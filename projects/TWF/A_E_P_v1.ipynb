{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f30743f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§­ Paso 3: Leyendo/estableciendo parÃ¡metros globales TWF_* ...\n",
      "   - TWF_SYMBOL                  : BTCUSDT\n",
      "   - TWF_TF                      : 15m\n",
      "   - TWF_START_DATE              : 2021-01-01\n",
      "   - TWF_END_DATE                : 2099-12-31\n",
      "   - TWF_SEED_BASE               : 42\n",
      "ğŸ•’ Paso 4: Forzando UTC para consistencia ...\n",
      "   â„¹ï¸  tzset() no disponible en este OS; se usarÃ¡ TZ=UTC en procesos hijos.\n",
      "ğŸ“ Paso 5: Resolviendo rutas base y sys.path ...\n",
      "   - PROJ_ROOT                   : C:\\Quant\\TWF\n",
      "ğŸ—‚ï¸  Paso 6: Creando estructura de carpetas por activo/timeframe ...\n",
      "   - PATHS['base']               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\n",
      "   - PATHS['data_raw']           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\data_raw\n",
      "   - PATHS['data_curated']       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\data_curated\n",
      "   - PATHS['features']           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\features\n",
      "   - PATHS['forward']            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\n",
      "   - PATHS['wf']                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\wf\n",
      "   - PATHS['models']             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\models\n",
      "   - PATHS['reports']            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\n",
      "   - PATHS['figures']            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\figures\n",
      "   - PATHS['logs']               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\n",
      "   - PATHS['tmp']                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\n",
      "ğŸ“ Paso 6.1: Validando permisos de escritura/lectura en outputs ...\n",
      "   âœ“ OK escritura/lectura en C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\_write_test.ok\n",
      "ğŸ² Paso 7: Fijando semillas para reproducibilidad ...\n",
      "   - SEED_BASE                   : 42\n",
      "ğŸ§¾ Paso 8: Creando snapshot de entorno ...\n",
      "   - snapshot_path               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\run_snapshot.json\n",
      "   âœ“ Snapshot escrito.\n",
      "\n",
      "âœ… RESUMEN DE SETUP\n",
      "   - SYMBOL                      : BTCUSDT\n",
      "   - TIMEFRAME                   : 15m\n",
      "   - START_DATE                  : 2021-01-01\n",
      "   - END_DATE                    : 2099-12-31\n",
      "   - OUTPUT_BASE                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\n",
      "   - LOG_FILE                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\run_snapshot.json\n",
      "   âœ“ Celda 0 finalizada sin errores.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_40828\\208170312.py:160: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "# Celda 0: [SETUP & PARÃMETROS GLOBALES]\n",
    "# ------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Fijar parÃ¡metros globales (sÃ­mbolo, timeframe, fechas, seed)\n",
    "# - Ajustar variables de entorno para reproducibilidad (threads/BLAS)\n",
    "# - Resolver rutas base y crear estructura de carpetas del proyecto\n",
    "# - Forzar UTC, fijar seeds y PATH del proyecto\n",
    "# - Guardar un \"snapshot\" de entorno (versiones / OS / env clave)\n",
    "# - Imprimir verificaciones exhaustivas de todo lo anterior\n",
    "# ------------------------------------------------------------\n",
    "# NOTA: Esta celda es la ÃšNICA que puede definir o modificar TWF_*\n",
    "#       El resto de celdas SOLO leen del entorno o de archivos generados aquÃ­.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ----(0) Variables de entorno para reproducibilidad (antes de imports cientÃ­ficos)----\n",
    "import os\n",
    "# Mono-hilo para BLAS/OMP (mejor determinismo). Si luego quieres velocidad, sube estos valores.\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "# Polars: por defecto usa todos los nÃºcleos. Si quieres restringir, define TWF_POLARS_THREADS (p.ej. \"1\" o \"0\" para auto).\n",
    "if \"TWF_POLARS_THREADS\" in os.environ:\n",
    "    os.environ[\"POLARS_MAX_THREADS\"] = os.environ[\"TWF_POLARS_THREADS\"]\n",
    "\n",
    "# ----(1) Imports mÃ­nimos (despuÃ©s de fijar variables de entorno)----\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Carga paulatina de libs cientÃ­ficas (manejar ausencias con mensajes claros)\n",
    "try:\n",
    "    import numpy as np\n",
    "    NP_OK = True\n",
    "except Exception as e:\n",
    "    NP_OK = False\n",
    "    print(f\"âš ï¸  [AVISO] NumPy no disponible: {e}\")\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "    PL_OK = True\n",
    "except Exception as e:\n",
    "    PL_OK = False\n",
    "    print(f\"âš ï¸  [AVISO] Polars no disponible: {e}\")\n",
    "\n",
    "# ----(2) Utilidades de impresiÃ³n y validaciÃ³n----\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<28}: {v}\")\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _is_valid_date(s: str) -> bool:\n",
    "    try:\n",
    "        datetime.fromisoformat(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ----(3) ParÃ¡metros globales (ÃšNICA FUENTE DE VERDAD)----\n",
    "# Si ya vienes con variables de entorno definidas desde fuera, las respetamos.\n",
    "# Si no, fijamos valores por defecto SEGUROS y los propagamos a os.environ.\n",
    "DEFAULTS = {\n",
    "    \"TWF_SYMBOL\": \"BTCUSDT\",         # Activo (ejemplos: BTCUSDT, ETHUSDT, ES, AAPL)\n",
    "    \"TWF_TF\": \"15m\",                  # Timeframe (ej.: 1m, 5m, 15m, 1h, 4h, 1d)\n",
    "    \"TWF_START_DATE\": \"2021-01-01\",  # ISO-8601 YYYY-MM-DD\n",
    "    \"TWF_END_DATE\": \"2099-12-31\",    # ISO-8601 YYYY-MM-DD (usar futuro amplio si harÃ¡s filtros luego)\n",
    "    \"TWF_SEED_BASE\": \"42\",           # Semilla base reproducible\n",
    "    # Opcional: fuerza nÃºmero de hilos de Polars (si lo deseas): os.environ[\"TWF_POLARS_THREADS\"]=\"1\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ§­ Paso 3: Leyendo/estableciendo parÃ¡metros globales TWF_* ...\")\n",
    "for k, default_v in DEFAULTS.items():\n",
    "    v = os.environ.get(k, default_v)\n",
    "    os.environ[k] = str(v)\n",
    "    _print_kv(k, os.environ[k])\n",
    "\n",
    "# Validaciones de formato de fechas\n",
    "assert _is_valid_date(os.environ[\"TWF_START_DATE\"]), \"Fecha invÃ¡lida en TWF_START_DATE (use YYYY-MM-DD)\"\n",
    "assert _is_valid_date(os.environ[\"TWF_END_DATE\"]), \"Fecha invÃ¡lida en TWF_END_DATE (use YYYY-MM-DD)\"\n",
    "dt_start = datetime.fromisoformat(os.environ[\"TWF_START_DATE\"])\n",
    "dt_end   = datetime.fromisoformat(os.environ[\"TWF_END_DATE\"])\n",
    "assert dt_start <= dt_end, \"TWF_START_DATE debe ser <= TWF_END_DATE\"\n",
    "\n",
    "# ----(4) Zona horaria (UTC)----\n",
    "print(\"ğŸ•’ Paso 4: Forzando UTC para consistencia ...\")\n",
    "os.environ[\"TZ\"] = \"UTC\"\n",
    "try:\n",
    "    # En sistemas Unix, tzset aplica TZ inmediatamente\n",
    "    import time as _time\n",
    "    if hasattr(_time, \"tzset\"):\n",
    "        _time.tzset()\n",
    "        print(\"   âœ“ TZ=UTC aplicado vÃ­a tzset()\")\n",
    "    else:\n",
    "        print(\"   â„¹ï¸  tzset() no disponible en este OS; se usarÃ¡ TZ=UTC en procesos hijos.\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  No se pudo aplicar tzset(): {e}\")\n",
    "\n",
    "# ----(5) RaÃ­z del proyecto y sys.path----\n",
    "print(\"ğŸ“ Paso 5: Resolviendo rutas base y sys.path ...\")\n",
    "# Puedes sobreescribir la raÃ­z con TWF_BASE si tu repo no coincide con Path.cwd()\n",
    "PROJ_ROOT = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "_print_kv(\"PROJ_ROOT\", PROJ_ROOT)\n",
    "\n",
    "# ----(6) ConstrucciÃ³n de rutas por activo/timeframe----\n",
    "print(\"ğŸ—‚ï¸  Paso 6: Creando estructura de carpetas por activo/timeframe ...\")\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF     = os.environ[\"TWF_TF\"]\n",
    "\n",
    "OUT_BASE = PROJ_ROOT / \"outputs\" / SYMBOL / TF\n",
    "PATHS = {\n",
    "    \"base\": OUT_BASE,\n",
    "    \"data_raw\": OUT_BASE / \"data_raw\",\n",
    "    \"data_curated\": OUT_BASE / \"data_curated\",\n",
    "    \"features\": OUT_BASE / \"features\",\n",
    "    \"forward\": OUT_BASE / \"forward\",\n",
    "    \"wf\": OUT_BASE / \"wf\",\n",
    "    \"models\": OUT_BASE / \"models\",\n",
    "    \"reports\": OUT_BASE / \"reports\",\n",
    "    \"figures\": OUT_BASE / \"figures\",\n",
    "    \"logs\": OUT_BASE / \"logs\",\n",
    "    \"tmp\": OUT_BASE / \"tmp\",\n",
    "}\n",
    "\n",
    "for name, p in PATHS.items():\n",
    "    _ensure_dir(p)\n",
    "    _print_kv(f\"PATHS['{name}']\", p)\n",
    "\n",
    "# Guardamos un archivo de prueba de escritura/lectura para validar permisos.\n",
    "print(\"ğŸ“ Paso 6.1: Validando permisos de escritura/lectura en outputs ...\")\n",
    "touch_file = PATHS[\"tmp\"] / \"_write_test.ok\"\n",
    "try:\n",
    "    touch_file.write_text(\"ok\")\n",
    "    assert touch_file.read_text() == \"ok\"\n",
    "    print(f\"   âœ“ OK escritura/lectura en {touch_file}\")\n",
    "    touch_file.unlink(missing_ok=True)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"âŒ Error de permisos en {PATHS['tmp']}: {e}\")\n",
    "\n",
    "# ----(7) Fijar semillas (reproducibilidad)----\n",
    "print(\"ğŸ² Paso 7: Fijando semillas para reproducibilidad ...\")\n",
    "SEED_BASE = int(os.environ[\"TWF_SEED_BASE\"])\n",
    "random.seed(SEED_BASE)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_BASE)\n",
    "if NP_OK:\n",
    "    np.random.seed(SEED_BASE)\n",
    "_print_kv(\"SEED_BASE\", SEED_BASE)\n",
    "\n",
    "# ----(8) Snapshot de entorno (versiones y sistema)----\n",
    "print(\"ğŸ§¾ Paso 8: Creando snapshot de entorno ...\")\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"platform\": {\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"implementation\": platform.python_implementation(),\n",
    "        \"system\": platform.system(),\n",
    "        \"release\": platform.release(),\n",
    "        \"machine\": platform.machine(),\n",
    "        \"processor\": platform.processor(),\n",
    "    },\n",
    "    \"packages\": {},\n",
    "    \"env\": {\n",
    "        \"TWF_SYMBOL\": os.environ[\"TWF_SYMBOL\"],\n",
    "        \"TWF_TF\": os.environ[\"TWF_TF\"],\n",
    "        \"TWF_START_DATE\": os.environ[\"TWF_START_DATE\"],\n",
    "        \"TWF_END_DATE\": os.environ[\"TWF_END_DATE\"],\n",
    "        \"TWF_SEED_BASE\": os.environ[\"TWF_SEED_BASE\"],\n",
    "        \"POLARS_MAX_THREADS\": os.environ.get(\"POLARS_MAX_THREADS\", None),\n",
    "        \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\", None),\n",
    "        \"OPENBLAS_NUM_THREADS\": os.environ.get(\"OPENBLAS_NUM_THREADS\", None),\n",
    "        \"MKL_NUM_THREADS\": os.environ.get(\"MKL_NUM_THREADS\", None),\n",
    "    },\n",
    "    \"paths\": {k: str(v) for k, v in PATHS.items()},\n",
    "}\n",
    "\n",
    "# Versiones de paquetes (si estÃ¡n disponibles)\n",
    "if NP_OK:\n",
    "    snapshot[\"packages\"][\"numpy\"] = np.__version__\n",
    "if PL_OK:\n",
    "    snapshot[\"packages\"][\"polars\"] = pl.__version__\n",
    "\n",
    "# Persistimos snapshot\n",
    "snapshot_path = PATHS[\"logs\"] / \"run_snapshot.json\"\n",
    "with snapshot_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(snapshot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_print_kv(\"snapshot_path\", snapshot_path)\n",
    "print(\"   âœ“ Snapshot escrito.\")\n",
    "\n",
    "# ----(9) Resumen final imprimible (para â€œojo clÃ­nicoâ€)----\n",
    "print(\"\\nâœ… RESUMEN DE SETUP\")\n",
    "_print_kv(\"SYMBOL\", SYMBOL)\n",
    "_print_kv(\"TIMEFRAME\", TF)\n",
    "_print_kv(\"START_DATE\", os.environ[\"TWF_START_DATE\"])\n",
    "_print_kv(\"END_DATE\", os.environ[\"TWF_END_DATE\"])\n",
    "_print_kv(\"OUTPUT_BASE\", PATHS[\"base\"])\n",
    "_print_kv(\"LOG_FILE\", snapshot_path)\n",
    "print(\"   âœ“ Celda 0 finalizada sin errores.\\n\")\n",
    "\n",
    "# ----(10) Contratos/Asserts para las siguientes celdas----\n",
    "# Las prÃ³ximas celdas SOLO deben leer TWF_* y PATHS desde disco/entorno. No modificar aquÃ­.\n",
    "assert all(k in os.environ for k in (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")), \\\n",
    "    \"Faltan variables TWF_* en entorno.\"\n",
    "for needed in (\"base\",\"logs\",\"tmp\"):\n",
    "    assert PATHS[needed].exists(), f\"Ruta obligatoria no existe: PATHS['{needed}']\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "30ac4542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“œ Leyendo variables TWF_* del entorno ...\n",
      "   - TWF_SYMBOL                  : BTCUSDT\n",
      "   - TWF_TF                      : 15m\n",
      "   - TWF_START_DATE              : 2021-01-01\n",
      "   - TWF_END_DATE                : 2099-12-31\n",
      "   - TWF_SEED_BASE               : 42\n",
      "   - PROJ_ROOT                   : C:\\Quant\\TWF\n",
      "   - OUT_BASE                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\n",
      "   - LOGS_DIR                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\n",
      "ğŸ§® Derivando parÃ¡metros de configuraciÃ³n ...\n",
      "   - expected_interval_ms        : 900000\n",
      "\n",
      "ğŸ§¾ CONFIG RESUMEN\n",
      "   - config_path                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\config_BTCUSDT_15m.json\n",
      "   - schema_version              : 1\n",
      "   - symbol                      : BTCUSDT\n",
      "   - timeframe                   : 15m\n",
      "   - start_date                  : 2021-01-01\n",
      "   - end_date                    : 2099-12-31\n",
      "   - seed_base                   : 42\n",
      "   - expected_interval_ms        : 900000\n",
      "   - features.ma.types           : ema,wma\n",
      "   - features.ma.windows         : [10, 20, 50]\n",
      "   - features.er.windows         : [10, 20, 50]\n",
      "   - features.er.thresholds      : [0.2, 0.3, 0.4]\n",
      "   - labeling.tb.horizon_bars    : 48\n",
      "   - walk_forward.train_bars     : 2000\n",
      "   - walk_forward.test_bars      : 500\n",
      "   - walk_forward.top_k          : 10\n",
      "   - compute.n_jobs              : 28\n",
      "   âœ“ ConfiguraciÃ³n persistida correctamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: [CONFIGURACIÃ“N PERSISTENTE â†’ config_{SYMBOL}_{TF}.json]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Leer TWF_* del entorno (definidos en Celda 0).\n",
    "# - Derivar parÃ¡metros estÃ¡ndar del pipeline (data/features/labeling/WF/compute).\n",
    "# - Persistir un JSON de configuraciÃ³n Ãºnico para este sÃ­mbolo/timeframe.\n",
    "# - Imprimir verificaciones exhaustivas del contenido y las rutas.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ----(A) Utilidades locales----\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<28}: {v}\")\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def _timeframe_to_ms(tf: str) -> int:\n",
    "    \"\"\"Convierte strings tipo '1m','5m','15m','1h','4h','1d' a milisegundos.\"\"\"\n",
    "    tf = tf.strip().lower()\n",
    "    if tf.endswith(\"ms\"):\n",
    "        return int(tf[:-2])\n",
    "    if tf.endswith(\"s\"):\n",
    "        return int(tf[:-1]) * 1_000\n",
    "    if tf.endswith(\"m\"):\n",
    "        return int(tf[:-1]) * 60_000\n",
    "    if tf.endswith(\"h\"):\n",
    "        return int(tf[:-1]) * 3_600_000\n",
    "    if tf.endswith(\"d\"):\n",
    "        return int(tf[:-1]) * 86_400_000\n",
    "    raise ValueError(f\"Timeframe no reconocido: {tf}\")\n",
    "\n",
    "# ----(B) Contratos de entrada (no tocar TWF_* aquÃ­)----\n",
    "print(\"ğŸ“œ Leyendo variables TWF_* del entorno ...\")\n",
    "needed = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in needed), \"Faltan TWF_* en el entorno; ejecuta primero la Celda 0.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF     = os.environ[\"TWF_TF\"]\n",
    "START  = os.environ[\"TWF_START_DATE\"]\n",
    "END    = os.environ[\"TWF_END_DATE\"]\n",
    "SEED   = int(os.environ[\"TWF_SEED_BASE\"])\n",
    "for k in needed: _print_kv(k, os.environ[k])\n",
    "\n",
    "# ----(C) Rutas base (derivadas de TWF_BASE o CWD; sin depender de variables en memoria)----\n",
    "PROJ_ROOT = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "OUT_BASE  = PROJ_ROOT / \"outputs\" / SYMBOL / TF\n",
    "LOGS_DIR  = _ensure_dir(OUT_BASE / \"logs\")\n",
    "_print_kv(\"PROJ_ROOT\", PROJ_ROOT)\n",
    "_print_kv(\"OUT_BASE\", OUT_BASE)\n",
    "_print_kv(\"LOGS_DIR\", LOGS_DIR)\n",
    "\n",
    "# ----(D) Derivados clave de configuraciÃ³n (valores razonables por defecto)----\n",
    "print(\"ğŸ§® Derivando parÃ¡metros de configuraciÃ³n ...\")\n",
    "interval_ms = _timeframe_to_ms(TF)\n",
    "_print_kv(\"expected_interval_ms\", interval_ms)\n",
    "\n",
    "# Nota: Estos valores son seguros para arrancar y pueden refinarse luego.\n",
    "#       MantÃ©n listas cortas para validaciÃ³n rÃ¡pida fin-a-fin; luego se amplÃ­an.\n",
    "config = {\n",
    "    \"schema_version\": 1,\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"run\": {\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"timeframe\": TF,\n",
    "        \"start_date\": START,\n",
    "        \"end_date\": END,\n",
    "        \"seed_base\": SEED,\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"source\": \"binance\",                 # Puedes cambiar a 'csv', 'parquet', etc. en el futuro\n",
    "        \"market_hours\": \"24x7\",\n",
    "        \"expected_interval_ms\": interval_ms,\n",
    "        \"allow_gaps\": False,                 # Si True, se loguean; si False, se consideran error aguas abajo\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"ma\": {\n",
    "            \"types\": [\"ema\",\"wma\"],          # Mantener corto para pruebas; luego agregar \"hma\",\"kama\" si quieres\n",
    "            \"windows\": [10, 20, 50],         # Grilla inicial corta; luego expandimos\n",
    "            \"price_col\": \"close\",\n",
    "        },\n",
    "        \"er\": {\n",
    "            \"windows\": [10, 20, 50],\n",
    "            \"thresholds\": [0.2, 0.3, 0.4],   # Filtro de â€œtendencia limpiaâ€\n",
    "            \"price_col\": \"close\",\n",
    "        },\n",
    "        \"slope\": {\n",
    "            \"window\": 10,\n",
    "            \"price_col\": \"close\",\n",
    "        }\n",
    "    },\n",
    "    \"labeling\": {\n",
    "        \"triple_barrier\": {\n",
    "            \"horizon_bars\": 48,              # ~2 dÃ­as en TF=1h (ajustable)\n",
    "            \"up_mult\": 2.0,                  # TP mÃºltiplo relativo (placeholder)\n",
    "            \"dn_mult\": 1.0,                  # SL mÃºltiplo relativo (placeholder)\n",
    "            \"timeout_bars\": 48               # Igual a horizon por defecto\n",
    "        }\n",
    "    },\n",
    "    \"walk_forward\": {\n",
    "        \"train_bars\": 2000,\n",
    "        \"test_bars\": 500,\n",
    "        \"embargo_bars\": 0,\n",
    "        \"top_k\": 10,                         # Top K combinaciones post-clustering\n",
    "        \"min_trades\": 50                     # Filtro mÃ­nimo de operaciones por combo\n",
    "    },\n",
    "    \"compute\": {\n",
    "        \"n_jobs\": os.cpu_count() or 4,\n",
    "        \"polars_threads\": os.environ.get(\"POLARS_MAX_THREADS\", None),\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"base\": str(OUT_BASE),\n",
    "        # Rutas estÃ¡ndar consumidas por celdas posteriores\n",
    "        \"data_raw\": str(OUT_BASE / \"data_raw\"),\n",
    "        \"data_curated\": str(OUT_BASE / \"data_curated\"),\n",
    "        \"features\": str(OUT_BASE / \"features\"),\n",
    "        \"forward\": str(OUT_BASE / \"forward\"),\n",
    "        \"wf\": str(OUT_BASE / \"wf\"),\n",
    "        \"models\": str(OUT_BASE / \"models\"),\n",
    "        \"reports\": str(OUT_BASE / \"reports\"),\n",
    "        \"figures\": str(OUT_BASE / \"figures\"),\n",
    "        \"logs\": str(OUT_BASE / \"logs\"),\n",
    "        \"tmp\": str(OUT_BASE / \"tmp\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----(E) Persistencia del config----\n",
    "CONFIG_PATH = LOGS_DIR / f\"config_{SYMBOL}_{TF}.json\"\n",
    "with CONFIG_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ----(F) Impresiones de verificaciÃ³n (resumen claro)----\n",
    "print(\"\\nğŸ§¾ CONFIG RESUMEN\")\n",
    "_print_kv(\"config_path\", CONFIG_PATH)\n",
    "_print_kv(\"schema_version\", config[\"schema_version\"])\n",
    "_print_kv(\"symbol\", config[\"run\"][\"symbol\"])\n",
    "_print_kv(\"timeframe\", config[\"run\"][\"timeframe\"])\n",
    "_print_kv(\"start_date\", config[\"run\"][\"start_date\"])\n",
    "_print_kv(\"end_date\", config[\"run\"][\"end_date\"])\n",
    "_print_kv(\"seed_base\", config[\"run\"][\"seed_base\"])\n",
    "_print_kv(\"expected_interval_ms\", config[\"data\"][\"expected_interval_ms\"])\n",
    "_print_kv(\"features.ma.types\", \",\".join(config[\"features\"][\"ma\"][\"types\"]))\n",
    "_print_kv(\"features.ma.windows\", config[\"features\"][\"ma\"][\"windows\"])\n",
    "_print_kv(\"features.er.windows\", config[\"features\"][\"er\"][\"windows\"])\n",
    "_print_kv(\"features.er.thresholds\", config[\"features\"][\"er\"][\"thresholds\"])\n",
    "_print_kv(\"labeling.tb.horizon_bars\", config[\"labeling\"][\"triple_barrier\"][\"horizon_bars\"])\n",
    "_print_kv(\"walk_forward.train_bars\", config[\"walk_forward\"][\"train_bars\"])\n",
    "_print_kv(\"walk_forward.test_bars\", config[\"walk_forward\"][\"test_bars\"])\n",
    "_print_kv(\"walk_forward.top_k\", config[\"walk_forward\"][\"top_k\"])\n",
    "_print_kv(\"compute.n_jobs\", config[\"compute\"][\"n_jobs\"])\n",
    "\n",
    "# ----(G) Contratos de salida----\n",
    "assert CONFIG_PATH.exists() and CONFIG_PATH.stat().st_size > 0, \"Config no se escribiÃ³ correctamente.\"\n",
    "print(\"   âœ“ ConfiguraciÃ³n persistida correctamente.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "55ecbc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Leyendo configuraciÃ³n ...\n",
      "   - symbol                      : BTCUSDT\n",
      "   - timeframe                   : 15m\n",
      "   - data_source                 : binance\n",
      "   - expected_interval_ms        : 900000\n",
      "   - start_date                  : 2021-01-01\n",
      "   - end_date                    : 2099-12-31\n",
      "   - start_ms                    : 1609459200000\n",
      "   - end_ms                      : 1762264747969\n",
      "   - now_ms                      : 1762264747969\n",
      "   - parquet_out                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\data_curated\\ohlc_BTCUSDT_15m.parquet\n",
      "   - manifest_out                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\manifest_ohlc_BTCUSDT_15m.json\n",
      "â™»ï¸  Archivo Parquet ya existe. Se validarÃ¡ y reutilizarÃ¡ si pasa controles...\n",
      "ğŸš« Descarga omitida (reutilizando Parquet existente).\n",
      "ğŸ“– Leyendo DataFrame desde Parquet existente ...\n",
      "   âœ“ Filas leÃ­das: 169643, columnas: 11\n",
      "ğŸ” Validando esquema y cobertura ...\n",
      "ğŸ§® MÃ©tricas:\n",
      "   - first_ts                    : 2021-01-01T00:00:00+00:00\n",
      "   - last_ts                     : 2025-11-03T20:00:00+00:00\n",
      "   - rows                        : 169643\n",
      "   - expected_bars               : 169713\n",
      "   - coverage_pct                : 99.9588%\n",
      "   - total_gaps                  : 7\n",
      "   - max_gap_bars                : 18\n",
      "ğŸ’¾ Persistiendo Parquet curado ...\n",
      "ğŸ§¾ Manifest escrito.\n",
      "   - manifest_path               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\manifest_ohlc_BTCUSDT_15m.json\n",
      "   - sha256                      : ac7ea6efcce1709d275d34609a11b8c6fa8944228464ee0630a636107d509e58\n",
      "âœ… Celda 2 finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: [INGESTA & CURACIÃ“N DE OHLCV]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Leer config_{SYMBOL}_{TF}.json\n",
    "# - Descargar (o reaprovechar) OHLCV del origen indicado (por defecto: Binance)\n",
    "# - Normalizar esquema, ordenar, deduplicar, validar gaps\n",
    "# - Persistir Parquet curado y manifest JSON con mÃ©tricas de cobertura\n",
    "# - Imprimir verificaciones exhaustivas y progreso por \"chunks\"\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, time, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict\n",
    "\n",
    "# ----(A) Utilidades de impresiÃ³n y tiempo----\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<28}: {v}\")\n",
    "\n",
    "def _to_utc_ms(dt_iso: str) -> int:\n",
    "    # Acepta 'YYYY-MM-DD' o ISO completo; retorna epoch ms en UTC.\n",
    "    if len(dt_iso) == 10:\n",
    "        dt = datetime.fromisoformat(dt_iso).replace(tzinfo=timezone.utc)\n",
    "    else:\n",
    "        dt = datetime.fromisoformat(dt_iso)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        else:\n",
    "            dt = dt.astimezone(timezone.utc)\n",
    "    return int(dt.timestamp() * 1000)\n",
    "\n",
    "def _ms_to_iso(ms: int) -> str:\n",
    "    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _timeframe_to_ms(tf: str) -> int:\n",
    "    tf = tf.strip().lower()\n",
    "    if tf.endswith(\"ms\"): return int(tf[:-2])\n",
    "    if tf.endswith(\"s\"):  return int(tf[:-1]) * 1_000\n",
    "    if tf.endswith(\"m\"):  return int(tf[:-1]) * 60_000\n",
    "    if tf.endswith(\"h\"):  return int(tf[:-1]) * 3_600_000\n",
    "    if tf.endswith(\"d\"):  return int(tf[:-1]) * 86_400_000\n",
    "    raise ValueError(f\"Timeframe no reconocido: {tf}\")\n",
    "\n",
    "# ----(B) Cargar configuraciÃ³n persistente (NO modificar TWF_* aquÃ­)----\n",
    "needed = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in needed), \"Faltan TWF_*; ejecuta primero Celda 0 y luego Celda 1.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF     = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe el archivo de configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "print(\"ğŸ“¥ Leyendo configuraciÃ³n ...\")\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "DATA_SOURCE = CFG[\"data\"][\"source\"]\n",
    "INTERVAL_MS = CFG[\"data\"][\"expected_interval_ms\"]\n",
    "START_ISO   = CFG[\"run\"][\"start_date\"]\n",
    "END_ISO     = CFG[\"run\"][\"end_date\"]\n",
    "\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "for k_name in (\"data_raw\",\"data_curated\",\"logs\",\"tmp\"):\n",
    "    PATHS[k_name].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_print_kv(\"symbol\", SYMBOL)\n",
    "_print_kv(\"timeframe\", TF)\n",
    "_print_kv(\"data_source\", DATA_SOURCE)\n",
    "_print_kv(\"expected_interval_ms\", INTERVAL_MS)\n",
    "_print_kv(\"start_date\", START_ISO)\n",
    "_print_kv(\"end_date\", END_ISO)\n",
    "\n",
    "# ----(C) Determinar rango temporal en ms (clamp a \"ahora\")----\n",
    "START_MS = _to_utc_ms(START_ISO)\n",
    "# Clamp final a \"ahora\" (UTC), para no pedir velas futuras\n",
    "NOW_MS   = int(datetime.now(timezone.utc).timestamp() * 1000)\n",
    "END_MS_RAW = _to_utc_ms(END_ISO)\n",
    "END_MS  = min(END_MS_RAW, NOW_MS)\n",
    "_print_kv(\"start_ms\", START_MS)\n",
    "_print_kv(\"end_ms\", END_MS)\n",
    "_print_kv(\"now_ms\", NOW_MS)\n",
    "\n",
    "# ----(D) Paths de salida----\n",
    "PARQUET_PATH = PATHS[\"data_curated\"] / f\"ohlc_{SYMBOL}_{TF}.parquet\"\n",
    "MANIFEST_PATH = PATHS[\"logs\"] / f\"manifest_ohlc_{SYMBOL}_{TF}.json\"\n",
    "_print_kv(\"parquet_out\", PARQUET_PATH)\n",
    "_print_kv(\"manifest_out\", MANIFEST_PATH)\n",
    "\n",
    "# ----(E) Intentar reutilizar si ya existe (idempotencia)----\n",
    "REUSE_IF_EXISTS = True\n",
    "if REUSE_IF_EXISTS and PARQUET_PATH.exists():\n",
    "    print(\"â™»ï¸  Archivo Parquet ya existe. Se validarÃ¡ y reutilizarÃ¡ si pasa controles...\")\n",
    "else:\n",
    "    print(\"ğŸ†• No existe Parquet curado previo o REUSE_IF_EXISTS=False. Se descargarÃ¡ y generarÃ¡ de cero.\")\n",
    "\n",
    "# ----(F) Descarga desde Binance (HTTP REST) si es necesario----\n",
    "rows: List[List] = []\n",
    "if (not PARQUET_PATH.exists()) or (not REUSE_IF_EXISTS):\n",
    "    assert DATA_SOURCE.lower() == \"binance\", \"Por ahora esta celda solo implementa 'binance' como origen.\"\n",
    "\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Falta el paquete 'requests' para descargar de Binance. InstÃ¡lalo con: pip install requests\"\n",
    "        ) from e\n",
    "\n",
    "    session = requests.Session()\n",
    "    endpoint = \"https://api.binance.com/api/v3/klines\"\n",
    "    limit = 1000\n",
    "    cursor = START_MS\n",
    "    n_calls = 0\n",
    "\n",
    "    print(\"ğŸŒ Iniciando paginaciÃ³n de klines (Binance) ...\")\n",
    "    while cursor < END_MS:\n",
    "        params = {\n",
    "            \"symbol\": SYMBOL.upper(),\n",
    "            \"interval\": TF.lower(),\n",
    "            \"startTime\": cursor,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        # Opcionalmente podrÃ­amos pasar \"endTime\", pero con startTime+limit avanzamos sin solapar.\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                resp = session.get(endpoint, params=params, timeout=30)\n",
    "                n_calls += 1\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    break\n",
    "                elif resp.status_code in (418, 429):  # rate limited / banned\n",
    "                    wait_s = 2 ** attempt\n",
    "                    print(f\"   âš ï¸  Rate limit HTTP {resp.status_code}. Reintento en {wait_s}s ... (attempt {attempt+1}/5)\")\n",
    "                    time.sleep(wait_s)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:200]}\")\n",
    "            except Exception as ex:\n",
    "                wait_s = 2 ** attempt\n",
    "                print(f\"   âš ï¸  Error de red: {ex}. Reintento en {wait_s}s ... (attempt {attempt+1}/5)\")\n",
    "                time.sleep(wait_s)\n",
    "        else:\n",
    "            raise RuntimeError(\"âŒ FallÃ³ la descarga tras mÃºltiples reintentos.\")\n",
    "\n",
    "        if not data:\n",
    "            # Si no hay datos, avanzamos una vela para evitar bucles infinitos\n",
    "            cursor += INTERVAL_MS\n",
    "            print(f\"   â„¹ï¸  Respuesta vacÃ­a, avanzando cursor a {_ms_to_iso(cursor)}\")\n",
    "            continue\n",
    "\n",
    "        # 'data' es una lista de klines. Cada kline:\n",
    "        # [ openTime, open, high, low, close, volume, closeTime, quoteVolume, trades, takerBase, takerQuote, ignore ]\n",
    "        first_open_ms = data[0][0]\n",
    "        last_open_ms  = data[-1][0]\n",
    "        rows.extend(data)\n",
    "\n",
    "        print(f\"   âœ“ Chunk recibido: {len(data):4d} velas | \"\n",
    "              f\"{_ms_to_iso(first_open_ms)} â†’ {_ms_to_iso(last_open_ms)} | \"\n",
    "              f\"total acumulado: {len(rows)}\")\n",
    "\n",
    "        # Avanzar cursor a la siguiente vela tras el Ãºltimo openTime\n",
    "        cursor = last_open_ms + INTERVAL_MS\n",
    "\n",
    "    print(f\"âœ… Descarga finalizada. Llamadas HTTP: {n_calls}, velas totales: {len(rows)}\")\n",
    "else:\n",
    "    print(\"ğŸš« Descarga omitida (reutilizando Parquet existente).\")\n",
    "\n",
    "# ----(G) ConstrucciÃ³n de DataFrame y normalizaciÃ³n de esquema----\n",
    "import polars as pl\n",
    "\n",
    "def _rows_to_df(data: List[List]) -> \"pl.DataFrame\":\n",
    "    if not data:\n",
    "        return pl.DataFrame(\n",
    "            schema={\n",
    "                \"ts\": pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"),\n",
    "                \"open\": pl.Float64, \"high\": pl.Float64, \"low\": pl.Float64, \"close\": pl.Float64,\n",
    "                \"volume\": pl.Float64,\n",
    "                \"close_time\": pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"),\n",
    "                \"quote_volume\": pl.Float64, \"trades\": pl.Int64,\n",
    "                \"taker_base\": pl.Float64, \"taker_quote\": pl.Float64\n",
    "            }\n",
    "        )\n",
    "    df = pl.DataFrame(data, orient=\"row\").select([\n",
    "        pl.col(\"column_0\").alias(\"open_time_ms\"),\n",
    "        pl.col(\"column_1\").cast(pl.Float64).alias(\"open\"),\n",
    "        pl.col(\"column_2\").cast(pl.Float64).alias(\"high\"),\n",
    "        pl.col(\"column_3\").cast(pl.Float64).alias(\"low\"),\n",
    "        pl.col(\"column_4\").cast(pl.Float64).alias(\"close\"),\n",
    "        pl.col(\"column_5\").cast(pl.Float64).alias(\"volume\"),\n",
    "        pl.col(\"column_6\").alias(\"close_time_ms\"),\n",
    "        pl.col(\"column_7\").cast(pl.Float64).alias(\"quote_volume\"),\n",
    "        pl.col(\"column_8\").cast(pl.Int64).alias(\"trades\"),\n",
    "        pl.col(\"column_9\").cast(pl.Float64).alias(\"taker_base\"),\n",
    "        pl.col(\"column_10\").cast(pl.Float64).alias(\"taker_quote\"),\n",
    "    ]).with_columns(\n",
    "        ts = pl.from_epoch(pl.col(\"open_time_ms\")/1000).dt.replace_time_zone(\"UTC\"),\n",
    "        close_time = pl.from_epoch(pl.col(\"close_time_ms\")/1000).dt.replace_time_zone(\"UTC\"),\n",
    "    ).drop([\"open_time_ms\",\"close_time_ms\"]).select([\n",
    "        \"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_base\",\"taker_quote\"\n",
    "    ])\n",
    "    # Orden, deduplicaciÃ³n por ts\n",
    "    df = df.sort(\"ts\").unique(subset=[\"ts\"], keep=\"last\")\n",
    "    return df\n",
    "\n",
    "if (not PARQUET_PATH.exists()) or (not REUSE_IF_EXISTS):\n",
    "    print(\"ğŸ§± Construyendo DataFrame Polars ...\")\n",
    "    df = _rows_to_df(rows)\n",
    "    print(f\"   âœ“ Filas tras normalizar: {df.height}, columnas: {df.width}\")\n",
    "else:\n",
    "    print(\"ğŸ“– Leyendo DataFrame desde Parquet existente ...\")\n",
    "    df = pl.read_parquet(PARQUET_PATH)\n",
    "    print(f\"   âœ“ Filas leÃ­das: {df.height}, columnas: {df.width}\")\n",
    "\n",
    "# ----(H) Validaciones y mÃ©tricas de cobertura----\n",
    "print(\"ğŸ” Validando esquema y cobertura ...\")\n",
    "expected_cols = [\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_base\",\"taker_quote\"]\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "assert not missing, f\"Faltan columnas en OHLCV: {missing}\"\n",
    "\n",
    "if df.height > 0:\n",
    "    # first_ms / last_ms usando expresiones Polars (evita objetos Python intermedios)\n",
    "    bounds = df.select(\n",
    "        pl.col(\"ts\").dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "        pl.col(\"ts\").dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    "    ).row(0)\n",
    "    first_ts, last_ts = int(bounds[0]), int(bounds[1])\n",
    "\n",
    "    # CÃ¡lculo de esperadas (bordes inclusivos)\n",
    "    expected_bars = (last_ts - first_ts) // INTERVAL_MS + 1\n",
    "    coverage = df.height / expected_bars if expected_bars > 0 else 0.0\n",
    "\n",
    "    # Gaps con diffs en ms (tambiÃ©n 100% en Polars)\n",
    "    diffs_ms = df.select(pl.col(\"ts\").dt.epoch(\"ms\").diff().alias(\"dms\")).get_column(\"dms\").drop_nulls()\n",
    "    gap_bars = (diffs_ms / INTERVAL_MS).round(0).cast(pl.Int64) - 1\n",
    "    total_gaps = int((gap_bars > 0).sum())\n",
    "    max_gap_bars = int(gap_bars.max()) if gap_bars.len() > 0 else 0\n",
    "\n",
    "    print(\"ğŸ§® MÃ©tricas:\")\n",
    "    _print_kv(\"first_ts\", _ms_to_iso(first_ts))\n",
    "    _print_kv(\"last_ts\", _ms_to_iso(last_ts))\n",
    "    _print_kv(\"rows\", df.height)\n",
    "    _print_kv(\"expected_bars\", expected_bars)\n",
    "    _print_kv(\"coverage_pct\", f\"{coverage*100:.4f}%\")\n",
    "    _print_kv(\"total_gaps\", total_gaps)\n",
    "    _print_kv(\"max_gap_bars\", max_gap_bars)\n",
    "else:\n",
    "    first_ts = last_ts = expected_bars = total_gaps = max_gap_bars = 0\n",
    "    coverage = 0.0\n",
    "    print(\"   â„¹ï¸  DataFrame vacÃ­o (sin filas).\")\n",
    "\n",
    "# ----(I) Persistencia a Parquet y manifest----\n",
    "print(\"ğŸ’¾ Persistiendo Parquet curado ...\")\n",
    "df.write_parquet(PARQUET_PATH)\n",
    "assert PARQUET_PATH.exists() and PARQUET_PATH.stat().st_size > 0, \"No se pudo escribir el Parquet curado.\"\n",
    "\n",
    "def _sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "manifest = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"source\": DATA_SOURCE,\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"timeframe\": TF,\n",
    "    \"expected_interval_ms\": INTERVAL_MS,\n",
    "    \"file\": str(PARQUET_PATH),\n",
    "    \"sha256\": _sha256_file(PARQUET_PATH),\n",
    "    \"rows\": int(df.height),\n",
    "    \"first_ts\": _ms_to_iso(first_ts) if df.height > 0 else None,\n",
    "    \"last_ts\": _ms_to_iso(last_ts) if df.height > 0 else None,\n",
    "    \"expected_bars\": int(expected_bars) if df.height > 0 else 0,\n",
    "    \"coverage_pct\": coverage,\n",
    "    \"total_gaps\": int(total_gaps),\n",
    "    \"max_gap_bars\": int(max_gap_bars),\n",
    "}\n",
    "\n",
    "with MANIFEST_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ§¾ Manifest escrito.\")\n",
    "_print_kv(\"manifest_path\", MANIFEST_PATH)\n",
    "_print_kv(\"sha256\", manifest[\"sha256\"])\n",
    "\n",
    "# ----(J) Contratos de salida----\n",
    "assert PARQUET_PATH.exists() and MANIFEST_PATH.exists(), \"Faltan artefactos de salida.\"\n",
    "print(\"âœ… Celda 2 finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "445f685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - OHLC_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\data_curated\\ohlc_BTCUSDT_15m.parquet\n",
      "   - FEAT_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\features\\features_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando OHLC curado ...\n",
      "   - ohlc_rows                     : 169643\n",
      "   - ohlc_cols                     : 11\n",
      "   - first_ts                      : 2021-01-01T00:00:00+00:00\n",
      "   - last_ts                       : 2025-11-03T20:00:00+00:00\n",
      "ğŸ”’ Validaciones base de OHLC ...\n",
      "   - ts_sorted_nondec              : True\n",
      "   - ts_unique                     : True\n",
      "ğŸ§± Preparando arrays NumPy ...\n",
      "   - n_samples                     : 169643\n",
      "ğŸ§® Generando features ...\n",
      "   - ma_types                      : ['ema', 'wma']\n",
      "   - ma_windows                    : [10, 20, 50]\n",
      "   - er_windows                    : [10, 20, 50]\n",
      "   - slope_window                  : 10\n",
      "   â€¢ EMA 10 ...\n",
      "   â€¢ WMA 10 ...\n",
      "   â€¢ EMA 20 ...\n",
      "   â€¢ WMA 20 ...\n",
      "   â€¢ EMA 50 ...\n",
      "   â€¢ WMA 50 ...\n",
      "   â€¢ ER 10 ...\n",
      "   â€¢ ER 20 ...\n",
      "   â€¢ ER 50 ...\n",
      "   â€¢ SLOPE 10 ...\n",
      "ğŸ§± Construyendo DataFrame de features ...\n",
      "ğŸ” Chequeos de calidad de features ...\n",
      "   - features_rows                 : 169643\n",
      "   - features_cols                 : 11\n",
      "   - first_ts(features)            : 2021-01-01T00:00:00+00:00\n",
      "   - last_ts(features)             : 2025-11-03T20:00:00+00:00\n",
      "   - dtype_ts                      : Datetime(time_unit='us', time_zone='UTC')\n",
      "   - num_cols_count                : 10\n",
      "   - sample_num_cols               : ['ema_10', 'wma_10', 'ema_20', 'wma_20', 'ema_50', 'wma_50', 'er_10', 'er_20']\n",
      "   - total_nan_cols>0              : 10\n",
      "   - NaN in ema_50                 : 49\n",
      "   - NaN in wma_50                 : 49\n",
      "   - NaN in er_50                  : 49\n",
      "   - NaN in ema_20                 : 19\n",
      "   - NaN in wma_20                 : 19\n",
      "   - NaN in er_20                  : 19\n",
      "   - NaN in ema_10                 : 9\n",
      "   - NaN in wma_10                 : 9\n",
      "   - NaN in er_10                  : 9\n",
      "   - NaN in slope_10               : 9\n",
      "ğŸ‘€ Muestra HEAD:\n",
      "shape: (3, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts                      â”† ema_10 â”† wma_10 â”† ema_20 â”† â€¦ â”† er_10 â”† er_20 â”† er_50 â”† slope_10 â”‚\n",
      "â”‚ ---                     â”† ---    â”† ---    â”† ---    â”†   â”† ---   â”† ---   â”† ---   â”† ---      â”‚\n",
      "â”‚ datetime[Î¼s, UTC]       â”† f64    â”† f64    â”† f64    â”†   â”† f64   â”† f64   â”† f64   â”† f64      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2021-01-01 00:00:00 UTC â”† NaN    â”† NaN    â”† NaN    â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â”‚ 2021-01-01 00:15:00 UTC â”† NaN    â”† NaN    â”† NaN    â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â”‚ 2021-01-01 00:30:00 UTC â”† NaN    â”† NaN    â”† NaN    â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ‘€ Muestra TAIL:\n",
      "shape: (3, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts         â”† ema_10     â”† wma_10    â”† ema_20    â”† â€¦ â”† er_10    â”† er_20    â”† er_50    â”† slope_10  â”‚\n",
      "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† ---      â”† ---      â”† ---      â”† ---       â”‚\n",
      "â”‚ datetime[Î¼ â”† f64        â”† f64       â”† f64       â”†   â”† f64      â”† f64      â”† f64      â”† f64       â”‚\n",
      "â”‚ s, UTC]    â”†            â”†           â”†           â”†   â”†          â”†          â”†          â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2025-11-03 â”† 107032.381 â”† 107193.81 â”† 107059.65 â”† â€¦ â”† 0.103175 â”† 0.139322 â”† 0.033303 â”† 21.083333 â”‚\n",
      "â”‚ 19:30:00   â”† 771        â”† 1818      â”† 0443      â”†   â”†          â”†          â”†          â”†           â”‚\n",
      "â”‚ UTC        â”†            â”†           â”†           â”†   â”†          â”†          â”†          â”†           â”‚\n",
      "â”‚ 2025-11-03 â”† 107042.857 â”† 107204.01 â”† 107062.54 â”† â€¦ â”† 0.233284 â”† 0.102648 â”† 0.033792 â”† -42.22111 â”‚\n",
      "â”‚ 19:45:00   â”† 813        â”† 4545      â”† 0877      â”†   â”†          â”†          â”†          â”† 1         â”‚\n",
      "â”‚ UTC        â”†            â”†           â”†           â”†   â”†          â”†          â”†          â”†           â”‚\n",
      "â”‚ 2025-11-03 â”† 107064.140 â”† 107137.67 â”† 107071.81 â”† â€¦ â”† 0.190379 â”† 0.302845 â”† 0.026843 â”† -35.69777 â”‚\n",
      "â”‚ 20:00:00   â”† 029        â”† 2182      â”† 4127      â”†   â”†          â”†          â”†          â”† 8         â”‚\n",
      "â”‚ UTC        â”†            â”†           â”†           â”†   â”†          â”†          â”†          â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ’¾ Guardando features ...\n",
      "   - features_parquet              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\features\\features_BTCUSDT_15m.parquet\n",
      "   - features_file_size(bytes)     : 11990777\n",
      "   - features_file_size(MB)        : 11.435\n",
      "âœ… Celda 3 finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: [FEATURES â†’ features_{SYMBOL}_{TF}.parquet]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Leer config y OHLC curado.\n",
    "# - Generar features: MA (ema/wma), ER (windows), slope.\n",
    "# - Persistir Parquet de features con 'ts' como clave temporal.\n",
    "# - Imprimir verificaciones exhaustivas (formas, NaNs, dtypes, fechas, muestras).\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import timezone, datetime\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# -------------------- Utils de impresiÃ³n/validaciÃ³n --------------------\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _assert_cols(df: pl.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"âŒ Faltan columnas: {missing}\"\n",
    "\n",
    "def _epoch_ms(series_dt_tz: pl.Series) -> pl.Series:\n",
    "    # Convierte Datetime TZ a epoch ms (Series Polars)\n",
    "    return series_dt_tz.dt.epoch(\"ms\")\n",
    "\n",
    "# -------------------- A) Cargar config y paths -------------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "needed = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in needed), \"Faltan TWF_*; ejecuta celdas 0 y 1 primero.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF     = os.environ[\"TWF_TF\"]\n",
    "PROJ_ROOT = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe el archivo de configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "OHLC_PARQ = PATHS[\"data_curated\"] / f\"ohlc_{SYMBOL}_{TF}.parquet\"\n",
    "FEAT_PARQ = PATHS[\"features\"] / f\"features_{SYMBOL}_{TF}.parquet\"\n",
    "_print_kv(\"OHLC_PARQ\", OHLC_PARQ)\n",
    "_print_kv(\"FEAT_PARQ\", FEAT_PARQ)\n",
    "assert OHLC_PARQ.exists(), f\"No existe el OHLC curado: {OHLC_PARQ}\"\n",
    "\n",
    "# -------------------- B) Leer OHLC en Polars ---------------------------\n",
    "print(\"ğŸ“¦ Cargando OHLC curado ...\")\n",
    "df = pl.read_parquet(OHLC_PARQ)\n",
    "_print_kv(\"ohlc_rows\", df.height)\n",
    "_print_kv(\"ohlc_cols\", df.width)\n",
    "_assert_cols(df, [\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "# Fechas primeras/Ãºltimas en texto (100% Polars)\n",
    "bounds = df.select(\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    ").row(0)\n",
    "first_ms, last_ms = int(bounds[0]), int(bounds[1])\n",
    "_print_kv(\"first_ts\", datetime.fromtimestamp(first_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"))\n",
    "_print_kv(\"last_ts\",  datetime.fromtimestamp(last_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"))\n",
    "\n",
    "print(\"ğŸ”’ Validaciones base de OHLC ...\")\n",
    "# Monotonicidad (no decreciente) y unicidad de ts (100% Polars)\n",
    "dms = df.select(pl.col(\"ts\").dt.epoch(\"ms\").diff().alias(\"dms\")).get_column(\"dms\").drop_nulls()\n",
    "is_sorted = bool((dms >= 0).all())\n",
    "unique_ts = (df[\"ts\"].n_unique() == df.height)\n",
    "_print_kv(\"ts_sorted_nondec\", is_sorted)\n",
    "_print_kv(\"ts_unique\", unique_ts)\n",
    "assert is_sorted and unique_ts, \"âŒ 'ts' debe estar ordenado y sin duplicados.\"\n",
    "\n",
    "# -------------------- C) Preparar arrays NumPy -------------------------\n",
    "print(\"ğŸ§± Preparando arrays NumPy ...\")\n",
    "ts_ms = _epoch_ms(df[\"ts\"]).to_numpy()          # epoch ms\n",
    "close = df[\"close\"].to_numpy()                  # <- corregido: sin dtype posicional\n",
    "open_  = df[\"open\"].to_numpy()\n",
    "high   = df[\"high\"].to_numpy()\n",
    "low    = df[\"low\"].to_numpy()\n",
    "n = close.size\n",
    "_print_kv(\"n_samples\", n)\n",
    "assert n > 0, \"âŒ No hay datos para generar features.\"\n",
    "\n",
    "# -------------------- D) Helpers NumPy para features -------------------\n",
    "def ema_numpy(x: np.ndarray, span: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    EMA con warm-up clÃ¡sico:\n",
    "    - NaN en [0:span-1)\n",
    "    - y[span-1] = media de x[0:span]\n",
    "    - y[i] = alpha*x[i] + (1-alpha)*y[i-1] para i>=span\n",
    "    \"\"\"\n",
    "    if span < 1:\n",
    "        raise ValueError(\"span debe ser >=1\")\n",
    "    y = np.full_like(x, np.nan, dtype=np.float64)\n",
    "    if x.size == 0:\n",
    "        return y\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    if x.size >= span:\n",
    "        y[span-1] = np.nanmean(x[:span])\n",
    "        for i in range(span, x.size):\n",
    "            y[i] = alpha * x[i] + (1 - alpha) * y[i - 1]\n",
    "    return y\n",
    "\n",
    "def wma_numpy(x: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"WMA lineal (pesos 1..window) con warm-up: NaN en [0:window-1).\"\"\"\n",
    "    if window < 1:\n",
    "        raise ValueError(\"window debe ser >=1\")\n",
    "    y = np.full_like(x, np.nan, dtype=np.float64)\n",
    "    if x.size < window:\n",
    "        return y\n",
    "    w = np.arange(1, window + 1, dtype=np.float64)\n",
    "    conv = np.convolve(x, w, mode=\"valid\") / w.sum()\n",
    "    y[window - 1:] = conv\n",
    "    return y\n",
    "\n",
    "def er_numpy(price: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"Efficiency Ratio (Kaufman) con warm-up NaN en [0:window-1).\"\"\"\n",
    "    if window < 2:\n",
    "        raise ValueError(\"ER window debe ser >=2\")\n",
    "    y = np.full_like(price, np.nan, dtype=np.float64)\n",
    "    N = price.size\n",
    "    if N < window:\n",
    "        return y\n",
    "    # |P_t - P_{t-w+1}|\n",
    "    net = np.abs(price[window - 1:] - price[:-window + 1])\n",
    "    # âˆ‘|Î”P| en ventana\n",
    "    diff = np.abs(np.diff(price))\n",
    "    csum = np.concatenate([[0.0], np.cumsum(diff)])\n",
    "    vol = csum[(window - 1):] - csum[:-(window - 1)]\n",
    "    er = np.divide(net, vol, out=np.zeros_like(net), where=vol > 0)\n",
    "    y[window - 1:] = er\n",
    "    return y\n",
    "\n",
    "def slope_simple(price: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"Pendiente simple: (P_t - P_{t-w+1}) / (w-1). Warm-up NaN.\"\"\"\n",
    "    if window < 2:\n",
    "        raise ValueError(\"slope window debe ser >=2\")\n",
    "    y = np.full_like(price, np.nan, dtype=np.float64)\n",
    "    if price.size >= window:\n",
    "        y[window - 1:] = (price[window - 1:] - price[:-window + 1]) / (window - 1)\n",
    "    return y\n",
    "\n",
    "# -------------------- E) Construir features segÃºn config ----------------\n",
    "print(\"ğŸ§® Generando features ...\")\n",
    "ma_types    = CFG[\"features\"][\"ma\"][\"types\"]\n",
    "ma_windows  = CFG[\"features\"][\"ma\"][\"windows\"]\n",
    "er_windows  = CFG[\"features\"][\"er\"][\"windows\"]\n",
    "er_thrs     = CFG[\"features\"][\"er\"][\"thresholds\"]  # referencia para celdas futuras\n",
    "slope_win   = CFG[\"features\"][\"slope\"][\"window\"]\n",
    "\n",
    "_print_kv(\"ma_types\", ma_types)\n",
    "_print_kv(\"ma_windows\", ma_windows)\n",
    "_print_kv(\"er_windows\", er_windows)\n",
    "_print_kv(\"slope_window\", slope_win)\n",
    "\n",
    "feat_cols = {}\n",
    "# MAs\n",
    "for w in ma_windows:\n",
    "    if \"ema\" in ma_types:\n",
    "        print(f\"   â€¢ EMA {w} ...\")\n",
    "        feat_cols[f\"ema_{w}\"] = ema_numpy(close, w)\n",
    "    if \"wma\" in ma_types:\n",
    "        print(f\"   â€¢ WMA {w} ...\")\n",
    "        feat_cols[f\"wma_{w}\"] = wma_numpy(close, w)\n",
    "\n",
    "# ER\n",
    "for w in er_windows:\n",
    "    print(f\"   â€¢ ER {w} ...\")\n",
    "    feat_cols[f\"er_{w}\"] = er_numpy(close, w)\n",
    "\n",
    "# Slope\n",
    "print(f\"   â€¢ SLOPE {slope_win} ...\")\n",
    "feat_cols[f\"slope_{slope_win}\"] = slope_simple(close, slope_win)\n",
    "\n",
    "# -------------------- F) Armar DataFrame de features -------------------\n",
    "print(\"ğŸ§± Construyendo DataFrame de features ...\")\n",
    "feat_df = pl.DataFrame({\"ts\": pl.from_epoch(ts_ms/1000).dt.replace_time_zone(\"UTC\")})\n",
    "for name, arr in feat_cols.items():\n",
    "    feat_df = feat_df.with_columns(pl.Series(name, arr.astype(np.float64)))\n",
    "\n",
    "# Orden y unicidad por ts\n",
    "feat_df = feat_df.sort(\"ts\").unique(subset=[\"ts\"], keep=\"last\")\n",
    "\n",
    "# -------------------- G) Chequeos de calidad ---------------------------\n",
    "print(\"ğŸ” Chequeos de calidad de features ...\")\n",
    "_print_kv(\"features_rows\", feat_df.height)\n",
    "_print_kv(\"features_cols\", feat_df.width)\n",
    "\n",
    "# Rango temporal de features\n",
    "b = feat_df.select(\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    ").row(0)\n",
    "_print_kv(\"first_ts(features)\", datetime.fromtimestamp(int(b[0])/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"))\n",
    "_print_kv(\"last_ts(features)\",  datetime.fromtimestamp(int(b[1])/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"))\n",
    "\n",
    "# Dtypes resumen\n",
    "dtype_map = {c: str(t) for c, t in feat_df.schema.items()}\n",
    "_print_kv(\"dtype_ts\", dtype_map.get(\"ts\"))\n",
    "numeric_cols = [c for c, t in feat_df.schema.items() if c != \"ts\"]  # todas las features son float64\n",
    "_print_kv(\"num_cols_count\", len(numeric_cols))\n",
    "_print_kv(\"sample_num_cols\", numeric_cols[:8])\n",
    "\n",
    "# NaN por columna (solo floats)\n",
    "if numeric_cols:\n",
    "    nan_df = feat_df.select([pl.col(c).is_nan().sum().alias(c) for c in numeric_cols])\n",
    "    nan_counts = {c: int(nan_df[0, c]) for c in numeric_cols}\n",
    "    _print_kv(\"total_nan_cols>0\", sum(1 for v in nan_counts.values() if v > 0))\n",
    "    # Top 10 columnas con mÃ¡s NaN (por warm-up)\n",
    "    top_nan = sorted(nan_counts.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "    for k, v in top_nan:\n",
    "        _print_kv(f\"NaN in {k}\", v)\n",
    "\n",
    "# Muestras visuales (head/tail reducidas)\n",
    "print(\"ğŸ‘€ Muestra HEAD:\")\n",
    "print(feat_df.head(3))\n",
    "print(\"ğŸ‘€ Muestra TAIL:\")\n",
    "print(feat_df.tail(3))\n",
    "\n",
    "# -------------------- H) Persistencia -------------------------------\n",
    "print(\"ğŸ’¾ Guardando features ...\")\n",
    "feat_df.write_parquet(FEAT_PARQ)\n",
    "assert FEAT_PARQ.exists() and FEAT_PARQ.stat().st_size > 0, \"âŒ No se pudo escribir features parquet.\"\n",
    "_print_kv(\"features_parquet\", FEAT_PARQ)\n",
    "_print_kv(\"features_file_size(bytes)\", FEAT_PARQ.stat().st_size)\n",
    "_print_kv(\"features_file_size(MB)\", round(FEAT_PARQ.stat().st_size/1024/1024, 3))\n",
    "\n",
    "print(\"âœ… Celda 3 finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8be9298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - OHLC_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\data_curated\\ohlc_BTCUSDT_15m.parquet\n",
      "   - FEAT_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\features\\features_BTCUSDT_15m.parquet\n",
      "   - JOINED_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\joined_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando OHLC y FEATURES ...\n",
      "   - ohlc_rows                     : 169643\n",
      "   - ohlc_cols                     : 11\n",
      "   - feat_rows                     : 169643\n",
      "   - feat_cols                     : 11\n",
      "   - ohlc.first_ts                 : 2021-01-01T00:00:00+00:00\n",
      "   - ohlc.last_ts                  : 2025-11-03T20:00:00+00:00\n",
      "   - feat.first_ts                 : 2021-01-01T00:00:00+00:00\n",
      "   - feat.last_ts                  : 2025-11-03T20:00:00+00:00\n",
      "ğŸ§© Uniendo por 'ts' (inner join) ...\n",
      "   - joined_rows                   : 169643\n",
      "   - joined_cols                   : 21\n",
      "   - ts_sorted_nondec              : True\n",
      "   - ts_unique                     : True\n",
      "   - joined.first_ts               : 2021-01-01T00:00:00+00:00\n",
      "   - joined.last_ts                : 2025-11-03T20:00:00+00:00\n",
      "ğŸ” NaNs en columnas de features (warm-up esperado) ...\n",
      "   - feat_cols_count               : 10\n",
      "   - feat_nan_cols>0               : 10\n",
      "   - NaN in ema_50                 : 49\n",
      "   - NaN in wma_50                 : 49\n",
      "   - NaN in er_50                  : 49\n",
      "   - NaN in ema_20                 : 19\n",
      "   - NaN in wma_20                 : 19\n",
      "   - NaN in er_20                  : 19\n",
      "   - NaN in ema_10                 : 9\n",
      "   - NaN in wma_10                 : 9\n",
      "   - NaN in er_10                  : 9\n",
      "   - NaN in slope_10               : 9\n",
      "ğŸ§Š CÃ¡lculo de warm-up global (primer Ã­ndice donde TODAS las features son vÃ¡lidas) ...\n",
      "   - warmup_global_bars            : 49\n",
      "   - first_valid_ts_global         : 2021-01-01 12:15:00+00:00\n",
      "ğŸ‘€ Muestra HEAD (post-join):\n",
      "shape: (3, 21)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts                      â”† open     â”† high     â”† low      â”† â€¦ â”† er_10 â”† er_20 â”† er_50 â”† slope_10 â”‚\n",
      "â”‚ ---                     â”† ---      â”† ---      â”† ---      â”†   â”† ---   â”† ---   â”† ---   â”† ---      â”‚\n",
      "â”‚ datetime[Î¼s, UTC]       â”† f64      â”† f64      â”† f64      â”†   â”† f64   â”† f64   â”† f64   â”† f64      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2021-01-01 00:00:00 UTC â”† 28923.63 â”† 29017.5  â”† 28690.17 â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â”‚ 2021-01-01 00:15:00 UTC â”† 28752.8  â”† 28875.55 â”† 28720.91 â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â”‚ 2021-01-01 00:30:00 UTC â”† 28836.63 â”† 28943.87 â”† 28836.62 â”† â€¦ â”† NaN   â”† NaN   â”† NaN   â”† NaN      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ‘€ Muestra TAIL (post-join):\n",
      "shape: (3, 21)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts         â”† open      â”† high      â”† low       â”† â€¦ â”† er_10    â”† er_20    â”† er_50    â”† slope_10   â”‚\n",
      "â”‚ ---        â”† ---       â”† ---       â”† ---       â”†   â”† ---      â”† ---      â”† ---      â”† ---        â”‚\n",
      "â”‚ datetime[Î¼ â”† f64       â”† f64       â”† f64       â”†   â”† f64      â”† f64      â”† f64      â”† f64        â”‚\n",
      "â”‚ s, UTC]    â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2025-11-03 â”† 106946.0  â”† 107395.25 â”† 106927.06 â”† â€¦ â”† 0.103175 â”† 0.139322 â”† 0.033303 â”† 21.083333  â”‚\n",
      "â”‚ 19:30:00   â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â”‚ UTC        â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â”‚ 2025-11-03 â”† 107269.75 â”† 107373.06 â”† 107040.0  â”† â€¦ â”† 0.233284 â”† 0.102648 â”† 0.033792 â”† -42.221111 â”‚\n",
      "â”‚ 19:45:00   â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â”‚ UTC        â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â”‚ 2025-11-03 â”† 107090.0  â”† 107159.91 â”† 107090.0  â”† â€¦ â”† 0.190379 â”† 0.302845 â”† 0.026843 â”† -35.697778 â”‚\n",
      "â”‚ 20:00:00   â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â”‚ UTC        â”†           â”†           â”†           â”†   â”†          â”†          â”†          â”†            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "   - dtype_ts                      : Datetime(time_unit='us', time_zone='UTC')\n",
      "   - numeric_cols_count            : 0\n",
      "ğŸ’¾ Guardando joined temporal ...\n",
      "   - joined_parquet                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\joined_BTCUSDT_15m.parquet\n",
      "   - joined_size(MB)               : 18.92\n",
      "âœ… Celda 4a finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 4a: [JOIN & SANEO MÃNIMO â†’ tmp/joined_{SYMBOL}_{TF}.parquet]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Leer config, OHLC curado y features\n",
    "# - Unir por 'ts' (inner join), mantener orden temporal\n",
    "# - Validar: filas/columnas, monotonicidad, unicidad, NaNs en features, warm-up global\n",
    "# - Persistir un Parquet temporal 'joined' para inspecciÃ³n de downstream\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _assert_cols(df: pl.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"âŒ Faltan columnas: {missing}\"\n",
    "\n",
    "def _epoch_bounds_iso(df: pl.DataFrame, ts_col: str = \"ts\"):\n",
    "    b = df.select(\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    "    ).row(0)\n",
    "    first_ms, last_ms = int(b[0]), int(b[1])\n",
    "    return (\n",
    "        datetime.fromtimestamp(first_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        datetime.fromtimestamp(last_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    )\n",
    "\n",
    "# -------------------- A) Cargar config y paths -------------------------\n",
    "needed = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in needed), \"Faltan TWF_*; ejecuta celdas 0 y 1 primero.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe el archivo de configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS       = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "OHLC_PARQ   = PATHS[\"data_curated\"] / f\"ohlc_{SYMBOL}_{TF}.parquet\"\n",
    "FEAT_PARQ   = PATHS[\"features\"] / f\"features_{SYMBOL}_{TF}.parquet\"\n",
    "JOINED_PARQ = PATHS[\"tmp\"] / f\"joined_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "_print_kv(\"OHLC_PARQ\", OHLC_PARQ)\n",
    "_print_kv(\"FEAT_PARQ\", FEAT_PARQ)\n",
    "_print_kv(\"JOINED_PARQ\", JOINED_PARQ)\n",
    "\n",
    "assert OHLC_PARQ.exists(), f\"No existe OHLC: {OHLC_PARQ}\"\n",
    "assert FEAT_PARQ.exists(), f\"No existe features: {FEAT_PARQ}\"\n",
    "\n",
    "# -------------------- B) Leer DataFrames -------------------------------\n",
    "print(\"ğŸ“¦ Cargando OHLC y FEATURES ...\")\n",
    "ohlc = pl.read_parquet(OHLC_PARQ)\n",
    "feat = pl.read_parquet(FEAT_PARQ)\n",
    "\n",
    "_print_kv(\"ohlc_rows\", ohlc.height); _print_kv(\"ohlc_cols\", ohlc.width)\n",
    "_print_kv(\"feat_rows\", feat.height); _print_kv(\"feat_cols\", feat.width)\n",
    "_assert_cols(ohlc, [\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "_assert_cols(feat, [\"ts\"])\n",
    "\n",
    "# Rango temporal previo\n",
    "f1, l1 = _epoch_bounds_iso(ohlc, \"ts\")\n",
    "f2, l2 = _epoch_bounds_iso(feat, \"ts\")\n",
    "_print_kv(\"ohlc.first_ts\", f1); _print_kv(\"ohlc.last_ts\", l1)\n",
    "_print_kv(\"feat.first_ts\", f2); _print_kv(\"feat.last_ts\", l2)\n",
    "\n",
    "# -------------------- C) Join por 'ts' (inner) ------------------------\n",
    "print(\"ğŸ§© Uniendo por 'ts' (inner join) ...\")\n",
    "joined = ohlc.join(feat, on=\"ts\", how=\"inner\", suffix=\"_f\")\n",
    "\n",
    "_print_kv(\"joined_rows\", joined.height)\n",
    "_print_kv(\"joined_cols\", joined.width)\n",
    "# Validaciones rÃ¡pidas\n",
    "dm = joined.select(pl.col(\"ts\").dt.epoch(\"ms\").diff().alias(\"dms\")).get_column(\"dms\").drop_nulls()\n",
    "is_sorted = bool((dm >= 0).all())\n",
    "unique_ts = (joined[\"ts\"].n_unique() == joined.height)\n",
    "_print_kv(\"ts_sorted_nondec\", is_sorted); _print_kv(\"ts_unique\", unique_ts)\n",
    "assert is_sorted and unique_ts, \"âŒ El 'ts' del join debe quedar ordenado y Ãºnico.\"\n",
    "\n",
    "jf, jl = _epoch_bounds_iso(joined, \"ts\")\n",
    "_print_kv(\"joined.first_ts\", jf); _print_kv(\"joined.last_ts\", jl)\n",
    "\n",
    "# -------------------- D) Chequeo de NaNs en features -------------------\n",
    "print(\"ğŸ” NaNs en columnas de features (warm-up esperado) ...\")\n",
    "feat_cols = [c for c in joined.columns if c not in [\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_base\",\"taker_quote\"]]\n",
    "nan_sums = {}\n",
    "if feat_cols:\n",
    "    nan_df = joined.select([pl.col(c).is_nan().sum().alias(c) for c in feat_cols])\n",
    "    nan_sums = {c: int(nan_df[0, c]) for c in feat_cols}\n",
    "    total_nan_cols = sum(1 for v in nan_sums.values() if v > 0)\n",
    "    _print_kv(\"feat_cols_count\", len(feat_cols))\n",
    "    _print_kv(\"feat_nan_cols>0\", total_nan_cols)\n",
    "    # Top 10\n",
    "    top_nan = sorted(nan_sums.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "    for k, v in top_nan:\n",
    "        _print_kv(f\"NaN in {k}\", v)\n",
    "else:\n",
    "    print(\"   â„¹ï¸  No se detectaron columnas de features (Â¿archivo vacÃ­o o columnas no esperadas?).\")\n",
    "\n",
    "# -------------------- E) Warm-up global (primer Ã­ndice usable) ---------\n",
    "print(\"ğŸ§Š CÃ¡lculo de warm-up global (primer Ã­ndice donde TODAS las features son vÃ¡lidas) ...\")\n",
    "def _first_valid_index(series: pl.Series) -> int:\n",
    "    # Ã­ndice del primer valor vÃ¡lido (not null y not NaN); -1 si ninguno\n",
    "    s = series\n",
    "    ok = s.is_not_null() & (~s.is_nan())\n",
    "    if ok.any():\n",
    "        # np.argmax devuelve el primer True\n",
    "        return int(np.argmax(ok.to_numpy()))\n",
    "    return -1\n",
    "\n",
    "if feat_cols:\n",
    "    first_valid_idxs = {c: _first_valid_index(joined.get_column(c)) for c in feat_cols}\n",
    "    # descartamos -1 (columnas totalmente NaN, no deberÃ­a pasar aquÃ­)\n",
    "    filtered = [ix for ix in first_valid_idxs.values() if ix >= 0]\n",
    "    warmup_global = max(filtered) if filtered else 0\n",
    "    _print_kv(\"warmup_global_bars\", warmup_global)\n",
    "    if joined.height > 0:\n",
    "        valid_ts = joined[\"ts\"][warmup_global] if warmup_global < joined.height else None\n",
    "        _print_kv(\"first_valid_ts_global\", str(valid_ts) if valid_ts is not None else \"None\")\n",
    "else:\n",
    "    warmup_global = 0\n",
    "    _print_kv(\"warmup_global_bars\", warmup_global)\n",
    "\n",
    "# -------------------- F) Muestras y dtypes -----------------------------\n",
    "print(\"ğŸ‘€ Muestra HEAD (post-join):\")\n",
    "print(joined.head(3))\n",
    "print(\"ğŸ‘€ Muestra TAIL (post-join):\")\n",
    "print(joined.tail(3))\n",
    "\n",
    "dtype_map = {c: str(t) for c, t in joined.schema.items()}\n",
    "_print_kv(\"dtype_ts\", dtype_map.get(\"ts\"))\n",
    "_print_kv(\"numeric_cols_count\", sum(1 for t in joined.schema.values() if \"f64\" in str(t)))\n",
    "\n",
    "# -------------------- G) Persistencia ----------------------------------\n",
    "print(\"ğŸ’¾ Guardando joined temporal ...\")\n",
    "JOINED_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "joined.write_parquet(JOINED_PARQ)\n",
    "assert JOINED_PARQ.exists() and JOINED_PARQ.stat().st_size > 0, \"âŒ No se pudo escribir joined parquet.\"\n",
    "_print_kv(\"joined_parquet\", JOINED_PARQ)\n",
    "_print_kv(\"joined_size(MB)\", round(JOINED_PARQ.stat().st_size/1024/1024, 3))\n",
    "\n",
    "print(\"âœ… Celda 4a finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f4a3c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - JOINED_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\joined_BTCUSDT_15m.parquet\n",
      "   - ENTRIES_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\entries_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando joined temporal ...\n",
      "   - joined_rows                   : 169643\n",
      "   - joined_cols                   : 21\n",
      "   - joined.first_ts               : 2021-01-01T00:00:00+00:00\n",
      "   - joined.last_ts                : 2025-11-03T20:00:00+00:00\n",
      "ğŸ§Š Recalculando warm-up global ...\n",
      "   - feature_cols_count            : 10\n",
      "   - warmup_global_bars            : 49\n",
      "   - first_valid_ts_global         : 2021-01-01 12:15:00+00:00\n",
      "ğŸ§® Preparando combos de ER (ventana/umbral) ...\n",
      "   - er_windows                    : [10, 20, 50]\n",
      "   - er_thresholds                 : [0.2, 0.3, 0.4]\n",
      "ğŸš¦ Generando banderas de entrada (ER â‰¥ umbral) ...\n",
      "   â€¢ entry_er10_ge_0p2 creado.\n",
      "   â€¢ entry_er10_ge_0p3 creado.\n",
      "   â€¢ entry_er10_ge_0p4 creado.\n",
      "   â€¢ entry_er20_ge_0p2 creado.\n",
      "   â€¢ entry_er20_ge_0p3 creado.\n",
      "   â€¢ entry_er20_ge_0p4 creado.\n",
      "   â€¢ entry_er50_ge_0p2 creado.\n",
      "   â€¢ entry_er50_ge_0p3 creado.\n",
      "   â€¢ entry_er50_ge_0p4 creado.\n",
      "ğŸ” Conteos de entradas por combo ...\n",
      "   - entry_cols_total              : 9\n",
      "   - signals in entry_er10_ge_0p2  : 105683\n",
      "   - signals in entry_er20_ge_0p2  : 77824\n",
      "   - signals in entry_er10_ge_0p3  : 77631\n",
      "   - signals in entry_er10_ge_0p4  : 53709\n",
      "   - signals in entry_er20_ge_0p3  : 45468\n",
      "   - signals in entry_er50_ge_0p2  : 42976\n",
      "   - signals in entry_er20_ge_0p4  : 23685\n",
      "   - signals in entry_er50_ge_0p3  : 16153\n",
      "   - signals in entry_er50_ge_0p4  : 4821\n",
      "   - rows_with_any_entry           : 134007\n",
      "   - max_entries_same_row          : 9\n",
      "   - first_ts_with_entry           : 2021-01-01T12:15:00+00:00\n",
      "   - last_ts_with_entry            : 2025-11-03T20:00:00+00:00\n",
      "ğŸ‘€ Muestra HEAD (entries):\n",
      "shape: (3, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts         â”† close    â”† entry_coun â”† entry_er10 â”† entry_er10 â”† entry_er1 â”† entry_er2 â”† entry_er2 â”‚\n",
      "â”‚ ---        â”† ---      â”† t_row      â”† _ge_0p2    â”† _ge_0p3    â”† 0_ge_0p4  â”† 0_ge_0p2  â”† 0_ge_0p3  â”‚\n",
      "â”‚ datetime[Î¼ â”† f64      â”† ---        â”† ---        â”† ---        â”† ---       â”† ---       â”† ---       â”‚\n",
      "â”‚ s, UTC]    â”†          â”† i8         â”† bool       â”† bool       â”† bool      â”† bool      â”† bool      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2021-01-01 â”† 28752.8  â”† 0          â”† false      â”† false      â”† false     â”† false     â”† false     â”‚\n",
      "â”‚ 00:00:00   â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â”‚ 2021-01-01 â”† 28836.63 â”† 0          â”† false      â”† false      â”† false     â”† false     â”† false     â”‚\n",
      "â”‚ 00:15:00   â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â”‚ 2021-01-01 â”† 28930.11 â”† 0          â”† false      â”† false      â”† false     â”† false     â”† false     â”‚\n",
      "â”‚ 00:30:00   â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†          â”†            â”†            â”†            â”†           â”†           â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ‘€ Muestra TAIL (entries):\n",
      "shape: (3, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ ts         â”† close     â”† entry_coun â”† entry_er10 â”† entry_er1 â”† entry_er1 â”† entry_er2 â”† entry_er2 â”‚\n",
      "â”‚ ---        â”† ---       â”† t_row      â”† _ge_0p2    â”† 0_ge_0p3  â”† 0_ge_0p4  â”† 0_ge_0p2  â”† 0_ge_0p3  â”‚\n",
      "â”‚ datetime[Î¼ â”† f64       â”† ---        â”† ---        â”† ---       â”† ---       â”† ---       â”† ---       â”‚\n",
      "â”‚ s, UTC]    â”†           â”† i8         â”† bool       â”† bool      â”† bool      â”† bool      â”† bool      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2025-11-03 â”† 107269.75 â”† 0          â”† false      â”† false     â”† false     â”† false     â”† false     â”‚\n",
      "â”‚ 19:30:00   â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ 2025-11-03 â”† 107090.0  â”† 1          â”† true       â”† false     â”† false     â”† false     â”† false     â”‚\n",
      "â”‚ 19:45:00   â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ 2025-11-03 â”† 107159.91 â”† 2          â”† false      â”† false     â”† false     â”† true      â”† true      â”‚\n",
      "â”‚ 20:00:00   â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â”‚ UTC        â”†           â”†            â”†            â”†           â”†           â”†           â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ’¾ Guardando entries ...\n",
      "   - entries_parquet               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\entries_BTCUSDT_15m.parquet\n",
      "   - entries_size(MB)              : 4.935\n",
      "   - entries_meta                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\entries_meta_BTCUSDT_15m.json\n",
      "âœ… Celda 4b finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 4b: [DETECCIÃ“N DE ENTRADAS (ER â‰¥ UMBRAL) â†’ tmp/entries_{SYMBOL}_{TF}.parquet]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Responsabilidad Ãºnica:\n",
    "# - Leer config y el joined temporal de 4a.\n",
    "# - Recalcular warm-up global (o usar el de 4a si lo hubiÃ©ramos persistido).\n",
    "# - Para cada ventana ER y umbral, crear una columna booleana de \"entrada\".\n",
    "# - Enmascarar entradas antes del warm-up global.\n",
    "# - Persistir Parquet con ts, close y banderas de entrada por combo.\n",
    "# - Imprimir verificaciones exhaustivas (conteos por combo, primeros/Ãºltimos ts, etc.).\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _assert_cols(df: pl.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"âŒ Faltan columnas: {missing}\"\n",
    "\n",
    "def _epoch_bounds_iso(df: pl.DataFrame, ts_col: str = \"ts\"):\n",
    "    b = df.select(\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    "    ).row(0)\n",
    "    first_ms, last_ms = int(b[0]), int(b[1])\n",
    "    return (\n",
    "        datetime.fromtimestamp(first_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        datetime.fromtimestamp(last_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    )\n",
    "\n",
    "def _first_valid_index(series: pl.Series) -> int:\n",
    "    ok = series.is_not_null() & (~series.is_nan())\n",
    "    if ok.any():\n",
    "        return int(np.argmax(ok.to_numpy()))\n",
    "    return -1\n",
    "\n",
    "# -------------------- A) Cargar config y paths -------------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas 0â†’4a primero.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS        = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "JOINED_PARQ  = PATHS[\"tmp\"] / f\"joined_{SYMBOL}_{TF}.parquet\"\n",
    "ENTRIES_PARQ = PATHS[\"tmp\"] / f\"entries_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "_print_kv(\"JOINED_PARQ\", JOINED_PARQ)\n",
    "_print_kv(\"ENTRIES_PARQ\", ENTRIES_PARQ)\n",
    "assert JOINED_PARQ.exists(), f\"No existe joined temporal: {JOINED_PARQ}\"\n",
    "\n",
    "# -------------------- B) Leer JOINED -------------------------\n",
    "print(\"ğŸ“¦ Cargando joined temporal ...\")\n",
    "df = pl.read_parquet(JOINED_PARQ)\n",
    "_print_kv(\"joined_rows\", df.height)\n",
    "_print_kv(\"joined_cols\", df.width)\n",
    "_assert_cols(df, [\"ts\",\"close\"])\n",
    "\n",
    "# Rango temporal para referencia\n",
    "fj, lj = _epoch_bounds_iso(df, \"ts\")\n",
    "_print_kv(\"joined.first_ts\", fj); _print_kv(\"joined.last_ts\", lj)\n",
    "\n",
    "# -------------------- C) Recalcular warm-up global ---------------------\n",
    "print(\"ğŸ§Š Recalculando warm-up global ...\")\n",
    "feature_cols = [c for c in df.columns if c.startswith((\"ema_\",\"wma_\",\"er_\",\"slope_\"))]\n",
    "_print_kv(\"feature_cols_count\", len(feature_cols))\n",
    "assert feature_cols, \"âŒ No se hallaron columnas de features en el joined.\"\n",
    "\n",
    "first_valid_idxs = {c: _first_valid_index(df.get_column(c)) for c in feature_cols}\n",
    "valid_ixs = [ix for ix in first_valid_idxs.values() if ix >= 0]\n",
    "warmup_global = max(valid_ixs) if valid_ixs else 0\n",
    "_print_kv(\"warmup_global_bars\", warmup_global)\n",
    "wts = df[\"ts\"][warmup_global] if warmup_global < df.height else None\n",
    "_print_kv(\"first_valid_ts_global\", str(wts) if wts is not None else \"None\")\n",
    "\n",
    "# -------------------- D) Config: ER windows y thresholds ----------------\n",
    "print(\"ğŸ§® Preparando combos de ER (ventana/umbral) ...\")\n",
    "er_windows = CFG[\"features\"][\"er\"][\"windows\"]\n",
    "er_thrs    = CFG[\"features\"][\"er\"][\"thresholds\"]\n",
    "_print_kv(\"er_windows\", er_windows)\n",
    "_print_kv(\"er_thresholds\", er_thrs)\n",
    "\n",
    "# Validar que existan columnas er_*\n",
    "missing_er_cols = [f\"er_{w}\" for w in er_windows if f\"er_{w}\" not in df.columns]\n",
    "assert not missing_er_cols, f\"âŒ Faltan columnas ER requeridas en joined: {missing_er_cols}\"\n",
    "\n",
    "# -------------------- E) ConstrucciÃ³n de flags de entrada ----------------\n",
    "print(\"ğŸš¦ Generando banderas de entrada (ER â‰¥ umbral) ...\")\n",
    "mask_after_warmup = pl.arange(0, df.height).cast(pl.Int64) >= warmup_global\n",
    "\n",
    "entry_cols = []\n",
    "work_df = df.select([\"ts\",\"close\"] + [f\"er_{w}\" for w in er_windows])\n",
    "\n",
    "for w in er_windows:\n",
    "    er_col = f\"er_{w}\"\n",
    "    for thr in er_thrs:\n",
    "        # Normalizar nombre de columna (ej: 0.3 -> 030)\n",
    "        thr_tag = str(thr).replace(\".\",\"p\").replace(\",\",\"p\")\n",
    "        col_name = f\"entry_er{w}_ge_{thr_tag}\"\n",
    "        entry_cols.append(col_name)\n",
    "        # entrada = (er_w >= thr) & no NaN & despues del warmup\n",
    "        work_df = work_df.with_columns(\n",
    "            pl.when(\n",
    "                (pl.col(er_col).is_not_null()) & (~pl.col(er_col).is_nan()) & (pl.col(er_col) >= pl.lit(thr)) & mask_after_warmup\n",
    "            ).then(pl.lit(True)).otherwise(pl.lit(False)).alias(col_name)\n",
    "        )\n",
    "        print(f\"   â€¢ {col_name} creado.\")\n",
    "\n",
    "# -------------------- F) Verificaciones de conteos ----------------------\n",
    "print(\"ğŸ” Conteos de entradas por combo ...\")\n",
    "counts = {}\n",
    "for c in entry_cols:\n",
    "    counts[c] = int(work_df[c].sum())\n",
    "# Top combos por nÃºmero de seÃ±ales\n",
    "top = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "_print_kv(\"entry_cols_total\", len(entry_cols))\n",
    "for k, v in top:\n",
    "    _print_kv(f\"signals in {k}\", v)\n",
    "\n",
    "# Conteo por fila (cuÃ¡ntos combos disparan simultÃ¡neo)\n",
    "work_df = work_df.with_columns(\n",
    "    pl.sum_horizontal([pl.col(c).cast(pl.Int8) for c in entry_cols]).alias(\"entry_count_row\")\n",
    ")\n",
    "_print_kv(\"rows_with_any_entry\", int((work_df[\"entry_count_row\"] > 0).sum()))\n",
    "_print_kv(\"max_entries_same_row\", int(work_df[\"entry_count_row\"].max()))\n",
    "\n",
    "# Rango temporal donde hubo al menos 1 entrada\n",
    "if int((work_df[\"entry_count_row\"] > 0).sum()) > 0:\n",
    "    sub = work_df.filter(pl.col(\"entry_count_row\") > 0)\n",
    "    f3, l3 = _epoch_bounds_iso(sub, \"ts\")\n",
    "    _print_kv(\"first_ts_with_entry\", f3)\n",
    "    _print_kv(\"last_ts_with_entry\", l3)\n",
    "else:\n",
    "    print(\"   â„¹ï¸  No hubo ninguna entrada con los umbrales actuales.\")\n",
    "\n",
    "# Muestras\n",
    "print(\"ğŸ‘€ Muestra HEAD (entries):\")\n",
    "print(work_df.head(3).select([\"ts\",\"close\",\"entry_count_row\"] + entry_cols[:min(5, len(entry_cols))]))\n",
    "print(\"ğŸ‘€ Muestra TAIL (entries):\")\n",
    "print(work_df.tail(3).select([\"ts\",\"close\",\"entry_count_row\"] + entry_cols[:min(5, len(entry_cols))]))\n",
    "\n",
    "# -------------------- G) Persistencia ----------------------------------\n",
    "print(\"ğŸ’¾ Guardando entries ...\")\n",
    "ENTRIES_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "work_df.write_parquet(ENTRIES_PARQ)\n",
    "assert ENTRIES_PARQ.exists() and ENTRIES_PARQ.stat().st_size > 0, \"âŒ No se pudo escribir entries parquet.\"\n",
    "_print_kv(\"entries_parquet\", ENTRIES_PARQ)\n",
    "_print_kv(\"entries_size(MB)\", round(ENTRIES_PARQ.stat().st_size/1024/1024, 3))\n",
    "\n",
    "# Meta opcional: guardar lista de columnas de entrada\n",
    "META_PATH = PATHS[\"logs\"] / f\"entries_meta_{SYMBOL}_{TF}.json\"\n",
    "meta = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"timeframe\": TF,\n",
    "    \"er_windows\": er_windows,\n",
    "    \"er_thresholds\": er_thrs,\n",
    "    \"entry_columns\": entry_cols,\n",
    "    \"warmup_global_bars\": int(warmup_global),\n",
    "    \"first_valid_ts_global\": str(wts) if wts is not None else None\n",
    "}\n",
    "with META_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "_print_kv(\"entries_meta\", META_PATH)\n",
    "\n",
    "print(\"âœ… Celda 4b finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fb8f3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - JOINED_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\joined_BTCUSDT_15m.parquet\n",
      "   - ENTRIES_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\tmp\\entries_BTCUSDT_15m.parquet\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando joined y entries ...\n",
      "ğŸ”— Alineando por 'ts' ...\n",
      "   - entries.first_ts              : 2021-01-01T00:00:00+00:00\n",
      "   - entries.last_ts               : 2025-11-03T20:00:00+00:00\n",
      "   - entries_rows                  : 169643\n",
      "âš™ï¸ ParÃ¡metros Triple-Barrier ...\n",
      "   - horizon_bars                  : 48\n",
      "   - timeout_bars                  : 48\n",
      "   - up_mult                       : 2.0\n",
      "   - dn_mult                       : 1.0\n",
      "   - atr_window(local)             : 14\n",
      "ğŸ“ˆ Calculando ATR% ...\n",
      "   - first_valid_atr_idx           : 13\n",
      "   - first_valid_atr_ts            : 2021-01-01 03:15:00+00:00\n",
      "ğŸ§± Preparando arrays para escaneo forward ...\n",
      "   - entry_cols_count              : 9\n",
      "   - entry_cols_sample             : ['entry_er10_ge_0p2', 'entry_er10_ge_0p3', 'entry_er10_ge_0p4', 'entry_er20_ge_0p2', 'entry_er20_ge_0p3', 'entry_er20_ge_0p4', 'entry_er50_ge_0p2', 'entry_er50_ge_0p3']\n",
      "ğŸ·ï¸ Etiquetando seÃ±ales (todas las columnas entry_er*) ...\n",
      "â†’ Procesando entry_er10_ge_0p2 ...\n",
      "   - signals_labeled               : 105683\n",
      "â†’ Procesando entry_er10_ge_0p3 ...\n",
      "   - signals_labeled               : 77631\n",
      "â†’ Procesando entry_er10_ge_0p4 ...\n",
      "   - signals_labeled               : 53709\n",
      "â†’ Procesando entry_er20_ge_0p2 ...\n",
      "   - signals_labeled               : 77823\n",
      "â†’ Procesando entry_er20_ge_0p3 ...\n",
      "   - signals_labeled               : 45467\n",
      "â†’ Procesando entry_er20_ge_0p4 ...\n",
      "   - signals_labeled               : 23685\n",
      "â†’ Procesando entry_er50_ge_0p2 ...\n",
      "   - signals_labeled               : 42976\n",
      "â†’ Procesando entry_er50_ge_0p3 ...\n",
      "   - signals_labeled               : 16153\n",
      "â†’ Procesando entry_er50_ge_0p4 ...\n",
      "   - signals_labeled               : 4821\n",
      "   - forward_rows_total            : 447948\n",
      "   - forward_cols                  : 11\n",
      "ğŸ“Š Resumen de etiquetas por columna ...\n",
      "shape: (9, 9)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col       â”† signals â”† tp    â”† sl    â”† â€¦ â”† ret_mean  â”† ret_median â”† bars_mean â”† bars_median â”‚\n",
      "â”‚ ---             â”† ---     â”† ---   â”† ---   â”†   â”† ---       â”† ---        â”† ---       â”† ---         â”‚\n",
      "â”‚ str             â”† u32     â”† i32   â”† i32   â”†   â”† f64       â”† f64        â”† f64       â”† f64         â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er10_ge_0 â”† 105683  â”† 34083 â”† 70289 â”† â€¦ â”† -0.000095 â”† -0.002269  â”† 7.118799  â”† 4.0         â”‚\n",
      "â”‚ p2              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er10_ge_0 â”† 77631   â”† 25041 â”† 51613 â”† â€¦ â”† -0.000083 â”† -0.002313  â”† 7.040499  â”† 4.0         â”‚\n",
      "â”‚ p3              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er10_ge_0 â”† 53709   â”† 17221 â”† 35780 â”† â€¦ â”† -0.000096 â”† -0.002385  â”† 6.994992  â”† 4.0         â”‚\n",
      "â”‚ p4              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er20_ge_0 â”† 77823   â”† 25104 â”† 51613 â”† â€¦ â”† -0.000095 â”† -0.002403  â”† 7.319391  â”† 4.0         â”‚\n",
      "â”‚ p2              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er20_ge_0 â”† 45467   â”† 14574 â”† 30143 â”† â€¦ â”† -0.000111 â”† -0.002627  â”† 7.470825  â”† 4.0         â”‚\n",
      "â”‚ p3              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er20_ge_0 â”† 23685   â”† 7588  â”† 15618 â”† â€¦ â”† -0.000084 â”† -0.002922  â”† 7.808064  â”† 4.0         â”‚\n",
      "â”‚ p4              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er50_ge_0 â”† 42976   â”† 13603 â”† 28659 â”† â€¦ â”† -0.000195 â”† -0.002861  â”† 7.884819  â”† 4.0         â”‚\n",
      "â”‚ p2              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er50_ge_0 â”† 16153   â”† 5073  â”† 10721 â”† â€¦ â”† -0.00017  â”† -0.003235  â”† 8.527394  â”† 5.0         â”‚\n",
      "â”‚ p3              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â”‚ entry_er50_ge_0 â”† 4821    â”† 1540  â”† 3167  â”† â€¦ â”† -0.000109 â”† -0.003714  â”† 8.751296  â”† 5.0         â”‚\n",
      "â”‚ p4              â”†         â”†       â”†       â”†   â”†           â”†            â”†           â”†             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ’¾ Guardando forward ...\n",
      "   - forward_parquet               : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - forward_size(MB)              : 6.827\n",
      "   - forward_meta                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\forward_meta_BTCUSDT_15m.json\n",
      "âœ… Celda 4c finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 4c (fix definitivo y mÃ©tricas estables): [TRIPLE-BARRIER / FORWARD LABELING â†’ forward/forward_{SYMBOL}_{TF}.parquet]\n",
    "# -------------------------------------------------------------------------------------\n",
    "# - Estudio estadÃ­stico (NO backtesting): etiquetado triple-barrier para medir ventaja.\n",
    "# - Lee config, joined (OHLC+features) y entries (seÃ±ales).\n",
    "# - Calcula ATR% como base de volatilidad.\n",
    "# - Etiqueta cada seÃ±al con Triple-Barrier (1=TP, -1=SL, 0=timeout).\n",
    "# - Persiste un Parquet por-seÃ±al con mÃ©tricas forward.\n",
    "# - Compatibilidad Polars \"legacy\": sin Series/DataFrame.take(); se usa epoch(us)+NumPy.\n",
    "# - FIX: usar DataFrame.group_by(...).agg(...) + Expr.sum() + pl.len().\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "TS_DTYPE = pl.Datetime(time_unit=\"us\", time_zone=\"UTC\")\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _assert_cols(df: pl.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"âŒ Faltan columnas: {missing}\"\n",
    "\n",
    "def _epoch_bounds_iso(df: pl.DataFrame, ts_col: str = \"ts\"):\n",
    "    b = df.select(\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "        pl.col(ts_col).dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    "    ).row(0)\n",
    "    first_ms, last_ms = int(b[0]), int(b[1])\n",
    "    return (\n",
    "        datetime.fromtimestamp(first_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        datetime.fromtimestamp(last_ms/1000, tz=timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    )\n",
    "\n",
    "def _first_valid_index(series: pl.Series) -> int:\n",
    "    ok = series.is_not_null() & (~series.is_nan())\n",
    "    if ok.any():\n",
    "        return int(np.argmax(ok.to_numpy()))\n",
    "    return -1\n",
    "\n",
    "# -------------------- A) Cargar config y paths -------------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas 0â†’4b primero.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS         = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "JOINED_PARQ   = PATHS[\"tmp\"] / f\"joined_{SYMBOL}_{TF}.parquet\"\n",
    "ENTRIES_PARQ  = PATHS[\"tmp\"] / f\"entries_{SYMBOL}_{TF}.parquet\"\n",
    "FORWARD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "META_PATH     = PATHS[\"logs\"]    / f\"forward_meta_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"JOINED_PARQ\", JOINED_PARQ)\n",
    "_print_kv(\"ENTRIES_PARQ\", ENTRIES_PARQ)\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "assert JOINED_PARQ.exists(), f\"No existe joined temporal: {JOINED_PARQ}\"\n",
    "assert ENTRIES_PARQ.exists(), f\"No existe entries: {ENTRIES_PARQ}\"\n",
    "\n",
    "# -------------------- B) Leer DataFrames base --------------------------\n",
    "print(\"ğŸ“¦ Cargando joined y entries ...\")\n",
    "joined  = pl.read_parquet(JOINED_PARQ)\n",
    "entries = pl.read_parquet(ENTRIES_PARQ)\n",
    "\n",
    "_assert_cols(joined, [\"ts\",\"open\",\"high\",\"low\",\"close\"])\n",
    "_assert_cols(entries, [\"ts\",\"close\"])\n",
    "\n",
    "# Sincronizar por ts (seguridad)\n",
    "print(\"ğŸ”— Alineando por 'ts' ...\")\n",
    "base = joined.select([\"ts\",\"open\",\"high\",\"low\",\"close\"])\n",
    "entries = entries.join(base, on=\"ts\", how=\"inner\", suffix=\"_j\")\n",
    "# Solo 'close' colisiona â†’ 'close_j'\n",
    "_assert_cols(entries, [\"ts\",\"close\",\"open\",\"high\",\"low\",\"close_j\"])\n",
    "\n",
    "fj, lj = _epoch_bounds_iso(entries, \"ts\")\n",
    "_print_kv(\"entries.first_ts\", fj); _print_kv(\"entries.last_ts\", lj)\n",
    "_print_kv(\"entries_rows\", entries.height)\n",
    "\n",
    "# -------------------- C) ParÃ¡metros Triple-Barrier ---------------------\n",
    "print(\"âš™ï¸ ParÃ¡metros Triple-Barrier ...\")\n",
    "TB_CFG = CFG[\"labeling\"][\"triple_barrier\"]\n",
    "HORIZON_BARS = int(TB_CFG.get(\"horizon_bars\", 48))\n",
    "UP_MULT      = float(TB_CFG.get(\"up_mult\", 2.0))\n",
    "DN_MULT      = float(TB_CFG.get(\"dn_mult\", 1.0))\n",
    "TIMEOUT_BARS = int(TB_CFG.get(\"timeout_bars\", HORIZON_BARS))\n",
    "ATR_WINDOW   = int(TB_CFG.get(\"atr_window\", 14))   # local\n",
    "\n",
    "_print_kv(\"horizon_bars\", HORIZON_BARS)\n",
    "_print_kv(\"timeout_bars\", TIMEOUT_BARS)\n",
    "_print_kv(\"up_mult\", UP_MULT)\n",
    "_print_kv(\"dn_mult\", DN_MULT)\n",
    "_print_kv(\"atr_window(local)\", ATR_WINDOW)\n",
    "\n",
    "# -------------------- D) Calcular ATR% (volatilidad base) --------------\n",
    "print(\"ğŸ“ˆ Calculando ATR% ...\")\n",
    "hi = entries[\"high\"].to_numpy()\n",
    "lo = entries[\"low\"].to_numpy()\n",
    "cl = entries[\"close_j\"].to_numpy()\n",
    "\n",
    "prev_close = np.roll(cl, 1); prev_close[0] = cl[0]\n",
    "tr = np.maximum.reduce([hi - lo, np.abs(hi - prev_close), np.abs(lo - prev_close)])\n",
    "\n",
    "atr = np.full_like(tr, np.nan, dtype=np.float64)\n",
    "if tr.size >= ATR_WINDOW:\n",
    "    csum = np.concatenate([[0.0], np.cumsum(tr, dtype=np.float64)])\n",
    "    atr_win = (csum[ATR_WINDOW:] - csum[:-ATR_WINDOW]) / ATR_WINDOW\n",
    "    atr[ATR_WINDOW-1:] = atr_win\n",
    "\n",
    "vol_pct = np.divide(atr, cl, out=np.full_like(atr, np.nan), where=cl != 0.0)\n",
    "\n",
    "entries = entries.with_columns(\n",
    "    pl.Series(\"atr\", atr),\n",
    "    pl.Series(\"vol_pct\", vol_pct)\n",
    ")\n",
    "\n",
    "first_valid_atr = _first_valid_index(entries[\"atr\"])\n",
    "_print_kv(\"first_valid_atr_idx\", first_valid_atr)\n",
    "if 0 <= first_valid_atr < entries.height:\n",
    "    _print_kv(\"first_valid_atr_ts\", str(entries[\"ts\"][first_valid_atr]))\n",
    "\n",
    "# -------------------- E) Arrays, epoch(us) y columnas de seÃ±ales -------\n",
    "print(\"ğŸ§± Preparando arrays para escaneo forward ...\")\n",
    "close_arr = entries[\"close_j\"].to_numpy()\n",
    "high_arr  = entries[\"high\"].to_numpy()\n",
    "low_arr   = entries[\"low\"].to_numpy()\n",
    "vol_arr   = entries[\"vol_pct\"].to_numpy()\n",
    "\n",
    "# Epoch(us) como NumPy para resolver ts/event_ts por Ã­ndices sin .take()\n",
    "ts_us_arr = entries.select(pl.col(\"ts\").dt.epoch(\"us\")).to_series().to_numpy()\n",
    "\n",
    "# SeÃ±ales: solo columnas entry_er*\n",
    "entry_cols = [c for c in entries.columns if c.startswith(\"entry_er\")]\n",
    "_print_kv(\"entry_cols_count\", len(entry_cols))\n",
    "_print_kv(\"entry_cols_sample\", entry_cols[:8])\n",
    "assert entry_cols, \"âŒ No se hallaron columnas entry_er* en entries.\"\n",
    "\n",
    "# -------------------- F) FunciÃ³n de etiquetado por seÃ±ales -------------\n",
    "def label_signals_for_column(col_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Estudio estadÃ­stico: recorre Ã­ndices True de `col_name`, calcula barreras up/dn\n",
    "    con base en ATR%, y etiqueta (TP/SL/timeout) dentro del horizonte.\n",
    "    \"\"\"\n",
    "    mask = entries[col_name].to_numpy().astype(bool)\n",
    "    idxs = np.where(mask)[0]\n",
    "    m = idxs.size\n",
    "    if m == 0:\n",
    "        return pl.DataFrame({\n",
    "            \"ts\": pl.Series([], dtype=TS_DTYPE),\n",
    "            \"entry_col\": pl.Series([], dtype=pl.Utf8),\n",
    "            \"entry_idx\": pl.Series([], dtype=pl.Int64),\n",
    "            \"label\": pl.Series([], dtype=pl.Int8),\n",
    "            \"bars_to_event\": pl.Series([], dtype=pl.Int32),\n",
    "            \"event_ts\": pl.Series([], dtype=TS_DTYPE),\n",
    "            \"entry_price\": pl.Series([], dtype=pl.Float64),\n",
    "            \"event_price\": pl.Series([], dtype=pl.Float64),\n",
    "            \"ret_event\": pl.Series([], dtype=pl.Float64),\n",
    "            \"up_price\": pl.Series([], dtype=pl.Float64),\n",
    "            \"dn_price\": pl.Series([], dtype=pl.Float64),\n",
    "        })\n",
    "\n",
    "    out_entry_idx, out_event_idx = [], []\n",
    "    out_label, out_bars = [], []\n",
    "    out_entry_px, out_evt_px, out_ret = [], [], []\n",
    "    out_up, out_dn = [], []\n",
    "    out_col = []\n",
    "\n",
    "    N = entries.height\n",
    "    H = HORIZON_BARS\n",
    "    T = TIMEOUT_BARS\n",
    "\n",
    "    t0 = time.time(); last_report = t0\n",
    "\n",
    "    for k, i in enumerate(idxs):\n",
    "        start = i + 1\n",
    "        if start >= N:\n",
    "            continue\n",
    "        end_h = min(i + H, N - 1)\n",
    "        end_t = min(i + T, N - 1)\n",
    "\n",
    "        entry_px = close_arr[i]\n",
    "        v = vol_arr[i]\n",
    "        if not (np.isfinite(entry_px) and np.isfinite(v)):\n",
    "            continue\n",
    "\n",
    "        up_px = entry_px * (1.0 + UP_MULT * v)\n",
    "        dn_px = entry_px * (1.0 - DN_MULT * v)\n",
    "\n",
    "        hi_seg = high_arr[start:end_h+1]\n",
    "        lo_seg = low_arr[start:end_h+1]\n",
    "\n",
    "        hit_up = np.where(hi_seg >= up_px)[0]\n",
    "        hit_dn = np.where(lo_seg <= dn_px)[0]\n",
    "\n",
    "        if hit_up.size == 0 and hit_dn.size == 0:\n",
    "            evt_idx = min(i + T, end_t)\n",
    "            label = 0\n",
    "            evt_px = close_arr[evt_idx]\n",
    "        else:\n",
    "            fu = hit_up[0] if hit_up.size > 0 else None\n",
    "            fd = hit_dn[0] if hit_dn.size > 0 else None\n",
    "            if fu is not None and fd is not None:\n",
    "                if fu < fd:\n",
    "                    label = 1; evt_idx = start + fu; evt_px = up_px\n",
    "                elif fd < fu:\n",
    "                    label = -1; evt_idx = start + fd; evt_px = dn_px\n",
    "                else:\n",
    "                    label = 1; evt_idx = start + fu; evt_px = up_px\n",
    "            elif fu is not None:\n",
    "                label = 1; evt_idx = start + fu; evt_px = up_px\n",
    "            else:\n",
    "                label = -1; evt_idx = start + fd; evt_px = dn_px\n",
    "\n",
    "        ret = (evt_px / entry_px) - 1.0 if (np.isfinite(evt_px) and entry_px != 0) else np.nan\n",
    "\n",
    "        out_entry_idx.append(int(i))\n",
    "        out_event_idx.append(int(evt_idx))\n",
    "        out_label.append(int(label))\n",
    "        out_bars.append(int(evt_idx - i))\n",
    "        out_entry_px.append(float(entry_px))\n",
    "        out_evt_px.append(float(evt_px))\n",
    "        out_ret.append(float(ret))\n",
    "        out_up.append(float(up_px))\n",
    "        out_dn.append(float(dn_px))\n",
    "        out_col.append(col_name)\n",
    "\n",
    "        if k % 10000 == 0:\n",
    "            now = time.time()\n",
    "            if now - last_report > 0.5:\n",
    "                print(f\"   â€¢ {col_name}: procesadas {k}/{m} seÃ±ales ...\")\n",
    "                last_report = now\n",
    "\n",
    "    # ---- ConstrucciÃ³n ts/event_ts desde epoch(us) por Ã­ndices ----\n",
    "    entry_idx_np = np.array(out_entry_idx, dtype=np.int64)\n",
    "    event_idx_np = np.array(out_event_idx, dtype=np.int64)\n",
    "\n",
    "    ts_us       = ts_us_arr[entry_idx_np]\n",
    "    event_ts_us = ts_us_arr[event_idx_np]\n",
    "\n",
    "    s_ts       = pl.Series(\"ts\", ts_us, dtype=pl.Int64).cast(TS_DTYPE)\n",
    "    s_event_ts = pl.Series(\"event_ts\", event_ts_us, dtype=pl.Int64).cast(TS_DTYPE)\n",
    "\n",
    "    df_out = pl.DataFrame({\n",
    "        \"ts\": s_ts,\n",
    "        \"entry_col\": pl.Series(\"entry_col\", out_col, dtype=pl.Utf8),\n",
    "        \"entry_idx\": pl.Series(\"entry_idx\", out_entry_idx, dtype=pl.Int64),\n",
    "        \"label\": pl.Series(\"label\", out_label, dtype=pl.Int8),\n",
    "        \"bars_to_event\": pl.Series(\"bars_to_event\", out_bars, dtype=pl.Int32),\n",
    "        \"event_ts\": s_event_ts,\n",
    "        \"entry_price\": pl.Series(\"entry_price\", out_entry_px, dtype=pl.Float64),\n",
    "        \"event_price\": pl.Series(\"event_price\", out_evt_px, dtype=pl.Float64),\n",
    "        \"ret_event\": pl.Series(\"ret_event\", out_ret, dtype=pl.Float64),\n",
    "        \"up_price\": pl.Series(\"up_price\", out_up, dtype=pl.Float64),\n",
    "        \"dn_price\": pl.Series(\"dn_price\", out_dn, dtype=pl.Float64),\n",
    "    })\n",
    "    return df_out\n",
    "\n",
    "# -------------------- G) Ejecutar etiquetado para todas las columnas ----\n",
    "print(\"ğŸ·ï¸ Etiquetando seÃ±ales (todas las columnas entry_er*) ...\")\n",
    "per_col = []\n",
    "for col in entry_cols:\n",
    "    print(f\"â†’ Procesando {col} ...\")\n",
    "    df_lab = label_signals_for_column(col)\n",
    "    _print_kv(\"signals_labeled\", df_lab.height)\n",
    "    per_col.append(df_lab)\n",
    "\n",
    "# Concatenar y ordenar por ts\n",
    "if per_col:\n",
    "    forward_df = pl.concat(per_col, how=\"vertical_relaxed\").sort([\"ts\",\"entry_col\"])\n",
    "else:\n",
    "    forward_df = pl.DataFrame({\n",
    "        \"ts\": pl.Series([], dtype=TS_DTYPE),\n",
    "        \"entry_col\": pl.Series([], dtype=pl.Utf8),\n",
    "        \"entry_idx\": pl.Series([], dtype=pl.Int64),\n",
    "        \"label\": pl.Series([], dtype=pl.Int8),\n",
    "        \"bars_to_event\": pl.Series([], dtype=pl.Int32),\n",
    "        \"event_ts\": pl.Series([], dtype=TS_DTYPE),\n",
    "        \"entry_price\": pl.Series([], dtype=pl.Float64),\n",
    "        \"event_price\": pl.Series([], dtype=pl.Float64),\n",
    "        \"ret_event\": pl.Series([], dtype=pl.Float64),\n",
    "        \"up_price\": pl.Series([], dtype=pl.Float64),\n",
    "        \"dn_price\": pl.Series([], dtype=pl.Float64),\n",
    "    })\n",
    "\n",
    "_print_kv(\"forward_rows_total\", forward_df.height)\n",
    "_print_kv(\"forward_cols\", forward_df.width)\n",
    "\n",
    "# -------------------- H) Resumen estadÃ­stico por columna ----------------\n",
    "print(\"ğŸ“Š Resumen de etiquetas por columna ...\")\n",
    "if forward_df.height > 0:\n",
    "    group = (\n",
    "        forward_df\n",
    "        .group_by(\"entry_col\")\n",
    "        .agg(\n",
    "            pl.len().alias(\"signals\"),\n",
    "            (pl.col(\"label\") == 1).cast(pl.Int32).sum().alias(\"tp\"),\n",
    "            (pl.col(\"label\") == -1).cast(pl.Int32).sum().alias(\"sl\"),\n",
    "            (pl.col(\"label\") == 0).cast(pl.Int32).sum().alias(\"timeout\"),\n",
    "            pl.col(\"ret_event\").mean().alias(\"ret_mean\"),\n",
    "            pl.col(\"ret_event\").median().alias(\"ret_median\"),\n",
    "            pl.col(\"bars_to_event\").mean().alias(\"bars_mean\"),\n",
    "            pl.col(\"bars_to_event\").median().alias(\"bars_median\"),\n",
    "        )\n",
    "        .sort(\"entry_col\")\n",
    "    )\n",
    "    print(group)\n",
    "else:\n",
    "    print(\"   â„¹ï¸  No hay seÃ±ales etiquetadas (Â¿no hubo entries vÃ¡lidas o ATR muy temprano?).\")\n",
    "\n",
    "# -------------------- I) Persistencia -----------------------------------\n",
    "print(\"ğŸ’¾ Guardando forward ...\")\n",
    "FORWARD_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "forward_df.write_parquet(FORWARD_PARQ)\n",
    "assert FORWARD_PARQ.exists() and FORWARD_PARQ.stat().st_size > 0, \"âŒ No se pudo escribir forward parquet.\"\n",
    "_print_kv(\"forward_parquet\", FORWARD_PARQ)\n",
    "_print_kv(\"forward_size(MB)\", round(FORWARD_PARQ.stat().st_size/1024/1024, 3))\n",
    "\n",
    "# Meta (solo estadÃ­stica descriptiva del etiquetado)\n",
    "meta = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"timeframe\": TF,\n",
    "    \"horizon_bars\": HORIZON_BARS,\n",
    "    \"timeout_bars\": TIMEOUT_BARS,\n",
    "    \"up_mult\": UP_MULT,\n",
    "    \"dn_mult\": DN_MULT,\n",
    "    \"atr_window_local\": ATR_WINDOW,\n",
    "    \"entry_cols\": entry_cols,\n",
    "}\n",
    "with META_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "_print_kv(\"forward_meta\", META_PATH)\n",
    "\n",
    "print(\"âœ… Celda 4c finalizada sin errores.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7a4e5970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "ğŸ“¦ Cargando forward (pre-dedup) ...\n",
      "   - rows_before                   : 447948\n",
      "   - rows_after                    : 447948\n",
      "ğŸ’¾ Sobrescribiendo forward limpio ...\n",
      "   - FORWARD_META                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\forward_meta_BTCUSDT_15m.json\n",
      "âœ… Celda 4d finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 4d (dedup v2): [DEDUP/QUALITY GATE + META â†’ forward/forward_{SYMBOL}_{TF}.parquet + logs/forward_meta_*.json]\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# - Entrada: forward_{SYMBOL}_{TF}.parquet (producido por 04c).\n",
    "# - Objetivo: asegurar unicidad por (ts, entry_col), remover duplicados exactos y escribir meta con dedup_completed.\n",
    "# - Salida: forward_{SYMBOL}_{TF}.parquet (sobrescrito limpio) + logs/forward_meta_{SYMBOL}_{TF}.json\n",
    "# - Gates: si el archivo no existe â†’ assert; imprime conteos before/after.\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<30}: {v}\")\n",
    "def _hash_file(p: Path, algo=\"sha256\") -> str:\n",
    "    h = hashlib.new(algo)\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "PATHS         = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "FORWARD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "FORWARD_META  = PATHS[\"logs\"]    / f\"forward_meta_{SYMBOL}_{TF}.json\"\n",
    "assert FORWARD_PARQ.exists(), f\"No existe forward: {FORWARD_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando forward (pre-dedup) ...\")\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\",\"bars_to_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta columna en forward: {c}\"\n",
    "_print_kv(\"rows_before\", fwd.height)\n",
    "\n",
    "# 1) Dedup exacto por columnas completas (si hay) y por clave (ts, entry_col)\n",
    "cols_key = [c for c in [\"ts\",\"entry_col\"] if c in fwd.columns]\n",
    "fwd = fwd.unique(keep=\"first\")  # dedup total row\n",
    "if cols_key:\n",
    "    fwd = fwd.unique(subset=cols_key, keep=\"first\")\n",
    "\n",
    "_print_kv(\"rows_after\", fwd.height)\n",
    "\n",
    "# 2) Persistimos limpio\n",
    "print(\"ğŸ’¾ Sobrescribiendo forward limpio ...\")\n",
    "fwd.write_parquet(FORWARD_PARQ)\n",
    "file_hash = _hash_file(FORWARD_PARQ)\n",
    "\n",
    "# 3) META con bandera obligatoria para Celda 5\n",
    "meta = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"timeframe\": TF,\n",
    "    \"rows\": int(fwd.height),\n",
    "    \"dedup_completed\": True,\n",
    "    \"dedup_method\": \"unique((ts,entry_col), keep=first)\",\n",
    "    \"forward_sha256\": file_hash,\n",
    "    \"version\": \"4d_v2\"\n",
    "}\n",
    "FORWARD_META.parent.mkdir(parents=True, exist_ok=True)\n",
    "FORWARD_META.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"FORWARD_META\", FORWARD_META)\n",
    "\n",
    "print(\"âœ… Celda 4d finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9165744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - KPI_PARQ                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\kpi_BTCUSDT_15m.parquet\n",
      "   - KPI_YEAR_PARQ                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\kpi_by_year_BTCUSDT_15m.parquet\n",
      "   - min_trades_cfg                : 1000\n",
      "ğŸ“¦ Cargando forward (por-seÃ±al) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - forward_rows                  : 447948\n",
      "   - forward_cols                  : 11\n",
      "ğŸ§® Agregando KPIs por 'entry_col' ...\n",
      "ğŸ—“ï¸  Estabilidad muestral por aÃ±o (ret_mean y n) ...\n",
      "ğŸ’¾ Guardando reportes ...\n",
      "ğŸ Rankings (filtro min_trades) ...\n",
      "   â–¶ Top 5 por ret_mean:\n",
      "shape: (5, 28)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col   â”† n_signals â”† tp    â”† sl    â”† â€¦ â”† ret_mean_ci â”† ret_mean_c â”† hit_rate_e â”† hit_rate_e â”‚\n",
      "â”‚ ---         â”† ---       â”† ---   â”† ---   â”†   â”† 95_lo       â”† i95_hi     â”† xcl_to_ci9 â”† xcl_to_ci9 â”‚\n",
      "â”‚ str         â”† u32       â”† i32   â”† i32   â”†   â”† ---         â”† ---        â”† 5_lo       â”† 5_hi       â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”† f64         â”† f64        â”† ---        â”† ---        â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”†             â”†            â”† f64        â”† f64        â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er10_ â”† 77631     â”† 25041 â”† 51613 â”† â€¦ â”† -0.000135   â”† -0.000031  â”† 0.323356   â”† 0.329996   â”‚\n",
      "â”‚ ge_0p3      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er20_ â”† 23685     â”† 7588  â”† 15618 â”† â€¦ â”† -0.000191   â”† 0.000023   â”† 0.320949   â”† 0.33302    â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 105683    â”† 34083 â”† 70289 â”† â€¦ â”† -0.000138   â”† -0.000051  â”† 0.323708   â”† 0.329398   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er20_ â”† 77823     â”† 25104 â”† 51613 â”† â€¦ â”† -0.000149   â”† -0.000042  â”† 0.323908   â”† 0.330549   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 53709     â”† 17221 â”† 35780 â”† â€¦ â”† -0.000159   â”† -0.000032  â”† 0.320931   â”† 0.328906   â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "   â–¶ Top 5 por profit_factor_stat:\n",
      "shape: (5, 28)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col   â”† n_signals â”† tp    â”† sl    â”† â€¦ â”† ret_mean_ci â”† ret_mean_c â”† hit_rate_e â”† hit_rate_e â”‚\n",
      "â”‚ ---         â”† ---       â”† ---   â”† ---   â”†   â”† 95_lo       â”† i95_hi     â”† xcl_to_ci9 â”† xcl_to_ci9 â”‚\n",
      "â”‚ str         â”† u32       â”† i32   â”† i32   â”†   â”† ---         â”† ---        â”† 5_lo       â”† 5_hi       â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”† f64         â”† f64        â”† ---        â”† ---        â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”†             â”†            â”† f64        â”† f64        â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er10_ â”† 77631     â”† 25041 â”† 51613 â”† â€¦ â”† -0.000135   â”† -0.000031  â”† 0.323356   â”† 0.329996   â”‚\n",
      "â”‚ ge_0p3      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 53709     â”† 17221 â”† 35780 â”† â€¦ â”† -0.000159   â”† -0.000032  â”† 0.320931   â”† 0.328906   â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 105683    â”† 34083 â”† 70289 â”† â€¦ â”† -0.000138   â”† -0.000051  â”† 0.323708   â”† 0.329398   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er20_ â”† 23685     â”† 7588  â”† 15618 â”† â€¦ â”† -0.000191   â”† 0.000023   â”† 0.320949   â”† 0.33302    â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er20_ â”† 77823     â”† 25104 â”† 51613 â”† â€¦ â”† -0.000149   â”† -0.000042  â”† 0.323908   â”† 0.330549   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "   â–¶ Top 5 por hit_rate_tp_excl_to:\n",
      "shape: (5, 28)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col   â”† n_signals â”† tp    â”† sl    â”† â€¦ â”† ret_mean_ci â”† ret_mean_c â”† hit_rate_e â”† hit_rate_e â”‚\n",
      "â”‚ ---         â”† ---       â”† ---   â”† ---   â”†   â”† 95_lo       â”† i95_hi     â”† xcl_to_ci9 â”† xcl_to_ci9 â”‚\n",
      "â”‚ str         â”† u32       â”† i32   â”† i32   â”†   â”† ---         â”† ---        â”† 5_lo       â”† 5_hi       â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”† f64         â”† f64        â”† ---        â”† ---        â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”†             â”†            â”† f64        â”† f64        â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er20_ â”† 77823     â”† 25104 â”† 51613 â”† â€¦ â”† -0.000149   â”† -0.000042  â”† 0.323908   â”† 0.330549   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er50_ â”† 4821      â”† 1540  â”† 3167  â”† â€¦ â”† -0.000367   â”† 0.00015    â”† 0.313769   â”† 0.340576   â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er20_ â”† 23685     â”† 7588  â”† 15618 â”† â€¦ â”† -0.000191   â”† 0.000023   â”† 0.320949   â”† 0.33302    â”‚\n",
      "â”‚ ge_0p4      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 77631     â”† 25041 â”† 51613 â”† â€¦ â”† -0.000135   â”† -0.000031  â”† 0.323356   â”† 0.329996   â”‚\n",
      "â”‚ ge_0p3      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 105683    â”† 34083 â”† 70289 â”† â€¦ â”† -0.000138   â”† -0.000051  â”† 0.323708   â”† 0.329398   â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "âœ… Celda 5 finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 5 (fix v3 c/ gate dedup): [KPIs ESTADÃSTICOS POR COMBO â†’ reports/kpi_*.parquet/json]\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# - Igual a tu 5 actual, pero aÃ±adimos el gate: exige logs/forward_meta_* con \"dedup_completed\": true.\n",
    "# - Sin cambios en expresiones; mismos KPIs; compat legacy (**0.5, sin pl.sqrt).\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): \n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "def _clamp01(expr: pl.Expr) -> pl.Expr:\n",
    "    return pl.when(expr < 0.0).then(0.0).otherwise(\n",
    "           pl.when(expr > 1.0).then(1.0).otherwise(expr))\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS        = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "FORWARD_PARQ = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "FORWARD_META = PATHS[\"logs\"]    / f\"forward_meta_{SYMBOL}_{TF}.json\"\n",
    "REPORTS_DIR  = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Gate dedup\n",
    "assert FORWARD_META.exists(), f\"âŒ Falta META de dedup: {FORWARD_META}. Ejecuta Celda 4d.\"\n",
    "meta = json.loads(FORWARD_META.read_text(encoding=\"utf-8\"))\n",
    "assert meta.get(\"dedup_completed\", False) is True, \"âŒ forward no estÃ¡ deduplicado (4d).\"\n",
    "\n",
    "KPI_PARQ      = REPORTS_DIR / f\"kpi_{SYMBOL}_{TF}.parquet\"\n",
    "KPI_JSON      = REPORTS_DIR / f\"kpi_{SYMBOL}_{TF}.json\"\n",
    "KPI_YEAR_PARQ = REPORTS_DIR / f\"kpi_by_year_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "min_trades_cfg = int((CFG.get(\"reporting\", {}) or {}).get(\"min_trades\", 1000))\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"KPI_PARQ\", KPI_PARQ)\n",
    "_print_kv(\"KPI_YEAR_PARQ\", KPI_YEAR_PARQ)\n",
    "_print_kv(\"min_trades_cfg\", min_trades_cfg)\n",
    "assert FORWARD_PARQ.exists(), f\"No existe forward: {FORWARD_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando forward (por-seÃ±al) ...\")\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "_print_kv(\"forward_rows\", fwd.height); _print_kv(\"forward_cols\", fwd.width)\n",
    "for col in [\"entry_col\",\"label\",\"ret_event\",\"bars_to_event\",\"ts\"]:\n",
    "    assert col in fwd.columns, f\"âŒ Falta columna en forward: {col}\"\n",
    "\n",
    "print(\"ğŸ§® Agregando KPIs por 'entry_col' ...\")\n",
    "tp_expr  = (pl.col(\"label\") == 1).cast(pl.Int32)\n",
    "sl_expr  = (pl.col(\"label\") == -1).cast(pl.Int32)\n",
    "to_expr  = (pl.col(\"label\") == 0).cast(pl.Int32)\n",
    "\n",
    "sum_tp_ret = pl.when(pl.col(\"label\")==1).then(pl.col(\"ret_event\")).otherwise(0.0).sum()\n",
    "sum_sl_abs = pl.when(pl.col(\"label\")==-1).then(-pl.col(\"ret_event\")).otherwise(0.0).sum()\n",
    "\n",
    "quantiles = [\n",
    "    pl.col(\"ret_event\").quantile(0.05).alias(\"ret_q05\"),\n",
    "    pl.col(\"ret_event\").quantile(0.25).alias(\"ret_q25\"),\n",
    "    pl.col(\"ret_event\").quantile(0.50).alias(\"ret_q50\"),\n",
    "    pl.col(\"ret_event\").quantile(0.75).alias(\"ret_q75\"),\n",
    "    pl.col(\"ret_event\").quantile(0.95).alias(\"ret_q95\"),\n",
    "]\n",
    "\n",
    "kpi_base = (\n",
    "    fwd.group_by(\"entry_col\").agg(\n",
    "        pl.len().alias(\"n_signals\"),\n",
    "        tp_expr.sum().alias(\"tp\"),\n",
    "        sl_expr.sum().alias(\"sl\"),\n",
    "        to_expr.sum().alias(\"timeout\"),\n",
    "        pl.col(\"ret_event\").mean().alias(\"ret_mean\"),\n",
    "        pl.col(\"ret_event\").median().alias(\"ret_median\"),\n",
    "        pl.col(\"ret_event\").std().alias(\"ret_std\"),\n",
    "        *quantiles,\n",
    "        pl.col(\"bars_to_event\").mean().alias(\"bars_mean\"),\n",
    "        pl.col(\"bars_to_event\").median().alias(\"bars_median\"),\n",
    "        pl.when(pl.col(\"label\")==1).then(pl.col(\"ret_event\")).otherwise(None).mean().alias(\"ret_mean_tp\"),\n",
    "        pl.when(pl.col(\"label\")==-1).then(pl.col(\"ret_event\")).otherwise(None).mean().alias(\"ret_mean_sl\"),\n",
    "        pl.when(pl.col(\"label\")==0).then(pl.col(\"ret_event\")).otherwise(None).mean().alias(\"ret_mean_timeout\"),\n",
    "        sum_tp_ret.alias(\"sum_ret_tp\"),\n",
    "        sum_sl_abs.alias(\"sum_abs_ret_sl\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "den_tp_sl = _safe_pos((pl.col(\"tp\") + pl.col(\"sl\")).cast(pl.Float64()), 1.0)\n",
    "den_pf    = _safe_pos(pl.col(\"sum_abs_ret_sl\"), 1e-12)\n",
    "\n",
    "kpi = (\n",
    "    kpi_base\n",
    "    .with_columns(\n",
    "        (pl.col(\"tp\") / pl.col(\"n_signals\")).alias(\"hit_rate_tp_incl_to\"),\n",
    "        (pl.col(\"tp\").cast(pl.Float64()) / den_tp_sl).alias(\"hit_rate_tp_excl_to\"),\n",
    "        (pl.col(\"sum_ret_tp\") / den_pf).alias(\"profit_factor_stat\"),\n",
    "        (pl.col(\"ret_std\") / (pl.col(\"n_signals\").cast(pl.Float64()) ** 0.5)).alias(\"ret_se\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"ret_mean\") - 1.96 * pl.col(\"ret_se\")).alias(\"ret_mean_ci95_lo\"),\n",
    "        (pl.col(\"ret_mean\") + 1.96 * pl.col(\"ret_se\")).alias(\"ret_mean_ci95_hi\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "p   = (pl.col(\"tp\").cast(pl.Float64()) / den_tp_sl)\n",
    "var = p * (1.0 - p) / den_tp_sl\n",
    "ci_lo = _clamp01(p - 1.96 * (var ** 0.5))\n",
    "ci_hi = _clamp01(p + 1.96 * (var ** 0.5))\n",
    "\n",
    "kpi = kpi.with_columns(\n",
    "    ci_lo.alias(\"hit_rate_excl_to_ci95_lo\"),\n",
    "    ci_hi.alias(\"hit_rate_excl_to_ci95_hi\"),\n",
    ").sort(\"entry_col\")\n",
    "\n",
    "print(\"ğŸ—“ï¸  Estabilidad muestral por aÃ±o (ret_mean y n) ...\")\n",
    "kpi_year = (\n",
    "    fwd.with_columns(pl.col(\"ts\").dt.year().alias(\"year\"))\n",
    "       .group_by([\"entry_col\",\"year\"])\n",
    "       .agg(\n",
    "            pl.len().alias(\"n_signals\"),\n",
    "            pl.col(\"ret_event\").mean().alias(\"ret_mean\"),\n",
    "            pl.col(\"ret_event\").median().alias(\"ret_median\"),\n",
    "       )\n",
    "       .sort([\"entry_col\",\"year\"])\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¾ Guardando reportes ...\")\n",
    "kpi.write_parquet(KPI_PARQ)\n",
    "kpi_year.write_parquet(KPI_YEAR_PARQ)\n",
    "with KPI_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "        \"min_trades_cfg\": int(min_trades_cfg),\n",
    "        \"rows\": kpi.to_dicts()\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ Rankings (filtro min_trades) ...\")\n",
    "kpi_f = kpi.filter(pl.col(\"n_signals\") >= min_trades_cfg)\n",
    "if kpi_f.height == 0:\n",
    "    print(f\"   â„¹ï¸ NingÃºn combo alcanza min_trades={min_trades_cfg}.\")\n",
    "else:\n",
    "    print(\"   â–¶ Top 5 por ret_mean:\"); print(kpi_f.sort(\"ret_mean\", descending=True).head(5))\n",
    "    print(\"   â–¶ Top 5 por profit_factor_stat:\"); print(kpi_f.sort(\"profit_factor_stat\", descending=True).head(5))\n",
    "    print(\"   â–¶ Top 5 por hit_rate_tp_excl_to:\"); print(kpi_f.sort(\"hit_rate_tp_excl_to\", descending=True).head(5))\n",
    "\n",
    "print(\"âœ… Celda 5 finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7957c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 5b QA FORMAL v1.4 FIX) ...\n",
      "   - FWD_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando forward ...\n",
      "   - forward_shape                     : 447948x11\n",
      "   - QA_JSON                           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\qa_results_BTCUSDT_15m.json\n",
      "âœ… Celda 5b (QA FORMAL v1.4 FIX) finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 5b (QA FORMAL v1.4 FIX FINAL): [PRUEBAS UNITARIAS Y SANIDAD â†’ reports/qa_results_* + run_log_records.csv]\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: NUEVA. ColÃ³cala *despuÃ©s* de la Celda 5 y *antes* de la Celda 6.\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/config_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/qa_results_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/run_log_records.csv   (append)\n",
    "# Objetivo:\n",
    "#   â€¢ Validaciones formales del artefacto 'forward' y labeling (contrato mÃ­nimo + checks de sanidad).\n",
    "#   â€¢ FIX v1.4: evita DuplicateError usando unique(subset=...) para contar pares Ãºnicos (entry_col, ts).\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, csv, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _status(ok: bool | None) -> str:\n",
    "    if ok is True: return \"ok\"\n",
    "    if ok is False: return \"fail\"\n",
    "    return \"skip\"\n",
    "\n",
    "def _md5_file(path: Path) -> str | None:\n",
    "    try:\n",
    "        h = hashlib.md5()\n",
    "        with path.open(\"rb\") as fh:\n",
    "            for chunk in iter(lambda: fh.read(8192), b\"\"):\n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _ensure_ts_datetime(df: pl.DataFrame) -> tuple[pl.DataFrame, bool, str]:\n",
    "    \"\"\"\n",
    "    Valida/coacciona 'ts' a Datetime sin depender de pl.datatypes.is_datetime.\n",
    "    1) Probar operaciÃ³n dt.year; 2) strptime; 3) cast.\n",
    "    \"\"\"\n",
    "    dtype_str = str(df.schema.get(\"ts\"))\n",
    "    try:\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "        return df, True, dtype_str\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "        return df, True, \"coerced:strptime\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "        return df, True, \"coerced:cast\"\n",
    "    except Exception:\n",
    "        return df, False, dtype_str\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 5b QA FORMAL v1.4 FIX) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FWD_PARQ   = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "QA_JSON    = PATHS[\"reports\"] / f\"qa_results_{SYMBOL}_{TF}.json\"\n",
    "RUNLOG_CSV = PATHS[\"reports\"] / \"run_log_records.csv\"\n",
    "\n",
    "_print_kv(\"FWD_PARQ\", FWD_PARQ)\n",
    "assert FWD_PARQ.exists(), f\"âŒ Falta {FWD_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando forward ...\")\n",
    "fwd = pl.read_parquet(FWD_PARQ)\n",
    "_print_kv(\"forward_shape\", f\"{fwd.height}x{fwd.width}\")\n",
    "\n",
    "# -------------------------------- Checks --------------------------------\n",
    "checks = []\n",
    "def add_check(name: str, ok: bool | None, detail: str = \"\", extras: dict | None = None):\n",
    "    row = {\"name\": name, \"status\": _status(ok), \"detail\": detail}\n",
    "    if extras: row.update(extras)\n",
    "    checks.append(row)\n",
    "\n",
    "# 1) Esquema mÃ­nimo\n",
    "required_cols = [\"ts\",\"entry_col\",\"label\",\"ret_event\"]\n",
    "missing = [c for c in required_cols if c not in fwd.columns]\n",
    "add_check(\"schema_min_required\", len(missing)==0, f\"missing={missing}\")\n",
    "\n",
    "# 2) Tipos/NA bÃ¡sicos + coerciÃ³n segura de ts a Datetime\n",
    "if set(required_cols).issubset(set(fwd.columns)):\n",
    "    fwd, ts_ok, ts_dtype = _ensure_ts_datetime(fwd)\n",
    "    add_check(\"dtype_ts_is_datetime\", ts_ok, f\"dtype_detected={ts_dtype}\")\n",
    "    # Dominio de label\n",
    "    try:\n",
    "        label_set = set(fwd.get_column(\"label\").unique().to_list())\n",
    "        labels_ok = label_set.issubset({-1,0,1})\n",
    "    except Exception:\n",
    "        label_set, labels_ok = set(), False\n",
    "    add_check(\"label_domain_-1_0_1\", labels_ok, f\"seen={sorted(list(label_set)) if label_set else 'NA'}\")\n",
    "    # NAs en columnas clave\n",
    "    na_counts = {}\n",
    "    na_ok = True\n",
    "    for c in required_cols:\n",
    "        n_na = int(fwd.select(pl.col(c).is_null().sum().alias(\"__sum__\")).item())\n",
    "        na_counts[c] = n_na\n",
    "        if n_na > 0: na_ok = False\n",
    "    add_check(\"no_nulls_in_required\", na_ok, f\"nulls={na_counts}\")\n",
    "\n",
    "# 3) Coherencia de conteos\n",
    "if set(required_cols).issubset(set(fwd.columns)):\n",
    "    n  = fwd.height\n",
    "    tp = int(fwd.select(((pl.col(\"label\")==1).cast(pl.Int32)).sum().alias(\"__tp__\")).item())\n",
    "    sl = int(fwd.select(((pl.col(\"label\")==-1).cast(pl.Int32)).sum().alias(\"__sl__\")).item())\n",
    "    to = int(fwd.select(((pl.col(\"label\")==0).cast(pl.Int32)).sum().alias(\"__to__\")).item())\n",
    "    add_check(\"counts_match_total\", (tp + sl + to) == n, f\"n={n}, tp={tp}, sl={sl}, timeout={to}\")\n",
    "\n",
    "# 4) Duplicados y monotonicidad de ts por entry_col (FIX DuplicateError)\n",
    "if {\"entry_col\",\"ts\"}.issubset(fwd.columns):\n",
    "    n_total  = fwd.height\n",
    "    # Conteo de pares Ãºnicos (entry_col, ts) sin .alias() duplicados:\n",
    "    n_unique = fwd.unique(subset=[\"entry_col\",\"ts\"]).height\n",
    "    dups = n_total - n_unique\n",
    "    add_check(\"no_duplicate_entry_ts\", dups==0, f\"duplicates={dups}\")\n",
    "\n",
    "    # Monotonicidad no decreciente por entry_col (vectorizado con shift)\n",
    "    sorted_df = fwd.select(\"entry_col\",\"ts\").sort([\"entry_col\",\"ts\"]).with_columns(\n",
    "        prev_entry = pl.col(\"entry_col\").shift(1),\n",
    "        prev_ts    = pl.col(\"ts\").shift(1),\n",
    "    )\n",
    "    violations = int(sorted_df.select(\n",
    "        ((pl.col(\"entry_col\")==pl.col(\"prev_entry\")) & (pl.col(\"ts\") < pl.col(\"prev_ts\"))).sum().alias(\"__viol__\")\n",
    "    ).item())\n",
    "    add_check(\"ts_monotonic_non_decreasing_by_entry\", violations==0, f\"violations={violations}\")\n",
    "\n",
    "# 5) â€œDoble cruceâ€ (si hay flags/tiempos). Regla: doble cruce â†’ -1.\n",
    "tp_flag_cols = [c for c in fwd.columns if c.lower() in {\"tp_hit\",\"hit_tp\",\"tp_reached\"}]\n",
    "sl_flag_cols = [c for c in fwd.columns if c.lower() in {\"sl_hit\",\"hit_sl\",\"sl_reached\"}]\n",
    "tp_time_cols = [c for c in fwd.columns if c.lower() in {\"tp_ts\",\"tp_time\",\"tp_idx\"}]\n",
    "sl_time_cols = [c for c in fwd.columns if c.lower() in {\"sl_ts\",\"sl_time\",\"sl_idx\"}]\n",
    "\n",
    "if tp_flag_cols and sl_flag_cols:\n",
    "    tp_flag = pl.col(tp_flag_cols[0]).cast(pl.Boolean).fill_null(False)\n",
    "    sl_flag = pl.col(sl_flag_cols[0]).cast(pl.Boolean).fill_null(False)\n",
    "    both = fwd.filter(tp_flag & sl_flag)\n",
    "    if both.height == 0:\n",
    "        add_check(\"double_cross_flags_detected\", True, \"0 rows con ambos flags\")\n",
    "    else:\n",
    "        bad = both.filter(pl.col(\"label\") != -1).height\n",
    "        info = \"(con tiempos)\" if (tp_time_cols and sl_time_cols) else \"(sin tiempos)\"\n",
    "        add_check(\"double_cross_labeled_minus_one\", bad==0,\n",
    "                  f\"{info} double_cross_rows={both.height}, wrong_label_rows={bad}\")\n",
    "else:\n",
    "    add_check(\"double_cross_check\", None, \"skip: no hay flags tp/sl\")\n",
    "\n",
    "# 6) Sanidad de ret_event y (si existen) mfe/mae\n",
    "if \"ret_event\" in fwd.columns:\n",
    "    finite = int(fwd.select(pl.col(\"ret_event\").is_finite().cast(pl.Int32).sum().alias(\"__sum__\")).item())\n",
    "    add_check(\"ret_event_all_finite\", finite == fwd.height, f\"finite={finite}/{fwd.height}\")\n",
    "    ret = fwd.get_column(\"ret_event\").fill_null(0.0).to_numpy()\n",
    "    if ret.size > 0:\n",
    "        p999 = float(np.nanpercentile(np.abs(ret), 99.9))\n",
    "        std  = float(np.nanstd(ret))\n",
    "        thr  = max(p999, 10.0*std) if np.isfinite(std) and std>0 else p999\n",
    "        bad  = int(np.sum(np.abs(ret) > (thr if np.isfinite(thr) and thr>0 else 1e9)))\n",
    "        add_check(\"ret_event_no_extreme_outliers\", bad==0, f\"outliers>{thr:.6g} = {bad}\")\n",
    "    else:\n",
    "        add_check(\"ret_event_no_extreme_outliers\", None, \"skip: sin datos\")\n",
    "else:\n",
    "    add_check(\"ret_event_present\", False, \"ret_event no existe en forward\")\n",
    "\n",
    "if {\"mfe\",\"mae\"}.issubset(fwd.columns):\n",
    "    mfef = int(fwd.select(pl.col(\"mfe\").is_finite().cast(pl.Int32).sum().alias(\"__sum__\")).item())\n",
    "    maef = int(fwd.select(pl.col(\"mae\").is_finite().cast(pl.Int32).sum().alias(\"__sum__\")).item())\n",
    "    add_check(\"mfe_mae_all_finite\", (mfef==fwd.height and maef==fwd.height),\n",
    "              f\"mfe_finite={mfef}/{fwd.height}, mae_finite={maef}/{fwd.height}\")\n",
    "    if {\"label\",\"ret_event\"}.issubset(fwd.columns):\n",
    "        dfc = fwd.select(\n",
    "            pl.when(pl.col(\"label\")==1).then(pl.col(\"mfe\") >= pl.col(\"ret_event\").abs()).otherwise(True).alias(\"__mfe_ok__\"),\n",
    "            pl.when(pl.col(\"label\")==-1).then(pl.col(\"mae\") >= pl.col(\"ret_event\").abs()).otherwise(True).alias(\"__mae_ok__\"),\n",
    "        )\n",
    "        mfe_ok = bool(dfc.select(pl.col(\"__mfe_ok__\").all()).item())\n",
    "        mae_ok = bool(dfc.select(pl.col(\"__mae_ok__\").all()).item())\n",
    "        add_check(\"mfe_coherence_when_tp\", mfe_ok, \"mfe >= |ret_event| si label=1\")\n",
    "        add_check(\"mae_coherence_when_sl\", mae_ok, \"mae >= |ret_event| si label=-1\")\n",
    "else:\n",
    "    add_check(\"mfe_mae_checks\", None, \"skip: no hay columnas mfe/mae\")\n",
    "\n",
    "# 7) MD5 bit-a-bit del parquet de forward\n",
    "md5 = _md5_file(FWD_PARQ)\n",
    "add_check(\"forward_md5_computed\", md5 is not None, f\"md5={md5}\")\n",
    "\n",
    "# ---------------------------- Guardado de resultados ----------------------------\n",
    "created_utc = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "summary = {\n",
    "    \"created_utc\": created_utc,\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"timeframe\": TF,\n",
    "    \"forward_shape\": {\"rows\": fwd.height, \"cols\": fwd.width},\n",
    "    \"forward_md5\": md5,\n",
    "    \"checks\": checks,\n",
    "    \"all_ok\": all(c[\"status\"] in (\"ok\",\"skip\") for c in checks),\n",
    "}\n",
    "\n",
    "QA_JSON.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"QA_JSON\", QA_JSON)\n",
    "\n",
    "# Append a run_log_records.csv\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header:\n",
    "        w.writerow(header)\n",
    "    w.writerow([\n",
    "        created_utc, SYMBOL, TF, \"5b_QA_formal_v1_4_fix\",\n",
    "        fwd.height, \"\", \"\", f\"QA formal; all_ok={summary['all_ok']}\"\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Celda 5b (QA FORMAL v1.4 FIX) finalizada.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a7311778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - STATES_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\states_BTCUSDT_15m.parquet\n",
      "   - ER_WINDOW                     : 100\n",
      "   - DENS_WINDOW                   : 100\n",
      "ğŸ“¦ Cargando forward ...\n",
      "   - forward_rows                  : 447948\n",
      "   - forward_cols                  : 11\n",
      "ğŸ§­ Derivando estados (tiempo y rÃ©gimen) ...\n",
      "   - q25_vol                       : 0.0024798316741538873\n",
      "   - q50_vol                       : 0.003859300154248846\n",
      "   - q75_vol                       : 0.005715393011875403\n",
      "   - price_series_col              : entry_price\n",
      "   - er_q33                        : 0.004691534416456593\n",
      "   - er_q66                        : 0.012106045792361658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_40828\\3044282886.py:132: DeprecationWarning: the argument `min_periods` for `Expr.rolling_sum` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  den_er = pl.col(\"__px__\").diff().abs().rolling_sum(window_size=ER_WINDOW, min_periods=ER_WINDOW)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_40828\\3044282886.py:156: DeprecationWarning: the argument `min_periods` for `Expr.rolling_quantile` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  rq75 = pl.col(\"__ret1__\").rolling_quantile(0.75, window_size=DENS_WINDOW, min_periods=DENS_WINDOW)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_40828\\3044282886.py:157: DeprecationWarning: the argument `min_periods` for `Expr.rolling_quantile` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  rq25 = pl.col(\"__ret1__\").rolling_quantile(0.25, window_size=DENS_WINDOW, min_periods=DENS_WINDOW)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - dens_q33                      : 1.0275171382464166\n",
      "   - dens_q66                      : 1.137051666878742\n",
      "ğŸ“Š Agregando por estados ...\n",
      "   â€¢ Estado: year\n",
      "   â€¢ Estado: month\n",
      "   â€¢ Estado: dow\n",
      "   â€¢ Estado: hour\n",
      "   â€¢ Estado: session\n",
      "   â€¢ Estado: vol_bin\n",
      "   â€¢ Estado: trend_er\n",
      "   â€¢ Estado: density\n",
      "   - states_rows                   : 567\n",
      "   - states_cols                   : 19\n",
      "ğŸ‘€ Sample HEAD:\n",
      "shape: (8, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col   â”† n_signals â”† tp    â”† sl    â”† â€¦ â”† hit_rate_tp â”† hit_rate_t â”† state_type â”† state_valu â”‚\n",
      "â”‚ ---         â”† ---       â”† ---   â”† ---   â”†   â”† _incl_to    â”† p_excl_to  â”† ---        â”† e          â”‚\n",
      "â”‚ str         â”† u32       â”† i32   â”† i32   â”†   â”† ---         â”† ---        â”† str        â”† ---        â”‚\n",
      "â”‚             â”†           â”†       â”†       â”†   â”† f64         â”† f64        â”†            â”† str        â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er10_ â”† 36025     â”† 11617 â”† 23961 â”† â€¦ â”† 0.322471    â”† 0.326522   â”† density    â”† HIGH       â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 34798     â”† 11269 â”† 23096 â”† â€¦ â”† 0.32384     â”† 0.327921   â”† density    â”† LOW        â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 34839     â”† 11188 â”† 23220 â”† â€¦ â”† 0.321134    â”† 0.325157   â”† density    â”† MID        â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 21        â”† 9     â”† 12    â”† â€¦ â”† 0.428571    â”† 0.428571   â”† density    â”† MISSING    â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 15688     â”† 5105  â”† 10412 â”† â€¦ â”† 0.325408    â”† 0.328994   â”† dow        â”† 1          â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 15160     â”† 4828  â”† 10165 â”† â€¦ â”† 0.31847     â”† 0.322017   â”† dow        â”† 2          â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 15274     â”† 5153  â”† 9912  â”† â€¦ â”† 0.337371    â”† 0.342051   â”† dow        â”† 3          â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â”‚ entry_er10_ â”† 15121     â”† 4570  â”† 10369 â”† â€¦ â”† 0.302229    â”† 0.305911   â”† dow        â”† 4          â”‚\n",
      "â”‚ ge_0p2      â”†           â”†       â”†       â”†   â”†             â”†            â”†            â”†            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ’¾ Guardando estados ...\n",
      "   - STATES_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\states_BTCUSDT_15m.parquet\n",
      "   - STATES_META                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\states_meta_BTCUSDT_15m.json\n",
      "âœ… Celda 6 finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 6 (states_base v2): [ESTADOS/MEDIDAS BASE â†’ reports/states_{SYMBOL}_{TF}.parquet]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# - Estudio estadÃ­stico (NO backtest): cortes por tiempo y rÃ©gimen del mercado del *entry*.\n",
    "# - Lee forward_{SYMBOL}_{TF}.parquet y genera mÃ©tricas por estado y entry_col.\n",
    "# - Estados SIEMPRE: year, month, dow(0=Mon), hour(0..23), session(ASIA/EU/US), vol_bin(LOW/MID/HIGH/ULTRA).\n",
    "# - Estados OPCIONALES (sÃ³lo si hay serie de precio para ventana): \n",
    "#       trend_er (ER de Kaufman sobre precio base), density (IQR de retornos, proxy de congestiÃ³n).\n",
    "#   * Ambos se calculan con ventana configurable y se discretizan en bins LOW/MID/HIGH.\n",
    "# - Reconstruye vol_pct_est = (up_price/entry_price - 1) / up_mult (usado en 4c) para vol_bin.\n",
    "# - Compatibilidad Polars \"legacy\": sin clip_min() ni pl.sqrt(); usa denominadores seguros y **0.5.\n",
    "# - No rompe Celda 8: Ã©sta detecta automÃ¡ticamente los 'state_type' presentes en states_*.parquet.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    # asegura expr >= floor (evita divisiÃ³n por 0)\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "# -------------------- A) Config y paths --------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS         = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "FORWARD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "REPORTS_DIR   = PATHS[\"reports\"]\n",
    "STATES_PARQ   = REPORTS_DIR / f\"states_{SYMBOL}_{TF}.parquet\"\n",
    "STATES_META   = REPORTS_DIR / f\"states_meta_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# ParÃ¡metros opcionales para nuevos estados\n",
    "STATES_CFG    = (CFG.get(\"states\", {}) or {})\n",
    "ER_WINDOW     = int(STATES_CFG.get(\"er_window\", 100))     # ventana para ER de Kaufman\n",
    "DENS_WINDOW   = int(STATES_CFG.get(\"density_window\", 100))# ventana para IQR de retornos\n",
    "USE_CLOSE_PREF= bool(STATES_CFG.get(\"prefer_close\", True))\n",
    "\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"STATES_PARQ\", STATES_PARQ)\n",
    "_print_kv(\"ER_WINDOW\", ER_WINDOW)\n",
    "_print_kv(\"DENS_WINDOW\", DENS_WINDOW)\n",
    "assert FORWARD_PARQ.exists(), f\"No existe forward: {FORWARD_PARQ}\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- B) Cargar forward -------------------\n",
    "print(\"ğŸ“¦ Cargando forward ...\")\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "_print_kv(\"forward_rows\", fwd.height)\n",
    "_print_kv(\"forward_cols\", fwd.width)\n",
    "\n",
    "req_cols = [\"ts\",\"entry_col\",\"label\",\"ret_event\",\"bars_to_event\",\"entry_price\",\"up_price\"]\n",
    "missing = [c for c in req_cols if c not in fwd.columns]\n",
    "assert not missing, f\"âŒ Faltan columnas en forward: {missing}\"\n",
    "\n",
    "# -------------------- C) Derivar estados -------------------\n",
    "print(\"ğŸ§­ Derivando estados (tiempo y rÃ©gimen) ...\")\n",
    "UP_MULT = float(CFG.get(\"labeling\", {}).get(\"triple_barrier\", {}).get(\"up_mult\", 2.0))\n",
    "\n",
    "# Tiempos bÃ¡sicos\n",
    "fwd = fwd.with_columns([\n",
    "    pl.col(\"ts\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"ts\").dt.month().alias(\"month\"),\n",
    "    pl.col(\"ts\").dt.weekday().alias(\"dow\"),   # 0=Mon ... 6=Sun\n",
    "    pl.col(\"ts\").dt.hour().alias(\"hour\"),\n",
    "])\n",
    "\n",
    "# SesiÃ³n por hora UTC (bandas simples, sin DST):\n",
    "#   ASIA:   [00..07], EUROPE: [08..15], US: [16..23]\n",
    "session_expr = (\n",
    "    pl.when((pl.col(\"hour\") >= 0) & (pl.col(\"hour\") <= 7)).then(pl.lit(\"ASIA\"))\n",
    "     .when((pl.col(\"hour\") >= 8) & (pl.col(\"hour\") <= 15)).then(pl.lit(\"EUROPE\"))\n",
    "     .otherwise(pl.lit(\"US\"))\n",
    ")\n",
    "fwd = fwd.with_columns(session_expr.alias(\"session\"))\n",
    "\n",
    "# Volatilidad estimada al entry (reconstruida desde up_mult)\n",
    "vol_pct_est = ((pl.col(\"up_price\") / pl.col(\"entry_price\")) - 1.0) / pl.lit(UP_MULT)\n",
    "fwd = fwd.with_columns(vol_pct_est.cast(pl.Float64()).alias(\"vol_pct_est\"))\n",
    "\n",
    "# Cortes por cuartiles de vol_pct_est (global)\n",
    "qdf = fwd.select([\n",
    "    pl.col(\"vol_pct_est\").quantile(0.25).alias(\"q25\"),\n",
    "    pl.col(\"vol_pct_est\").quantile(0.50).alias(\"q50\"),\n",
    "    pl.col(\"vol_pct_est\").quantile(0.75).alias(\"q75\"),\n",
    "])\n",
    "q25, q50, q75 = qdf.row(0)\n",
    "vol_bin_expr = pl.when(pl.col(\"vol_pct_est\").is_null()).then(pl.lit(\"MISSING\")).otherwise(\n",
    "    pl.when(pl.col(\"vol_pct_est\") < pl.lit(q25)).then(pl.lit(\"LOW\"))\n",
    "     .when(pl.col(\"vol_pct_est\") < pl.lit(q50)).then(pl.lit(\"MID\"))\n",
    "     .when(pl.col(\"vol_pct_est\") < pl.lit(q75)).then(pl.lit(\"HIGH\"))\n",
    "     .otherwise(pl.lit(\"ULTRA\"))\n",
    ")\n",
    "fwd = fwd.with_columns(vol_bin_expr.alias(\"vol_bin\"))\n",
    "_print_kv(\"q25_vol\", q25); _print_kv(\"q50_vol\", q50); _print_kv(\"q75_vol\", q75)\n",
    "\n",
    "# --------- C2) Estados adicionales opcionales: ER y DENSITY ----------\n",
    "# Precio base para ventanas: preferimos 'close' si existe; si no, usamos 'entry_price'.\n",
    "price_col = None\n",
    "if USE_CLOSE_PREF and \"close\" in fwd.columns:\n",
    "    price_col = \"close\"\n",
    "elif \"entry_price\" in fwd.columns:\n",
    "    price_col = \"entry_price\"\n",
    "\n",
    "added_optional_states = []\n",
    "if price_col is not None and fwd.height >= max(ER_WINDOW, DENS_WINDOW) + 2:\n",
    "    _print_kv(\"price_series_col\", price_col)\n",
    "\n",
    "    # Serie de log-precio y retornos 1-bar\n",
    "    fwd = fwd.with_columns([\n",
    "        pl.col(price_col).cast(pl.Float64()).alias(\"__px__\"),\n",
    "    ])\n",
    "    fwd = fwd.with_columns([\n",
    "        pl.col(\"__px__\").log().diff().alias(\"__ret1__\"),\n",
    "    ])\n",
    "\n",
    "    # --- ER de Kaufman (rolling) ---\n",
    "    # ER_t = |px_t - px_{t-N}| / sum_{i=t-N+1..t} |px_i - px_{i-1}|\n",
    "    num_er = pl.col(\"__px__\").diff(n=ER_WINDOW).abs()\n",
    "    den_er = pl.col(\"__px__\").diff().abs().rolling_sum(window_size=ER_WINDOW, min_periods=ER_WINDOW)\n",
    "    er_val = (num_er / _safe_pos(den_er, 1e-12)).alias(\"__er__\")\n",
    "    fwd = fwd.with_columns(er_val)\n",
    "\n",
    "    # Bins para ER (terciles robustos)\n",
    "    er_q = fwd.select([\n",
    "        pl.col(\"__er__\").quantile(0.33).alias(\"er_q33\"),\n",
    "        pl.col(\"__er__\").quantile(0.66).alias(\"er_q66\"),\n",
    "    ])\n",
    "    er_q33, er_q66 = er_q.row(0)\n",
    "    fwd = fwd.with_columns(\n",
    "        pl.when(pl.col(\"__er__\").is_null()).then(pl.lit(\"MISSING\")).otherwise(\n",
    "            pl.when(pl.col(\"__er__\") < pl.lit(er_q33)).then(pl.lit(\"LOW\"))\n",
    "             .when(pl.col(\"__er__\") < pl.lit(er_q66)).then(pl.lit(\"MID\"))\n",
    "             .otherwise(pl.lit(\"HIGH\"))\n",
    "        ).alias(\"trend_er\")\n",
    "    )\n",
    "    added_optional_states.append(\"trend_er\")\n",
    "    _print_kv(\"er_q33\", er_q33); _print_kv(\"er_q66\", er_q66)\n",
    "\n",
    "    # --- DENSITY (IQR de retornos en ventana, proxy de congestiÃ³n) ---\n",
    "    # IQR = Q75(ret1) - Q25(ret1) sobre ventana; discretizado LOW/MID/HIGH\n",
    "    # Nota: rolling_quantile requiere Polars >=0.20; si no estÃ¡, se omite automÃ¡ticamente.\n",
    "    try:\n",
    "        rq75 = pl.col(\"__ret1__\").rolling_quantile(0.75, window_size=DENS_WINDOW, min_periods=DENS_WINDOW)\n",
    "        rq25 = pl.col(\"__ret1__\").rolling_quantile(0.25, window_size=DENS_WINDOW, min_periods=DENS_WINDOW)\n",
    "        dens_val = (rq75 - rq25).alias(\"__dens__\")\n",
    "        fwd = fwd.with_columns(dens_val)\n",
    "\n",
    "        dens_q = fwd.select([\n",
    "            pl.col(\"__dens__\").quantile(0.33).alias(\"dens_q33\"),\n",
    "            pl.col(\"__dens__\").quantile(0.66).alias(\"dens_q66\"),\n",
    "        ])\n",
    "        dens_q33, dens_q66 = dens_q.row(0)\n",
    "        fwd = fwd.with_columns(\n",
    "            pl.when(pl.col(\"__dens__\").is_null()).then(pl.lit(\"MISSING\")).otherwise(\n",
    "                pl.when(pl.col(\"__dens__\") < pl.lit(dens_q33)).then(pl.lit(\"LOW\"))\n",
    "                 .when(pl.col(\"__dens__\") < pl.lit(dens_q66)).then(pl.lit(\"MID\"))\n",
    "                 .otherwise(pl.lit(\"HIGH\"))\n",
    "            ).alias(\"density\")\n",
    "        )\n",
    "        added_optional_states.append(\"density\")\n",
    "        _print_kv(\"dens_q33\", dens_q33); _print_kv(\"dens_q66\", dens_q66)\n",
    "    except Exception as e:\n",
    "        _print_kv(\"density_skip_warn\", f\"omitido (rolling_quantile no disponible): {e}\")\n",
    "\n",
    "else:\n",
    "    _print_kv(\"price_series_col\", \"NO DISPONIBLE â€” se omiten trend_er y density\")\n",
    "\n",
    "# -------------------- D) Agregadores reutilizables -----------\n",
    "tp_expr  = (pl.col(\"label\") == 1).cast(pl.Int32)\n",
    "sl_expr  = (pl.col(\"label\") == -1).cast(pl.Int32)\n",
    "to_expr  = (pl.col(\"label\") == 0).cast(pl.Int32)\n",
    "\n",
    "def _agg_by_state(df: pl.DataFrame, state_col: str, state_name: str) -> pl.DataFrame:\n",
    "    base = (\n",
    "        df.group_by([\"entry_col\", state_col])\n",
    "          .agg(\n",
    "              pl.len().alias(\"n_signals\"),\n",
    "              tp_expr.sum().alias(\"tp\"),\n",
    "              sl_expr.sum().alias(\"sl\"),\n",
    "              to_expr.sum().alias(\"timeout\"),\n",
    "              pl.col(\"ret_event\").mean().alias(\"ret_mean\"),\n",
    "              pl.col(\"ret_event\").median().alias(\"ret_median\"),\n",
    "              pl.col(\"ret_event\").std().alias(\"ret_std\"),\n",
    "              pl.col(\"ret_event\").quantile(0.05).alias(\"ret_q05\"),\n",
    "              pl.col(\"ret_event\").quantile(0.25).alias(\"ret_q25\"),\n",
    "              pl.col(\"ret_event\").quantile(0.50).alias(\"ret_q50\"),\n",
    "              pl.col(\"ret_event\").quantile(0.75).alias(\"ret_q75\"),\n",
    "              pl.col(\"ret_event\").quantile(0.95).alias(\"ret_q95\"),\n",
    "              pl.col(\"bars_to_event\").mean().alias(\"bars_mean\"),\n",
    "              pl.col(\"bars_to_event\").median().alias(\"bars_median\"),\n",
    "          )\n",
    "          .with_columns([\n",
    "              (pl.col(\"tp\") / pl.col(\"n_signals\")).alias(\"hit_rate_tp_incl_to\"),\n",
    "              (pl.col(\"tp\").cast(pl.Float64()) /\n",
    "               _safe_pos((pl.col(\"tp\")+pl.col(\"sl\")).cast(pl.Float64()), 1.0)\n",
    "              ).alias(\"hit_rate_tp_excl_to\"),\n",
    "              pl.lit(state_name).alias(\"state_type\"),\n",
    "              pl.col(state_col).cast(pl.Utf8).alias(\"state_value\"),\n",
    "          ])\n",
    "          .drop(state_col)\n",
    "    )\n",
    "    return base\n",
    "\n",
    "# -------------------- E) CÃ¡lculo por cada estado --------------\n",
    "print(\"ğŸ“Š Agregando por estados ...\")\n",
    "states_list = []\n",
    "\n",
    "# Estados base siempre presentes\n",
    "for sc, sn in [(\"year\",\"year\"), (\"month\",\"month\"), (\"dow\",\"dow\"), (\"hour\",\"hour\"),\n",
    "               (\"session\",\"session\"), (\"vol_bin\",\"vol_bin\")]:\n",
    "    print(f\"   â€¢ Estado: {sn}\")\n",
    "    states_list.append(_agg_by_state(fwd, sc, sn))\n",
    "\n",
    "# Estados opcionales (si fueron generados)\n",
    "if \"trend_er\" in fwd.columns:\n",
    "    print(\"   â€¢ Estado: trend_er\")\n",
    "    states_list.append(_agg_by_state(fwd, \"trend_er\", \"trend_er\"))\n",
    "if \"density\" in fwd.columns:\n",
    "    print(\"   â€¢ Estado: density\")\n",
    "    states_list.append(_agg_by_state(fwd, \"density\", \"density\"))\n",
    "\n",
    "states_df = pl.concat(states_list, how=\"vertical_relaxed\").sort([\"entry_col\",\"state_type\",\"state_value\"])\n",
    "_print_kv(\"states_rows\", states_df.height)\n",
    "_print_kv(\"states_cols\", states_df.width)\n",
    "print(\"ğŸ‘€ Sample HEAD:\")\n",
    "print(states_df.head(8))\n",
    "\n",
    "# -------------------- F) Persistencia --------------------------\n",
    "print(\"ğŸ’¾ Guardando estados ...\")\n",
    "states_df.write_parquet(STATES_PARQ)\n",
    "with STATES_META.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"timeframe\": TF,\n",
    "        \"up_mult\": UP_MULT,\n",
    "        \"vol_quantiles\": {\"q25\": float(q25), \"q50\": float(q50), \"q75\": float(q75)},\n",
    "        \"state_types\": [\"year\",\"month\",\"dow\",\"hour\",\"session\",\"vol_bin\"] + added_optional_states,\n",
    "        \"price_series_col\": price_col if price_col is not None else \"N/A\",\n",
    "        \"er_window\": ER_WINDOW,\n",
    "        \"density_window\": DENS_WINDOW,\n",
    "        \"source\": str(FORWARD_PARQ),\n",
    "        \"output\": str(STATES_PARQ),\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_print_kv(\"STATES_PARQ\", STATES_PARQ)\n",
    "_print_kv(\"STATES_META\", STATES_META)\n",
    "print(\"âœ… Celda 6 finalizada sin errores.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b6310d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "ğŸ“¦ Cargando forward ...\n",
      "   - forward.first_ts              : 2021-01-01T12:15:00+00:00\n",
      "   - forward.last_ts               : 2025-11-03T19:45:00+00:00\n",
      "   - forward_rows                  : 447948\n",
      "   - train_days                    : 365\n",
      "   - test_days                     : 30\n",
      "   - step_days                     : 30\n",
      "   - min_train_signals             : 0\n",
      "   - min_test_signals              : 0\n",
      "   - wf_start(used)                : 2021-01-01T12:15:00+00:00\n",
      "   - wf_end(used)                  : 2025-11-03T19:45:00+00:00\n",
      "ğŸ§± Construyendo ventanas TRAIN/TEST (sin solape) ...\n",
      "   - wf_windows_count              : 4\n",
      "ğŸ‘€ Muestra de ventanas:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"wf_000\",\n",
      "    \"train\": {\n",
      "      \"start\": \"2021-01-01T12:15:00+00:00\",\n",
      "      \"end\": \"2022-01-01T12:14:59+00:00\",\n",
      "      \"n\": 95928\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"start\": \"2022-01-01T12:15:00+00:00\",\n",
      "      \"end\": \"2022-01-31T12:14:59+00:00\",\n",
      "      \"n\": 7144\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"wf_001\",\n",
      "    \"train\": {\n",
      "      \"start\": \"2022-01-31T12:15:00+00:00\",\n",
      "      \"end\": \"2023-01-31T12:14:59+00:00\",\n",
      "      \"n\": 93625\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"start\": \"2023-01-31T12:15:00+00:00\",\n",
      "      \"end\": \"2023-03-02T12:14:59+00:00\",\n",
      "      \"n\": 7637\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"wf_002\",\n",
      "    \"train\": {\n",
      "      \"start\": \"2023-03-02T12:15:00+00:00\",\n",
      "      \"end\": \"2024-03-01T12:14:59+00:00\",\n",
      "      \"n\": 87889\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"start\": \"2024-03-01T12:15:00+00:00\",\n",
      "      \"end\": \"2024-03-31T12:14:59+00:00\",\n",
      "      \"n\": 7267\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"wf_003\",\n",
      "    \"train\": {\n",
      "      \"start\": \"2024-03-31T12:15:00+00:00\",\n",
      "      \"end\": \"2025-03-31T12:14:59+00:00\",\n",
      "      \"n\": 92738\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"start\": \"2025-03-31T12:15:00+00:00\",\n",
      "      \"end\": \"2025-04-30T12:14:59+00:00\",\n",
      "      \"n\": 7698\n",
      "    }\n",
      "  }\n",
      "]\n",
      "ğŸ’¾ Guardando wf_windows ...\n",
      "   - WF_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "âœ… Celda 7a finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7a (wf_windows v1): [VENTANAS WALK-FORWARD (SIN SOLAPE) â†’ logs/wf_windows_{SYMBOL}_{TF}.json]\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# - Estudio estadÃ­stico (NO backtest): solo define ventanas TRAIN/TEST sin solape.\n",
    "# - Lee config + forward_{SYMBOL}_{TF}.parquet, respeta TWF_START_DATE/TWF_END_DATE.\n",
    "# - Usa CFG[\"wf\"] si existe; si no, defaults: train_days=365, test_days=30, step_days=test_days.\n",
    "# - Persiste wf_windows_{SYMBOL}_{TF}.json con lista de ventanas y conteos muestrales (train/test).\n",
    "# - Compatibilidad \"legacy\": Polars bÃ¡sico, nada exÃ³tico.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _to_utc(dt: datetime) -> datetime:\n",
    "    # Asegura timezone UTC\n",
    "    return dt if dt.tzinfo is not None else dt.replace(tzinfo=timezone.utc)\n",
    "\n",
    "def _parse_date_env(key: str) -> datetime:\n",
    "    s = os.environ[key]\n",
    "    # Permite 'YYYY-MM-DD' o ISO completo; si es solo fecha, asume 00:00 UTC\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        # Fallback robusto (solo por si hay formato raro)\n",
    "        y, m, d = map(int, s.split(\"-\"))\n",
    "        dt = datetime(y, m, d)\n",
    "    dt = _to_utc(dt)\n",
    "    if dt.hour == 0 and dt.minute == 0 and dt.second == 0:\n",
    "        # Mantener como lÃ­mite inferior/superior segÃºn el uso\n",
    "        pass\n",
    "    return dt\n",
    "\n",
    "# -------------------- A) Config y paths --------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS        = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "FORWARD_PARQ = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "LOGS_DIR     = PATHS[\"logs\"]\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_JSON = LOGS_DIR / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"WF_JSON\", WF_JSON)\n",
    "assert FORWARD_PARQ.exists(), f\"No existe forward: {FORWARD_PARQ}\"\n",
    "\n",
    "# -------------------- B) Cargar forward y rango disponible -------------\n",
    "print(\"ğŸ“¦ Cargando forward ...\")\n",
    "fwd = pl.read_parquet(FORWARD_PARQ).select([\"ts\"])\n",
    "first_ms, last_ms = fwd.select(\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").min().alias(\"first_ms\"),\n",
    "    pl.col(\"ts\").dt.epoch(\"ms\").max().alias(\"last_ms\")\n",
    ").row(0)\n",
    "first_ts = datetime.fromtimestamp(int(first_ms)/1000, tz=timezone.utc)\n",
    "last_ts  = datetime.fromtimestamp(int(last_ms)/1000, tz=timezone.utc)\n",
    "_print_kv(\"forward.first_ts\", first_ts.isoformat(timespec=\"seconds\"))\n",
    "_print_kv(\"forward.last_ts\",  last_ts.isoformat(timespec=\"seconds\"))\n",
    "_print_kv(\"forward_rows\", fwd.height)\n",
    "\n",
    "# -------------------- C) WF config (CFG['wf'] o defaults) --------------\n",
    "WF_CFG = CFG.get(\"wf\", {})\n",
    "train_days = int(WF_CFG.get(\"train_days\", 365))\n",
    "test_days  = int(WF_CFG.get(\"test_days\", 30))\n",
    "step_days  = int(WF_CFG.get(\"step_days\", test_days))  # sin solape por defecto\n",
    "min_train_signals = int(WF_CFG.get(\"min_train_signals\", 0))  # solo informativo en 7a\n",
    "min_test_signals  = int(WF_CFG.get(\"min_test_signals\", 0))   # (el filtro real se aplica en 7b si lo decides)\n",
    "\n",
    "_print_kv(\"train_days\", train_days)\n",
    "_print_kv(\"test_days\",  test_days)\n",
    "_print_kv(\"step_days\",  step_days)\n",
    "_print_kv(\"min_train_signals\", min_train_signals)\n",
    "_print_kv(\"min_test_signals\",  min_test_signals)\n",
    "\n",
    "# Rango WF acotado por entorno + datos\n",
    "env_start = _parse_date_env(\"TWF_START_DATE\")\n",
    "env_end   = _parse_date_env(\"TWF_END_DATE\")\n",
    "wf_start  = max(env_start, first_ts)\n",
    "wf_end    = min(env_end,   last_ts)\n",
    "_print_kv(\"wf_start(used)\", wf_start.isoformat(timespec=\"seconds\"))\n",
    "_print_kv(\"wf_end(used)\",   wf_end.isoformat(timespec=\"seconds\"))\n",
    "\n",
    "# -------------------- D) ConstrucciÃ³n de ventanas ----------------------\n",
    "print(\"ğŸ§± Construyendo ventanas TRAIN/TEST (sin solape) ...\")\n",
    "ts_series = fwd[\"ts\"]  # para conteos rÃ¡pidos\n",
    "\n",
    "windows = []\n",
    "cursor = wf_start\n",
    "one_us = timedelta(microseconds=1)  # para inclusividad sobre lÃ­mites\n",
    "\n",
    "while True:\n",
    "    train_start = cursor\n",
    "    train_end   = train_start + timedelta(days=train_days) - one_us\n",
    "    test_start  = train_end + one_us\n",
    "    test_end    = test_start + timedelta(days=test_days) - one_us\n",
    "\n",
    "    if test_end > wf_end:\n",
    "        break\n",
    "\n",
    "    # Conteos (muestrales, no filtros estrictos aquÃ­)\n",
    "    n_train = fwd.filter((pl.col(\"ts\") >= train_start) & (pl.col(\"ts\") <= train_end)).height\n",
    "    n_test  = fwd.filter((pl.col(\"ts\") >= test_start)  & (pl.col(\"ts\") <= test_end)).height\n",
    "\n",
    "    windows.append({\n",
    "        \"id\":       f\"wf_{len(windows):03d}\",\n",
    "        \"train\":    {\"start\": train_start.isoformat(timespec=\"seconds\"),\n",
    "                     \"end\":   train_end.isoformat(timespec=\"seconds\"),\n",
    "                     \"n\": int(n_train)},\n",
    "        \"test\":     {\"start\": test_start.isoformat(timespec=\"seconds\"),\n",
    "                     \"end\":   test_end.isoformat(timespec=\"seconds\"),\n",
    "                     \"n\": int(n_test)},\n",
    "    })\n",
    "\n",
    "    cursor = test_start + timedelta(days=step_days)\n",
    "\n",
    "_print_kv(\"wf_windows_count\", len(windows))\n",
    "if windows:\n",
    "    print(\"ğŸ‘€ Muestra de ventanas:\")\n",
    "    print(json.dumps(windows[:2] + windows[-2:], indent=2))\n",
    "\n",
    "# -------------------- E) Persistencia ----------------------------------\n",
    "print(\"ğŸ’¾ Guardando wf_windows ...\")\n",
    "with WF_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"timeframe\": TF,\n",
    "        \"config_used\": {\n",
    "            \"train_days\": train_days,\n",
    "            \"test_days\": test_days,\n",
    "            \"step_days\": step_days,\n",
    "            \"min_train_signals\": min_train_signals,\n",
    "            \"min_test_signals\": min_test_signals,\n",
    "            \"wf_start\": wf_start.isoformat(timespec=\"seconds\"),\n",
    "            \"wf_end\":   wf_end.isoformat(timespec=\"seconds\"),\n",
    "        },\n",
    "        \"windows\": windows\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_print_kv(\"WF_JSON\", WF_JSON)\n",
    "print(\"âœ… Celda 7a finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "932600cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 7b v3 FINAL TZ+RANK) ...\n",
      "ğŸ“¦ Cargando forward y ventanas ...\n",
      "   - fwd_shape                         : 447948x11\n",
      "   - ts_dtype                          : Datetime(time_unit='us', time_zone='UTC')\n",
      "   - train_windows                     : 4\n",
      "ğŸ§® Scoring en TRAIN por (wf_id, entry_col) ...\n",
      "   - train_scores_rows                 : 36\n",
      "   - train_scores_cols                 : 15\n",
      "   - ranked_rows                       : 36\n",
      "   - OUT_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - OUT_JSON                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.json\n",
      "âœ… Celda 7b (v3 FINAL, TZ-UTC + RANK FIX) completada.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_40828\\287493429.py:210: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  .with_row_count(\"rank_in_window_train\", offset=1)\n"
     ]
    }
   ],
   "source": [
    "# Celda 7b (v3 FINAL, TZ-UTC + RANK FIX): [SCORING EN TRAIN + N_min + desempates â†’ wf_train_scores_*]\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: REEMPLAZA la Celda 7b actual (despuÃ©s de 7a y antes de 7c).\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/config_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/wf_windows_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_train_scores_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_train_scores_{SYMBOL}_{TF}.json\n",
    "# Objetivo:\n",
    "#   â€¢ Por (wf_id, entry_col) en TRAIN: KPIs + L_b_H_train, n_eff_hit_train y Score_train estable:\n",
    "#       - support_excl_to = tp+sl ; p_good = tp/(tp+sl)\n",
    "#       - EV (ret_mean), ret_std, profit_factor_stat_train\n",
    "#       - L_b_H_train por ACF (dos lags en banda Â±1.96/âˆšn_pairs)\n",
    "#       - n_eff_hit_train = max(1, n_pairs / L_b_H_train)\n",
    "#       - Score_train := EV_train Ã— (2Â·p_goodâˆ’1) Ã— log1p(n_eff_hit_train)\n",
    "#   â€¢ Aplicar N_min por soporte (GOOD y BAD) y ordenar con desempates normados.\n",
    "#   â€¢ FIX: ranking sin GroupBy.apply (compat con tu Polars).\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _z_acf_band(n_pairs: int) -> float:\n",
    "    n_pairs = max(1, int(n_pairs))\n",
    "    return 1.96 / math.sqrt(n_pairs)\n",
    "\n",
    "def _acf(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - np.nanmean(x)\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.zeros(max_lag+1)\n",
    "    denom = float(np.dot(x, x))\n",
    "    if denom <= 0:\n",
    "        return np.zeros(max_lag+1)\n",
    "    out = np.empty(max_lag+1, dtype=float)\n",
    "    for k in range(0, max_lag+1):\n",
    "        out[k] = float(np.dot(x[:n-k], x[k:]) / denom)\n",
    "    return out\n",
    "\n",
    "def _estimate_block_len_train(labels_vec: np.ndarray) -> int:\n",
    "    \"\"\"L_b_H_train: primer H con 2 lags consecutivos en banda.\"\"\"\n",
    "    if labels_vec.size == 0:\n",
    "        return 1\n",
    "    y = np.zeros_like(labels_vec, dtype=float)\n",
    "    y[labels_vec == 1]  = 1.0\n",
    "    y[labels_vec == -1] = -1.0\n",
    "    n_pairs = int(np.sum(labels_vec != 0))\n",
    "    band = _z_acf_band(n_pairs)\n",
    "    max_lag = max(10, min(200, int(len(y) * 0.2)))\n",
    "    r = _acf(y, max_lag)\n",
    "    for H in range(1, max_lag-1):\n",
    "        if abs(r[H]) <= band and abs(r[H+1]) <= band:\n",
    "            return max(1, H)\n",
    "    return max(1, int(np.ceil(np.sqrt(max(n_pairs, 1)))))  # fallback conservador\n",
    "\n",
    "def _score_train_fn(ev: float | None, p_good: float | None, n_eff: int | float | None) -> float | None:\n",
    "    if ev is None or p_good is None or n_eff is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(ev) * (2.0*float(p_good) - 1.0) * math.log1p(max(1.0, float(n_eff)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _ensure_ts_datetime_utc(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Asegura ts como Datetime[UTC].\"\"\"\n",
    "    # Garantizar Datetime\n",
    "    try:\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "        except Exception:\n",
    "            df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "    # Forzar UTC\n",
    "    try:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\"))\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").dt.convert_time_zone(\"UTC\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 7b v3 FINAL TZ+RANK) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "# N_min (ajustables por env)\n",
    "NMIN_GOOD = int(os.environ.get(\"TWF_NMIN_GOOD\", \"50\"))\n",
    "NMIN_BAD  = int(os.environ.get(\"TWF_NMIN_BAD\",  \"50\"))\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "WF_JSON     = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "FWD_PARQ    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"forward\" / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "OUT_PARQ    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "OUT_JSON    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / f\"wf_train_scores_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "assert WF_JSON.exists(),     f\"âŒ No existe ventanas WF: {WF_JSON}\"\n",
    "assert FWD_PARQ.exists(),    f\"âŒ Falta forward: {FWD_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando forward y ventanas ...\")\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fwd = pl.read_parquet(FWD_PARQ)\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta columna en forward: {c}\"\n",
    "fwd = _ensure_ts_datetime_utc(fwd)\n",
    "_print_kv(\"fwd_shape\", f\"{fwd.height}x{fwd.width}\")\n",
    "_print_kv(\"ts_dtype\", str(fwd.schema.get(\"ts\")))\n",
    "\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "assert len(windows) > 0, \"âŒ wf_windows vacÃ­o.\"\n",
    "wf_map = {}\n",
    "for idx, w in enumerate(windows):\n",
    "    wf_key = str(w.get(\"wf_id\", f\"wf_{idx:03d}\"))  # STRING\n",
    "    t0 = datetime.fromisoformat(w[\"train\"][\"start\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    t1 = datetime.fromisoformat(w[\"train\"][\"end\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    wf_map[wf_key] = (t0, t1)\n",
    "_print_kv(\"train_windows\", len(wf_map))\n",
    "\n",
    "# ----------------------------- CÃ¡lculo por ventana/entry_col -----------------------------\n",
    "print(\"ğŸ§® Scoring en TRAIN por (wf_id, entry_col) ...\")\n",
    "rows = []\n",
    "for wf_id, (t0, t1) in wf_map.items():\n",
    "    sub_wf = fwd.filter((pl.col(\"ts\") >= pl.lit(t0)) & (pl.col(\"ts\") <= pl.lit(t1)))\n",
    "    if sub_wf.is_empty():\n",
    "        continue\n",
    "    for entry in sub_wf.get_column(\"entry_col\").unique().to_list():\n",
    "        g = sub_wf.filter(pl.col(\"entry_col\") == entry).select([\"ts\",\"label\",\"ret_event\"]).sort(\"ts\")\n",
    "\n",
    "        n  = g.height\n",
    "        tp = int(g.select(((pl.col(\"label\")==1).cast(pl.Int32)).sum().alias(\"__tp__\")).item()) if n>0 else 0\n",
    "        sl = int(g.select(((pl.col(\"label\")==-1).cast(pl.Int32)).sum().alias(\"__sl__\")).item()) if n>0 else 0\n",
    "        to = int(g.select(((pl.col(\"label\")==0).cast(pl.Int32)).sum().alias(\"__to__\")).item()) if n>0 else 0\n",
    "\n",
    "        n_pairs = tp + sl\n",
    "        p_good  = (tp / n_pairs) if n_pairs > 0 else None\n",
    "        ev      = float(g.select(pl.col(\"ret_event\").mean()).item()) if n>0 else None\n",
    "        ev_std  = float(g.select(pl.col(\"ret_event\").std()).item()) if n>0 else None\n",
    "\n",
    "        if n>0:\n",
    "            pos_sum = float(g.filter(pl.col(\"ret_event\")>0).select(pl.col(\"ret_event\").sum()).item() or 0.0)\n",
    "            neg_sum = float(g.filter(pl.col(\"ret_event\")<0).select(pl.col(\"ret_event\").sum()).item() or 0.0)\n",
    "            pf_train = (pos_sum / abs(neg_sum)) if abs(neg_sum) > 1e-12 else None\n",
    "        else:\n",
    "            pf_train = None\n",
    "\n",
    "        labels_np = g.get_column(\"label\").to_numpy() if n>0 else np.array([], dtype=int)\n",
    "        L_b_H = _estimate_block_len_train(labels_np)\n",
    "        n_eff = max(1, int(round(n_pairs / max(1, L_b_H))))\n",
    "\n",
    "        score = _score_train_fn(ev, p_good, n_eff)\n",
    "\n",
    "        rows.append({\n",
    "            \"wf_id\": wf_id,                 # STRING\n",
    "            \"entry_col\": entry,\n",
    "            \"n_signals\": n,\n",
    "            \"tp\": tp, \"sl\": sl, \"timeout\": to,\n",
    "            \"support_excl_to\": n_pairs,\n",
    "            \"hit_rate_tp_excl_to\": (tp / n_pairs) if n_pairs>0 else None,\n",
    "            \"ret_mean\": ev,\n",
    "            \"ret_std\": ev_std,\n",
    "            \"profit_factor_stat\": pf_train,\n",
    "            \"L_b_H_train\": L_b_H,\n",
    "            \"n_eff_hit_train\": n_eff,\n",
    "            \"score_train\": score,\n",
    "            \"meets_Nmin\": (tp >= NMIN_GOOD and sl >= NMIN_BAD)\n",
    "        })\n",
    "\n",
    "df = pl.DataFrame(rows) if rows else pl.DataFrame({\"wf_id\": pl.Series([], dtype=pl.Utf8)})\n",
    "_print_kv(\"train_scores_rows\", df.height)\n",
    "_print_kv(\"train_scores_cols\", df.width)\n",
    "\n",
    "# ----------------------------- Ranking con desempates (sin GroupBy.apply) ----------------\n",
    "ranked_parts = []\n",
    "if df.height > 0:\n",
    "    wf_ids = df.get_column(\"wf_id\").unique().to_list()\n",
    "    for wfid in wf_ids:\n",
    "        part = (\n",
    "            df.filter(pl.col(\"wf_id\")==wfid)\n",
    "              .with_columns(\n",
    "                  p_good = pl.when(pl.col(\"support_excl_to\")>0)\n",
    "                              .then(pl.col(\"tp\")/pl.col(\"support_excl_to\"))\n",
    "                              .otherwise(None)\n",
    "              )\n",
    "              .sort(\n",
    "                  by=[\n",
    "                      pl.col(\"score_train\").fill_null(-1e308),\n",
    "                      pl.col(\"p_good\").fill_null(-1.0),\n",
    "                      pl.col(\"ret_mean\").fill_null(-1e308),\n",
    "                      pl.col(\"L_b_H_train\").fill_null(1e9),\n",
    "                      pl.col(\"entry_col\")\n",
    "                  ],\n",
    "                  descending=[True, True, True, False, False]\n",
    "              )\n",
    "              .with_row_count(\"rank_in_window_train\", offset=1)\n",
    "        )\n",
    "        ranked_parts.append(part)\n",
    "\n",
    "ranked = pl.concat(ranked_parts) if ranked_parts else df\n",
    "_print_kv(\"ranked_rows\", ranked.height)\n",
    "\n",
    "# ----------------------------- Guardado -----------------------------\n",
    "REPORTS_DIR = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ranked.write_parquet(OUT_PARQ)\n",
    "(Path(OUT_JSON)).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"NMIN_GOOD\": NMIN_GOOD, \"NMIN_BAD\": NMIN_BAD,\n",
    "    \"block_method\": \"nonoverlap\",\n",
    "    \"acf_band\": \"Â±1.96/sqrt(n_pairs)\",\n",
    "    \"score_train_definition\": \"EV_train * (2*p_good-1) * log1p(n_eff_hit_train)\",\n",
    "    \"tie_breakers\": [\"score_train desc\",\"p_good desc\",\"ret_mean desc\",\"L_b_H_train asc\",\"entry_col asc\"],\n",
    "    \"rows_head\": ranked.head(50).to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"OUT_PARQ\", OUT_PARQ)\n",
    "_print_kv(\"OUT_JSON\", OUT_JSON)\n",
    "print(\"âœ… Celda 7b (v3 FINAL, TZ-UTC + RANK FIX) completada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dee1574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 7c v3 FINAL) ...\n",
      "ğŸ“¦ Cargando artefactos ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - scores_shape                      : 36x17\n",
      "   - fwd_shape                         : 447948x3\n",
      "   - windows_count                     : 4\n",
      "ğŸ§® Construyendo conjuntos TRAIN por ventana (tp|sl) ...\n",
      "ğŸ§ª SelecciÃ³n + dedup Greedy (Jaccard â‰¥ 0.80) ...\n",
      "   - selected_rows                     : 36\n",
      "   - selected_cols                     : 5\n",
      "   - SEL_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - SEL_JSON                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.json\n",
      "âœ… Celda 7c (v3 FINAL) completada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7c (v3 FINAL): [SELECCIÃ“N & DEDUP Greedy( Jaccard â‰¥ 0.80 ) â†’ wf_selected_*]\n",
    "# -----------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: REEMPLAZA la Celda 7c actual (despuÃ©s de 7b y antes de 7d).\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_train_scores_{SYMBOL}_{TF}.parquet   (7b v3 FINAL)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/wf_windows_{SYMBOL}_{TF}.json\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_selected_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_selected_{SYMBOL}_{TF}.json  (metadatos + resumen)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/run_log_records.csv              (append)\n",
    "# Contrato del plan:\n",
    "#   â€¢ Ranking por ventana: score_train desc â†’ p_good desc â†’ EV desc â†’ L_b_H_train asc â†’ entry asc.\n",
    "#   â€¢ Gate N_min: usar meets_Nmin==True para elegibles.\n",
    "#   â€¢ DeduplicaciÃ³n Greedy Step-Down por Jaccardâ‰¥0.80 sobre el conjunto de seÃ±ales TRAIN (tp|sl).\n",
    "#   â€¢ cluster_id por ventana; cluster_size = tamaÃ±o del cluster del representante.\n",
    "#   â€¢ wf_id tratado como STRING (compatible con â€œwf_003â€).\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, csv, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _ensure_ts_datetime_utc(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    try:\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "        except Exception:\n",
    "            df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "    try:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\"))\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").dt.convert_time_zone(\"UTC\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 7c v3 FINAL) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "# Umbral Jaccard configurable (default 0.80)\n",
    "J_THR = float(os.environ.get(\"TWF_JACCARD_THR\", \"0.80\"))\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "WF_JSON     = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "FWD_PARQ    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"forward\" / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "SCORES_PARQ = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_JSON    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / f\"wf_selected_{SYMBOL}_{TF}.json\"\n",
    "RUNLOG_CSV  = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"reports\" / \"run_log_records.csv\"\n",
    "\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "assert WF_JSON.exists(),     f\"âŒ No existe ventanas WF: {WF_JSON}\"\n",
    "assert FWD_PARQ.exists(),    f\"âŒ Falta forward: {FWD_PARQ}\"\n",
    "assert SCORES_PARQ.exists(), f\"âŒ Falta scores 7b: {SCORES_PARQ}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“¦ Cargando artefactos ...\")\n",
    "scores = pl.read_parquet(SCORES_PARQ)\n",
    "fwd    = pl.read_parquet(FWD_PARQ).select([\"ts\",\"entry_col\",\"label\"])\n",
    "fwd    = _ensure_ts_datetime_utc(fwd)\n",
    "\n",
    "# wf_id como STRING\n",
    "if scores.get_column(\"wf_id\").dtype != pl.Utf8:\n",
    "    scores = scores.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "assert windows, \"âŒ wf_windows vacÃ­o.\"\n",
    "wf_map = {}\n",
    "for idx, w in enumerate(windows):\n",
    "    wf_key = str(w.get(\"wf_id\", f\"wf_{idx:03d}\"))\n",
    "    t0 = datetime.fromisoformat(w[\"train\"][\"start\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    t1 = datetime.fromisoformat(w[\"train\"][\"end\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    wf_map[wf_key] = (t0, t1)\n",
    "\n",
    "_print_kv(\"scores_shape\", f\"{scores.height}x{scores.width}\")\n",
    "_print_kv(\"fwd_shape\",    f\"{fwd.height}x{fwd.width}\")\n",
    "_print_kv(\"windows_count\", len(wf_map))\n",
    "\n",
    "# Campos esperados en scores\n",
    "need_cols = {\"wf_id\",\"entry_col\",\"score_train\",\"ret_mean\",\"L_b_H_train\",\n",
    "             \"tp\",\"sl\",\"support_excl_to\",\"meets_Nmin\"}\n",
    "missing = [c for c in need_cols if c not in scores.columns]\n",
    "assert not missing, f\"âŒ Falta columnas en wf_train_scores: {missing}\"\n",
    "\n",
    "# p_good para desempate (si no viene)\n",
    "if \"p_good\" not in scores.columns:\n",
    "    scores = scores.with_columns(\n",
    "        pl.when(pl.col(\"support_excl_to\")>0)\n",
    "          .then(pl.col(\"tp\")/pl.col(\"support_excl_to\"))\n",
    "          .otherwise(None)\n",
    "          .alias(\"p_good\")\n",
    "    )\n",
    "\n",
    "# Orden canÃ³nico de ranking en TRAIN\n",
    "def _sort_train(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.sort(\n",
    "        by=[\n",
    "            pl.col(\"score_train\").fill_null(-1e308),\n",
    "            pl.col(\"p_good\").fill_null(-1.0),\n",
    "            pl.col(\"ret_mean\").fill_null(-1e308),\n",
    "            pl.col(\"L_b_H_train\").fill_null(1e9),\n",
    "            pl.col(\"entry_col\")\n",
    "        ],\n",
    "        descending=[True, True, True, False, False]\n",
    "    )\n",
    "\n",
    "# Precalcular, por ventana TRAIN, los sets de timestamps (tp|sl) por entry_col\n",
    "print(\"ğŸ§® Construyendo conjuntos TRAIN por ventana (tp|sl) ...\")\n",
    "train_sets: dict[str, dict[str, set]] = {}\n",
    "for wf_id, (t0, t1) in wf_map.items():\n",
    "    sub = (\n",
    "        fwd.filter( (pl.col(\"ts\")>=pl.lit(t0)) & (pl.col(\"ts\")<=pl.lit(t1)) & (pl.col(\"label\")!=0) )\n",
    "           .select([\"entry_col\",\"ts\"])\n",
    "           .unique()  # asegurar (entry, ts) Ãºnicos\n",
    "    )\n",
    "    by_entry = {}\n",
    "    if sub.height > 0:\n",
    "        for rec in sub.group_by(\"entry_col\").agg(pl.col(\"ts\").alias(\"__ts__\")).to_dicts():\n",
    "            by_entry[rec[\"entry_col\"]] = set(rec[\"__ts__\"])\n",
    "    train_sets[wf_id] = by_entry\n",
    "\n",
    "def _jaccard(A: set, B: set) -> float:\n",
    "    if not A and not B:\n",
    "        return 0.0\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return inter/union if union>0 else 0.0\n",
    "\n",
    "# Greedy Step-Down con Jaccard â‰¥ J_THR (por ventana)\n",
    "print(\"ğŸ§ª SelecciÃ³n + dedup Greedy (Jaccard â‰¥ %.2f) ...\" % J_THR)\n",
    "kept_rows = []\n",
    "drop_rows = []\n",
    "clusters  = []  # (wf_id, cluster_head_entry, members)\n",
    "for wf_id in scores.get_column(\"wf_id\").unique().to_list():\n",
    "    cand = scores.filter( (pl.col(\"wf_id\")==wf_id) & (pl.col(\"meets_Nmin\")==True) )\n",
    "    if cand.is_empty():\n",
    "        continue\n",
    "    cand = _sort_train(cand)\n",
    "    kept = []          # lista de entry_col elegidos (orden ranking)\n",
    "    cluster_members = {}  # head -> set(members)\n",
    "    sets_map = train_sets.get(wf_id, {})\n",
    "\n",
    "    for rec in cand.to_dicts():\n",
    "        e = rec[\"entry_col\"]\n",
    "        e_set = sets_map.get(e, set())\n",
    "        drop_to_head = None\n",
    "        # comparar contra representantes ya kept\n",
    "        for head in kept:\n",
    "            h_set = sets_map.get(head, set())\n",
    "            jac = _jaccard(e_set, h_set)\n",
    "            if jac >= J_THR:\n",
    "                drop_to_head = head\n",
    "                break\n",
    "        if drop_to_head is None:\n",
    "            kept.append(e)\n",
    "            cluster_members.setdefault(e, set([e]))\n",
    "            kept_rows.append({**rec})\n",
    "        else:\n",
    "            drop_rows.append({**rec, \"_dropped_to\": drop_to_head})\n",
    "\n",
    "    # cluster sizes\n",
    "    for head in kept:\n",
    "        # agregar a su cluster los que cayeron a ese head\n",
    "        members = {head}\n",
    "        for r in drop_rows:\n",
    "            if r.get(\"_dropped_to\")==head and r[\"wf_id\"]==wf_id:\n",
    "                members.add(r[\"entry_col\"])\n",
    "        clusters.append((wf_id, head, members))\n",
    "\n",
    "# Construir DataFrame final de seleccionados con cluster_id/size y rank_in_window\n",
    "sel_records = []\n",
    "for wf_id in scores.get_column(\"wf_id\").unique().to_list():\n",
    "    # map head->members para wf\n",
    "    cl_heads = [(h, m) for (w,h,m) in clusters if w==wf_id]\n",
    "    head_to_size = {h: len(m) for (h,m) in cl_heads}\n",
    "\n",
    "    kept_df = pl.DataFrame([r for r in kept_rows if r[\"wf_id\"]==wf_id]) if kept_rows else pl.DataFrame()\n",
    "    if kept_df.height==0:\n",
    "        continue\n",
    "    # rank_in_window = posiciÃ³n entre kept (orden ya es de cand ordenado; reordenamos por la misma regla)\n",
    "    kept_df = _sort_train(kept_df).with_row_index(\"rank_in_window\", offset=1)\n",
    "\n",
    "    # asignar cluster_id/size (head = entry_col)\n",
    "    kept_df = kept_df.with_columns(\n",
    "        pl.col(\"entry_col\").map_elements(lambda x: f\"{wf_id}::cluster::{x}\").alias(\"cluster_id\"),\n",
    "        pl.col(\"entry_col\").map_elements(lambda x: head_to_size.get(x, 1)).alias(\"cluster_size\")\n",
    "    )\n",
    "\n",
    "    sel_records.append(kept_df.select([\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"]))\n",
    "\n",
    "sel_df = pl.concat(sel_records) if sel_records else pl.DataFrame(\n",
    "    {\"wf_id\": pl.Series([], dtype=pl.Utf8),\n",
    "     \"entry_col\": pl.Series([], dtype=pl.Utf8),\n",
    "     \"rank_in_window\": pl.Series([], dtype=pl.Int64),\n",
    "     \"cluster_id\": pl.Series([], dtype=pl.Utf8),\n",
    "     \"cluster_size\": pl.Series([], dtype=pl.Int64)}\n",
    ")\n",
    "\n",
    "_print_kv(\"selected_rows\", sel_df.height)\n",
    "_print_kv(\"selected_cols\", sel_df.width)\n",
    "\n",
    "# Guardado\n",
    "sel_df.write_parquet(SEL_PARQ)\n",
    "\n",
    "meta = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"dedup_method\": \"greedy_stepdown\",\n",
    "    \"jaccard_thr\": J_THR,\n",
    "    \"rank_order\": [\"score_train desc\",\"p_good desc\",\"ret_mean desc\",\"L_b_H_train asc\",\"entry_col asc\"],\n",
    "    \"wf_id_type\": \"string\",\n",
    "    \"notes\": \"Jaccard sobre TRAIN (tp|sl) por (entry_col). Se guardan sÃ³lo representantes (kept).\",\n",
    "    \"summary\": {\n",
    "        \"n_windows\": len(wf_map),\n",
    "        \"n_kept_total\": int(sel_df.height),\n",
    "        \"n_dropped_total\": int(len(drop_rows))\n",
    "    },\n",
    "    \"dropped_examples_head\": drop_rows[:20]\n",
    "}\n",
    "SEL_JSON.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"SEL_PARQ\", SEL_PARQ)\n",
    "_print_kv(\"SEL_JSON\", SEL_JSON)\n",
    "\n",
    "# run_log_records append\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header: w.writerow(header)\n",
    "    w.writerow([\n",
    "        datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        SYMBOL, TF, \"7c_v3_final_greedy_0p80\",\n",
    "        sel_df.height, \"\", \"\",\n",
    "        f\"Greedy step-down; J={J_THR:.2f}; kept={sel_df.height}; dropped={len(drop_rows)}\"\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Celda 7c (v3 FINAL) completada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "cdfa72f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "   - SCORES_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - PBO_JSON                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_pbo_BTCUSDT_15m.json\n",
      "   - min_train_signals             : 1000\n",
      "   - sort_metric_requested         : profit_factor_stat\n",
      "ğŸ§® Calculando PBO por ventana ...\n",
      "   - [wf_000] train_candidates_after_minN: 8\n",
      "   - [wf_000] sort_metric_used     : profit_factor_stat\n",
      "   - [wf_000] top1_train           : entry_er10_ge_0p4\n",
      "   - [wf_000] test_rows            : 7144\n",
      "   - [wf_000] test_candidates_measured: 8\n",
      "   - [wf_000] test_percentile(top1): 0.625\n",
      "   - [wf_000] overfit              : False\n",
      "   - [wf_001] train_candidates_after_minN: 9\n",
      "   - [wf_001] sort_metric_used     : profit_factor_stat\n",
      "   - [wf_001] top1_train           : entry_er50_ge_0p4\n",
      "   - [wf_001] test_rows            : 7637\n",
      "   - [wf_001] test_candidates_measured: 9\n",
      "   - [wf_001] test_percentile(top1): 1.0\n",
      "   - [wf_001] overfit              : False\n",
      "   - [wf_002] train_candidates_after_minN: 9\n",
      "   - [wf_002] sort_metric_used     : profit_factor_stat\n",
      "   - [wf_002] top1_train           : entry_er20_ge_0p3\n",
      "   - [wf_002] test_rows            : 7267\n",
      "   - [wf_002] test_candidates_measured: 9\n",
      "   - [wf_002] test_percentile(top1): 0.4444444444444444\n",
      "   - [wf_002] overfit              : True\n",
      "   - [wf_003] train_candidates_after_minN: 8\n",
      "   - [wf_003] sort_metric_used     : profit_factor_stat\n",
      "   - [wf_003] top1_train           : entry_er10_ge_0p2\n",
      "   - [wf_003] test_rows            : 7698\n",
      "   - [wf_003] test_candidates_measured: 8\n",
      "   - [wf_003] test_percentile(top1): 0.875\n",
      "   - [wf_003] overfit              : False\n",
      "   - windows_usable                : 4\n",
      "   - PBO                           : 0.25\n",
      "ğŸ’¾ Guardando wf_pbo ...\n",
      "   - PBO_JSON                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_pbo_BTCUSDT_15m.json\n",
      "âœ… Celda 7c_bis_pbo finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7c_bis_pbo (FIX v2): [PBO QUICK-CSCV APROX â†’ reports/wf_pbo_*.json]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# - Entrada: forward_{..}.parquet, wf_windows_{..}.json, wf_train_scores_{..}.parquet (7b v2).\n",
    "# - Para cada ventana:\n",
    "#     1) rank TRAIN por mÃ©trica efectiva (respeta sort_metric del config si existe; si no, fallback).\n",
    "#     2) en TEST: ret_mean por entry_col de TODOS los candidatos de TRAIN.\n",
    "#     3) Percentil del top-1 TRAIN dentro de la dist. TEST. Si â‰¤ 0.5 â‡’ \"overfit\".\n",
    "# - PBO = proporciÃ³n de ventanas marcadas como overfit (solo sobre ventanas \"usables\").\n",
    "# - Salida: reports/wf_pbo_{SYMBOL}_{TF}.json   (JSON 100% serializable con tipos nativos)\n",
    "# - Logs: impresiones detalladas por ventana.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): \n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _parse_iso(ts_str: str):\n",
    "    return datetime.fromisoformat(ts_str.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CFG         = json.loads((PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\").read_text(encoding=\"utf-8\"))\n",
    "PATHS         = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "\n",
    "FORWARD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON       = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "SCORES_PARQ   = PATHS[\"reports\"] / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "PBO_JSON      = PATHS[\"reports\"] / f\"wf_pbo_{SYMBOL}_{TF}.json\"\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert FORWARD_PARQ.exists(), f\"âŒ Falta {FORWARD_PARQ}\"\n",
    "assert WF_JSON.exists(),      f\"âŒ Falta {WF_JSON}\"\n",
    "assert SCORES_PARQ.exists(),  f\"âŒ Falta {SCORES_PARQ}\"\n",
    "\n",
    "SEL_CFG = (CFG.get(\"wf\", {}).get(\"selection\", {}) or CFG.get(\"selection\", {}) or CFG.get(\"wf_selection\", {}) or {})\n",
    "MIN_TRAIN_SIGNALS = int(SEL_CFG.get(\"min_train_signals\", 1000))\n",
    "SORT_MET_REQ      = str(SEL_CFG.get(\"sort_metric\", \"profit_factor_stat\"))\n",
    "\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"WF_JSON\", WF_JSON)\n",
    "_print_kv(\"SCORES_PARQ\", SCORES_PARQ)\n",
    "_print_kv(\"PBO_JSON\", PBO_JSON)\n",
    "_print_kv(\"min_train_signals\", MIN_TRAIN_SIGNALS)\n",
    "_print_kv(\"sort_metric_requested\", SORT_MET_REQ)\n",
    "\n",
    "# Cargas\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta {c} en forward.\"\n",
    "\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "train   = pl.read_parquet(SCORES_PARQ)\n",
    "\n",
    "def _resolve_sort_metric(cols: list[str], requested: str) -> str:\n",
    "    if requested in cols: \n",
    "        return requested\n",
    "    for cand in (\"score_train\", \"profit_factor_stat\", \"ret_mean\"):\n",
    "        if cand in cols:\n",
    "            print(f\"   âš ï¸ sort_metric '{requested}' no existe; usando fallback '{cand}'.\")\n",
    "            return cand\n",
    "    raise AssertionError(\"âŒ No hay mÃ©trica vÃ¡lida para ranking (falta score_train/profit_factor_stat/ret_mean).\")\n",
    "\n",
    "print(\"ğŸ§® Calculando PBO por ventana ...\")\n",
    "details = []\n",
    "overfit_flags = []\n",
    "usable = 0\n",
    "\n",
    "for w in windows:\n",
    "    wid = w[\"id\"]\n",
    "    sc_win = train.filter(pl.col(\"wf_id\")==wid)\n",
    "    if \"n_signals\" in sc_win.columns:\n",
    "        sc_win = sc_win.filter(pl.col(\"n_signals\") >= MIN_TRAIN_SIGNALS)\n",
    "    _print_kv(f\"[{wid}] train_candidates_after_minN\", sc_win.height)\n",
    "    if sc_win.height == 0:\n",
    "        details.append({\"wf_id\": str(wid), \"note\": \"no_train_candidates\"}); \n",
    "        continue\n",
    "\n",
    "    sort_met = _resolve_sort_metric(sc_win.columns, SORT_MET_REQ)\n",
    "    sc_win = sc_win.sort([sort_met], descending=[True])\n",
    "\n",
    "    top1 = sc_win[\"entry_col\"][0]\n",
    "    _print_kv(f\"[{wid}] sort_metric_used\", sort_met)\n",
    "    _print_kv(f\"[{wid}] top1_train\", top1)\n",
    "\n",
    "    # Subset TEST\n",
    "    t0 = _parse_iso(w[\"test\"][\"start\"]); t1 = _parse_iso(w[\"test\"][\"end\"])\n",
    "    sub = fwd.filter((pl.col(\"ts\") >= t0) & (pl.col(\"ts\") <= t1))\n",
    "    _print_kv(f\"[{wid}] test_rows\", sub.height)\n",
    "    if sub.height == 0:\n",
    "        details.append({\"wf_id\": str(wid), \"top_train\": str(top1), \"note\": \"no_test_rows\"}); \n",
    "        continue\n",
    "\n",
    "    cand_list = sc_win[\"entry_col\"].to_list()\n",
    "    kpi_test = (\n",
    "        sub.filter(pl.col(\"entry_col\").is_in(cand_list))\n",
    "           .group_by(\"entry_col\")\n",
    "           .agg(pl.len().alias(\"n\"), pl.col(\"ret_event\").mean().alias(\"ret_mean\"))\n",
    "    )\n",
    "    _print_kv(f\"[{wid}] test_candidates_measured\", kpi_test.height)\n",
    "    if kpi_test.height == 0:\n",
    "        details.append({\"wf_id\": str(wid), \"top_train\": str(top1), \"note\": \"no_test_metrics\"}); \n",
    "        continue\n",
    "\n",
    "    # Dist TEST (tipos nativos)\n",
    "    dist_vals = [float(x) for x in kpi_test[\"ret_mean\"].to_list()]\n",
    "    top_vals  = kpi_test.filter(pl.col(\"entry_col\")==top1)[\"ret_mean\"].to_list()\n",
    "    if len(top_vals)==0:\n",
    "        details.append({\"wf_id\": str(wid), \"top_train\": str(top1), \"note\": \"top1_missing_in_test\"}); \n",
    "        continue\n",
    "\n",
    "    top_val = float(top_vals[0])\n",
    "    pct = float(sum(1 for v in dist_vals if v <= top_val) / max(len(dist_vals), 1))\n",
    "    overfit = bool(pct <= 0.5)\n",
    "\n",
    "    usable += 1\n",
    "    overfit_flags.append(1 if overfit else 0)\n",
    "    details.append({\n",
    "        \"wf_id\": str(wid), \n",
    "        \"top_train\": str(top1), \n",
    "        \"sort_metric\": str(sort_met),\n",
    "        \"test_percentile\": float(pct), \n",
    "        \"overfit\": bool(overfit),\n",
    "        \"n_test_candidates\": int(len(dist_vals))\n",
    "    })\n",
    "    _print_kv(f\"[{wid}] test_percentile(top1)\", pct)\n",
    "    _print_kv(f\"[{wid}] overfit\", overfit)\n",
    "\n",
    "PBO = (sum(overfit_flags) / usable) if usable > 0 else None\n",
    "_print_kv(\"windows_usable\", usable)\n",
    "_print_kv(\"PBO\", PBO if PBO is not None else \"N/A\")\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_pbo ...\")\n",
    "PBO_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": str(SYMBOL), \n",
    "    \"timeframe\": str(TF),\n",
    "    \"min_train_signals\": int(MIN_TRAIN_SIGNALS),\n",
    "    \"sort_metric_requested\": str(SORT_MET_REQ),\n",
    "    \"windows_usable\": int(usable),\n",
    "    \"PBO\": (float(PBO) if PBO is not None else None),\n",
    "    \"details\": details\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"PBO_JSON\", PBO_JSON)\n",
    "print(\"âœ… Celda 7c_bis_pbo finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cba76865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n ...\n",
      "   - FORWARD_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "   - DS_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_ds_BTCUSDT_15m.json\n",
      "ğŸ§® Dataset shift por ventana ...\n",
      "   - [wf_000] n_train              : 95928\n",
      "   - [wf_000] n_test               : 7144\n",
      "   - [wf_000] tr_hit_excl          : 0.3124625429770053\n",
      "   - [wf_000] te_hit_excl          : 0.27262459409854584\n",
      "   - [wf_000] delta_hit_excl       : -0.039837948878459484\n",
      "   - [wf_000] PSI(ret_event)       : 0.10705857356008333\n",
      "   - [wf_000] KS(ret_event)        : 0.08471772362306451\n",
      "   - [wf_000] drift_flag           : False\n",
      "   - [wf_001] n_train              : 93625\n",
      "   - [wf_001] n_test               : 7637\n",
      "   - [wf_001] tr_hit_excl          : 0.3347800548231301\n",
      "   - [wf_001] te_hit_excl          : 0.3209909429941396\n",
      "   - [wf_001] delta_hit_excl       : -0.013789111828990508\n",
      "   - [wf_001] PSI(ret_event)       : 0.3592209954246238\n",
      "   - [wf_001] KS(ret_event)        : 0.1250684872833806\n",
      "   - [wf_001] drift_flag           : True\n",
      "   - [wf_002] n_train              : 87889\n",
      "   - [wf_002] n_test               : 7267\n",
      "   - [wf_002] tr_hit_excl          : 0.34138529449446187\n",
      "   - [wf_002] te_hit_excl          : 0.34472696957065446\n",
      "   - [wf_002] delta_hit_excl       : 0.003341675076192596\n",
      "   - [wf_002] PSI(ret_event)       : 0.9283032695884611\n",
      "   - [wf_002] KS(ret_event)        : 0.2777309303646568\n",
      "   - [wf_002] drift_flag           : True\n",
      "   - [wf_003] n_train              : 92738\n",
      "   - [wf_003] n_test               : 7698\n",
      "   - [wf_003] tr_hit_excl          : 0.3226655598396557\n",
      "   - [wf_003] te_hit_excl          : 0.33895402450270057\n",
      "   - [wf_003] delta_hit_excl       : 0.016288464663044844\n",
      "   - [wf_003] PSI(ret_event)       : 0.020806039113256902\n",
      "   - [wf_003] KS(ret_event)        : 0.055339049103663274\n",
      "   - [wf_003] drift_flag           : False\n",
      "   - drift_windows                 : 2\n",
      "   - drift_rate                    : 0.5\n",
      "ğŸ’¾ Guardando wf_ds ...\n",
      "   - DS_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_ds_BTCUSDT_15m.json\n",
      "âœ… Celda 7c_bis_ds finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7c_bis_ds (v1.1 FIX): [DATASET SHIFT por ventana â†’ reports/wf_ds_*.json]\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# FIXES:\n",
    "#  - Evita pivot() sin Ã­ndice: conteos de label via dict por compresiÃ³n (robusto y sin deprecations).\n",
    "#  - PSI/KS: bordes estrictamente crecientes con np.nextafter para evitar fallos si hay cuantiles iguales.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): \n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _parse_iso(ts_str: str):\n",
    "    return datetime.fromisoformat(ts_str.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n",
    "\n",
    "def _safe_div(a: float, b: float, eps: float = 1e-12) -> float:\n",
    "    return float(a) / float(b if abs(b) > eps else eps)\n",
    "\n",
    "def _make_edges_from_quantiles(q_edges: list[float]) -> list[float]:\n",
    "    \"\"\"Construye [-inf, q..., +inf] y fuerza monotonicidad estricta (maneja cuantiles repetidos).\"\"\"\n",
    "    edges = [-np.inf] + [float(x) for x in q_edges] + [np.inf]\n",
    "    cleaned = [edges[0]]\n",
    "    for x in edges[1:]:\n",
    "        if x <= cleaned[-1]:\n",
    "            x = float(np.nextafter(cleaned[-1], np.inf))\n",
    "        cleaned.append(x)\n",
    "    return cleaned\n",
    "\n",
    "def _psi_from_bins(train_vals: np.ndarray, test_vals: np.ndarray, q_edges: list[float]) -> float:\n",
    "    edges = _make_edges_from_quantiles(q_edges)\n",
    "    tr_hist, _ = np.histogram(train_vals, bins=edges)\n",
    "    te_hist, _ = np.histogram(test_vals,  bins=edges)\n",
    "    tr_p = tr_hist / max(tr_hist.sum(), 1)\n",
    "    te_p = te_hist / max(te_hist.sum(), 1)\n",
    "    tr_p = np.where(tr_p<=0, 1e-12, tr_p)\n",
    "    te_p = np.where(te_p<=0, 1e-12, te_p)\n",
    "    return float(np.sum((tr_p - te_p) * np.log(tr_p / te_p)))\n",
    "\n",
    "def _ks_from_bins(train_vals: np.ndarray, test_vals: np.ndarray, q_edges: list[float]) -> float:\n",
    "    edges = _make_edges_from_quantiles(q_edges)\n",
    "    tr_hist, _ = np.histogram(train_vals, bins=edges)\n",
    "    te_hist, _ = np.histogram(test_vals,  bins=edges)\n",
    "    tr_cdf = np.cumsum(tr_hist) / max(tr_hist.sum(), 1)\n",
    "    te_cdf = np.cumsum(te_hist) / max(te_hist.sum(), 1)\n",
    "    return float(np.max(np.abs(tr_cdf - te_cdf)))\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CFG         = json.loads((PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\").read_text(encoding=\"utf-8\"))\n",
    "PATHS         = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "\n",
    "FORWARD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON       = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "DS_JSON       = PATHS[\"reports\"] / f\"wf_ds_{SYMBOL}_{TF}.json\"\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert FORWARD_PARQ.exists(), f\"âŒ Falta {FORWARD_PARQ}\"\n",
    "assert WF_JSON.exists(),      f\"âŒ Falta {WF_JSON}\"\n",
    "\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"WF_JSON\", WF_JSON)\n",
    "_print_kv(\"DS_JSON\", DS_JSON)\n",
    "\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta {c} en forward.\"\n",
    "\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "\n",
    "print(\"ğŸ§® Dataset shift por ventana ...\")\n",
    "win_rows = []\n",
    "\n",
    "for w in windows:\n",
    "    wid = w[\"id\"]\n",
    "    t0_tr = _parse_iso(w[\"train\"][\"start\"]); t1_tr = _parse_iso(w[\"train\"][\"end\"])\n",
    "    t0_te = _parse_iso(w[\"test\"][\"start\"]);  t1_te = _parse_iso(w[\"test\"][\"end\"])\n",
    "\n",
    "    tr = fwd.filter((pl.col(\"ts\") >= t0_tr) & (pl.col(\"ts\") <= t1_tr))\n",
    "    te = fwd.filter((pl.col(\"ts\") >= t0_te) & (pl.col(\"ts\") <= t1_te))\n",
    "    _print_kv(f\"[{wid}] n_train\", tr.height)\n",
    "    _print_kv(f\"[{wid}] n_test\",  te.height)\n",
    "\n",
    "    if tr.height==0 or te.height==0:\n",
    "        win_rows.append({\n",
    "            \"wf_id\": str(wid), \"note\": \"empty_train_or_test\",\n",
    "            \"n_train\": int(tr.height), \"n_test\": int(te.height),\n",
    "            \"psi_ret_event\": None, \"ks_ret_event\": None, \n",
    "            \"delta_hit_excl\": None, \"drift_flag\": None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Conteos por label (SIN pivot): dict por comprensiÃ³n â†’ robusto\n",
    "    tr_lbl = tr.group_by(\"label\").agg(pl.len().alias(\"n\")).to_dicts()\n",
    "    te_lbl = te.group_by(\"label\").agg(pl.len().alias(\"n\")).to_dicts()\n",
    "    tr_counts = { int(row[\"label\"]): int(row[\"n\"]) for row in tr_lbl }\n",
    "    te_counts = { int(row[\"label\"]): int(row[\"n\"]) for row in te_lbl }\n",
    "\n",
    "    tr_tp = tr_counts.get(1, 0);   tr_sl = tr_counts.get(-1, 0);  tr_to = tr_counts.get(0, 0)\n",
    "    te_tp = te_counts.get(1, 0);   te_sl = te_counts.get(-1, 0);  te_to = te_counts.get(0, 0)\n",
    "\n",
    "    tr_den = tr_tp + tr_sl\n",
    "    te_den = te_tp + te_sl\n",
    "    tr_hit_excl = (_safe_div(tr_tp, tr_den) if tr_den > 0 else None)\n",
    "    te_hit_excl = (_safe_div(te_tp, te_den) if te_den > 0 else None)\n",
    "    delta_hit_excl = (float(te_hit_excl - tr_hit_excl) if (tr_hit_excl is not None and te_hit_excl is not None) else None)\n",
    "\n",
    "    # PSI / KS de ret_event (bins por cuantiles de TRAIN)\n",
    "    qdf = tr.select([\n",
    "        pl.col(\"ret_event\").quantile(0.05).alias(\"q05\"),\n",
    "        pl.col(\"ret_event\").quantile(0.25).alias(\"q25\"),\n",
    "        pl.col(\"ret_event\").quantile(0.50).alias(\"q50\"),\n",
    "        pl.col(\"ret_event\").quantile(0.75).alias(\"q75\"),\n",
    "        pl.col(\"ret_event\").quantile(0.95).alias(\"q95\"),\n",
    "    ])\n",
    "    qrow = qdf.to_dicts()[0]\n",
    "    q_edges = [float(qrow[\"q05\"]), float(qrow[\"q25\"]), float(qrow[\"q50\"]), float(qrow[\"q75\"]), float(qrow[\"q95\"])]\n",
    "\n",
    "    tr_vals = np.asarray(tr[\"ret_event\"].to_numpy(), dtype=float)\n",
    "    te_vals = np.asarray(te[\"ret_event\"].to_numpy(), dtype=float)\n",
    "    psi_val = _psi_from_bins(tr_vals, te_vals, q_edges)\n",
    "    ks_val  = _ks_from_bins(tr_vals, te_vals, q_edges)\n",
    "\n",
    "    # Reglas de flag\n",
    "    drift_flag = bool( (psi_val > 0.25) or (ks_val > 0.10) or ((delta_hit_excl is not None) and (abs(delta_hit_excl) > 0.05)) )\n",
    "\n",
    "    _print_kv(f\"[{wid}] tr_hit_excl\", tr_hit_excl if tr_hit_excl is not None else \"N/A\")\n",
    "    _print_kv(f\"[{wid}] te_hit_excl\", te_hit_excl if te_hit_excl is not None else \"N/A\")\n",
    "    _print_kv(f\"[{wid}] delta_hit_excl\", delta_hit_excl if delta_hit_excl is not None else \"N/A\")\n",
    "    _print_kv(f\"[{wid}] PSI(ret_event)\", psi_val)\n",
    "    _print_kv(f\"[{wid}] KS(ret_event)\", ks_val)\n",
    "    _print_kv(f\"[{wid}] drift_flag\", drift_flag)\n",
    "\n",
    "    win_rows.append({\n",
    "        \"wf_id\": str(wid),\n",
    "        \"n_train\": int(tr.height), \"n_test\": int(te.height),\n",
    "        \"train_label_counts\": {\"tp\": int(tr_tp), \"sl\": int(tr_sl), \"timeout\": int(tr_to)},\n",
    "        \"test_label_counts\":  {\"tp\": int(te_tp), \"sl\": int(te_sl), \"timeout\": int(te_to)},\n",
    "        \"tr_hit_excl\": (float(tr_hit_excl) if tr_hit_excl is not None else None),\n",
    "        \"te_hit_excl\": (float(te_hit_excl) if te_hit_excl is not None else None),\n",
    "        \"delta_hit_excl\": (float(delta_hit_excl) if delta_hit_excl is not None else None),\n",
    "        \"psi_ret_event\": float(psi_val),\n",
    "        \"ks_ret_event\": float(ks_val),\n",
    "        \"drift_flag\": bool(drift_flag)\n",
    "    })\n",
    "\n",
    "# Resumen global\n",
    "drift_count = sum(1 for r in win_rows if (r.get(\"drift_flag\") is True))\n",
    "drift_rate  = (float(drift_count) / float(len(win_rows))) if len(win_rows)>0 else None\n",
    "_print_kv(\"drift_windows\", drift_count)\n",
    "_print_kv(\"drift_rate\", drift_rate if drift_rate is not None else \"N/A\")\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_ds ...\")\n",
    "DS_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": str(SYMBOL), \n",
    "    \"timeframe\": str(TF),\n",
    "    \"rules\": {\n",
    "        \"psi_gt\": 0.25,\n",
    "        \"ks_gt\": 0.10,\n",
    "        \"abs_delta_hit_excl_gt\": 0.05\n",
    "    },\n",
    "    \"drift_windows\": int(drift_count),\n",
    "    \"drift_rate\": (float(drift_rate) if drift_rate is not None else None),\n",
    "    \"windows\": win_rows\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"DS_JSON\", DS_JSON)\n",
    "print(\"âœ… Celda 7c_bis_ds finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b240586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 7d v3 FINAL) ...\n",
      "   - SEL_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - FWD_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "   - sel_shape                       : 36x5\n",
      "   - fwd_shape                       : 447948x4\n",
      "   - windows_count                   : 4\n",
      "ğŸ§® Evaluando TEST con bootstrap por bloques ...\n",
      "   - eval_rows                       : 36\n",
      "   - eval_cols                       : 22\n",
      "ğŸ’¾ Guardando wf_select_eval ...\n",
      "   - OUT_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "   - OUT_JSON                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.json\n",
      "âœ… Celda 7d (v3 FINAL) completada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7d (v3 FINAL, TZ-UTC + wf_id string): [EVALUACIÃ“N EN TEST ROBUSTA â†’ reports/wf_select_eval_*]\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: REEMPLAZA la Celda 7d actual (despuÃ©s de 7c y antes de 8b).\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_selected_{SYMBOL}_{TF}.parquet   (7c v3 FINAL)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet       (eventos)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/wf_windows_{SYMBOL}_{TF}.json          (ventanas WF)\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_select_eval_{SYMBOL}_{TF}.parquet / .json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/run_log_records.csv (append)\n",
    "# MÃ©todo:\n",
    "#   â€¢ L_b_H_test (ACF: dos lags consecutivos en banda Â±1.96/âˆšn_pairs), n_eff = max(1, n_pairs/L_b_H_test).\n",
    "#   â€¢ Bootstrap por bloques no solapados (B=2000) para EV (ret_mean).\n",
    "#   â€¢ Intervalo de Wilson para p_obs (hit-rate excluyendo timeouts).\n",
    "#   â€¢ Profit Factor estimado desde ret_event (si hay pÃ©rdidas).\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<32}: {v}\")\n",
    "\n",
    "def _ensure_ts_datetime_utc(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # fuerza ts a Datetime[Î¼s, UTC]\n",
    "    try:\n",
    "        _ = df.select(pl.col(\"ts\").dt.year).height\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "        except Exception:\n",
    "            df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "    # aÃ±adir/convertir zona\n",
    "    try:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\"))\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = df.with_columns(pl.col(\"ts\").dt.convert_time_zone(\"UTC\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def _z_acf_band(n: int) -> float:\n",
    "    n = max(1, int(n))\n",
    "    return 1.96 / math.sqrt(n)\n",
    "\n",
    "def _acf(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x - np.nanmean(x)\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    n = len(x)\n",
    "    if n == 0: return np.zeros(max_lag+1)\n",
    "    denom = np.dot(x, x)\n",
    "    if denom <= 0: return np.zeros(max_lag+1)\n",
    "    r = np.empty(max_lag+1, dtype=float)\n",
    "    for k in range(max_lag+1):\n",
    "        r[k] = np.dot(x[:n-k], x[k:]) / denom\n",
    "    return r\n",
    "\n",
    "def _estimate_block_len(labels_vec: np.ndarray) -> int:\n",
    "    # serie y = +1 (tp), -1 (sl), 0 (timeout); usa N_pairs para la banda\n",
    "    if labels_vec.size == 0:\n",
    "        return 1\n",
    "    y = np.zeros_like(labels_vec, dtype=float)\n",
    "    y[labels_vec == 1]  = 1.0\n",
    "    y[labels_vec == -1] = -1.0\n",
    "    n_pairs = int(np.sum(labels_vec != 0))\n",
    "    band = _z_acf_band(max(n_pairs, 1))\n",
    "    max_lag = max(10, min(200, int(len(y) * 0.2)))\n",
    "    r = _acf(y, max_lag)\n",
    "    for H in range(1, max_lag-1):\n",
    "        if abs(r[H]) <= band and abs(r[H+1]) <= band:\n",
    "            return max(1, H)\n",
    "    return max(1, int(np.ceil(np.sqrt(max(n_pairs, 1)))))  # fallback conservador\n",
    "\n",
    "def _wilson_interval(tp: int, n: int, z: float = 1.96):\n",
    "    n = int(max(0, n))\n",
    "    if n == 0: return (None, None, None)\n",
    "    p = tp / n\n",
    "    denom = 1 + z*z/n\n",
    "    center = (p + z*z/(2*n)) / denom\n",
    "    half = z * math.sqrt((p*(1-p) + z*z/(4*n)) / n) / denom\n",
    "    return (p, max(0.0, center - half), min(1.0, center + half))\n",
    "\n",
    "def _block_bootstrap_mean(ret: np.ndarray, block_len: int, B: int = 2000, rng: np.random.Generator | None = None):\n",
    "    n = len(ret)\n",
    "    if n == 0: return (None, None, None, 0.0)\n",
    "    if block_len <= 0: block_len = 1\n",
    "    k = max(1, int(math.ceil(n / block_len)))\n",
    "    starts = np.arange(0, max(1, n - block_len + 1))  # inicios de bloques no solapados\n",
    "    if starts.size == 0: starts = np.array([0], dtype=int)\n",
    "    rng = rng or np.random.default_rng(123)\n",
    "    boots = np.empty(B, dtype=float); ok = 0\n",
    "    for b in range(B):\n",
    "        idxs = rng.choice(starts, size=k, replace=True)\n",
    "        sample = []\n",
    "        for s in idxs:\n",
    "            sample.extend(ret[s:s+block_len])\n",
    "        if len(sample) >= 1:\n",
    "            sample = np.asarray(sample[:n], dtype=float)\n",
    "            boots[b] = np.nanmean(sample); ok += 1\n",
    "        else:\n",
    "            boots[b] = np.nan\n",
    "    mean = float(np.nanmean(boots)) if ok>0 else None\n",
    "    ci_lo = float(np.nanpercentile(boots, 2.5)) if ok>0 else None\n",
    "    ci_hi = float(np.nanpercentile(boots, 97.5)) if ok>0 else None\n",
    "    return (mean, ci_lo, ci_hi, ok / B * 100.0)\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 7d v3 FINAL) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]; SEED_BASE = int(os.environ[\"TWF_SEED_BASE\"])\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEL_PARQ   = PATHS[\"reports\"] / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "FWD_PARQ   = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON    = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "OUT_PARQ   = PATHS[\"reports\"] / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "OUT_JSON   = PATHS[\"reports\"] / f\"wf_select_eval_{SYMBOL}_{TF}.json\"\n",
    "RUNLOG_CSV = PATHS[\"reports\"] / \"run_log_records.csv\"\n",
    "\n",
    "_print_kv(\"SEL_PARQ\", SEL_PARQ); _print_kv(\"FWD_PARQ\", FWD_PARQ); _print_kv(\"WF_JSON\", WF_JSON)\n",
    "assert SEL_PARQ.exists(), \"âŒ Falta wf_selected\"\n",
    "assert FWD_PARQ.exists(), \"âŒ Falta forward\"\n",
    "assert WF_JSON.exists(),  \"âŒ Falta wf_windows\"\n",
    "\n",
    "sel = pl.read_parquet(SEL_PARQ).select([\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"]).unique()\n",
    "# normaliza wf_id a string\n",
    "if sel.get_column(\"wf_id\").dtype != pl.Utf8:\n",
    "    sel = sel.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "\n",
    "fwd = pl.read_parquet(FWD_PARQ).select([\"ts\",\"entry_col\",\"label\",\"ret_event\"])\n",
    "fwd = _ensure_ts_datetime_utc(fwd)\n",
    "\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta columna en forward: {c}\"\n",
    "\n",
    "_print_kv(\"sel_shape\", f\"{sel.height}x{sel.width}\")\n",
    "_print_kv(\"fwd_shape\", f\"{fwd.height}x{fwd.width}\")\n",
    "\n",
    "# ventanas TEST â†’ claves string\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "wf_map: dict[str, tuple[datetime, datetime]] = {}\n",
    "for idx, w in enumerate(windows):\n",
    "    wf_key = str(w.get(\"wf_id\", f\"wf_{idx:03d}\"))\n",
    "    t0 = datetime.fromisoformat(w[\"test\"][\"start\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    t1 = datetime.fromisoformat(w[\"test\"][\"end\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    wf_map[wf_key] = (t0, t1)\n",
    "_print_kv(\"windows_count\", len(wf_map))\n",
    "\n",
    "rows = []\n",
    "rng_global = np.random.default_rng(SEED_BASE)\n",
    "\n",
    "print(\"ğŸ§® Evaluando TEST con bootstrap por bloques ...\")\n",
    "for r in sel.to_dicts():\n",
    "    wf_id = str(r[\"wf_id\"])\n",
    "    entry = r[\"entry_col\"]\n",
    "    if wf_id not in wf_map:\n",
    "        print(f\"   âš ï¸ wf_id {wf_id} no encontrado en wf_windows; se salta.\")\n",
    "        continue\n",
    "    t0, t1 = wf_map[wf_id]\n",
    "\n",
    "    sub = (\n",
    "        fwd.filter((pl.col(\"entry_col\")==entry) & (pl.col(\"ts\")>=pl.lit(t0)) & (pl.col(\"ts\")<=pl.lit(t1)))\n",
    "           .select([\"ts\",\"label\",\"ret_event\"])\n",
    "           .sort(\"ts\")\n",
    "    )\n",
    "\n",
    "    n = sub.height\n",
    "    tp = int(sub.select(((pl.col(\"label\")==1).cast(pl.Int32)).sum().alias(\"__tp__\")).item()) if n>0 else 0\n",
    "    sl = int(sub.select(((pl.col(\"label\")==-1).cast(pl.Int32)).sum().alias(\"__sl__\")).item()) if n>0 else 0\n",
    "    timeout = int(sub.select(((pl.col(\"label\")==0).cast(pl.Int32)).sum().alias(\"__to__\")).item()) if n>0 else 0\n",
    "    ret_mean = float(sub.select(pl.col(\"ret_event\").mean()).item()) if n>0 else None\n",
    "    ret_std  = float(sub.select(pl.col(\"ret_event\").std()).item()) if n>0 else None\n",
    "\n",
    "    # Profit Factor\n",
    "    if n>0:\n",
    "        pos_sum = float(sub.filter(pl.col(\"ret_event\")>0).select(pl.col(\"ret_event\").sum()).item() or 0.0)\n",
    "        neg_sum = float(sub.filter(pl.col(\"ret_event\")<0).select(pl.col(\"ret_event\").sum()).item() or 0.0)\n",
    "        test_pf = (pos_sum / abs(neg_sum)) if abs(neg_sum) > 1e-12 else None\n",
    "    else:\n",
    "        test_pf = None\n",
    "\n",
    "    labels_np = sub.get_column(\"label\").to_numpy() if n>0 else np.array([], dtype=int)\n",
    "    n_pairs = int(tp + sl)\n",
    "    L_b_H = _estimate_block_len(labels_np)\n",
    "    n_eff = max(1, int(round(n_pairs / max(1, L_b_H))))\n",
    "\n",
    "    p_obs, p_lo, p_hi = _wilson_interval(tp, n_pairs)\n",
    "\n",
    "    ret_np = sub.get_column(\"ret_event\").to_numpy() if n>0 else np.array([], dtype=float)\n",
    "    mean_b, ci_lo, ci_hi, ok_pct = _block_bootstrap_mean(\n",
    "        ret_np, block_len=L_b_H, B=2000, rng=np.random.default_rng(SEED_BASE + (abs(hash(wf_id)) % 10_000))\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"wf_id\": wf_id,\n",
    "        \"entry_col\": entry,\n",
    "        \"rank_in_window\": r.get(\"rank_in_window\"),\n",
    "        \"cluster_id\": r.get(\"cluster_id\"),\n",
    "        \"cluster_size\": r.get(\"cluster_size\"),\n",
    "        \"test_n\": n,\n",
    "        \"tp\": tp,\n",
    "        \"sl\": sl,\n",
    "        \"timeout\": timeout,\n",
    "        \"hit_rate_tp_excl_to\": (tp / n_pairs) if n_pairs>0 else None,\n",
    "        \"ret_mean\": ret_mean,\n",
    "        \"ret_std\": ret_std,\n",
    "        \"profit_factor_stat\": test_pf,\n",
    "        \"L_b_H_test\": L_b_H,\n",
    "        \"n_eff_hit_test\": n_eff,\n",
    "        \"ev_boot_mean\": mean_b,\n",
    "        \"ev_boot_ci_lo\": ci_lo,\n",
    "        \"ev_boot_ci_hi\": ci_hi,\n",
    "        \"p_obs_wilson\": p_obs,\n",
    "        \"p_obs_ci_lo\": p_lo,\n",
    "        \"p_obs_ci_hi\": p_hi,\n",
    "        \"boot_ok_pct\": ok_pct,\n",
    "    })\n",
    "\n",
    "eval_df = pl.DataFrame(rows) if rows else pl.DataFrame({\"wf_id\": pl.Series([], dtype=pl.Utf8)})\n",
    "_print_kv(\"eval_rows\", eval_df.height); _print_kv(\"eval_cols\", eval_df.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_select_eval ...\")\n",
    "eval_df.write_parquet(OUT_PARQ)\n",
    "OUT_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"block_method\": \"nonoverlap\", \"block_bootstrap_B\": 2000,\n",
    "    \"acf_band\": \"Â±1.96/sqrt(n_pairs)\",\n",
    "    \"notes\": \"Wilson para p_obs; EV block-bootstrap; n_eff = n_pairs/L_b_H_test.\",\n",
    "    \"rows_head\": eval_df.head(50).to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header: w.writerow(header)\n",
    "    w.writerow([\n",
    "        datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        SYMBOL, TF, \"7d_v3_final\", eval_df.height, \"nonoverlap\", 2000,\n",
    "        \"eval TEST robusto (Wilson + block-bootstrap, wf_id string)\"\n",
    "    ])\n",
    "\n",
    "_print_kv(\"OUT_PARQ\", OUT_PARQ); _print_kv(\"OUT_JSON\", OUT_JSON)\n",
    "print(\"âœ… Celda 7d (v3 FINAL) completada.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "665e615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 7e v1.1) ...\n",
      "   - SCORES_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - SELECTED_PARQ                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - EVAL_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "   - train_shape                   : 36x17\n",
      "   - sel_shape                     : 36x5\n",
      "   - test_shape                    : 36x22\n",
      "ğŸ§® Calculando decay por ventana/entry_col ...\n",
      "   - decay_rows                    : 36\n",
      "   - decay_cols                    : 24\n",
      "ğŸ’¾ Guardando wf_decay ...\n",
      "ğŸ“ˆ Resumen global (ponderado por test_n si existe) ...\n",
      "   - DECAY_PARQ                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_BTCUSDT_15m.parquet\n",
      "   - DECAY_JSON                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_BTCUSDT_15m.json\n",
      "   - SUMMARY_JSON                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_summary_BTCUSDT_15m.json\n",
      "âœ… Celda 7e (v1.1 FINAL) completada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 7e (v1.1 FINAL): [DECAY TRAINâ†’TEST DE LOS SELECCIONADOS â†’ reports/wf_decay_*]\n",
    "# ------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: OPCIONAL. ColÃ³cala despuÃ©s de 7d y antes de 8b (no bloquea 9a/9b/9c).\n",
    "# Cambios clave (FIX):\n",
    "#   - wf_id como string.\n",
    "#   - Coalesce seguro de contadores: train_n â† (train_n | n_signals), test_n â† (test_n | n_signals).\n",
    "#   - Renombrados seguros (sÃ³lo si existen) y propagaciÃ³n de nulls.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "def _coalesce_alias(df: pl.DataFrame, target: str, candidates: list[str], dtype=None) -> pl.DataFrame:\n",
    "    exprs = [pl.col(c).cast(dtype) if (dtype and c in df.columns) else pl.col(c) for c in candidates if c in df.columns]\n",
    "    if exprs:\n",
    "        df = df.with_columns(pl.coalesce(exprs).alias(target))\n",
    "    return df\n",
    "\n",
    "def _safe_rename(df: pl.DataFrame, mapping: dict[str,str]) -> pl.DataFrame:\n",
    "    use = {k:v for k,v in mapping.items() if k in df.columns}\n",
    "    return df.rename(use) if use else df\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 7e v1.1) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCORES_PARQ   = REPORTS_DIR / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SELECTED_PARQ = REPORTS_DIR / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "EVAL_PARQ     = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "DECAY_PARQ    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.parquet\"\n",
    "DECAY_JSON    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.json\"\n",
    "SUMMARY_JSON  = REPORTS_DIR / f\"wf_decay_summary_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"SCORES_PARQ\", SCORES_PARQ)\n",
    "_print_kv(\"SELECTED_PARQ\", SELECTED_PARQ)\n",
    "_print_kv(\"EVAL_PARQ\", EVAL_PARQ)\n",
    "assert SCORES_PARQ.exists(),  f\"âŒ Falta {SCORES_PARQ}\"\n",
    "assert SELECTED_PARQ.exists(),f\"âŒ Falta {SELECTED_PARQ}\"\n",
    "assert EVAL_PARQ.exists(),    f\"âŒ Falta {EVAL_PARQ}\"\n",
    "\n",
    "train = pl.read_parquet(SCORES_PARQ)\n",
    "sel   = pl.read_parquet(SELECTED_PARQ)\n",
    "test  = pl.read_parquet(EVAL_PARQ)\n",
    "\n",
    "# wf_id como string\n",
    "for name, df in ((\"train\", train), (\"sel\", sel), (\"test\", test)):\n",
    "    if \"wf_id\" in df.columns and df[\"wf_id\"].dtype != pl.Utf8:\n",
    "        df = df.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "    locals()[name] = df\n",
    "\n",
    "_print_kv(\"train_shape\", f\"{train.height}x{train.width}\")\n",
    "_print_kv(\"sel_shape\",   f\"{sel.height}x{sel.width}\")\n",
    "_print_kv(\"test_shape\",  f\"{test.height}x{test.width}\")\n",
    "\n",
    "# ----------- Reducir columnas y normalizar nombres -----------\n",
    "\n",
    "# TRAIN: admite 'train_n' o 'n_signals'\n",
    "train_keep = [c for c in [\"wf_id\",\"entry_col\",\"train_n\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\"] if c in train.columns]\n",
    "train_k = train.select(train_keep)\n",
    "train_k = _coalesce_alias(train_k, \"train_n\", [\"train_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "train_k = _safe_rename(train_k, {\n",
    "    \"ret_mean\": \"train_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\": \"train_hit_excl\",\n",
    "    \"profit_factor_stat\": \"train_pf\",\n",
    "})\n",
    "\n",
    "# TEST (7d): admite 'test_n' o 'n_signals'\n",
    "test_keep = [c for c in [\n",
    "    \"wf_id\",\"entry_col\",\"test_n\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\n",
    "    \"profit_factor_stat\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"\n",
    "] if c in test.columns]\n",
    "test_k = test.select(test_keep)\n",
    "test_k = _coalesce_alias(test_k, \"test_n\", [\"test_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "test_k = _safe_rename(test_k, {\n",
    "    \"ret_mean\": \"test_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\": \"test_hit_excl\",\n",
    "    \"profit_factor_stat\": \"test_pf\",\n",
    "})\n",
    "\n",
    "# Seleccionados\n",
    "sel_k = sel.select([c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in sel.columns]).unique()\n",
    "\n",
    "print(\"ğŸ§® Calculando decay por ventana/entry_col ...\")\n",
    "decay = (\n",
    "    sel_k\n",
    "    .join(train_k, on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "    .join(test_k,  on=[\"wf_id\",\"entry_col\"], how=\"left\", suffix=\"_dup\")\n",
    "    .with_columns(\n",
    "        # Deltas absolutos\n",
    "        (pl.col(\"test_ret_mean\") - pl.col(\"train_ret_mean\")).alias(\"delta_ret_mean\"),\n",
    "        (pl.col(\"test_hit_excl\") - pl.col(\"train_hit_excl\")).alias(\"delta_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")       - pl.col(\"train_pf\")).alias(\"delta_pf\"),\n",
    "        # Ratios (seguros ante 0/NA)\n",
    "        (pl.col(\"test_ret_mean\").abs() / _safe_pos(pl.col(\"train_ret_mean\").abs(), 1e-12)).alias(\"ratio_ret_mean_abs\"),\n",
    "        (pl.col(\"test_hit_excl\")       / _safe_pos(pl.col(\"train_hit_excl\"), 1e-12)).alias(\"ratio_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")             / _safe_pos(pl.col(\"train_pf\"), 1e-12)).alias(\"ratio_pf\"),\n",
    "        # SeÃ±ales totales vistas (usa coalesce seguro)\n",
    "        (pl.col(\"train_n\").fill_null(0) + pl.col(\"test_n\").fill_null(0)).alias(\"n_total_seen\"),\n",
    "    )\n",
    "    .sort([\"wf_id\",\"rank_in_window\",\"entry_col\"])\n",
    ")\n",
    "\n",
    "_print_kv(\"decay_rows\", decay.height); _print_kv(\"decay_cols\", decay.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_decay ...\")\n",
    "decay.write_parquet(DECAY_PARQ)\n",
    "Path(DECAY_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rows_head\": decay.head(50).to_dicts(),\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"ğŸ“ˆ Resumen global (ponderado por test_n si existe) ...\")\n",
    "has_test_n = \"test_n\" in decay.columns\n",
    "w = (pl.col(\"test_n\").fill_null(0).cast(pl.Float64())) if has_test_n else pl.lit(1.0)\n",
    "\n",
    "summary = (\n",
    "    decay.with_columns(w.alias(\"__w__\"))\n",
    "         .select(\n",
    "            ((pl.col(\"delta_ret_mean\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_ret_mean\"),\n",
    "            ((pl.col(\"delta_hit_excl\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_hit_excl\"),\n",
    "            ((pl.col(\"delta_pf\")       * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_pf\"),\n",
    "            ((pl.col(\"ratio_ret_mean_abs\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_ret_mean_abs\"),\n",
    "            ((pl.col(\"ratio_hit_excl\")     * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_hit_excl\"),\n",
    "            ((pl.col(\"ratio_pf\")           * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_pf\"),\n",
    "         )\n",
    ")\n",
    "\n",
    "rank = (\n",
    "    decay\n",
    "    .select(\n",
    "        \"wf_id\",\"entry_col\",\"rank_in_window\",\n",
    "        \"train_ret_mean\",\"test_ret_mean\",\"delta_ret_mean\",\"ratio_ret_mean_abs\",\n",
    "        \"train_hit_excl\",\"test_hit_excl\",\"delta_hit_excl\",\"ratio_hit_excl\",\n",
    "        \"train_pf\",\"test_pf\",\"delta_pf\",\"ratio_pf\",\n",
    "        \"train_n\",\"test_n\",\"n_total_seen\"\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"ratio_pf\").fill_null(0.0).alias(\"_ratio_pf_\"),\n",
    "        pl.col(\"ratio_hit_excl\").fill_null(0.0).alias(\"_ratio_hit_\"),\n",
    "        pl.col(\"delta_ret_mean\").abs().fill_null(0.0).alias(\"_abs_dret_\")\n",
    "    )\n",
    "    .with_columns(((pl.col(\"_ratio_pf_\")+pl.col(\"_ratio_hit_\"))/2.0 - pl.col(\"_abs_dret_\")).alias(\"robustness_score\"))\n",
    "    .drop([\"_ratio_pf_\",\"_ratio_hit_\",\"_abs_dret_\"])\n",
    "    .sort([\"wf_id\",\"robustness_score\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "Path(SUMMARY_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"global_summary\": (summary.to_dicts()[0] if summary.height>0 else {}),\n",
    "    \"top5_by_robustness\": rank.sort(\"robustness_score\", descending=True).to_dicts()[:5],\n",
    "    \"bottom5_by_robustness\": rank.sort(\"robustness_score\", descending=False).to_dicts()[:5],\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"DECAY_PARQ\", DECAY_PARQ)\n",
    "_print_kv(\"DECAY_JSON\", DECAY_JSON)\n",
    "_print_kv(\"SUMMARY_JSON\", SUMMARY_JSON)\n",
    "print(\"âœ… Celda 7e (v1.1 FINAL) completada.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2a6b5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 8 surfaces) ...\n",
      "   - CONFIG_PATH                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\config_BTCUSDT_15m.json\n",
      "   - REPORTS_DIR                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\n",
      "   - STATES_PARQ                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\states_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando states ...\n",
      "   - states_rows                       : 567\n",
      "   - states_cols                       : 19\n",
      "   - state_types                       : ['year', 'density', 'session', 'trend_er', 'dow', 'month', 'vol_bin', 'hour']\n",
      "   - metrics_detected                  : ['n_signals', 'tp', 'sl', 'timeout', 'ret_mean', 'ret_median', 'ret_std', 'ret_q05', 'ret_q25', 'ret_q50', 'ret_q75', 'ret_q95', 'bars_mean', 'bars_median', 'hit_rate_tp_incl_to', 'hit_rate_tp_excl_to']\n",
      "ğŸ—ºï¸  Generando superficies por state_type ...\n",
      "   - [year] rows                       : 45\n",
      "   - [year] saved_long                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_year.parquet\n",
      "   - [year] wide_ret_mean_shape        : 9x6\n",
      "   - [year] saved_wide_ret             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_year.parquet\n",
      "   - [year] wide_hit_excl_shape        : 9x6\n",
      "   - [year] saved_wide_hit             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_year.parquet\n",
      "   - [year] stability_rows             : 9\n",
      "   - [year] saved_stability            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_year.parquet\n",
      "   - [density] rows                    : 36\n",
      "   - [density] saved_long              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_density.parquet\n",
      "   - [density] wide_ret_mean_shape     : 9x5\n",
      "   - [density] saved_wide_ret          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_density.parquet\n",
      "   - [density] wide_hit_excl_shape     : 9x5\n",
      "   - [density] saved_wide_hit          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_density.parquet\n",
      "   - [density] stability_rows          : 9\n",
      "   - [density] saved_stability         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_density.parquet\n",
      "   - [session] rows                    : 27\n",
      "   - [session] saved_long              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_session.parquet\n",
      "   - [session] wide_ret_mean_shape     : 9x4\n",
      "   - [session] saved_wide_ret          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_session.parquet\n",
      "   - [session] wide_hit_excl_shape     : 9x4\n",
      "   - [session] saved_wide_hit          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_session.parquet\n",
      "   - [session] stability_rows          : 9\n",
      "   - [session] saved_stability         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_session.parquet\n",
      "   - [trend_er] rows                   : 36\n",
      "   - [trend_er] saved_long             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_trend_er.parquet\n",
      "   - [trend_er] wide_ret_mean_shape    : 9x5\n",
      "   - [trend_er] saved_wide_ret         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_trend_er.parquet\n",
      "   - [trend_er] wide_hit_excl_shape    : 9x5\n",
      "   - [trend_er] saved_wide_hit         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_trend_er.parquet\n",
      "   - [trend_er] stability_rows         : 9\n",
      "   - [trend_er] saved_stability        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_trend_er.parquet\n",
      "   - [dow] rows                        : 63\n",
      "   - [dow] saved_long                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_dow.parquet\n",
      "   - [dow] wide_ret_mean_shape         : 9x8\n",
      "   - [dow] saved_wide_ret              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_dow.parquet\n",
      "   - [dow] wide_hit_excl_shape         : 9x8\n",
      "   - [dow] saved_wide_hit              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_dow.parquet\n",
      "   - [dow] stability_rows              : 9\n",
      "   - [dow] saved_stability             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_dow.parquet\n",
      "   - [month] rows                      : 108\n",
      "   - [month] saved_long                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_month.parquet\n",
      "   - [month] wide_ret_mean_shape       : 9x13\n",
      "   - [month] saved_wide_ret            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_month.parquet\n",
      "   - [month] wide_hit_excl_shape       : 9x13\n",
      "   - [month] saved_wide_hit            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_month.parquet\n",
      "   - [month] stability_rows            : 9\n",
      "   - [month] saved_stability           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_month.parquet\n",
      "   - [vol_bin] rows                    : 36\n",
      "   - [vol_bin] saved_long              : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_vol_bin.parquet\n",
      "   - [vol_bin] wide_ret_mean_shape     : 9x5\n",
      "   - [vol_bin] saved_wide_ret          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_vol_bin.parquet\n",
      "   - [vol_bin] wide_hit_excl_shape     : 9x5\n",
      "   - [vol_bin] saved_wide_hit          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_vol_bin.parquet\n",
      "   - [vol_bin] stability_rows          : 9\n",
      "   - [vol_bin] saved_stability         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_vol_bin.parquet\n",
      "   - [hour] rows                       : 216\n",
      "   - [hour] saved_long                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_long_BTCUSDT_15m_hour.parquet\n",
      "   - [hour] wide_ret_mean_shape        : 9x25\n",
      "   - [hour] saved_wide_ret             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_ret_mean_BTCUSDT_15m_hour.parquet\n",
      "   - [hour] wide_hit_excl_shape        : 9x25\n",
      "   - [hour] saved_wide_hit             : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_wide_hit_excl_BTCUSDT_15m_hour.parquet\n",
      "   - [hour] stability_rows             : 9\n",
      "   - [hour] saved_stability            : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_stability_BTCUSDT_15m_hour.parquet\n",
      "âœ… Celda 8 (surfaces v1 FIX) finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 8 (surfaces v1 FIX): [SUPERFICIES/HEATMAPS DE ESTABILIDAD POR ESTADO â†’ reports/surfaces_*]\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# - Entrada: reports/states_{SYMBOL}_{TF}.parquet (deriva de Celda 6).\n",
    "# - Objetivo: generar superficies (long y wide) por cada state_type (year, month, dow, hour, session, vol_bin),\n",
    "#             con mÃ©tricas clave: n_signals, ret_mean, ret_median, ret_std, hit_rate_tp_incl_to,\n",
    "#             hit_rate_tp_excl_to, profit_factor_stat, etc. (solo si existen en el dataset).\n",
    "# - Salida:\n",
    "#     reports/surfaces_long_{SYMBOL}_{TF}_{state}.parquet / .csv\n",
    "#     reports/surfaces_wide_ret_mean_{SYMBOL}_{TF}_{state}.parquet / .csv\n",
    "#     reports/surfaces_wide_hit_excl_{SYMBOL}_{TF}_{state}.parquet / .csv\n",
    "#     reports/surfaces_stability_{SYMBOL}_{TF}_{state}.parquet / .csv   (dispersion intra-estado por entry_col)\n",
    "# - Notas:\n",
    "#     * Mantiene rutas y nombres originales (drop-in replacement).\n",
    "#     * Corrige DeprecationWarning de pivot: usa on=\"state_value\".\n",
    "#     * Prints exhaustivos para auditorÃ­a.\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v):\n",
    "    print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _save(df: pl.DataFrame, base: Path):\n",
    "    df.write_parquet(base.with_suffix(\".parquet\"))\n",
    "    # CSV auxiliar (siempre best-effort)\n",
    "    try:\n",
    "        df.write_csv(base.with_suffix(\".csv\"))\n",
    "    except Exception as e:\n",
    "        _print_kv(\"csv_warn\", f\"{base.name}: {e}\")\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 8 surfaces) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF     = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "PATHS       = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STATES_PARQ = REPORTS_DIR / f\"states_{SYMBOL}_{TF}.parquet\"\n",
    "assert STATES_PARQ.exists(), f\"âŒ No existe states: {STATES_PARQ}\"\n",
    "\n",
    "_print_kv(\"CONFIG_PATH\", CONFIG_PATH)\n",
    "_print_kv(\"REPORTS_DIR\", REPORTS_DIR)\n",
    "_print_kv(\"STATES_PARQ\", STATES_PARQ)\n",
    "\n",
    "print(\"ğŸ“¦ Cargando states ...\")\n",
    "st = pl.read_parquet(STATES_PARQ)\n",
    "\n",
    "# Chequeos mÃ­nimos\n",
    "required_cols = [\"entry_col\",\"state_type\",\"state_value\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\"]\n",
    "for c in required_cols:\n",
    "    assert c in st.columns, f\"âŒ Falta columna en states: {c}\"\n",
    "\n",
    "_print_kv(\"states_rows\", st.height)\n",
    "_print_kv(\"states_cols\", st.width)\n",
    "\n",
    "# Lista de tipos de estado disponibles\n",
    "state_types = st[\"state_type\"].unique().to_list()\n",
    "_print_kv(\"state_types\", state_types)\n",
    "\n",
    "# Columnas mÃ©tricas candidatas (guardamos solo las que existan)\n",
    "CAND_METRICS = [\n",
    "    \"n_signals\",\"tp\",\"sl\",\"timeout\",\n",
    "    \"ret_mean\",\"ret_median\",\"ret_std\",\"ret_q05\",\"ret_q25\",\"ret_q50\",\"ret_q75\",\"ret_q95\",\n",
    "    \"bars_mean\",\"bars_median\",\n",
    "    \"ret_mean_tp\",\"ret_mean_sl\",\"ret_mean_timeout\",\n",
    "    \"sum_ret_tp\",\"sum_abs_ret_sl\",\n",
    "    \"hit_rate_tp_incl_to\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\"\n",
    "]\n",
    "METRICS = [c for c in CAND_METRICS if c in st.columns]\n",
    "_print_kv(\"metrics_detected\", METRICS)\n",
    "\n",
    "print(\"ğŸ—ºï¸  Generando superficies por state_type ...\")\n",
    "for s in state_types:\n",
    "    sub = (\n",
    "        st.filter(pl.col(\"state_type\")==s)\n",
    "          .with_columns(pl.col(\"state_value\").cast(pl.Utf8))\n",
    "    )\n",
    "    _print_kv(f\"[{s}] rows\", sub.height)\n",
    "\n",
    "    # Long (ya estÃ¡ long, pero filtramos columnas relevantes y ordenamos)\n",
    "    long_cols = [\"entry_col\",\"state_type\",\"state_value\"] + METRICS\n",
    "    long_cols = [c for c in long_cols if c in sub.columns]\n",
    "    long_df = sub.select(long_cols).sort([\"entry_col\",\"state_value\"])\n",
    "    base_long = REPORTS_DIR / f\"surfaces_long_{SYMBOL}_{TF}_{s}\"\n",
    "    _save(long_df, base_long)\n",
    "    _print_kv(f\"[{s}] saved_long\", str(base_long.with_suffix('.parquet')))\n",
    "\n",
    "    # Wide (ret_mean)\n",
    "    if \"ret_mean\" in sub.columns:\n",
    "        wide_ret = (\n",
    "            sub\n",
    "            .select([\"entry_col\",\"state_value\",\"ret_mean\"])\n",
    "            .pivot(values=\"ret_mean\", index=\"entry_col\", on=\"state_value\", aggregate_function=\"first\")\n",
    "            .sort(\"entry_col\")\n",
    "        )\n",
    "        base_wide_ret = REPORTS_DIR / f\"surfaces_wide_ret_mean_{SYMBOL}_{TF}_{s}\"\n",
    "        _save(wide_ret, base_wide_ret)\n",
    "        _print_kv(f\"[{s}] wide_ret_mean_shape\", f\"{wide_ret.height}x{wide_ret.width}\")\n",
    "        _print_kv(f\"[{s}] saved_wide_ret\", str(base_wide_ret.with_suffix('.parquet')))\n",
    "\n",
    "    # Wide (hit_rate_tp_excl_to)\n",
    "    if \"hit_rate_tp_excl_to\" in sub.columns:\n",
    "        wide_hit = (\n",
    "            sub\n",
    "            .select([\"entry_col\",\"state_value\",\"hit_rate_tp_excl_to\"])\n",
    "            .pivot(values=\"hit_rate_tp_excl_to\", index=\"entry_col\", on=\"state_value\", aggregate_function=\"first\")\n",
    "            .sort(\"entry_col\")\n",
    "        )\n",
    "        base_wide_hit = REPORTS_DIR / f\"surfaces_wide_hit_excl_{SYMBOL}_{TF}_{s}\"\n",
    "        _save(wide_hit, base_wide_hit)\n",
    "        _print_kv(f\"[{s}] wide_hit_excl_shape\", f\"{wide_hit.height}x{wide_hit.width}\")\n",
    "        _print_kv(f\"[{s}] saved_wide_hit\", str(base_wide_hit.with_suffix('.parquet')))\n",
    "\n",
    "    # Estabilidad: dispersiÃ³n intra-estado por entry_col (std y rango de ret_mean)\n",
    "    if \"ret_mean\" in sub.columns:\n",
    "        stab = (\n",
    "            sub.group_by(\"entry_col\")\n",
    "               .agg(\n",
    "                   pl.len().alias(\"states_count\"),\n",
    "                   pl.col(\"ret_mean\").mean().alias(\"ret_mean_avg\"),\n",
    "                   pl.col(\"ret_mean\").std().alias(\"ret_mean_std\"),\n",
    "                   (pl.col(\"ret_mean\").max() - pl.col(\"ret_mean\").min()).alias(\"ret_mean_range\"),\n",
    "               )\n",
    "               .sort([\"ret_mean_std\",\"ret_mean_range\"])\n",
    "        )\n",
    "        base_stab = REPORTS_DIR / f\"surfaces_stability_{SYMBOL}_{TF}_{s}\"\n",
    "        _save(stab, base_stab)\n",
    "        _print_kv(f\"[{s}] stability_rows\", stab.height)\n",
    "        _print_kv(f\"[{s}] saved_stability\", str(base_stab.with_suffix('.parquet')))\n",
    "\n",
    "print(\"âœ… Celda 8 (surfaces v1 FIX) finalizada sin errores.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "980f6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 8b v2.1 FINAL FIX) ...\n",
      "   - TRAIN_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - SEL_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - TEST_PARQ                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "ğŸ”— Join (seleccionados âˆ© train âˆ© test) ...\n",
      "ğŸ§® CalibraciÃ³n por CUARTILES de score_train ...\n",
      "   - score_q25                         : -0.0003192229613169593\n",
      "   - score_q50                         : 0.0002411707977396477\n",
      "   - score_q75                         : 0.0006464070960267863\n",
      "   - ECE_score_quart                   : 0.015963611324844335\n",
      "   - Brier_avg_score_quart             : 0.21699351563500063\n",
      "ğŸ§® CalibraciÃ³n por TERCILES de delta1 := test_ret_mean - train_ret_mean ...\n",
      "   - delta1_t33                        : -0.0006883242693972503\n",
      "   - delta1_t66                        : 0.00014105747682573866\n",
      "   - ECE_delta1_terc                   : 0.025811519660562138\n",
      "   - Brier_avg_delta1_terc             : 0.2148212763258047\n",
      "âš¡ Potencia (hitrate) con n_eff_test ...\n",
      "ğŸ’¾ Guardando calibraciones y potencia ...\n",
      "   - CAL_Q_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\calib_score_quart_BTCUSDT_15m.parquet\n",
      "   - CAL_Q_JSON                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\calib_score_quart_BTCUSDT_15m.json\n",
      "   - CAL_D_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\calib_delta1_terc_BTCUSDT_15m.parquet\n",
      "   - CAL_D_JSON                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\calib_delta1_terc_BTCUSDT_15m.json\n",
      "   - PWR_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\power_hitrate_BTCUSDT_15m.parquet\n",
      "   - PWR_JSON                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\power_hitrate_BTCUSDT_15m.json\n",
      "âœ… Celda 8b (v2.1 FINAL FIX) finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 8b (v2.1 FINAL FIX): [CALIBRACIÃ“N y POTENCIA con n_eff_test]\n",
    "# ---------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: DESPUÃ‰S de 7d (y 7e si la usas) y ANTES de 9b.\n",
    "# Cambios clave:\n",
    "#   - Renombrado SEGURO (sÃ³lo si existe) + coalesce:\n",
    "#       train_n â† coalesce(train.train_n, train.n_signals)\n",
    "#       test_n  â† coalesce(test.test_n,  test.n_signals)\n",
    "#   - wf_id como string para consistencia.\n",
    "#   - CÃ¡lculo de n_pairs y n_eff_used en pasos separados.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _norm_cdf(z: float) -> float:\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "def _power_two_sided(p0: float, delta: float, n_eff: float, alpha: float = 0.05) -> float:\n",
    "    p0 = min(max(float(p0), 1e-9), 1-1e-9)\n",
    "    n_eff = max(1.0, float(n_eff))\n",
    "    se0 = math.sqrt(p0*(1-p0)/n_eff)\n",
    "    zcrit = 1.9599639845400538  # ~N^{-1}(1-alpha/2)\n",
    "    ncp = abs(delta) / max(se0, 1e-12)\n",
    "    return _norm_cdf(-zcrit - ncp) + (1.0 - _norm_cdf(zcrit - ncp))\n",
    "\n",
    "def _safe_rename(df: pl.DataFrame, mapping: dict[str, str]) -> pl.DataFrame:\n",
    "    use = {k: v for k, v in mapping.items() if k in df.columns}\n",
    "    return df.rename(use) if use else df\n",
    "\n",
    "def _coalesce_alias(df: pl.DataFrame, target: str, candidates: list[str], dtype=None) -> pl.DataFrame:\n",
    "    exprs = [pl.col(c).cast(dtype) if (dtype and c in df.columns) else pl.col(c)\n",
    "             for c in candidates if c in df.columns]\n",
    "    if exprs:\n",
    "        df = df.with_columns(pl.coalesce(exprs).alias(target))\n",
    "    return df\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 8b v2.1 FINAL FIX) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PARQ = REPORTS_DIR / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ   = REPORTS_DIR / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "TEST_PARQ  = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "CAL_Q_PARQ = REPORTS_DIR / f\"calib_score_quart_{SYMBOL}_{TF}.parquet\"\n",
    "CAL_Q_JSON = REPORTS_DIR / f\"calib_score_quart_{SYMBOL}_{TF}.json\"\n",
    "CAL_D_PARQ = REPORTS_DIR / f\"calib_delta1_terc_{SYMBOL}_{TF}.parquet\"\n",
    "CAL_D_JSON = REPORTS_DIR / f\"calib_delta1_terc_{SYMBOL}_{TF}.json\"\n",
    "PWR_PARQ   = REPORTS_DIR / f\"power_hitrate_{SYMBOL}_{TF}.parquet\"\n",
    "PWR_JSON   = REPORTS_DIR / f\"power_hitrate_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"TRAIN_PARQ\", TRAIN_PARQ); _print_kv(\"SEL_PARQ\", SEL_PARQ); _print_kv(\"TEST_PARQ\", TEST_PARQ)\n",
    "assert TRAIN_PARQ.exists(), f\"âŒ Falta {TRAIN_PARQ}\"\n",
    "assert SEL_PARQ.exists(),   f\"âŒ Falta {SEL_PARQ}\"\n",
    "assert TEST_PARQ.exists(),  f\"âŒ Falta {TEST_PARQ}\"\n",
    "\n",
    "train = pl.read_parquet(TRAIN_PARQ)\n",
    "sel   = pl.read_parquet(SEL_PARQ)\n",
    "test  = pl.read_parquet(TEST_PARQ)\n",
    "\n",
    "# wf_id como string\n",
    "for name, df in ((\"train\", train), (\"sel\", sel), (\"test\", test)):\n",
    "    if \"wf_id\" in df.columns and df[\"wf_id\"].dtype != pl.Utf8:\n",
    "        df = df.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "    locals()[name] = df\n",
    "\n",
    "# -------------------- TRAIN --------------------\n",
    "train_keep = [c for c in [\n",
    "    \"wf_id\",\"entry_col\",\"train_n\",\"n_signals\",\"hit_rate_tp_excl_to\",\n",
    "    \"ret_mean\",\"ret_std\",\"score_train\",\"p_pred_train\"\n",
    "] if c in train.columns]\n",
    "train_k = train.select(train_keep)\n",
    "# p_pred_train â† coalesce(p_pred_train, hit_rate_tp_excl_to), fallback 0.5 si ninguna existe\n",
    "if \"p_pred_train\" not in train_k.columns:\n",
    "    if \"hit_rate_tp_excl_to\" in train_k.columns:\n",
    "        train_k = _safe_rename(train_k, {\"hit_rate_tp_excl_to\": \"p_pred_train\"})\n",
    "    else:\n",
    "        train_k = train_k.with_columns(pl.lit(0.5).alias(\"p_pred_train\"))\n",
    "# train_n â† coalesce(train_n, n_signals)\n",
    "train_k = _coalesce_alias(train_k, \"train_n\", [\"train_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "# renombres mÃ©tricas\n",
    "train_k = _safe_rename(train_k, {\"ret_mean\": \"train_ret_mean\", \"ret_std\": \"train_ret_std\"})\n",
    "\n",
    "# -------------------- SELECTED --------------------\n",
    "sel_k = sel.select([c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in sel.columns]).unique()\n",
    "\n",
    "# -------------------- TEST (7d) --------------------\n",
    "test_keep = [c for c in [\n",
    "    \"wf_id\",\"entry_col\",\"test_n\",\"n_signals\",\"tp\",\"sl\",\"timeout\",\n",
    "    \"ret_mean\",\"ret_std\",\"n_eff_hit_test\"\n",
    "] if c in test.columns]\n",
    "test_k = test.select(test_keep)\n",
    "# test_n â† coalesce(test_n, n_signals)\n",
    "test_k = _coalesce_alias(test_k, \"test_n\", [\"test_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "# renombres mÃ©tricas\n",
    "test_k = _safe_rename(test_k, {\"ret_mean\": \"test_ret_mean\", \"ret_std\": \"test_ret_std\"})\n",
    "\n",
    "print(\"ğŸ”— Join (seleccionados âˆ© train âˆ© test) ...\")\n",
    "df = (sel_k.join(train_k, on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "          .join(test_k,  on=[\"wf_id\",\"entry_col\"], how=\"left\", suffix=\"_dup\"))\n",
    "\n",
    "# ---------------- CalibraciÃ³n por CUARTILES de score_train ----------------\n",
    "print(\"ğŸ§® CalibraciÃ³n por CUARTILES de score_train ...\")\n",
    "if \"score_train\" not in df.columns or df.select(pl.col(\"score_train\").is_not_null().sum()).item() == 0:\n",
    "    calib_q = pl.DataFrame({\"bucket\": pl.Series([], dtype=pl.Utf8)})\n",
    "    _print_kv(\"warn\", \"Falta score_train â†’ calib vacÃ­a\")\n",
    "else:\n",
    "    df_q = df.filter(pl.col(\"score_train\").is_not_null())\n",
    "    q25 = float(df_q.select(pl.col(\"score_train\").quantile(0.25)).item())\n",
    "    q50 = float(df_q.select(pl.col(\"score_train\").quantile(0.50)).item())\n",
    "    q75 = float(df_q.select(pl.col(\"score_train\").quantile(0.75)).item())\n",
    "    _print_kv(\"score_q25\", q25); _print_kv(\"score_q50\", q50); _print_kv(\"score_q75\", q75)\n",
    "\n",
    "    df_b = df_q.with_columns(\n",
    "        pl.when(pl.col(\"score_train\") <= q25).then(pl.lit(\"Q1\"))\n",
    "         .when(pl.col(\"score_train\") <= q50).then(pl.lit(\"Q2\"))\n",
    "         .when(pl.col(\"score_train\") <= q75).then(pl.lit(\"Q3\"))\n",
    "         .otherwise(pl.lit(\"Q4\")).alias(\"score_quart\"),\n",
    "        (pl.col(\"tp\").fill_null(0) + pl.col(\"sl\").fill_null(0)).alias(\"__w__\"),\n",
    "        (pl.col(\"tp\").fill_null(0) * (1.0 - pl.col(\"p_pred_train\").fill_null(0.5))**2\n",
    "         + pl.col(\"sl\").fill_null(0) * (pl.col(\"p_pred_train\").fill_null(0.5))**2).alias(\"__brier_num__\"),\n",
    "    )\n",
    "\n",
    "    calib_q = (\n",
    "        df_b.group_by(\"score_quart\")\n",
    "            .agg(\n",
    "                pl.col(\"__w__\").sum().alias(\"support_excl_to\"),\n",
    "                pl.col(\"tp\").sum().alias(\"tp_sum\"),\n",
    "                pl.col(\"sl\").sum().alias(\"sl_sum\"),\n",
    "                ((pl.col(\"p_pred_train\").fill_null(0.5) * pl.col(\"__w__\")).sum()\n",
    "                 / pl.when(pl.col(\"__w__\").sum()<=0).then(1.0).otherwise(pl.col(\"__w__\").sum())).alias(\"p_pred_bucket\"),\n",
    "                (pl.col(\"__brier_num__\").sum()\n",
    "                 / pl.when(pl.col(\"__w__\").sum()<=0).then(1.0).otherwise(pl.col(\"__w__\").sum())).alias(\"brier_bucket\"),\n",
    "                pl.col(\"score_train\").mean().alias(\"score_train_mean\"),\n",
    "                pl.col(\"p_pred_train\").mean().alias(\"p_pred_mean_unw\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col(\"tp_sum\")+pl.col(\"sl_sum\")).alias(\"n_pairs\"),\n",
    "                (pl.col(\"tp_sum\") / pl.when((pl.col(\"tp_sum\")+pl.col(\"sl_sum\"))<=0).then(1.0)\n",
    "                 .otherwise(pl.col(\"tp_sum\")+pl.col(\"sl_sum\"))).alias(\"p_obs_bucket\"),\n",
    "            )\n",
    "            .sort(\"score_quart\")\n",
    "            .rename({\"score_quart\":\"bucket\"})\n",
    "    )\n",
    "\n",
    "    total_support = float(calib_q.select(pl.col(\"n_pairs\").sum()).item())\n",
    "    if total_support > 0:\n",
    "        ECE = sum(\n",
    "            (float(r[\"n_pairs\"])/total_support) * abs(float(r[\"p_obs_bucket\"]) - float(r[\"p_pred_bucket\"]))\n",
    "            for r in calib_q.to_dicts()\n",
    "        )\n",
    "    else:\n",
    "        ECE = None\n",
    "    _print_kv(\"ECE_score_quart\", ECE if ECE is not None else \"N/A\")\n",
    "    _print_kv(\"Brier_avg_score_quart\", float(calib_q[\"brier_bucket\"].mean()) if calib_q.height>0 else \"N/A\")\n",
    "\n",
    "# ---------------- CalibraciÃ³n por TERCILES de delta1 ----------------\n",
    "print(\"ğŸ§® CalibraciÃ³n por TERCILES de delta1 := test_ret_mean - train_ret_mean ...\")\n",
    "if (\"test_ret_mean\" not in df.columns) or (\"train_ret_mean\" not in df.columns) \\\n",
    "   or df.select(pl.col(\"test_ret_mean\").is_not_null().sum()).item()==0 \\\n",
    "   or df.select(pl.col(\"train_ret_mean\").is_not_null().sum()).item()==0:\n",
    "    calib_d = pl.DataFrame({\"bucket\": pl.Series([], dtype=pl.Utf8)})\n",
    "    _print_kv(\"warn\", \"Faltan ret_mean train/test â†’ calib_delta vacÃ­a\")\n",
    "else:\n",
    "    df_dsrc = df.filter(pl.col(\"test_ret_mean\").is_not_null() & pl.col(\"train_ret_mean\").is_not_null()) \\\n",
    "                .with_columns(\n",
    "                    (pl.col(\"test_ret_mean\") - pl.col(\"train_ret_mean\")).alias(\"delta1\"),\n",
    "                    (pl.col(\"tp\").fill_null(0) + pl.col(\"sl\").fill_null(0)).alias(\"__w__\"),\n",
    "                    (pl.col(\"tp\").fill_null(0) * (1.0 - pl.col(\"p_pred_train\").fill_null(0.5))**2\n",
    "                     + pl.col(\"sl\").fill_null(0) * (pl.col(\"p_pred_train\").fill_null(0.5))**2).alias(\"__brier_num__\"),\n",
    "                )\n",
    "    if df_dsrc.height > 0:\n",
    "        t33 = float(df_dsrc.select(pl.col(\"delta1\").quantile(0.3333333333)).item())\n",
    "        t66 = float(df_dsrc.select(pl.col(\"delta1\").quantile(0.6666666667)).item())\n",
    "    else:\n",
    "        t33 = t66 = 0.0\n",
    "    _print_kv(\"delta1_t33\", t33); _print_kv(\"delta1_t66\", t66)\n",
    "\n",
    "    df_db = df_dsrc.with_columns(\n",
    "        pl.when(pl.col(\"delta1\") <= t33).then(pl.lit(\"T1\"))\n",
    "         .when(pl.col(\"delta1\") <= t66).then(pl.lit(\"T2\"))\n",
    "         .otherwise(pl.lit(\"T3\")).alias(\"delta1_terc\")\n",
    "    )\n",
    "\n",
    "    calib_d = (\n",
    "        df_db.group_by(\"delta1_terc\")\n",
    "             .agg(\n",
    "                 pl.col(\"__w__\").sum().alias(\"support_excl_to\"),\n",
    "                 pl.col(\"tp\").sum().alias(\"tp_sum\"),\n",
    "                 pl.col(\"sl\").sum().alias(\"sl_sum\"),\n",
    "                 ((pl.col(\"p_pred_train\").fill_null(0.5) * pl.col(\"__w__\")).sum()\n",
    "                  / pl.when(pl.col(\"__w__\").sum()<=0).then(1.0).otherwise(pl.col(\"__w__\").sum())).alias(\"p_pred_bucket\"),\n",
    "                 (pl.col(\"__brier_num__\").sum()\n",
    "                  / pl.when(pl.col(\"__w__\").sum()<=0).then(1.0).otherwise(pl.col(\"__w__\").sum())).alias(\"brier_bucket\"),\n",
    "                 pl.col(\"delta1\").mean().alias(\"delta1_mean\"),\n",
    "                 pl.col(\"p_pred_train\").mean().alias(\"p_pred_mean_unw\"),\n",
    "             )\n",
    "             .with_columns(\n",
    "                 (pl.col(\"tp_sum\")+pl.col(\"sl_sum\")).alias(\"n_pairs\"),\n",
    "                 (pl.col(\"tp_sum\") / pl.when((pl.col(\"tp_sum\")+pl.col(\"sl_sum\"))<=0).then(1.0)\n",
    "                  .otherwise(pl.col(\"tp_sum\")+pl.col(\"sl_sum\"))).alias(\"p_obs_bucket\"),\n",
    "             )\n",
    "             .sort(\"delta1_terc\")\n",
    "             .rename({\"delta1_terc\":\"bucket\"})\n",
    "    )\n",
    "\n",
    "    total_support_d = float(calib_d.select(pl.col(\"n_pairs\").sum()).item())\n",
    "    if total_support_d > 0:\n",
    "        ECE_d = sum(\n",
    "            (float(r[\"n_pairs\"])/total_support_d) * abs(float(r[\"p_obs_bucket\"]) - float(r[\"p_pred_bucket\"]))\n",
    "            for r in calib_d.to_dicts()\n",
    "        )\n",
    "    else:\n",
    "        ECE_d = None\n",
    "    _print_kv(\"ECE_delta1_terc\", ECE_d if ECE_d is not None else \"N/A\")\n",
    "    _print_kv(\"Brier_avg_delta1_terc\", float(calib_d[\"brier_bucket\"].mean()) if calib_d.height>0 else \"N/A\")\n",
    "\n",
    "# ---------------- Potencia usando n_eff_hit_test ----------------\n",
    "print(\"âš¡ Potencia (hitrate) con n_eff_test ...\")\n",
    "use_neff = \"n_eff_hit_test\" in df.columns\n",
    "\n",
    "pwr_step1 = df.with_columns(\n",
    "    (pl.col(\"tp\").fill_null(0) + pl.col(\"sl\").fill_null(0)).alias(\"n_pairs\"),\n",
    "    (pl.col(\"tp\").fill_null(0).cast(pl.Float64())\n",
    "     / pl.when((pl.col(\"tp\").fill_null(0)+pl.col(\"sl\").fill_null(0))<=0).then(1.0)\n",
    "       .otherwise(pl.col(\"tp\").fill_null(0)+pl.col(\"sl\").fill_null(0))).alias(\"p_obs\")\n",
    ")\n",
    "\n",
    "pwr = (\n",
    "    pwr_step1.with_columns(\n",
    "        pl.when(use_neff).then(pl.col(\"n_eff_hit_test\").fill_null(pl.col(\"n_pairs\")))\n",
    "         .otherwise(pl.col(\"n_pairs\")).cast(pl.Float64()).alias(\"n_eff_used\")\n",
    "    )\n",
    "    .select(\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\",\n",
    "            \"p_pred_train\",\"p_obs\",\"n_eff_used\",\"n_pairs\")\n",
    "    .with_columns(\n",
    "        pl.struct([\"p_pred_train\",\"n_eff_used\"]).map_elements(\n",
    "            lambda s: _power_two_sided(float(s[\"p_pred_train\"]) if s[\"p_pred_train\"] is not None else 0.5,\n",
    "                                       0.05, max(float(s[\"n_eff_used\"]),1.0))\n",
    "        ).alias(\"power_delta_0p05\"),\n",
    "        pl.struct([\"p_pred_train\",\"n_eff_used\"]).map_elements(\n",
    "            lambda s: _power_two_sided(float(s[\"p_pred_train\"]) if s[\"p_pred_train\"] is not None else 0.5,\n",
    "                                       0.10, max(float(s[\"n_eff_used\"]),1.0))\n",
    "        ).alias(\"power_delta_0p10\"),\n",
    "    )\n",
    "    .sort([\"wf_id\",\"rank_in_window\",\"entry_col\"])\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¾ Guardando calibraciones y potencia ...\")\n",
    "calib_q.write_parquet(CAL_Q_PARQ)\n",
    "Path(CAL_Q_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"note\": \"ECE/Brier micro; pred = p_pred_train ponderada por soporte.\",\n",
    "    \"rows\": calib_q.to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"CAL_Q_PARQ\", CAL_Q_PARQ); _print_kv(\"CAL_Q_JSON\", CAL_Q_JSON)\n",
    "\n",
    "calib_d.write_parquet(CAL_D_PARQ)\n",
    "Path(CAL_D_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"delta1_def\": \"test_ret_mean - train_ret_mean\",\n",
    "    \"rows\": calib_d.to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"CAL_D_PARQ\", CAL_D_PARQ); _print_kv(\"CAL_D_JSON\", CAL_D_JSON)\n",
    "\n",
    "pwr.write_parquet(PWR_PARQ)\n",
    "Path(PWR_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"alpha\": 0.05,\n",
    "    \"delta_list\": [0.05, 0.10],\n",
    "    \"n_eff_used\": \"n_eff_hit_test si existe; si no, n_pairs\",\n",
    "    \"rows\": pwr.to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"PWR_PARQ\", PWR_PARQ); _print_kv(\"PWR_JSON\", PWR_JSON)\n",
    "\n",
    "print(\"âœ… Celda 8b (v2.1 FINAL FIX) finalizada.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1241b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9a v1.2 FIX) ...\n",
      "   - TRAIN_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - SEL_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - TEST_PARQ                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando artefactos ...\n",
      "   - df_shape                          : 36x19\n",
      "ğŸ§® Construyendo grids de umbral ...\n",
      "   - p_pred_grid                       : [np.float64(0.5)]\n",
      "   - score_grid                        : none+0.000241,0.000437,0.000464,0.00086,0.00116\n",
      "ğŸ“Š Agregando mÃ©tricas en la cuadrÃ­cula ...\n",
      "   - surfaces_rows                     : 6\n",
      "   - surfaces_cols                     : 13\n",
      "ğŸ† Generando rankings de thresholds ...\n",
      "ğŸ’¾ Guardando surfaces/rankings ...\n",
      "   - SURF_PARQ                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_goodbad_BTCUSDT_15m.parquet\n",
      "   - SURF_JSON                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_goodbad_BTCUSDT_15m.json\n",
      "âœ… Celda 9a (v1.2 FIX DEFINITIVO) finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9a (NUEVA v1.2 FIX DEFINITIVO): [SUPERFICIES GOODÃ—BAD & THRESHOLDS (TEST)]\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: despuÃ©s de 8b y antes de 9b (orden: ... 8b â†’ 9a â†’ 9b â†’ 9c)\n",
    "# Cambios clave vs v1.1:\n",
    "#   1) Dos pasos para columnas dependientes:\n",
    "#        df1 = ... .with_columns(n_pairs=tp+sl)\n",
    "#        df  = df1.with_columns(n_eff_used = coalesce(n_eff_hit_test, n_pairs))\n",
    "#   2) p_pred_train siempre existe por coalesce (p_pred_train | hit_rate_tp_excl_to | 0.5)\n",
    "#   3) test_n y train_n normalizados con coalesce(..., n_signals)\n",
    "#   4) El grid de score sÃ³lo si existe score_train\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _norm_cdf(z: float) -> float:\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "def _power_two_sided(p0: float, delta: float, n_eff: float, alpha: float = 0.05) -> float:\n",
    "    p0 = min(max(float(p0 if p0 is not None else 0.5), 1e-9), 1-1e-9)\n",
    "    n_eff = max(1.0, float(n_eff if n_eff is not None else 1.0))\n",
    "    se0 = math.sqrt(p0*(1-p0)/n_eff)\n",
    "    zcrit = 1.9599639845400538\n",
    "    ncp = abs(delta) / max(se0, 1e-12)\n",
    "    return _norm_cdf(-zcrit - ncp) + (1.0 - _norm_cdf(zcrit - ncp))\n",
    "\n",
    "def _coalesce_alias(df: pl.DataFrame, target: str, candidates: list[str], dtype=None) -> pl.DataFrame:\n",
    "    exprs = [pl.col(c).cast(dtype) if (dtype and c in df.columns) else pl.col(c)\n",
    "             for c in candidates if c in df.columns]\n",
    "    if exprs:\n",
    "        df = df.with_columns(pl.coalesce(exprs).alias(target))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).alias(target))\n",
    "    return df\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9a v1.2 FIX) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS = PATHS[\"reports\"]; REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PARQ = REPORTS / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"   # 7b\n",
    "SEL_PARQ   = REPORTS / f\"wf_selected_{SYMBOL}_{TF}.parquet\"       # 7c\n",
    "TEST_PARQ  = REPORTS / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"    # 7d\n",
    "\n",
    "SURF_PARQ  = REPORTS / f\"surfaces_goodbad_{SYMBOL}_{TF}.parquet\"\n",
    "SURF_JSON  = REPORTS / f\"surfaces_goodbad_{SYMBOL}_{TF}.json\"\n",
    "RANK_PARQ  = REPORTS / f\"rankings_thresholds_{SYMBOL}_{TF}.parquet\"\n",
    "RUNLOG_CSV = REPORTS / \"run_log_records.csv\"\n",
    "\n",
    "_print_kv(\"TRAIN_PARQ\", TRAIN_PARQ); _print_kv(\"SEL_PARQ\", SEL_PARQ); _print_kv(\"TEST_PARQ\", TEST_PARQ)\n",
    "assert TRAIN_PARQ.exists(), \"âŒ Falta 7b (wf_train_scores)\"\n",
    "assert SEL_PARQ.exists(),   \"âŒ Falta 7c (wf_selected)\"\n",
    "assert TEST_PARQ.exists(),  \"âŒ Falta 7d (wf_select_eval)\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando artefactos ...\")\n",
    "train = pl.read_parquet(TRAIN_PARQ)\n",
    "sel   = pl.read_parquet(SEL_PARQ)\n",
    "test  = pl.read_parquet(TEST_PARQ)\n",
    "\n",
    "# wf_id como string para consistencia\n",
    "for name, df in ((\"train\", train), (\"sel\", sel), (\"test\", test)):\n",
    "    if \"wf_id\" in df.columns and df[\"wf_id\"].dtype != pl.Utf8:\n",
    "        df = df.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "    locals()[name] = df\n",
    "\n",
    "# -------------------------- TRAIN (normalizaciÃ³n) --------------------------\n",
    "train_keep = [c for c in [\"wf_id\",\"entry_col\",\"score_train\",\"p_pred_train\",\"hit_rate_tp_excl_to\",\"train_n\",\"n_signals\"] if c in train.columns]\n",
    "train_k = train.select(train_keep)\n",
    "# p_pred_train â† coalesce(p_pred_train, hit_rate_tp_excl_to, 0.5)\n",
    "if \"p_pred_train\" not in train_k.columns or train_k.select(pl.col(\"p_pred_train\").is_null().any()).item():\n",
    "    candidates = []\n",
    "    if \"p_pred_train\" in train_k.columns: candidates.append(\"p_pred_train\")\n",
    "    if \"hit_rate_tp_excl_to\" in train_k.columns: candidates.append(\"hit_rate_tp_excl_to\")\n",
    "    train_k = _coalesce_alias(train_k, \"p_pred_train\", candidates, dtype=pl.Float64)\n",
    "    train_k = train_k.with_columns(pl.col(\"p_pred_train\").fill_null(0.5))\n",
    "# train_n â† coalesce(train_n, n_signals)\n",
    "train_k = _coalesce_alias(train_k, \"train_n\", [\"train_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "\n",
    "# -------------------------- SELECTED --------------------------\n",
    "sel_k = sel.select([c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in sel.columns]).unique()\n",
    "\n",
    "# -------------------------- TEST (7d) --------------------------\n",
    "test_keep = [c for c in [\"wf_id\",\"entry_col\",\"test_n\",\"n_signals\",\"tp\",\"sl\",\"timeout\",\"ret_mean\",\"ev_boot_mean\",\"n_eff_hit_test\"] if c in test.columns]\n",
    "test_k = test.select(test_keep)\n",
    "# test_n â† coalesce(test_n, n_signals)\n",
    "test_k = _coalesce_alias(test_k, \"test_n\", [\"test_n\",\"n_signals\"], dtype=pl.Int64)\n",
    "# renombre mÃ©trica\n",
    "if \"ret_mean\" in test_k.columns:\n",
    "    test_k = test_k.rename({\"ret_mean\":\"test_ret_mean\"})\n",
    "\n",
    "# -------------------------- Join principal --------------------------\n",
    "df1 = (\n",
    "    sel_k.join(train_k, on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "         .join(test_k,  on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "         .with_columns(\n",
    "             (pl.col(\"tp\").fill_null(0) + pl.col(\"sl\").fill_null(0)).alias(\"n_pairs\")\n",
    "         )\n",
    ")\n",
    "\n",
    "# Paso 2: usar n_pairs YA materializado\n",
    "df = (\n",
    "    df1.with_columns(\n",
    "        pl.coalesce([pl.col(\"n_eff_hit_test\"), pl.col(\"n_pairs\")]).cast(pl.Float64).alias(\"n_eff_used\")\n",
    "    )\n",
    ")\n",
    "\n",
    "_print_kv(\"df_shape\", f\"{df.height}x{df.width}\")\n",
    "\n",
    "# ---------------------- Grids de umbrales ----------------------\n",
    "print(\"ğŸ§® Construyendo grids de umbral ...\")\n",
    "# p_pred_train: [0.50, 0.90] paso 0.05, recortado al rango observado\n",
    "pmin_obs = float(df.select(pl.col(\"p_pred_train\").min()).item() or 0.5)\n",
    "pmax_obs = float(df.select(pl.col(\"p_pred_train\").max()).item() or 0.9)\n",
    "p_lo = max(0.50, min(pmin_obs, 0.90))\n",
    "p_hi = min(0.90, max(pmax_obs, 0.50))\n",
    "p_grid = [round(x,2) for x in np.arange(math.floor(p_lo*20)/20, math.ceil(p_hi*20)/20 + 1e-9, 0.05)]\n",
    "if not p_grid: p_grid = [0.50]\n",
    "\n",
    "# score_train: cuantiles 50/60/70/80/90 + \"sin filtro\" SOLO si existe la columna\n",
    "score_grid_vals = []\n",
    "if \"score_train\" in df.columns and df.select(pl.col(\"score_train\").is_not_null().sum()).item() > 0:\n",
    "    for q in (0.50, 0.60, 0.70, 0.80, 0.90):\n",
    "        try:\n",
    "            score_grid_vals.append(float(df.select(pl.col(\"score_train\").quantile(q)).item()))\n",
    "        except Exception:\n",
    "            pass\n",
    "score_grid_vals = sorted(list({round(s, 12) for s in score_grid_vals}))\n",
    "score_grid = [None] + score_grid_vals  # None = sin filtro de score\n",
    "\n",
    "_print_kv(\"p_pred_grid\", p_grid)\n",
    "_print_kv(\"score_grid\", (\"none+\" + \",\".join([f\"{x:.3g}\" for x in score_grid_vals])) if score_grid_vals else \"none\")\n",
    "\n",
    "# ParÃ¡metros\n",
    "MIN_PAIRS_CELL = 30\n",
    "MIN_REPS_CELL  = 3\n",
    "\n",
    "# ---------------------- AgregaciÃ³n por celda de umbrales ----------------------\n",
    "print(\"ğŸ“Š Agregando mÃ©tricas en la cuadrÃ­cula ...\")\n",
    "rows = []\n",
    "fusion_suggestions = []\n",
    "\n",
    "base_cols = [\"wf_id\",\"entry_col\",\"p_pred_train\",\"score_train\",\"n_pairs\",\"tp\",\"sl\",\"test_n\",\"n_eff_used\",\"ev_boot_mean\",\"test_ret_mean\"]\n",
    "used_cols = [c for c in base_cols if c in df.columns]\n",
    "df_local = df.select(used_cols)\n",
    "\n",
    "for p_thr in p_grid:\n",
    "    for s_thr in score_grid:\n",
    "        mask = (pl.col(\"p_pred_train\") >= p_thr)\n",
    "        if s_thr is not None and \"score_train\" in df_local.columns:\n",
    "            mask = mask & (pl.col(\"score_train\").fill_null(float(\"-inf\")) >= s_thr)\n",
    "        sub = df_local.filter(mask)\n",
    "\n",
    "        n_reps = sub.height\n",
    "        tp_sum = int(sub.select(pl.col(\"tp\").fill_null(0).sum().alias(\"__s\")).item() or 0)\n",
    "        sl_sum = int(sub.select(pl.col(\"sl\").fill_null(0).sum().alias(\"__s\")).item() or 0)\n",
    "        n_pairs = tp_sum + sl_sum\n",
    "        n_eff_total = float(sub.select(pl.col(\"n_eff_used\").fill_null(0).sum().alias(\"__s\")).item() or 0.0)\n",
    "        test_n_sum = float(sub.select(pl.col(\"test_n\").fill_null(0).sum().alias(\"__s\")).item() or 0.0)\n",
    "\n",
    "        p_obs_pool = (tp_sum / n_pairs) if n_pairs > 0 else None\n",
    "\n",
    "        # p0_pool: promedio ponderado por n_eff_used\n",
    "        if n_eff_total > 0 and sub.height > 0:\n",
    "            p0_pool = float(sub.select((pl.col(\"p_pred_train\") * pl.col(\"n_eff_used\")).sum() / pl.col(\"n_eff_used\").sum()).item())\n",
    "        else:\n",
    "            p0_pool = None\n",
    "\n",
    "        # EV_pool: usa ev_boot_mean si existe; si no, test_ret_mean (ponderado por test_n)\n",
    "        chosen_ev_col = \"ev_boot_mean\" if \"ev_boot_mean\" in sub.columns else (\"test_ret_mean\" if \"test_ret_mean\" in sub.columns else None)\n",
    "        if chosen_ev_col and test_n_sum > 0:\n",
    "            ev_pool = float(sub.select((pl.col(chosen_ev_col).fill_null(0.0) * pl.col(\"test_n\").fill_null(0.0)).sum()\n",
    "                                       / pl.col(\"test_n\").fill_null(0.0).sum()).item())\n",
    "        elif chosen_ev_col:\n",
    "            ev_pool = float(sub.select(pl.col(chosen_ev_col).mean()).item() or 0.0)\n",
    "        else:\n",
    "            ev_pool = None\n",
    "\n",
    "        pow_005 = _power_two_sided(p0_pool if p0_pool is not None else 0.5, 0.05, n_eff_total)\n",
    "        pow_010 = _power_two_sided(p0_pool if p0_pool is not None else 0.5, 0.10, n_eff_total)\n",
    "\n",
    "        low_support = (n_pairs < MIN_PAIRS_CELL) or (n_reps < MIN_REPS_CELL)\n",
    "        if low_support:\n",
    "            fusion_suggestions.append({\n",
    "                \"p_thr\": p_thr, \"score_thr\": (None if s_thr is None else float(s_thr)),\n",
    "                \"reason\": f\"low_support: n_pairs={n_pairs} < {MIN_PAIRS_CELL} or n_reps={n_reps} < {MIN_REPS_CELL}\",\n",
    "                \"suggestion\": \"relajar umbral (p_thr-0.05 o quitar score_thr) y re-evaluar\"\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"p_min\": float(p_thr),\n",
    "            \"score_min\": (None if s_thr is None else float(s_thr)),\n",
    "            \"n_reps\": n_reps,\n",
    "            \"n_pairs\": n_pairs,\n",
    "            \"n_eff_total\": n_eff_total,\n",
    "            \"tp_sum\": tp_sum,\n",
    "            \"sl_sum\": sl_sum,\n",
    "            \"p_obs_pool\": (None if p_obs_pool is None else float(p_obs_pool)),\n",
    "            \"p0_pool\": (None if p0_pool is None else float(p0_pool)),\n",
    "            \"ev_pool\": (None if ev_pool is None else float(ev_pool)),\n",
    "            \"power_delta_0p05\": float(pow_005),\n",
    "            \"power_delta_0p10\": float(pow_010),\n",
    "            \"low_support_flag\": bool(low_support),\n",
    "        })\n",
    "\n",
    "surfaces = pl.DataFrame(rows) if rows else pl.DataFrame({\"p_min\": pl.Series([], dtype=pl.Float64), \"score_min\": pl.Series([], dtype=pl.Float64)})\n",
    "_print_kv(\"surfaces_rows\", surfaces.height); _print_kv(\"surfaces_cols\", surfaces.width)\n",
    "\n",
    "# ---------------------- Rankings de thresholds ----------------------\n",
    "print(\"ğŸ† Generando rankings de thresholds ...\")\n",
    "valid = surfaces.filter(pl.col(\"low_support_flag\")==False)\n",
    "rank_power_005 = valid.sort([\"power_delta_0p05\",\"ev_pool\",\"n_pairs\"], descending=[True, True, True]).with_columns(pl.lit(\"power_0p05\").alias(\"criterion\")).head(25)\n",
    "rank_power_010 = valid.sort([\"power_delta_0p10\",\"ev_pool\",\"n_pairs\"], descending=[True, True, True]).with_columns(pl.lit(\"power_0p10\").alias(\"criterion\")).head(25)\n",
    "rank_ev        = valid.sort([\"ev_pool\",\"n_pairs\",\"power_delta_0p05\"], descending=[True, True, True]).with_columns(pl.lit(\"ev_pool\").alias(\"criterion\")).head(25)\n",
    "rankings = pl.concat([rank_power_005, rank_power_010, rank_ev], how=\"vertical\") if valid.height>0 else valid\n",
    "\n",
    "# ---------------------- Guardado ----------------------\n",
    "print(\"ğŸ’¾ Guardando surfaces/rankings ...\")\n",
    "if surfaces.height > 0:\n",
    "    surfaces.write_parquet(SURF_PARQ)\n",
    "    SURF_JSON.write_text(json.dumps({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "        \"grid\": {\"p_min_values\": [float(x) for x in p_grid], \"score_min_quantiles\": [None] + [float(x) for x in score_grid_vals]},\n",
    "        \"rows\": surfaces.head(50).to_dicts(),\n",
    "        \"fusions_suggestions\": fusion_suggestions[:50]\n",
    "    }, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    _print_kv(\"SURF_PARQ\", SURF_PARQ); _print_kv(\"SURF_JSON\", SURF_JSON)\n",
    "\n",
    "if rankings.height > 0:\n",
    "    rankings.write_parquet(RANK_PARQ)\n",
    "    _print_kv(\"RANK_PARQ\", RANK_PARQ)\n",
    "\n",
    "# run_log append\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header:\n",
    "        w.writerow(header)\n",
    "    w.writerow([\n",
    "        datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        SYMBOL, TF, \"9a_surfaces_goodbad_v1_2_fixdef\",\n",
    "        surfaces.height, \"\", \"\",\n",
    "        f\"rankings={rankings.height}; low_support={int(surfaces.select(pl.col('low_support_flag').sum()).item() or 0)}\"\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Celda 9a (v1.2 FIX DEFINITIVO) finalizada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f68bc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9b v1.3 FINAL ROBUST) ...\n",
      "   - FWD_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - SEL_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "ğŸ“¦ Cargando artefactos ...\n",
      "   - windows_count                     : 4\n",
      "ğŸ§® Agregando por mes (TEST) ...\n",
      "   - alpha_rows                        : 36\n",
      "   - alpha_cols                        : 11\n",
      "ğŸ’¾ Guardando alpha-decay mensual ...\n",
      "   - OUT_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_monthly_BTCUSDT_15m.parquet\n",
      "   - OUT_JSON                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_monthly_BTCUSDT_15m.json\n",
      "âœ… Celda 9b (v1.3 FINAL ROBUST) finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9b (v1.3 FINAL ROBUST): [ALPHA-DECAY MENSUAL & RÃ‰GIMEN â†’ alpha_decay_monthly_*]\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: NUEVA. ColÃ³cala **despuÃ©s** de la Celda 9a y **antes** de la Celda 9c. (Orden: ... 8b â†’ 9a â†’ 9b â†’ 9c)\n",
    "#\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet           (4c/4d)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_selected_{SYMBOL}_{TF}.parquet       (7c v3 FINAL)\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/wf_windows_{SYMBOL}_{TF}.json              (7a)\n",
    "#\n",
    "# Objetivo:\n",
    "#   â€¢ Para cada (wf_id, entry_col) seleccionado, medir tendencia mensual en TEST:\n",
    "#       - Theilâ€“Sen slope sobre ret_mean mensual\n",
    "#       - Kendall Ï„ y Z de Mannâ€“Kendall (aprox. sin empates)\n",
    "#       - Half-life lineal (meses) si slope<0\n",
    "#   â€¢ SeÃ±alizar soportes y meses efectivos por ventana.\n",
    "#\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/alpha_decay_monthly_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/alpha_decay_monthly_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/run_log_records.csv  (append)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _parse_iso_utc(s: str):\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # Normaliza 'Z' y convierte a UTC\n",
    "    dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _ensure_ts(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Asegura 'ts' Datetime[us, UTC], robusto a string/naive\n",
    "    if \"ts\" not in df.columns:\n",
    "        raise RuntimeError(\"âŒ forward no tiene columna 'ts'\")\n",
    "    col = pl.col(\"ts\")\n",
    "    # 1) si es texto â†’ strptime flexible\n",
    "    if df[\"ts\"].dtype == pl.Utf8:\n",
    "        df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "    # 2) si no es datetime â†’ cast\n",
    "    if str(df.schema[\"ts\"]).startswith(\"Datetime\") is False:\n",
    "        df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "    # 3) aÃ±adir/convertir zona horaria a UTC\n",
    "    dtype_str = str(df.schema.get(\"ts\"))\n",
    "    if \"time_zone=\" not in dtype_str:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\"))\n",
    "    elif \"UTC\" not in dtype_str:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.convert_time_zone(\"UTC\"))\n",
    "    return df\n",
    "\n",
    "def _theil_sen(y: np.ndarray) -> float | None:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[np.isfinite(y)]\n",
    "    n = y.size\n",
    "    if n < 2:\n",
    "        return None\n",
    "    x = np.arange(n, dtype=float)\n",
    "    # median of pairwise slopes (j>i): (y[j]-y[i])/(x[j]-x[i]) ; aquÃ­ x es 0..n-1 â‡’ dj>0\n",
    "    slopes = []\n",
    "    for i in range(n-1):\n",
    "        di = y[i+1:] - y[i]\n",
    "        if di.size:\n",
    "            slopes.extend((di / np.arange(1, di.size+1, dtype=float)).tolist())\n",
    "    return float(np.median(slopes)) if slopes else None\n",
    "\n",
    "def _kendall_tau_mk(y: np.ndarray) -> tuple[float | None, float | None, float | None]:\n",
    "    # Ï„, Z y p bilateral (aprox. sin correcciÃ³n por empates)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[np.isfinite(y)]\n",
    "    n = y.size\n",
    "    if n < 2:\n",
    "        return None, None, None\n",
    "    s = 0\n",
    "    for i in range(n-1):\n",
    "        s += int(np.sign(y[i+1:] - y[i]).sum())\n",
    "    tau = s / (n*(n-1)/2.0)\n",
    "    var_s = n*(n-1)*(2*n+5)/18.0\n",
    "    if var_s <= 0:\n",
    "        return float(tau), None, None\n",
    "    if s > 0:\n",
    "        z = (s - 1)/math.sqrt(var_s)\n",
    "    elif s < 0:\n",
    "        z = (s + 1)/math.sqrt(var_s)\n",
    "    else:\n",
    "        z = 0.0\n",
    "    # p bilateral ~ 2*(1 - Phi(|z|))\n",
    "    p = 2.0*(1.0 - 0.5*(1.0 + math.erf(abs(z)/math.sqrt(2.0))))\n",
    "    return float(tau), float(z), float(p)\n",
    "\n",
    "def _half_life_linear(ev_mean: float | None, slope_per_month: float | None) -> float | None:\n",
    "    if ev_mean is None or slope_per_month is None:\n",
    "        return None\n",
    "    if slope_per_month >= 0:\n",
    "        return float(\"inf\")\n",
    "    if ev_mean <= 0:\n",
    "        return None\n",
    "    return float(ev_mean / (2.0*abs(slope_per_month))) if abs(slope_per_month) > 0 else float(\"inf\")\n",
    "\n",
    "def _load_wf_windows(path: Path):\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip()\n",
    "    # 1) JSON estÃ¡ndar\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # 2) NDJSON/JSONL\n",
    "        lines = [json.loads(ln) for ln in raw.splitlines() if ln.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Normalizaciones\n",
    "    if isinstance(obj, list):\n",
    "        # lista de dicts o lista de strings con JSON embebido\n",
    "        if obj and isinstance(obj[0], str):\n",
    "            items = []\n",
    "            for s in obj:\n",
    "                try:\n",
    "                    items.append(json.loads(s))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return items if items else obj\n",
    "        return obj\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        # dict con clave contenedora\n",
    "        for k in (\"windows\", \"wf_windows\", \"data\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        # dict de wf_id -> ventana\n",
    "        if obj and all(isinstance(v, dict) for v in obj.values()):\n",
    "            return list(obj.values())\n",
    "        # JSON como string\n",
    "        for k in (\"text\", \"content\", \"value\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, str):\n",
    "                try:\n",
    "                    inner = json.loads(v)\n",
    "                    if isinstance(inner, list):\n",
    "                        return inner\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # un solo objeto con campos de test\n",
    "        if {\"wf_id\",\"test_start\",\"test_end\"}.intersection(obj.keys()):\n",
    "            return [obj]\n",
    "        raise RuntimeError(\"Formato inesperado en wf_windows (dict).\")\n",
    "\n",
    "    if isinstance(obj, str):\n",
    "        try:\n",
    "            inner = json.loads(obj)\n",
    "            if isinstance(inner, list):\n",
    "                return inner\n",
    "        except Exception:\n",
    "            lines = [json.loads(ln) for ln in obj.splitlines() if ln.strip()]\n",
    "            if lines:\n",
    "                return lines\n",
    "        raise RuntimeError(\"wf_windows: string JSON no parseable.\")\n",
    "\n",
    "    raise RuntimeError(\"wf_windows: formato no soportado.\")\n",
    "\n",
    "def _pick_time(w: dict, candidates: list[str]):\n",
    "    for k in candidates:\n",
    "        v = w.get(k)\n",
    "        if v:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9b v1.3 FINAL ROBUST) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "\n",
    "FWD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ  = PATHS[\"reports\"] / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON   = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "OUT_PARQ  = PATHS[\"reports\"] / f\"alpha_decay_monthly_{SYMBOL}_{TF}.parquet\"\n",
    "OUT_JSON  = PATHS[\"reports\"] / f\"alpha_decay_monthly_{SYMBOL}_{TF}.json\"\n",
    "RUNLOG_CSV= PATHS[\"reports\"] / \"run_log_records.csv\"\n",
    "\n",
    "_print_kv(\"FWD_PARQ\", FWD_PARQ); _print_kv(\"SEL_PARQ\", SEL_PARQ); _print_kv(\"WF_JSON\", WF_JSON)\n",
    "assert FWD_PARQ.exists(), f\"âŒ Falta {FWD_PARQ}\"\n",
    "assert SEL_PARQ.exists(), f\"âŒ Falta {SEL_PARQ}\"\n",
    "assert WF_JSON.exists(),  f\"âŒ Falta {WF_JSON}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando artefactos ...\")\n",
    "fwd = pl.read_parquet(FWD_PARQ)\n",
    "fwd = _ensure_ts(fwd)\n",
    "\n",
    "need_cols = {\"ts\",\"entry_col\"}\n",
    "missing = sorted([c for c in need_cols if c not in fwd.columns])\n",
    "assert not missing, f\"âŒ forward le faltan columnas: {missing}\"\n",
    "\n",
    "# normaliza entry_col a texto\n",
    "if fwd[\"entry_col\"].dtype != pl.Utf8:\n",
    "    fwd = fwd.with_columns(pl.col(\"entry_col\").cast(pl.Utf8))\n",
    "\n",
    "# EV fila: ret_event si existe; si no, fallback con label (1/-1/0). Si tampoco hay label, error.\n",
    "has_ret = \"ret_event\" in fwd.columns\n",
    "if has_ret:\n",
    "    fwd = fwd.with_columns(pl.col(\"ret_event\").cast(pl.Float64).alias(\"ev\"))\n",
    "else:\n",
    "    if \"label\" not in fwd.columns:\n",
    "        raise RuntimeError(\"âŒ forward no tiene 'ret_event' ni 'label' para construir EV\")\n",
    "    fwd = fwd.with_columns(\n",
    "        pl.when(pl.col(\"label\")==1).then(1.0)\n",
    "         .when(pl.col(\"label\")==-1).then(-1.0)\n",
    "         .otherwise(0.0).cast(pl.Float64).alias(\"ev\")\n",
    "    )\n",
    "\n",
    "sel = pl.read_parquet(SEL_PARQ)\n",
    "keep_sel = [c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\"] if c in sel.columns]\n",
    "sel = sel.select(keep_sel).unique()\n",
    "if \"wf_id\" in sel.columns and sel[\"wf_id\"].dtype != pl.Utf8:\n",
    "    sel = sel.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "\n",
    "wf_windows = _load_wf_windows(WF_JSON)\n",
    "\n",
    "# Mapa wf_id(str) -> (t0_test, t1_test)\n",
    "wf_map = {}\n",
    "for idx, w in enumerate(wf_windows):\n",
    "    if not isinstance(w, dict):\n",
    "        try:\n",
    "            w = json.loads(w)\n",
    "        except Exception:\n",
    "            continue\n",
    "    key = str(w.get(\"wf_id\", f\"wf_{idx:03d}\"))\n",
    "    t0s = _pick_time(w, [\"test_start\",\"test_t0\",\"t0_test\",\"t0\",\"start\",\"testStart\"])\n",
    "    t1s = _pick_time(w, [\"test_end\",\"test_t1\",\"t1_test\",\"t1\",\"end\",\"testEnd\"])\n",
    "    if not (t0s and t1s):\n",
    "        # nested: \"test\": {\"start\":..., \"end\":...}\n",
    "        test_block = w.get(\"test\") if isinstance(w.get(\"test\"), dict) else {}\n",
    "        t0s = t0s or _pick_time(test_block, [\"start\",\"t0\"])\n",
    "        t1s = t1s or _pick_time(test_block, [\"end\",\"t1\"])\n",
    "    if not (t0s and t1s):\n",
    "        continue\n",
    "    t0 = _parse_iso_utc(t0s); t1 = _parse_iso_utc(t1s)\n",
    "    if (t0 is None) or (t1 is None):\n",
    "        continue\n",
    "    wf_map[key] = (t0, t1)\n",
    "\n",
    "_print_kv(\"windows_count\", len(wf_map))\n",
    "assert len(wf_map) > 0, \"âŒ No se pudo construir ninguna ventana de TEST desde wf_windows_*.json\"\n",
    "\n",
    "# ------------------------ AgregaciÃ³n mensual por (wf_id, entry_col) en TEST ------------------------\n",
    "print(\"ğŸ§® Agregando por mes (TEST) ...\")\n",
    "rows = []\n",
    "for r in sel.to_dicts():\n",
    "    wf_id = str(r.get(\"wf_id\"))\n",
    "    en    = r.get(\"entry_col\")\n",
    "    if (wf_id not in wf_map) or (en is None):\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": 0, \"any_test_month\": False})\n",
    "        continue\n",
    "\n",
    "    t0, t1 = wf_map[wf_id]\n",
    "    sub = fwd.filter(\n",
    "        (pl.col(\"entry_col\") == en) &\n",
    "        (pl.col(\"ts\") >= pl.lit(t0)) &\n",
    "        (pl.col(\"ts\") <= pl.lit(t1))\n",
    "    )\n",
    "    if sub.is_empty():\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": 0, \"any_test_month\": False})\n",
    "        continue\n",
    "\n",
    "    # Mes calendario UTC\n",
    "    sub = sub.with_columns(month = pl.col(\"ts\").dt.truncate(\"1mo\"))\n",
    "\n",
    "    # AgregaciÃ³n mensual\n",
    "    ag = (\n",
    "        sub.group_by(\"month\")\n",
    "           .agg(\n",
    "               pl.col(\"ev\").mean().alias(\"ret_mean_month\"),\n",
    "               # soporte si existe label:\n",
    "               (pl.when(pl.col(\"label\")==1).then(1).otherwise(0)).sum().alias(\"tp_sum\") if \"label\" in sub.columns else pl.lit(None).alias(\"tp_sum\"),\n",
    "               (pl.when(pl.col(\"label\")==-1).then(1).otherwise(0)).sum().alias(\"sl_sum\") if \"label\" in sub.columns else pl.lit(None).alias(\"sl_sum\"),\n",
    "               pl.len().alias(\"n_obs\")\n",
    "           )\n",
    "           .sort(\"month\")\n",
    "    )\n",
    "\n",
    "    n_m = ag.height\n",
    "    if n_m == 0:\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": 0, \"any_test_month\": False})\n",
    "        continue\n",
    "\n",
    "    y = ag[\"ret_mean_month\"].to_numpy()\n",
    "    slope = _theil_sen(y)\n",
    "    tau, z, p = _kendall_tau_mk(y)\n",
    "    ev_overall = float(np.nanmean(y)) if y.size>0 else None\n",
    "    hl = _half_life_linear(ev_overall, slope)\n",
    "\n",
    "    support_pairs_total = None\n",
    "    if \"tp_sum\" in ag.columns and \"sl_sum\" in ag.columns and ag[\"tp_sum\"].dtype is not None:\n",
    "        try:\n",
    "            support_pairs_total = int(ag.select((pl.col(\"tp_sum\").fill_null(0)+pl.col(\"sl_sum\").fill_null(0)).sum()).item() or 0)\n",
    "        except Exception:\n",
    "            support_pairs_total = None\n",
    "\n",
    "    rows.append({\n",
    "        \"wf_id\": wf_id,\n",
    "        \"entry_col\": en,\n",
    "        \"n_months\": int(n_m),\n",
    "        \"any_test_month\": True,\n",
    "        \"ev_mean_overall\": (None if ev_overall is None else float(ev_overall)),\n",
    "        \"theil_sen_slope_per_month\": (None if slope is None else float(slope)),\n",
    "        \"kendall_tau\": (None if tau is None else float(tau)),\n",
    "        \"mk_Z\": (None if z is None else float(z)),\n",
    "        \"mk_p_value_two_sided\": (None if p is None else float(p)),\n",
    "        \"half_life_linear_months\": (None if hl is None else (float(\"inf\") if math.isinf(hl) else float(hl))),\n",
    "        \"support_pairs_total\": support_pairs_total,\n",
    "    })\n",
    "\n",
    "alpha = pl.DataFrame(rows)\n",
    "_print_kv(\"alpha_rows\", alpha.height); _print_kv(\"alpha_cols\", alpha.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando alpha-decay mensual ...\")\n",
    "alpha.write_parquet(OUT_PARQ)\n",
    "Path(OUT_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rows_head\": alpha.head(50).to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"OUT_PARQ\", OUT_PARQ); _print_kv(\"OUT_JSON\", OUT_JSON)\n",
    "\n",
    "# run_log append\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header:\n",
    "        w.writerow(header)\n",
    "    w.writerow([\n",
    "        datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        SYMBOL, TF, \"9b_alpha_decay_monthly_v1_3_final\",\n",
    "        alpha.height, \"\", \"\",\n",
    "        \"theil_sen/kendall/MK + half-life lineal (loader robusto wf_windows)\"\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Celda 9b (v1.3 FINAL ROBUST) finalizada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b1257b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9c v1.3 FINAL) ...\n",
      "   - FWD_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - SEL_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                           : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "ğŸ“¦ Cargando artefactos ...\n",
      "   - windows_count                     : 4\n",
      "ğŸ§® Detectando rupturas mensuales (BIC, segâ‰¥6m) ...\n",
      "   - breaks_rows                       : 36\n",
      "   - breaks_cols                       : 12\n",
      "ğŸ’¾ Guardando alpha-decay breaks ...\n",
      "   - OUT_PARQ                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_breaks_BTCUSDT_15m.parquet\n",
      "   - OUT_JSON                          : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_breaks_BTCUSDT_15m.json\n",
      "âœ… Celda 9c (v1.3 FINAL) finalizada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9c (v1.3 FINAL): [RUPTURAS EN ALPHA (BIC, segâ‰¥6m) â†’ alpha_decay_breaks_*]\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# UbicaciÃ³n: despuÃ©s de 9b. (Orden: ... 8b â†’ 9a â†’ 9b â†’ 9c)\n",
    "#\n",
    "# Entradas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/forward/forward_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/wf_selected_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/logs/wf_windows_{SYMBOL}_{TF}.json\n",
    "#\n",
    "# Salidas:\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/alpha_decay_breaks_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/alpha_decay_breaks_{SYMBOL}_{TF}.json\n",
    "#   â€¢ outputs/{SYMBOL}/{TF}/reports/run_log_records.csv (append)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<34}: {v}\")\n",
    "\n",
    "def _parse_iso_utc(s: str):\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _ensure_ts(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"ts\" not in df.columns:\n",
    "        raise RuntimeError(\"âŒ forward no tiene columna 'ts'\")\n",
    "    if df[\"ts\"].dtype == pl.Utf8:\n",
    "        df = df.with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, strict=False, fmt=None))\n",
    "    if str(df.schema[\"ts\"]).startswith(\"Datetime\") is False:\n",
    "        df = df.with_columns(pl.col(\"ts\").cast(pl.Datetime))\n",
    "    dtype_str = str(df.schema.get(\"ts\"))\n",
    "    if \"time_zone=\" not in dtype_str:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\"))\n",
    "    elif \"UTC\" not in dtype_str:\n",
    "        df = df.with_columns(pl.col(\"ts\").dt.convert_time_zone(\"UTC\"))\n",
    "    return df\n",
    "\n",
    "def _load_wf_windows(path: Path):\n",
    "    raw = path.read_text(encoding=\"utf-8\").strip()\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        return [json.loads(ln) for ln in raw.splitlines() if ln.strip()]\n",
    "    if isinstance(obj, list):\n",
    "        return [json.loads(s) if isinstance(s, str) else s for s in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        for k in (\"windows\",\"wf_windows\",\"data\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        if obj and all(isinstance(v, dict) for v in obj.values()):\n",
    "            return list(obj.values())\n",
    "        for k in (\"text\",\"content\",\"value\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, str):\n",
    "                try:\n",
    "                    inner = json.loads(v)\n",
    "                    if isinstance(inner, list):\n",
    "                        return inner\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if {\"wf_id\",\"test_start\",\"test_end\"}.intersection(obj.keys()):\n",
    "            return [obj]\n",
    "    if isinstance(obj, str):\n",
    "        try:\n",
    "            inner = json.loads(obj)\n",
    "            if isinstance(inner, list):\n",
    "                return inner\n",
    "        except Exception:\n",
    "            lines = [json.loads(ln) for ln in obj.splitlines() if ln.strip()]\n",
    "            if lines: return lines\n",
    "    raise RuntimeError(\"wf_windows: formato no soportado.\")\n",
    "\n",
    "def _pick_time(w: dict, ks: list[str]):\n",
    "    for k in ks:\n",
    "        v = w.get(k)\n",
    "        if v: return v\n",
    "    return None\n",
    "\n",
    "def _bic_piecewise_const(y: np.ndarray, breaks: list[int]) -> tuple[float, list[tuple[int,int]], list[float], float]:\n",
    "    \"\"\"\n",
    "    y: serie (n,)\n",
    "    breaks: Ã­ndices de inicio de cada segmento EXCEPTO el 0; e.g., [b1, b2] con 0<b1<b2<n\n",
    "    Retorna: (BIC, segmentos [(s,e)], medias_segmento, sse_total)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = int(y.size)\n",
    "    idx = [0] + sorted(breaks) + [n]\n",
    "    segs = [(idx[i], idx[i+1]) for i in range(len(idx)-1)]\n",
    "    means = []\n",
    "    sse = 0.0\n",
    "    for s,e in segs:\n",
    "        seg = y[s:e]\n",
    "        mu = float(np.nanmean(seg)) if seg.size>0 else 0.0\n",
    "        means.append(mu)\n",
    "        sse += float(np.nansum((seg - mu)**2))\n",
    "    k = len(segs)  # nÂº segmentos â‡’ parÃ¡metros: k medias + 1 var\n",
    "    # verosimilitud normal con var comÃºn estimada: sse/n\n",
    "    sigma2 = sse / max(n,1)\n",
    "    if sigma2 <= 0: sigma2 = 1e-12\n",
    "    loglik = -0.5*n*(math.log(2*math.pi*sigma2) + 1.0)\n",
    "    p = k + 1  # k medias + var\n",
    "    bic = -2.0*loglik + p*math.log(max(n,1))\n",
    "    return bic, segs, means, sse\n",
    "\n",
    "def _best_breaks(y: np.ndarray, minseg: int = 6, max_breaks: int = 2):\n",
    "    \"\"\"\n",
    "    Explora 0,1,2 rupturas con segmentos >= minseg.\n",
    "    Retorna: dict con 'k', 'break_idxs', 'bic', 'segments', 'means'.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = int(y.size)\n",
    "    best = {\"k\":0, \"break_idxs\":[], \"bic\":None, \"segments\":[(0,n)], \"means\":[float(np.nanmean(y)) if n>0 else None]}\n",
    "    # 0 rupturas\n",
    "    bic0, segs0, means0, _ = _bic_piecewise_const(y, [])\n",
    "    best.update({\"k\":0, \"break_idxs\":[], \"bic\":bic0, \"segments\":segs0, \"means\":means0})\n",
    "    # 1 ruptura\n",
    "    for b1 in range(minseg, n-minseg+1):\n",
    "        bic1, segs1, means1, _ = _bic_piecewise_const(y, [b1])\n",
    "        if (best[\"bic\"] is None) or (bic1 < best[\"bic\"]):\n",
    "            best = {\"k\":1, \"break_idxs\":[b1], \"bic\":bic1, \"segments\":segs1, \"means\":means1}\n",
    "    # 2 rupturas\n",
    "    if max_breaks >= 2:\n",
    "        for b1 in range(minseg, n-2*minseg+1):\n",
    "            for b2 in range(b1+minseg, n-minseg+1):\n",
    "                bic2, segs2, means2, _ = _bic_piecewise_const(y, [b1,b2])\n",
    "                if (best[\"bic\"] is None) or (bic2 < best[\"bic\"]):\n",
    "                    best = {\"k\":2, \"break_idxs\":[b1,b2], \"bic\":bic2, \"segments\":segs2, \"means\":means2}\n",
    "    return best\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9c v1.3 FINAL) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "\n",
    "FWD_PARQ  = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ  = PATHS[\"reports\"] / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON   = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "OUT_PARQ  = PATHS[\"reports\"] / f\"alpha_decay_breaks_{SYMBOL}_{TF}.parquet\"\n",
    "OUT_JSON  = PATHS[\"reports\"] / f\"alpha_decay_breaks_{SYMBOL}_{TF}.json\"\n",
    "RUNLOG_CSV= PATHS[\"reports\"] / \"run_log_records.csv\"\n",
    "\n",
    "_print_kv(\"FWD_PARQ\", FWD_PARQ); _print_kv(\"SEL_PARQ\", SEL_PARQ); _print_kv(\"WF_JSON\", WF_JSON)\n",
    "assert FWD_PARQ.exists(), f\"âŒ Falta {FWD_PARQ}\"\n",
    "assert SEL_PARQ.exists(), f\"âŒ Falta {SEL_PARQ}\"\n",
    "assert WF_JSON.exists(),  f\"âŒ Falta {WF_JSON}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando artefactos ...\")\n",
    "fwd = pl.read_parquet(FWD_PARQ)\n",
    "fwd = _ensure_ts(fwd)\n",
    "if \"entry_col\" in fwd.columns and fwd[\"entry_col\"].dtype != pl.Utf8:\n",
    "    fwd = fwd.with_columns(pl.col(\"entry_col\").cast(pl.Utf8))\n",
    "\n",
    "# EV fila\n",
    "has_ret = \"ret_event\" in fwd.columns\n",
    "if has_ret:\n",
    "    fwd = fwd.with_columns(pl.col(\"ret_event\").cast(pl.Float64).alias(\"ev\"))\n",
    "else:\n",
    "    if \"label\" not in fwd.columns:\n",
    "        raise RuntimeError(\"âŒ forward no tiene 'ret_event' ni 'label' para construir EV\")\n",
    "    fwd = fwd.with_columns(\n",
    "        pl.when(pl.col(\"label\")==1).then(1.0)\n",
    "         .when(pl.col(\"label\")==-1).then(-1.0)\n",
    "         .otherwise(0.0).cast(pl.Float64).alias(\"ev\")\n",
    "    )\n",
    "\n",
    "sel = pl.read_parquet(SEL_PARQ).select([c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\"] if c in pl.read_parquet(SEL_PARQ).columns]).unique()\n",
    "if \"wf_id\" in sel.columns and sel[\"wf_id\"].dtype != pl.Utf8:\n",
    "    sel = sel.with_columns(pl.col(\"wf_id\").cast(pl.Utf8))\n",
    "\n",
    "# ventanas TEST\n",
    "raw_windows = _load_wf_windows(WF_JSON)\n",
    "wf_map = {}\n",
    "for idx, w in enumerate(raw_windows):\n",
    "    if not isinstance(w, dict):\n",
    "        try: w = json.loads(w)\n",
    "        except Exception: continue\n",
    "    key = str(w.get(\"wf_id\", f\"wf_{idx:03d}\"))\n",
    "    t0s = _pick_time(w, [\"test_start\",\"test_t0\",\"t0_test\",\"t0\",\"start\",\"testStart\"])\n",
    "    t1s = _pick_time(w, [\"test_end\",\"test_t1\",\"t1_test\",\"t1\",\"end\",\"testEnd\"])\n",
    "    if not (t0s and t1s):\n",
    "        tb = w.get(\"test\") if isinstance(w.get(\"test\"), dict) else {}\n",
    "        t0s = t0s or _pick_time(tb, [\"start\",\"t0\"])\n",
    "        t1s = t1s or _pick_time(tb, [\"end\",\"t1\"])\n",
    "    if not (t0s and t1s): \n",
    "        continue\n",
    "    t0 = _parse_iso_utc(t0s); t1 = _parse_iso_utc(t1s)\n",
    "    if (t0 is None) or (t1 is None): \n",
    "        continue\n",
    "    wf_map[key] = (t0, t1)\n",
    "\n",
    "_print_kv(\"windows_count\", len(wf_map))\n",
    "assert len(wf_map) > 0, \"âŒ No se pudo construir ninguna ventana de TEST.\"\n",
    "\n",
    "# ---- Serie mensual y rupturas ----\n",
    "print(\"ğŸ§® Detectando rupturas mensuales (BIC, segâ‰¥6m) ...\")\n",
    "rows = []\n",
    "MINSEG = 6\n",
    "EPS = 1e-9\n",
    "\n",
    "for r in sel.to_dicts():\n",
    "    wf_id = str(r.get(\"wf_id\"))\n",
    "    en    = r.get(\"entry_col\")\n",
    "    if (wf_id not in wf_map) or (en is None):\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": 0, \"any_test_month\": False})\n",
    "        continue\n",
    "\n",
    "    t0, t1 = wf_map[wf_id]\n",
    "    sub = fwd.filter(\n",
    "        (pl.col(\"entry_col\")==en) & (pl.col(\"ts\")>=pl.lit(t0)) & (pl.col(\"ts\")<=pl.lit(t1))\n",
    "    )\n",
    "    if sub.is_empty():\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": 0, \"any_test_month\": False})\n",
    "        continue\n",
    "\n",
    "    g = (sub.with_columns(month = pl.col(\"ts\").dt.truncate(\"1mo\"))\n",
    "             .group_by(\"month\")\n",
    "             .agg(pl.col(\"ev\").mean().alias(\"ret_mean_month\"), pl.len().alias(\"n_obs\"))\n",
    "             .sort(\"month\"))\n",
    "\n",
    "    y = g[\"ret_mean_month\"].to_numpy()\n",
    "    n = int(g.height)\n",
    "    if n < 2:\n",
    "        rows.append({\"wf_id\": wf_id, \"entry_col\": en, \"n_months\": n, \"any_test_month\": n>0})\n",
    "        continue\n",
    "\n",
    "    # si n insuficiente para seg>=6m, sÃ³lo 0 rupturas\n",
    "    if n < 2*MINSEG:\n",
    "        best = {\"k\":0, \"break_idxs\":[], \"segments\":[(0,n)], \"means\":[float(np.nanmean(y))]}\n",
    "    else:\n",
    "        best = _best_breaks(y, minseg=MINSEG, max_breaks=2)\n",
    "\n",
    "    # materializar info legible\n",
    "    months = g[\"month\"].to_list()\n",
    "    breaks_iso = []\n",
    "    if best[\"k\"] >= 1:\n",
    "        for b in best[\"break_idxs\"]:\n",
    "            if 0 <= b < n:\n",
    "                breaks_iso.append(months[b].isoformat())\n",
    "    seg_means = [None if m is None else float(m) for m in best[\"means\"]]\n",
    "    last_mean = seg_means[-1] if seg_means else None\n",
    "    prev_mean = seg_means[-2] if len(seg_means) >= 2 else None\n",
    "    adverse_recent = (last_mean is not None) and ( (last_mean <= 0.0) or (prev_mean is not None and last_mean < (prev_mean - EPS)) )\n",
    "\n",
    "    rows.append({\n",
    "        \"wf_id\": wf_id,\n",
    "        \"entry_col\": en,\n",
    "        \"n_months\": n,\n",
    "        \"any_test_month\": True,\n",
    "        \"best_k_breaks\": int(best[\"k\"]),\n",
    "        \"break_months_iso\": breaks_iso,\n",
    "        \"segment_means\": seg_means,\n",
    "        \"last_segment_start_iso\": (months[best[\"segments\"][-1][0]].isoformat() if best[\"segments\"] else None),\n",
    "        \"last_segment_mean\": last_mean,\n",
    "        \"prev_segment_mean\": prev_mean,\n",
    "        \"adverse_recent_break\": bool(adverse_recent),\n",
    "        \"bic\": float(best[\"bic\"]) if best.get(\"bic\") is not None else None,\n",
    "    })\n",
    "\n",
    "out = pl.DataFrame(rows)\n",
    "_print_kv(\"breaks_rows\", out.height); _print_kv(\"breaks_cols\", out.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando alpha-decay breaks ...\")\n",
    "out.write_parquet(OUT_PARQ)\n",
    "Path(OUT_JSON).write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rows_head\": out.head(50).to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"OUT_PARQ\", OUT_PARQ); _print_kv(\"OUT_JSON\", OUT_JSON)\n",
    "\n",
    "# run_log append\n",
    "RUNLOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "header = [\"created_utc\",\"symbol\",\"tf\",\"cell\",\"rows\",\"block_method\",\"B\",\"comment\"]\n",
    "append_header = not RUNLOG_CSV.exists()\n",
    "with RUNLOG_CSV.open(\"a\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    if append_header: w.writerow(header)\n",
    "    w.writerow([\n",
    "        datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        SYMBOL, TF, \"9c_alpha_decay_breaks_v1_3_final\",\n",
    "        out.height, \"\", \"\",\n",
    "        \"BIC piecewise-const, seg>=6m, adverse_recent_break flag\"\n",
    "    ])\n",
    "\n",
    "print(\"âœ… Celda 9c (v1.3 FINAL) finalizada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5126add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9 v1.1 TOLERANT) ...\n",
      "   - SCORES_PARQ                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_train_scores_BTCUSDT_15m.parquet\n",
      "   - SELECTED_PARQ                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - EVAL_PARQ                     : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando datasets WF ...\n",
      "   - train_rows                    : 36\n",
      "   - train_cols                    : 32\n",
      "   - sel_rows                      : 12\n",
      "   - sel_cols                      : 11\n",
      "   - test_rows                     : 12\n",
      "   - test_cols                     : 22\n",
      "ğŸ§® Calculando decay por ventana/entry_col ...\n",
      "   - decay_rows                    : 12\n",
      "   - decay_cols                    : 23\n",
      "ğŸ’¾ Guardando wf_decay ...\n",
      "ğŸ“ˆ Resumen de decay (global y ranking) ...\n",
      "   - DECAY_PARQ                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_BTCUSDT_15m.parquet\n",
      "   - DECAY_JSON                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_BTCUSDT_15m.json\n",
      "   - SUMMARY_JSON                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_summary_BTCUSDT_15m.json\n",
      "âœ… Celda 9 (decay v1.1 TOLERANT) finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9 (decay v1.1 TOLERANT): [DECAY TRAINâ†’TEST DE LOS SELECCIONADOS â†’ reports/wf_decay_*]\n",
    "# -----------------------------------------------------------------------------------\n",
    "# - Entradas:\n",
    "#     â€¢ reports/wf_train_scores_{SYMBOL}_{TF}.parquet  (7b)\n",
    "#     â€¢ reports/wf_selected_{SYMBOL}_{TF}.parquet      (7c)\n",
    "#     â€¢ reports/wf_select_eval_{SYMBOL}_{TF}.parquet   (7d)\n",
    "# - Objetivo:\n",
    "#     â€¢ Î” y ratio TEST vs TRAIN para: ret_mean, hit_rate_tp_excl_to, profit_factor_stat (si existe)\n",
    "#     â€¢ Resumen global (ponderado por test_n si existe)\n",
    "#     â€¢ Ranking de robustez (simple)\n",
    "# - Salidas:\n",
    "#     â€¢ reports/wf_decay_{SYMBOL}_{TF}.parquet / .json\n",
    "#     â€¢ reports/wf_decay_summary_{SYMBOL}_{TF}.json\n",
    "# - Notas (FIX):\n",
    "#     â€¢ Renombrado tolerante: solo renombra columnas presentes (evita ColumnNotFoundError).\n",
    "#     â€¢ Si falta profit_factor_stat, crea train_pf/test_pf = null y continÃºa.\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "def _rename_if_exists(df: pl.DataFrame, mapping: dict) -> pl.DataFrame:\n",
    "    # filtra solo las claves presentes para evitar errores\n",
    "    m = {k: v for k, v in mapping.items() if k in df.columns}\n",
    "    return df.rename(m)\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9 v1.1 TOLERANT) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS       = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCORES_PARQ   = REPORTS_DIR / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SELECTED_PARQ = REPORTS_DIR / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "EVAL_PARQ     = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "DECAY_PARQ    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.parquet\"\n",
    "DECAY_JSON    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.json\"\n",
    "SUMMARY_JSON  = REPORTS_DIR / f\"wf_decay_summary_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"SCORES_PARQ\", SCORES_PARQ)\n",
    "_print_kv(\"SELECTED_PARQ\", SELECTED_PARQ)\n",
    "_print_kv(\"EVAL_PARQ\", EVAL_PARQ)\n",
    "assert SCORES_PARQ.exists(),  f\"âŒ Falta entrada: {SCORES_PARQ}\"\n",
    "assert SELECTED_PARQ.exists(),f\"âŒ Falta entrada: {SELECTED_PARQ}\"\n",
    "assert EVAL_PARQ.exists(),    f\"âŒ Falta entrada: {EVAL_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando datasets WF ...\")\n",
    "train = pl.read_parquet(SCORES_PARQ)\n",
    "sel   = pl.read_parquet(SELECTED_PARQ)\n",
    "test  = pl.read_parquet(EVAL_PARQ)\n",
    "\n",
    "_print_kv(\"train_rows\", train.height); _print_kv(\"train_cols\", train.width)\n",
    "_print_kv(\"sel_rows\", sel.height);     _print_kv(\"sel_cols\", sel.width)\n",
    "_print_kv(\"test_rows\", test.height);   _print_kv(\"test_cols\", test.width)\n",
    "\n",
    "# Chequeo de llaves\n",
    "for c in [\"wf_id\",\"entry_col\"]:\n",
    "    assert (c in train.columns) and (c in sel.columns) and (c in test.columns), f\"âŒ Falta {c} en alguno de los datasets.\"\n",
    "\n",
    "# TRAIN -> seleccionar y renombrar tolerante\n",
    "keep_train = [c for c in [\"wf_id\",\"entry_col\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\"] if c in train.columns]\n",
    "train_k = train.select(keep_train)\n",
    "train_k = _rename_if_exists(train_k, {\n",
    "    \"n_signals\": \"train_n\",\n",
    "    \"ret_mean\": \"train_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\": \"train_hit_excl\",\n",
    "    \"profit_factor_stat\": \"train_pf\",\n",
    "})\n",
    "# Asegurar columnas esperadas aunque no existan en origen\n",
    "if \"train_pf\" not in train_k.columns:\n",
    "    train_k = train_k.with_columns(pl.lit(None).cast(pl.Float64()).alias(\"train_pf\"))\n",
    "if \"train_hit_excl\" not in train_k.columns:\n",
    "    train_k = train_k.with_columns(pl.lit(None).cast(pl.Float64()).alias(\"train_hit_excl\"))\n",
    "if \"train_ret_mean\" not in train_k.columns:\n",
    "    train_k = train_k.with_columns(pl.lit(None).cast(pl.Float64()).alias(\"train_ret_mean\"))\n",
    "if \"train_n\" not in train_k.columns:\n",
    "    train_k = train_k.with_columns(pl.lit(None).cast(pl.Int64()).alias(\"train_n\"))\n",
    "\n",
    "# TEST -> seleccionar y renombrar tolerante\n",
    "keep_test = [c for c in [\n",
    "    \"wf_id\",\"entry_col\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\",\n",
    "    \"rank_in_window\",\"cluster_id\",\"cluster_size\"\n",
    "] if c in test.columns]\n",
    "test_k = test.select(keep_test)\n",
    "test_k = _rename_if_exists(test_k, {\n",
    "    \"n_signals\": \"test_n\",\n",
    "    \"ret_mean\": \"test_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\": \"test_hit_excl\",\n",
    "    \"profit_factor_stat\": \"test_pf\",\n",
    "})\n",
    "# Asegurar columnas esperadas aunque no existan en origen\n",
    "for colname, dtype in [\n",
    "    (\"test_pf\", pl.Float64()), (\"test_hit_excl\", pl.Float64()),\n",
    "    (\"test_ret_mean\", pl.Float64()), (\"test_n\", pl.Int64())\n",
    "]:\n",
    "    if colname not in test_k.columns:\n",
    "        test_k = test_k.with_columns(pl.lit(None).cast(dtype).alias(colname))\n",
    "\n",
    "# Seleccionados (Ãºnicos)\n",
    "sel_cols = [c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in sel.columns]\n",
    "sel_k = sel.select(sel_cols).unique()\n",
    "\n",
    "print(\"ğŸ§® Calculando decay por ventana/entry_col ...\")\n",
    "decay = (\n",
    "    sel_k\n",
    "    .join(train_k, on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "    .join(test_k,  on=[\"wf_id\",\"entry_col\"], how=\"left\", suffix=\"_dup\")\n",
    "    .with_columns(\n",
    "        # Deltas absolutos\n",
    "        (pl.col(\"test_ret_mean\") - pl.col(\"train_ret_mean\")).alias(\"delta_ret_mean\"),\n",
    "        (pl.col(\"test_hit_excl\") - pl.col(\"train_hit_excl\")).alias(\"delta_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")       - pl.col(\"train_pf\")).alias(\"delta_pf\"),\n",
    "        # Ratios (seguros)\n",
    "        (pl.col(\"test_ret_mean\").abs() / _safe_pos(pl.col(\"train_ret_mean\").abs(), 1e-12)).alias(\"ratio_ret_mean_abs\"),\n",
    "        (pl.col(\"test_hit_excl\")       / _safe_pos(pl.col(\"train_hit_excl\"), 1e-12)).alias(\"ratio_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")             / _safe_pos(pl.col(\"train_pf\"), 1e-12)).alias(\"ratio_pf\"),\n",
    "        # SeÃ±ales totales vistas (si faltan, 0 en null-safe)\n",
    "        (pl.col(\"train_n\").fill_null(0) + pl.col(\"test_n\").fill_null(0)).alias(\"n_total_seen\"),\n",
    "    )\n",
    "    .sort([\"wf_id\",\"rank_in_window\",\"entry_col\"])\n",
    ")\n",
    "\n",
    "_print_kv(\"decay_rows\", decay.height)\n",
    "_print_kv(\"decay_cols\", decay.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_decay ...\")\n",
    "decay.write_parquet(DECAY_PARQ)\n",
    "DECAY_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rows_head\": decay.head(50).to_dicts(),\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"ğŸ“ˆ Resumen de decay (global y ranking) ...\")\n",
    "has_test_n = \"test_n\" in decay.columns\n",
    "w = (pl.col(\"test_n\").fill_null(0).cast(pl.Float64())) if has_test_n else pl.lit(1.0)\n",
    "\n",
    "summary = (\n",
    "    decay\n",
    "    .with_columns(w.alias(\"__w__\"))\n",
    "    .select(\n",
    "        ((pl.col(\"delta_ret_mean\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_ret_mean\"),\n",
    "        ((pl.col(\"delta_hit_excl\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_hit_excl\"),\n",
    "        ((pl.col(\"delta_pf\")       * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_delta_pf\"),\n",
    "        ((pl.col(\"ratio_ret_mean_abs\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_ret_mean_abs\"),\n",
    "        ((pl.col(\"ratio_hit_excl\")     * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_hit_excl\"),\n",
    "        ((pl.col(\"ratio_pf\")           * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12)).alias(\"avg_ratio_pf\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "rank = (\n",
    "    decay\n",
    "    .select(\n",
    "        \"wf_id\",\"entry_col\",\"rank_in_window\",\n",
    "        \"train_ret_mean\",\"test_ret_mean\",\"delta_ret_mean\",\"ratio_ret_mean_abs\",\n",
    "        \"train_hit_excl\",\"test_hit_excl\",\"delta_hit_excl\",\"ratio_hit_excl\",\n",
    "        \"train_pf\",\"test_pf\",\"delta_pf\",\"ratio_pf\",\n",
    "        \"train_n\",\"test_n\",\"n_total_seen\"\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"ratio_pf\").fill_null(0.0).alias(\"_ratio_pf_\"),\n",
    "        pl.col(\"ratio_hit_excl\").fill_null(0.0).alias(\"_ratio_hit_\"),\n",
    "        pl.col(\"delta_ret_mean\").abs().fill_null(0.0).alias(\"_abs_dret_\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (((pl.col(\"_ratio_pf_\") + pl.col(\"_ratio_hit_\"))/2.0) - pl.col(\"_abs_dret_\")).alias(\"robustness_score\")\n",
    "    )\n",
    "    .drop([\"_ratio_pf_\",\"_ratio_hit_\",\"_abs_dret_\"])\n",
    "    .sort([\"wf_id\",\"robustness_score\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "SUMMARY_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"global_summary\": (summary.to_dicts()[0] if summary.height>0 else {}),\n",
    "    \"top5_by_robustness\": rank.sort(\"robustness_score\", descending=True).to_dicts()[:5],\n",
    "    \"bottom5_by_robustness\": rank.sort(\"robustness_score\", descending=False).to_dicts()[:5],\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"DECAY_PARQ\", DECAY_PARQ)\n",
    "_print_kv(\"DECAY_JSON\", DECAY_JSON)\n",
    "_print_kv(\"SUMMARY_JSON\", SUMMARY_JSON)\n",
    "print(\"âœ… Celda 9 (decay v1.1 TOLERANT) finalizada sin errores.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9b NUEVA) ...\n",
      "   - FORWARD_PARQ                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\forward\\forward_BTCUSDT_15m.parquet\n",
      "   - SEL_PARQ                        : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_selected_BTCUSDT_15m.parquet\n",
      "   - WF_JSON                         : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\logs\\wf_windows_BTCUSDT_15m.json\n",
      "ğŸ“¦ Cargando forward y seleccionados ...\n",
      "   - fwd_shape                       : 447948x11\n",
      "   - sel_shape                       : 12x5\n",
      "   - ts dtype=Datetime(time_unit='us', time_zone='UTC') OK\n",
      "   - entry_selected_unique           : 7\n",
      "ğŸ§® Agregando serie mensual por entry_col ...\n",
      "   - monthly_rows                    : 412\n",
      "   - monthly_cols                    : 11\n",
      "ğŸ“Š Vista mensual (primeras 10 filas):\n",
      "shape: (10, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col    â”† year_month â”† n_signals â”† tp  â”† â€¦ â”† ret_std  â”† any_test_mon â”† n_excl_to â”† hit_excl â”‚\n",
      "â”‚ ---          â”† ---        â”† ---       â”† --- â”†   â”† ---      â”† th           â”† ---       â”† ---      â”‚\n",
      "â”‚ str          â”† str        â”† u32       â”† i64 â”†   â”† f64      â”† ---          â”† i64       â”† f64      â”‚\n",
      "â”‚              â”†            â”†           â”†     â”†   â”†          â”† bool         â”†           â”†          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er10_g â”† 2021-01    â”† 1959      â”† 570 â”† â€¦ â”† 0.017262 â”† false        â”† 1954      â”† 0.291709 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-02    â”† 1731      â”† 587 â”† â€¦ â”† 0.012752 â”† false        â”† 1726      â”† 0.340093 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-03    â”† 1934      â”† 685 â”† â€¦ â”† 0.009707 â”† false        â”† 1931      â”† 0.354738 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-04    â”† 1822      â”† 496 â”† â€¦ â”† 0.008581 â”† false        â”† 1820      â”† 0.272527 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-05    â”† 1972      â”† 538 â”† â€¦ â”† 0.016184 â”† false        â”† 1966      â”† 0.273652 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-06    â”† 1855      â”† 605 â”† â€¦ â”† 0.011882 â”† false        â”† 1847      â”† 0.327558 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-07    â”† 1855      â”† 625 â”† â€¦ â”† 0.007759 â”† false        â”† 1840      â”† 0.339674 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-08    â”† 1904      â”† 610 â”† â€¦ â”† 0.007592 â”† false        â”† 1894      â”† 0.32207  â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-09    â”† 1735      â”† 541 â”† â€¦ â”† 0.007939 â”† false        â”† 1703      â”† 0.317675 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â”‚ entry_er10_g â”† 2021-10    â”† 1794      â”† 574 â”† â€¦ â”† 0.007344 â”† false        â”† 1784      â”† 0.321749 â”‚\n",
      "â”‚ e_0p2        â”†            â”†           â”†     â”†   â”†          â”†              â”†           â”†          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ§ª DiagnÃ³stico por entry_col (Theilâ€“Sen, Kendall Ï„, Mannâ€“Kendall, Half-life) ...\n",
      "   - diag_rows                       : 7\n",
      "ğŸ“‹ DiagnÃ³stico (top 10 por |slope_ret|):\n",
      "shape: (7, 13)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ entry_col  â”† months â”† n_total â”† slope_ret_ â”† â€¦ â”† mann_kenda â”† mann_kenda â”† half_life â”† any_test_ â”‚\n",
      "â”‚ ---        â”† ---    â”† ---     â”† theil_sen  â”†   â”† ll_Z       â”† ll_p       â”† _months_l â”† months    â”‚\n",
      "â”‚ str        â”† i64    â”† i64     â”† ---        â”†   â”† ---        â”† ---        â”† inear     â”† ---       â”‚\n",
      "â”‚            â”†        â”†         â”† f64        â”†   â”† f64        â”† f64        â”† ---       â”† i64       â”‚\n",
      "â”‚            â”†        â”†         â”†            â”†   â”†            â”†            â”† f64       â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ entry_er50 â”† 58     â”† 4821    â”† -0.000025  â”† â€¦ â”† -1.717244  â”† 0.085935   â”† null      â”† 4         â”‚\n",
      "â”‚ _ge_0p4    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â”‚ entry_er50 â”† 59     â”† 42976   â”† 0.000007   â”† â€¦ â”† 1.438687   â”† 0.150239   â”† 80.697827 â”† 7         â”‚\n",
      "â”‚ _ge_0p2    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â”‚ entry_er10 â”† 59     â”† 105683  â”† 0.000005   â”† â€¦ â”† 1.307897   â”† 0.190908   â”† 94.333604 â”† 7         â”‚\n",
      "â”‚ _ge_0p2    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â”‚ entry_er20 â”† 59     â”† 77823   â”† 0.000004   â”† â€¦ â”† 0.850133   â”† 0.395251   â”† 79.090783 â”† 7         â”‚\n",
      "â”‚ _ge_0p2    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â”‚ entry_er10 â”† 59     â”† 53709   â”† 0.000002   â”† â€¦ â”† 0.601633   â”† 0.547419   â”† 36.931801 â”† 7         â”‚\n",
      "â”‚ _ge_0p4    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â”‚ entry_er20 â”† 59     â”† 45467   â”† 0.000002   â”† â€¦ â”† 0.405448   â”† 0.685148   â”† 274.62115 â”† 7         â”‚\n",
      "â”‚ _ge_0p3    â”†        â”†         â”†            â”†   â”†            â”†            â”† 6         â”†           â”‚\n",
      "â”‚ entry_er20 â”† 59     â”† 23685   â”† -0.000001  â”† â€¦ â”† -0.274658  â”† 0.783579   â”† null      â”† 7         â”‚\n",
      "â”‚ _ge_0p4    â”†        â”†         â”†            â”†   â”†            â”†            â”†           â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ğŸ’¾ Guardando monthly y diag ...\n",
      "   - MONTHLY_PARQ                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_monthly_BTCUSDT_15m.parquet\n",
      "   - MONTHLY_JSON                    : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_monthly_BTCUSDT_15m.json\n",
      "   - DIAG_PARQ                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_diag_BTCUSDT_15m.parquet\n",
      "   - DIAG_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_diag_BTCUSDT_15m.json\n",
      "âœ… Celda 9b (alpha-decay mensual v1.1 FIX) finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9b (FINAL): [ALPHA-DECAY MENSUAL y DIAGNÃ“STICO DE RÃ‰GIMEN]\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Entradas:\n",
    "#   â€¢ forward_{SYMBOL}_{TF}.parquet   (ts, entry_col, label, ret_event)\n",
    "#   â€¢ reports/wf_selected_{SYMBOL}_{TF}.parquet\n",
    "#   â€¢ logs/wf_windows_{SYMBOL}_{TF}.json\n",
    "# Salidas:\n",
    "#   â€¢ reports/alpha_decay_monthly_{SYMBOL}_{TF}.parquet / .json\n",
    "#   â€¢ reports/alpha_decay_diag_{SYMBOL}_{TF}.parquet / .json\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<32}: {v}\")\n",
    "\n",
    "def _theil_sen(y: np.ndarray):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[~np.isnan(y)]\n",
    "    n = len(y)\n",
    "    if n < 3: return None\n",
    "    slopes = []\n",
    "    for i in range(n-1):\n",
    "        dy = y[i+1:] - y[i]\n",
    "        dx = np.arange(1, n - i, dtype=float)\n",
    "        slopes.extend(list(dy / dx))\n",
    "    return float(np.median(slopes)) if slopes else None\n",
    "\n",
    "def _kendall_tau(y: np.ndarray):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[~np.isnan(y)]\n",
    "    n = len(y)\n",
    "    if n < 3: return None\n",
    "    x = np.arange(n, dtype=float)\n",
    "    conc = 0; disc = 0\n",
    "    for i in range(n-1):\n",
    "        s = np.sign(y[i+1:] - y[i]) * np.sign(x[i+1:] - x[i])\n",
    "        conc += np.sum(s > 0); disc += np.sum(s < 0)\n",
    "    denom = n*(n-1)/2\n",
    "    return float((conc - disc) / denom) if denom>0 else None\n",
    "\n",
    "def _mann_kendall(y: np.ndarray):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[~np.isnan(y)]\n",
    "    n = len(y)\n",
    "    if n < 3: return (None,None,None,None)\n",
    "    S = 0\n",
    "    for i in range(n-1): S += np.sum(np.sign(y[i+1:] - y[i]))\n",
    "    VarS = (n*(n-1)*(2*n+5))/18.0\n",
    "    if VarS <= 0: return (float(S), None, None, None)\n",
    "    if S>0: Z = (S-1)/math.sqrt(VarS)\n",
    "    elif S<0: Z = (S+1)/math.sqrt(VarS)\n",
    "    else: Z = 0.0\n",
    "    p = 2.0 * (1.0 - 0.5*(1.0 + math.erf(abs(Z)/math.sqrt(2.0))))\n",
    "    return (float(S), float(VarS), float(Z), float(p))\n",
    "\n",
    "def _half_life_linear(y: np.ndarray, slope):\n",
    "    if slope is None or np.all(np.isnan(y)): return None\n",
    "    y = y[~np.isnan(y)]\n",
    "    if y.size==0: return None\n",
    "    edge0 = float(y[0])\n",
    "    if edge0 == 0: return None\n",
    "    if slope == 0 or np.sign(edge0) == np.sign(slope): return None\n",
    "    t_half = (abs(edge0)/2.0) / abs(slope)\n",
    "    return float(t_half) if np.isfinite(t_half) and t_half>=0 else None\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9b FINAL) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FORWARD_PARQ = PATHS[\"forward\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ     = PATHS[\"reports\"] / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "WF_JSON      = PATHS[\"logs\"]    / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "MONTHLY_PARQ = PATHS[\"reports\"] / f\"alpha_decay_monthly_{SYMBOL}_{TF}.parquet\"\n",
    "MONTHLY_JSON = PATHS[\"reports\"] / f\"alpha_decay_monthly_{SYMBOL}_{TF}.json\"\n",
    "DIAG_PARQ    = PATHS[\"reports\"] / f\"alpha_decay_diag_{SYMBOL}_{TF}.parquet\"\n",
    "DIAG_JSON    = PATHS[\"reports\"] / f\"alpha_decay_diag_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"FORWARD_PARQ\", FORWARD_PARQ)\n",
    "_print_kv(\"SEL_PARQ\",     SEL_PARQ)\n",
    "_print_kv(\"WF_JSON\",      WF_JSON)\n",
    "\n",
    "assert FORWARD_PARQ.exists(), f\"âŒ Falta {FORWARD_PARQ}\"\n",
    "assert SEL_PARQ.exists(),     f\"âŒ Falta {SEL_PARQ}\"\n",
    "assert WF_JSON.exists(),      f\"âŒ Falta {WF_JSON}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando forward y seleccionados ...\")\n",
    "fwd = pl.read_parquet(FORWARD_PARQ)\n",
    "sel = pl.read_parquet(SEL_PARQ).select([\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"]).unique()\n",
    "_print_kv(\"fwd_shape\", f\"{fwd.height}x{fwd.width}\")\n",
    "_print_kv(\"sel_shape\", f\"{sel.height}x{sel.width}\")\n",
    "\n",
    "for c in [\"ts\",\"entry_col\",\"label\",\"ret_event\"]:\n",
    "    assert c in fwd.columns, f\"âŒ Falta columna en forward: {c}\"\n",
    "\n",
    "windows = json.loads(WF_JSON.read_text(encoding=\"utf-8\")).get(\"windows\", [])\n",
    "test_ors = []\n",
    "for w in windows:\n",
    "    t0 = datetime.fromisoformat(w[\"test\"][\"start\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    t1 = datetime.fromisoformat(w[\"test\"][\"end\"].replace(\"Z\",\"+00:00\")).astimezone(timezone.utc)\n",
    "    test_ors.append(((pl.col(\"ts\") >= pl.lit(t0)) & (pl.col(\"ts\") <= pl.lit(t1))))\n",
    "any_test_expr = pl.lit(False) if len(test_ors)==0 else pl.fold(False, lambda a,b: a|b, test_ors)\n",
    "\n",
    "entry_list = sel.get_column(\"entry_col\").unique().to_list()\n",
    "_print_kv(\"entry_selected_unique\", len(entry_list))\n",
    "fwd_sel = fwd.filter(pl.col(\"entry_col\").is_in(entry_list)) \\\n",
    "             .with_columns(\n",
    "                 pl.col(\"ts\").dt.strftime(\"%Y-%m\").alias(\"year_month\"),\n",
    "                 any_test_expr.alias(\"any_test_month\")\n",
    "             )\n",
    "\n",
    "print(\"ğŸ§® Agregando serie mensual por entry_col ...\")\n",
    "monthly = (\n",
    "    fwd_sel\n",
    "    .group_by([\"entry_col\",\"year_month\"])\n",
    "    .agg(\n",
    "        pl.len().alias(\"n_signals\"),\n",
    "        pl.sum((pl.col(\"label\")==1).cast(pl.Int32)).alias(\"tp\"),\n",
    "        pl.sum((pl.col(\"label\")==-1).cast(pl.Int32)).alias(\"sl\"),\n",
    "        pl.sum((pl.col(\"label\")==0).cast(pl.Int32)).alias(\"timeout\"),\n",
    "        pl.col(\"ret_event\").mean().alias(\"ret_mean\"),\n",
    "        pl.col(\"ret_event\").std().alias(\"ret_std\"),\n",
    "        pl.any(\"any_test_month\").alias(\"any_test_month\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"tp\") + pl.col(\"sl\")).alias(\"n_excl_to\"),\n",
    "        (pl.col(\"tp\").cast(pl.Float64) / pl.when((pl.col(\"tp\")+pl.col(\"sl\"))<=0).then(1.0)\n",
    "             .otherwise(pl.col(\"tp\")+pl.col(\"sl\"))).alias(\"hit_excl\"),\n",
    "    )\n",
    "    .sort([\"entry_col\",\"year_month\"])\n",
    ")\n",
    "\n",
    "_print_kv(\"monthly_rows\", monthly.height)\n",
    "_print_kv(\"monthly_cols\", monthly.width)\n",
    "print(\"ğŸ“Š Vista mensual (primeras 10 filas):\")\n",
    "print(monthly.head(10))\n",
    "\n",
    "print(\"ğŸ§ª DiagnÃ³stico por entry_col ...\")\n",
    "diag_rows = []\n",
    "for entry in entry_list:\n",
    "    m = monthly.filter(pl.col(\"entry_col\")==entry).sort(\"year_month\")\n",
    "    y_ret = np.array(m.get_column(\"ret_mean\").to_list(), dtype=float)\n",
    "    y_hit = np.array(m.get_column(\"hit_excl\").to_list(), dtype=float)\n",
    "\n",
    "    slope_ret = _theil_sen(y_ret)\n",
    "    slope_hit = _theil_sen(y_hit)\n",
    "    tau_ret   = _kendall_tau(y_ret)\n",
    "    tau_hit   = _kendall_tau(y_hit)\n",
    "    S, VarS, Z, p = _mann_kendall(y_ret)\n",
    "    t_half    = _half_life_linear(y_ret, slope_ret)\n",
    "\n",
    "    diag_rows.append({\n",
    "        \"entry_col\": entry,\n",
    "        \"months\": int(m.height),\n",
    "        \"n_total\": int(m.get_column(\"n_signals\").sum()) if m.height>0 else 0,\n",
    "        \"slope_ret_theil_sen\": slope_ret,\n",
    "        \"slope_hit_theil_sen\": slope_hit,\n",
    "        \"kendall_tau_ret\": tau_ret,\n",
    "        \"kendall_tau_hit\": tau_hit,\n",
    "        \"mann_kendall_S\": S,\n",
    "        \"mann_kendall_VarS\": VarS,\n",
    "        \"mann_kendall_Z\": Z,\n",
    "        \"mann_kendall_p\": p,\n",
    "        \"half_life_months_linear\": t_half,\n",
    "        \"any_test_months\": int(m.get_column(\"any_test_month\").sum()) if (\"any_test_month\" in m.columns and m.height>0) else 0,\n",
    "    })\n",
    "\n",
    "diag = pl.DataFrame(diag_rows) if len(diag_rows) else pl.DataFrame({\"entry_col\": pl.Series([], dtype=pl.Utf8)})\n",
    "_print_kv(\"diag_rows\", diag.height)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando monthly y diag ...\")\n",
    "MONTHLY_PARQ.parent.mkdir(parents=True, exist_ok=True)\n",
    "monthly.write_parquet(MONTHLY_PARQ)\n",
    "MONTHLY_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"note\": \"Serie mensual por entry_col (seleccionados); any_test_month indica si el mes cae en alguna ventana TEST.\",\n",
    "    \"rows_head\": monthly.head(50).to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "diag.write_parquet(DIAG_PARQ)\n",
    "DIAG_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"metrics\": [\"Theilâ€“Sen(ret_mean/hit_excl)\",\"Kendall Ï„\",\"Mannâ€“Kendall(ret_mean)\",\"Half-life(lineal)\"],\n",
    "    \"rows\": diag.to_dicts()\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"MONTHLY_PARQ\", MONTHLY_PARQ)\n",
    "_print_kv(\"MONTHLY_JSON\", MONTHLY_JSON)\n",
    "_print_kv(\"DIAG_PARQ\", DIAG_PARQ)\n",
    "_print_kv(\"DIAG_JSON\", DIAG_JSON)\n",
    "print(\"âœ… Celda 9b (FINAL) completada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d83731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n (Celda 9c NUEVA) ...\n",
      "   - MONTHLY_PARQ                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_monthly_BTCUSDT_15m.parquet\n",
      "ğŸ“¦ Cargando monthly ...\n",
      "   - monthly_rows                      : 412\n",
      "   - monthly_cols                      : 11\n",
      "ğŸ§® Detectando rupturas (BIC, seg>=6m) ...\n",
      "   - entries                           : 7\n",
      "ğŸ’¾ Guardando rupturas ...\n",
      "   - BREAKS_PARQ                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_breaks_BTCUSDT_15m.parquet\n",
      "   - BREAKS_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\alpha_decay_breaks_BTCUSDT_15m.json\n",
      "âœ… Celda 9c (rupturas BIC, seg>=6m, JSON-safe) finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 9c (FINAL): [DETECCIÃ“N DE RUPTURAS EN SERIE MENSUAL â†’ alpha_decay_breaks_*]\n",
    "# ---------------------------------------------------------------------------------\n",
    "# - Entrada: reports/alpha_decay_monthly_{SYMBOL}_{TF}.parquet\n",
    "# - Salida : reports/alpha_decay_breaks_{SYMBOL}_{TF}.parquet / .json\n",
    "# - Nota   : Segmentos mÃ­nimos de 6 meses; JSON seguro con casting a tipos nativos.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<32}: {v}\")\n",
    "\n",
    "def _bic_of_segments(y: np.ndarray, cuts: list[int]):\n",
    "    # y: serie; cuts: Ã­ndices fin de segmentos (exclusivo), p.ej. [j1, j2, ..., n]\n",
    "    # Modelo: media por segmento, varianza comÃºn (MLE); BIC ~ -2*loglik + k*log(n)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if n == 0: return (np.inf, None)\n",
    "    segs = []\n",
    "    last = 0\n",
    "    for c in cuts:\n",
    "        segs.append(y[last:c])\n",
    "        last = c\n",
    "    if last < n:\n",
    "        segs.append(y[last:n])\n",
    "    # cada segmento debe tener al menos 6 obs\n",
    "    for s in segs:\n",
    "        if len(s) < 6: return (np.inf, None)\n",
    "    mu = [np.nanmean(s) for s in segs]\n",
    "    resid = []\n",
    "    last = 0\n",
    "    for m, c in zip(mu, cuts + [n]):\n",
    "        seg = y[last:c]\n",
    "        resid.extend((seg - m))\n",
    "        last = c\n",
    "    resid = np.asarray(resid, dtype=float)\n",
    "    sigma2 = np.nanmean(resid**2) if resid.size>0 else 0.0\n",
    "    if sigma2 <= 0: sigma2 = 1e-12\n",
    "    loglik = -0.5*n*(math.log(2*math.pi*sigma2) + 1.0)\n",
    "    k = len(mu)  # parÃ¡metros de medias; sigma2 tratada como MLE comÃºn\n",
    "    bic = -2*loglik + k * math.log(max(n,1))\n",
    "    return (bic, mu)\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n (Celda 9c FINAL) ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "PATHS[\"reports\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MONTHLY_PARQ = PATHS[\"reports\"] / f\"alpha_decay_monthly_{SYMBOL}_{TF}.parquet\"\n",
    "BREAKS_PARQ  = PATHS[\"reports\"] / f\"alpha_decay_breaks_{SYMBOL}_{TF}.parquet\"\n",
    "BREAKS_JSON  = PATHS[\"reports\"] / f\"alpha_decay_breaks_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"MONTHLY_PARQ\", MONTHLY_PARQ)\n",
    "assert MONTHLY_PARQ.exists(), \"âŒ Falta monthly\"\n",
    "\n",
    "monthly = pl.read_parquet(MONTHLY_PARQ)\n",
    "_print_kv(\"monthly_rows\", monthly.height); _print_kv(\"monthly_cols\", monthly.width)\n",
    "\n",
    "print(\"ğŸ§® Detectando rupturas (BIC, segâ‰¥6m) ...\")\n",
    "entries = monthly.get_column(\"entry_col\").unique().to_list()\n",
    "_print_kv(\"entries\", len(entries))\n",
    "\n",
    "break_rows = []\n",
    "summary = []\n",
    "for entry in entries:\n",
    "    m = monthly.filter(pl.col(\"entry_col\")==entry).sort(\"year_month\")\n",
    "    y = np.asarray(m.get_column(\"ret_mean\").to_list(), dtype=float)\n",
    "    n = len(y)\n",
    "    # cortes candidatos: posiciones 6..n-6 (fin de segmento), permitiendo 0..k cortes\n",
    "    candidates = list(range(6, max(6, n-5)))\n",
    "    best_bic = np.inf; best_cuts = []; best_mu = None\n",
    "    # 0 cortes\n",
    "    bic0, mu0 = _bic_of_segments(y, [n])\n",
    "    if bic0 < best_bic: best_bic, best_cuts, best_mu = bic0, [n], mu0\n",
    "    # 1 corte\n",
    "    for j in candidates:\n",
    "        bic1, mu1 = _bic_of_segments(y, [j, n])\n",
    "        if bic1 < best_bic:\n",
    "            best_bic, best_cuts, best_mu = bic1, [j, n], mu1\n",
    "    # 2 cortes (opcional; coste O(n^2))\n",
    "    for j1 in candidates:\n",
    "        for j2 in candidates:\n",
    "            if j2 <= j1: continue\n",
    "            bic2, mu2 = _bic_of_segments(y, [j1, j2, n])\n",
    "            if bic2 < best_bic:\n",
    "                best_bic, best_cuts, best_mu = bic2, [j1, j2, n], mu2\n",
    "\n",
    "    # construir filas\n",
    "    last = 0\n",
    "    seg_idx = 0\n",
    "    for c in best_cuts:\n",
    "        seg = y[last:c]\n",
    "        if seg.size >= 1:\n",
    "            break_rows.append({\n",
    "                \"entry_col\": entry,\n",
    "                \"seg_idx\": seg_idx,\n",
    "                \"start_idx\": last,\n",
    "                \"end_idx_exclusive\": c,\n",
    "                \"n_months\": int(seg.size),\n",
    "                \"ret_mean_seg\": float(np.nanmean(seg)),\n",
    "                \"hit_excl_seg\": float(np.nanmean(\n",
    "                    np.asarray(\n",
    "                        monthly.filter((pl.col(\"entry_col\")==entry) & (pl.col(\"year_month\").is_in(m.get_column(\"year_month\").to_list()[last:c])))\n",
    "                               .get_column(\"hit_excl\").to_list(), dtype=float\n",
    "                    )\n",
    "                )) if seg.size>0 else None\n",
    "            })\n",
    "        last = c\n",
    "        seg_idx += 1\n",
    "\n",
    "    summary.append({\n",
    "        \"entry_col\": entry,\n",
    "        \"n\": int(n),\n",
    "        \"best_bic\": float(best_bic),\n",
    "        \"n_segments\": int(len(best_cuts)),\n",
    "        \"cuts\": [int(x) for x in best_cuts],\n",
    "    })\n",
    "\n",
    "breaks_df = pl.DataFrame(break_rows) if break_rows else pl.DataFrame({\n",
    "    \"entry_col\": pl.Series([], dtype=pl.Utf8),\n",
    "    \"seg_idx\": pl.Series([], dtype=pl.Int32),\n",
    "    \"start_idx\": pl.Series([], dtype=pl.Int32),\n",
    "    \"end_idx_exclusive\": pl.Series([], dtype=pl.Int32),\n",
    "    \"n_months\": pl.Series([], dtype=pl.Int32),\n",
    "    \"ret_mean_seg\": pl.Series([], dtype=pl.Float64),\n",
    "    \"hit_excl_seg\": pl.Series([], dtype=pl.Float64),\n",
    "})\n",
    "\n",
    "print(\"ğŸ’¾ Guardando rupturas ...\")\n",
    "breaks_df.write_parquet(BREAKS_PARQ)\n",
    "BREAKS_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": os.environ[\"TWF_SYMBOL\"],\n",
    "    \"timeframe\": os.environ[\"TWF_TF\"],\n",
    "    \"note\": \"Rupturas por particionado Ã³ptimo con penalizaciÃ³n BIC (min 6 meses).\",\n",
    "    \"entries\": summary\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "_print_kv(\"BREAKS_PARQ\", BREAKS_PARQ)\n",
    "_print_kv(\"BREAKS_JSON\", BREAKS_JSON)\n",
    "print(\"âœ… Celda 9c (FINAL) completada.\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9 (v1 FIX FINAL): [DECAY TRAINâ†’TEST DE LOS SELECCIONADOS â†’ reports/wf_decay_*]\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "def _print_kv(k, v): print(f\"   - {k:<30}: {v}\")\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "assert CONFIG_PATH.exists(), f\"âŒ No existe configuraciÃ³n: {CONFIG_PATH}\"\n",
    "\n",
    "CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "PATHS       = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCORES_PARQ   = REPORTS_DIR / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SELECTED_PARQ = REPORTS_DIR / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "EVAL_PARQ     = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "DECAY_PARQ    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.parquet\"\n",
    "DECAY_JSON    = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.json\"\n",
    "SUMMARY_JSON  = REPORTS_DIR / f\"wf_decay_summary_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "_print_kv(\"SCORES_PARQ\", SCORES_PARQ)\n",
    "_print_kv(\"SELECTED_PARQ\", SELECTED_PARQ)\n",
    "_print_kv(\"EVAL_PARQ\", EVAL_PARQ)\n",
    "assert SCORES_PARQ.exists(),  f\"âŒ Falta entrada: {SCORES_PARQ}\"\n",
    "assert SELECTED_PARQ.exists(),f\"âŒ Falta entrada: {SELECTED_PARQ}\"\n",
    "assert EVAL_PARQ.exists(),    f\"âŒ Falta entrada: {EVAL_PARQ}\"\n",
    "\n",
    "print(\"ğŸ“¦ Cargando datasets WF ...\")\n",
    "train = pl.read_parquet(SCORES_PARQ)\n",
    "sel   = pl.read_parquet(SELECTED_PARQ)\n",
    "test  = pl.read_parquet(EVAL_PARQ)\n",
    "\n",
    "_print_kv(\"train_rows\", train.height); _print_kv(\"train_cols\", train.width)\n",
    "_print_kv(\"sel_rows\", sel.height);     _print_kv(\"sel_cols\", sel.width)\n",
    "_print_kv(\"test_rows\", test.height);   _print_kv(\"test_cols\", test.width)\n",
    "\n",
    "for c in [\"wf_id\",\"entry_col\"]:\n",
    "    assert (c in train.columns) and (c in sel.columns) and (c in test.columns), f\"âŒ Falta {c} en alguno de los datasets.\"\n",
    "\n",
    "keep_train = [c for c in [\"wf_id\",\"entry_col\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\"] if c in train.columns]\n",
    "train_k = train.select(keep_train).rename({\n",
    "    \"n_signals\":\"train_n\",\n",
    "    \"ret_mean\":\"train_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\":\"train_hit_excl\",\n",
    "    \"profit_factor_stat\":\"train_pf\",\n",
    "}, strict=False)  # <â€” tolerante\n",
    "\n",
    "keep_test = [c for c in [\"wf_id\",\"entry_col\",\"n_signals\",\"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in test.columns]\n",
    "test_k = test.select(keep_test).rename({\n",
    "    \"n_signals\":\"test_n\",\n",
    "    \"ret_mean\":\"test_ret_mean\",\n",
    "    \"hit_rate_tp_excl_to\":\"test_hit_excl\",\n",
    "    \"profit_factor_stat\":\"test_pf\",\n",
    "}, strict=False)  # <â€” tolerante\n",
    "\n",
    "sel_cols = [c for c in [\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in sel.columns]\n",
    "sel_k = sel.select(sel_cols).unique()\n",
    "\n",
    "print(\"ğŸ§® Calculando decay por ventana/entry_col ...\")\n",
    "decay = (\n",
    "    sel_k\n",
    "    .join(train_k, on=[\"wf_id\",\"entry_col\"], how=\"left\")\n",
    "    .join(test_k,  on=[\"wf_id\",\"entry_col\"], how=\"left\", suffix=\"_testdup\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"test_ret_mean\") - pl.col(\"train_ret_mean\")).alias(\"delta_ret_mean\"),\n",
    "        (pl.col(\"test_hit_excl\") - pl.col(\"train_hit_excl\")).alias(\"delta_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")       - pl.col(\"train_pf\")).alias(\"delta_pf\"),\n",
    "        (pl.col(\"test_ret_mean\").abs() / _safe_pos(pl.col(\"train_ret_mean\").abs(), 1e-12)).alias(\"ratio_ret_mean_abs\"),\n",
    "        (pl.col(\"test_hit_excl\")       / _safe_pos(pl.col(\"train_hit_excl\"), 1e-12)).alias(\"ratio_hit_excl\"),\n",
    "        (pl.col(\"test_pf\")             / _safe_pos(pl.col(\"train_pf\"), 1e-12)).alias(\"ratio_pf\"),\n",
    "        (pl.col(\"train_n\").fill_null(0) + pl.col(\"test_n\").fill_null(0)).alias(\"n_total_seen\"),\n",
    "    )\n",
    "    .sort([\"wf_id\",\"rank_in_window\",\"entry_col\"])\n",
    ")\n",
    "\n",
    "_print_kv(\"decay_rows\", decay.height)\n",
    "_print_kv(\"decay_cols\", decay.width)\n",
    "\n",
    "print(\"ğŸ’¾ Guardando wf_decay ...\")\n",
    "decay.write_parquet(DECAY_PARQ)\n",
    "with DECAY_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"timeframe\": TF,\n",
    "        \"rows_head\": decay.head(50).to_dicts(),\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ“ˆ Resumen de decay (global y ranking) ...\")\n",
    "has_test_n = \"test_n\" in decay.columns\n",
    "w = (pl.col(\"test_n\").fill_null(0).cast(pl.Float64())) if has_test_n else pl.lit(1.0)\n",
    "\n",
    "summary = (\n",
    "    decay\n",
    "    .with_columns(w.alias(\"__w__\"))\n",
    "    .select(\n",
    "        ( (pl.col(\"delta_ret_mean\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_delta_ret_mean\"),\n",
    "        ( (pl.col(\"delta_hit_excl\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_delta_hit_excl\"),\n",
    "        ( (pl.col(\"delta_pf\")       * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_delta_pf\"),\n",
    "        ( (pl.col(\"ratio_ret_mean_abs\") * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_ratio_ret_mean_abs\"),\n",
    "        ( (pl.col(\"ratio_hit_excl\")     * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_ratio_hit_excl\"),\n",
    "        ( (pl.col(\"ratio_pf\")           * pl.col(\"__w__\")).sum() / _safe_pos(pl.col(\"__w__\").sum(), 1e-12) ).alias(\"avg_ratio_pf\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "rank = (\n",
    "    decay\n",
    "    .select(\n",
    "        \"wf_id\",\"entry_col\",\"rank_in_window\",\n",
    "        \"train_ret_mean\",\"test_ret_mean\",\"delta_ret_mean\",\"ratio_ret_mean_abs\",\n",
    "        \"train_hit_excl\",\"test_hit_excl\",\"delta_hit_excl\",\"ratio_hit_excl\",\n",
    "        \"train_pf\",\"test_pf\",\"delta_pf\",\"ratio_pf\",\n",
    "        \"train_n\",\"test_n\",\"n_total_seen\"\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"ratio_pf\").fill_null(0.0).alias(\"_ratio_pf_\"),\n",
    "        pl.col(\"ratio_hit_excl\").fill_null(0.0).alias(\"_ratio_hit_\"),\n",
    "        pl.col(\"delta_ret_mean\").abs().fill_null(0.0).alias(\"_abs_dret_\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        ( (pl.col(\"_ratio_pf_\") + pl.col(\"_ratio_hit_\"))/2.0 - pl.col(\"_abs_dret_\") ).alias(\"robustness_score\")\n",
    "    )\n",
    "    .drop([\"_ratio_pf_\",\"_ratio_hit_\",\"_abs_dret_\"])\n",
    "    .sort([\"wf_id\",\"robustness_score\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "with SUMMARY_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL,\n",
    "        \"timeframe\": TF,\n",
    "        \"global_summary\": (summary.to_dicts()[0] if summary.height>0 else {}),\n",
    "        \"top5_by_robustness\": rank.sort(\"robustness_score\", descending=True).to_dicts()[:5],\n",
    "        \"bottom5_by_robustness\": rank.sort(\"robustness_score\", descending=False).to_dicts()[:5],\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_print_kv(\"DECAY_PARQ\", DECAY_PARQ)\n",
    "_print_kv(\"DECAY_JSON\", DECAY_JSON)\n",
    "_print_kv(\"SUMMARY_JSON\", SUMMARY_JSON)\n",
    "print(\"âœ… Celda 9 (v1 FIX FINAL) completada.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783261f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n & gates ...\n",
      "   - SYMBOL                        : BTCUSDT\n",
      "   - TF                            : 15m\n",
      "   - GATE_REQUIRE_DS_OK            : False\n",
      "   - GATE_MAX_PBO                  : 0.7\n",
      "   - MIN_TEST_N                    : 100\n",
      "   - MIN_TEST_HIT_EXCL             : 0.3\n",
      "   - MIN_TEST_PF                   : 0.9\n",
      "   - MIN_RATIO_PF                  : 0.85\n",
      "   - MIN_RATIO_HIT_EXCL            : 0.9\n",
      "   - MAX_DELTA_RET_MEAN_ABS        : None\n",
      "   - RANK_METRIC                   : test_pf\n",
      "   - MAX_CHAMPIONS_PER_WF          : 1\n",
      "   - MIN_WINDOWS_PASS_GLOBAL       : 1\n",
      "ğŸ“¦ Cargando datasets WF ...\n",
      "   - PBO_value                     : 0.25\n",
      "   - PBO_OK                        : True\n",
      "   - rows_base_before_gates        : 12\n",
      "ğŸš§ Aplicando gates por combo/ventana ...\n",
      "   - passed_gate_rows              : 0\n",
      "ğŸ† Seleccionando campeones por ventana ...\n",
      "   - wf_champions_rows             : 0\n",
      "   - wf_champions_cols             : 1\n",
      "ğŸ’¾ Guardando wf_champions ...\n",
      "   - WF_CHAMPS_PARQ                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_champions_BTCUSDT_15m.parquet\n",
      "   - WF_CHAMPS_JSON                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_champions_BTCUSDT_15m.json\n",
      "ğŸ“ˆ Construyendo resumen global y manifest ...\n",
      "   - GLOBAL_JSON                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\champions_global_BTCUSDT_15m.json\n",
      "   - MANIFEST_JSON                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\prod_manifest_BTCUSDT_15m.json\n",
      "âœ… Celda 10 finalizada sin errores.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 10 (wf_gate_export v1): [WF GATES & EXPORT CHAMPIONS â†’ reports/wf_champions_*, champions_global_*, prod_manifest_*]\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# - Requisitos previos (celdas 5â€“9, 7c_bis):\n",
    "#     forward_meta_{..}.json (dedup_completed==true), wf_train_scores_{..}.parquet, wf_selected_{..}.parquet,\n",
    "#     wf_select_eval_{..}.parquet, wf_decay_{..}.parquet/.json, wf_pbo_{..}.json, wf_ds_{..}.json (si se exige DS).\n",
    "# - Gates (configurables desde config_{..}.json â†’ cfg['wf']['champions'] / cfg['champions']):\n",
    "#     * PBO global <= max_pbo\n",
    "#     * DS ok por ventana (opcional)\n",
    "#     * En TEST por combo: n>=min_test_n, hit_excl>=min_test_hit, pf>=min_test_pf,\n",
    "#                          ratio_pf>=min_ratio_pf, ratio_hit_excl>=min_ratio_hit, |Î”ret_mean|<=max_delta_abs (opcional)\n",
    "# - SelecciÃ³n: por ventana, toma hasta K (max_champions_per_window) entre los que pasan gates, ordenados por rank_metric.\n",
    "# - Salidas:\n",
    "#     reports/wf_champions_{SYMBOL}_{TF}.parquet / .json\n",
    "#     reports/champions_global_{SYMBOL}_{TF}.json\n",
    "#     reports/prod_manifest_{SYMBOL}_{TF}.json\n",
    "# - Compat \"legacy\": no pl.sqrt(); manejo seguro de nulos/divisiones.\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "\n",
    "# -------------------------------------- Helpers --------------------------------------\n",
    "def _print_kv(k, v): print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _safe_pos(expr: pl.Expr, floor: float) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(floor)).then(pl.lit(floor)).otherwise(expr)\n",
    "\n",
    "def _read_json(path: Path) -> dict:\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"âŒ Error leyendo JSON: {path} â†’ {e}\")\n",
    "\n",
    "def _parse_cfg_gate(cfg: dict, section_keys: list[str], key: str, default):\n",
    "    \"\"\"\n",
    "    Busca cfg[section_keys[0]][section_keys[1]]...[key]; si no existe, devuelve default.\n",
    "    \"\"\"\n",
    "    node = cfg\n",
    "    try:\n",
    "        for s in section_keys:\n",
    "            node = (node.get(s, {}) if isinstance(node, dict) else {})\n",
    "        return node.get(key, default) if isinstance(node, dict) else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _extract_ds_ok_map(ds_json: dict, wf_ids: list[str]) -> dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Intenta extraer 'ok' por ventana desde estructuras flexibles:\n",
    "    - details: [{wf_id, ds_ok/shift_ok/ok, ...}]\n",
    "    - windows: idem\n",
    "    - global: ds_ok/shift_ok/ok\n",
    "    Por defecto: True si no se puede inferir.\n",
    "    \"\"\"\n",
    "    m: dict[str, bool] = {}\n",
    "    details = []\n",
    "    for k in (\"details\", \"windows\"):\n",
    "        if isinstance(ds_json.get(k, None), list):\n",
    "            details = ds_json[k]\n",
    "            break\n",
    "\n",
    "    for it in (details or []):\n",
    "        wid = it.get(\"wf_id\") or it.get(\"id\")\n",
    "        val = None\n",
    "        for kk in (\"ds_ok\", \"shift_ok\", \"ok\"):\n",
    "            if kk in it and isinstance(it[kk], bool):\n",
    "                val = bool(it[kk]); break\n",
    "        if wid is not None and isinstance(val, bool):\n",
    "            m[str(wid)] = val\n",
    "\n",
    "    if not m:\n",
    "        # Fallback global\n",
    "        global_ok = None\n",
    "        for kk in (\"ds_ok\", \"shift_ok\", \"ok\"):\n",
    "            if kk in ds_json and isinstance(ds_json[kk], bool):\n",
    "                global_ok = bool(ds_json[kk]); break\n",
    "        if isinstance(global_ok, bool):\n",
    "            for wid in wf_ids:\n",
    "                m[str(wid)] = global_ok\n",
    "\n",
    "    # Si aÃºn faltan ids, asumir True (no bloqueante)\n",
    "    for wid in wf_ids:\n",
    "        if str(wid) not in m:\n",
    "            m[str(wid)] = True\n",
    "\n",
    "    return m\n",
    "\n",
    "# -------------------------------------- A) Config/paths --------------------------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n & gates ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"âŒ Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CONFIG_PATH = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "CFG = _read_json(CONFIG_PATH)\n",
    "\n",
    "PATHS           = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR     = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR        = PATHS[\"logs\"]\n",
    "FORWARD_META    = LOGS_DIR / f\"forward_meta_{SYMBOL}_{TF}.json\"\n",
    "WF_JSON         = LOGS_DIR / f\"wf_windows_{SYMBOL}_{TF}.json\"\n",
    "TRAIN_SCORES_PQ = REPORTS_DIR / f\"wf_train_scores_{SYMBOL}_{TF}.parquet\"\n",
    "SEL_PARQ        = REPORTS_DIR / f\"wf_selected_{SYMBOL}_{TF}.parquet\"\n",
    "EVAL_PARQ       = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "DECAY_PARQ      = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.parquet\"\n",
    "DECAY_JSON      = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.json\"\n",
    "PBO_JSON        = REPORTS_DIR / f\"wf_pbo_{SYMBOL}_{TF}.json\"\n",
    "DS_JSON         = REPORTS_DIR / f\"wf_ds_{SYMBOL}_{TF}.json\"  # creado en 7c_bis_ds\n",
    "\n",
    "# Salidas\n",
    "WF_CHAMPS_PARQ  = REPORTS_DIR / f\"wf_champions_{SYMBOL}_{TF}.parquet\"\n",
    "WF_CHAMPS_JSON  = REPORTS_DIR / f\"wf_champions_{SYMBOL}_{TF}.json\"\n",
    "GLOBAL_JSON     = REPORTS_DIR / f\"champions_global_{SYMBOL}_{TF}.json\"\n",
    "MANIFEST_JSON   = REPORTS_DIR / f\"prod_manifest_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# Gates/params (con defaults razonables; se pueden ajustar en config)\n",
    "GATE_REQUIRE_DS_OK      = bool(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"require_ds_ok\", False))\n",
    "GATE_MAX_PBO            = float(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"max_pbo\", 0.70))\n",
    "MIN_TEST_N              = int(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_test_n_signals\", 100))\n",
    "MIN_TEST_HIT_EXCL       = float(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_test_hit_excl\", 0.30))\n",
    "MIN_TEST_PF             = float(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_test_pf\", 0.90))\n",
    "MIN_RATIO_PF            = float(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_ratio_pf\", 0.85))\n",
    "MIN_RATIO_HIT_EXCL      = float(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_ratio_hit_excl\", 0.90))\n",
    "MAX_DELTA_RET_MEAN_ABS  = _parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"max_delta_ret_mean_abs\", None)  # None => no gate\n",
    "RANK_METRIC             = str(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"rank_metric\", \"test_pf\"))\n",
    "MAX_CHAMPIONS_PER_WF    = int(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"max_champions_per_window\", 1))\n",
    "MIN_WINDOWS_PASS_GLOBAL = int(_parse_cfg_gate(CFG, [\"wf\",\"champions\"], \"min_windows_pass\", 1))\n",
    "\n",
    "# Dedup gate\n",
    "assert FORWARD_META.exists(), f\"âŒ Falta META de dedup: {FORWARD_META}. Ejecuta 4d.\"\n",
    "meta = _read_json(FORWARD_META)\n",
    "assert meta.get(\"dedup_completed\", False) is True, \"âŒ forward no estÃ¡ deduplicado (Celda 4d).\"\n",
    "\n",
    "# Archivos base\n",
    "for p in [TRAIN_SCORES_PQ, SEL_PARQ, EVAL_PARQ, DECAY_PARQ, DECAY_JSON, PBO_JSON, WF_JSON]:\n",
    "    assert Path(p).exists(), f\"âŒ Falta entrada requerida: {p}\"\n",
    "if GATE_REQUIRE_DS_OK:\n",
    "    assert DS_JSON.exists(), f\"âŒ Falta {DS_JSON}. Ejecuta 07c_bis_ds.\"\n",
    "\n",
    "_print_kv(\"SYMBOL\", SYMBOL); _print_kv(\"TF\", TF)\n",
    "_print_kv(\"GATE_REQUIRE_DS_OK\", GATE_REQUIRE_DS_OK)\n",
    "_print_kv(\"GATE_MAX_PBO\", GATE_MAX_PBO)\n",
    "_print_kv(\"MIN_TEST_N\", MIN_TEST_N)\n",
    "_print_kv(\"MIN_TEST_HIT_EXCL\", MIN_TEST_HIT_EXCL)\n",
    "_print_kv(\"MIN_TEST_PF\", MIN_TEST_PF)\n",
    "_print_kv(\"MIN_RATIO_PF\", MIN_RATIO_PF)\n",
    "_print_kv(\"MIN_RATIO_HIT_EXCL\", MIN_RATIO_HIT_EXCL)\n",
    "_print_kv(\"MAX_DELTA_RET_MEAN_ABS\", MAX_DELTA_RET_MEAN_ABS)\n",
    "_print_kv(\"RANK_METRIC\", RANK_METRIC)\n",
    "_print_kv(\"MAX_CHAMPIONS_PER_WF\", MAX_CHAMPIONS_PER_WF)\n",
    "_print_kv(\"MIN_WINDOWS_PASS_GLOBAL\", MIN_WINDOWS_PASS_GLOBAL)\n",
    "\n",
    "# -------------------------------------- B) Carga datasets --------------------------------------\n",
    "print(\"ğŸ“¦ Cargando datasets WF ...\")\n",
    "eval_df   = pl.read_parquet(EVAL_PARQ)\n",
    "decay_df  = pl.read_parquet(DECAY_PARQ)\n",
    "sel_df    = pl.read_parquet(SEL_PARQ)\n",
    "wf_obj    = _read_json(WF_JSON)\n",
    "wf_ids    = [w.get(\"id\") for w in wf_obj.get(\"windows\", []) if \"id\" in w]\n",
    "pbo_obj   = _read_json(PBO_JSON)\n",
    "ds_obj    = _read_json(DS_JSON) if DS_JSON.exists() else {}\n",
    "\n",
    "# PBO gate (global)\n",
    "pbo_val = pbo_obj.get(\"PBO\", None)\n",
    "pbo_ok  = True if (pbo_val is None) else (pbo_val <= GATE_MAX_PBO)\n",
    "_print_kv(\"PBO_value\", pbo_val); _print_kv(\"PBO_OK\", pbo_ok)\n",
    "\n",
    "# DS gate (por ventana)\n",
    "ds_ok_map = _extract_ds_ok_map(ds_obj, [str(x) for x in wf_ids])\n",
    "ds_ok_df  = pl.DataFrame({\"wf_id\": list(ds_ok_map.keys()), \"ds_ok\": list(ds_ok_map.values())}) if ds_ok_map else pl.DataFrame({\"wf_id\":[], \"ds_ok\":[]})\n",
    "\n",
    "# Reducimos eval a metadatos (evita colisiones con decay)\n",
    "keep_eval_cols = [c for c in [\"wf_id\",\"test_start\",\"test_end\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\"] if c in eval_df.columns]\n",
    "eval_meta = eval_df.select(keep_eval_cols) if keep_eval_cols else pl.DataFrame({\"wf_id\": pl.Series([], dtype=pl.Utf8)})\n",
    "\n",
    "# Decay trae train_* y test_* + ratios/deltas; lo usaremos como base\n",
    "need_decay_cols = [\n",
    "    \"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\",  # por si viniera de decay (tenemos algunos)\n",
    "    \"train_n\",\"test_n\",\n",
    "    \"train_ret_mean\",\"test_ret_mean\",\"delta_ret_mean\",\"ratio_ret_mean_abs\",\n",
    "    \"train_hit_excl\",\"test_hit_excl\",\"delta_hit_excl\",\"ratio_hit_excl\",\n",
    "    \"train_pf\",\"test_pf\",\"delta_pf\",\"ratio_pf\"\n",
    "]\n",
    "have_decay_cols = [c for c in need_decay_cols if c in decay_df.columns]\n",
    "base = decay_df.select(have_decay_cols)\n",
    "\n",
    "# Unimos metadatos (para fechas de test y clusters si faltan)\n",
    "if eval_meta.height > 0:\n",
    "    base = base.join(eval_meta, on=[\"wf_id\",\"entry_col\"], how=\"left\", suffix=\"_evaldup\")\n",
    "\n",
    "# DS por ventana\n",
    "if ds_ok_df.height > 0:\n",
    "    base = base.join(ds_ok_df, on=\"wf_id\", how=\"left\")\n",
    "else:\n",
    "    base = base.with_columns(pl.lit(True).alias(\"ds_ok\"))\n",
    "\n",
    "# PBO global\n",
    "base = base.with_columns(pl.lit(bool(pbo_ok)).alias(\"pbo_ok\"))\n",
    "\n",
    "_print_kv(\"rows_base_before_gates\", base.height)\n",
    "assert base.height > 0, \"âŒ No hay filas en base para evaluar gates.\"\n",
    "\n",
    "# -------------------------------------- C) Aplicar Gates --------------------------------------\n",
    "print(\"ğŸš§ Aplicando gates por combo/ventana ...\")\n",
    "\n",
    "# Condiciones dinÃ¡micas\n",
    "gate_expr = pl.lit(True)\n",
    "\n",
    "# n seÃ±ales en TEST\n",
    "if \"test_n\" in base.columns and MIN_TEST_N is not None and MIN_TEST_N > 0:\n",
    "    gate_expr = gate_expr & (pl.col(\"test_n\").fill_null(0) >= pl.lit(MIN_TEST_N))\n",
    "\n",
    "# hit rate excl TO en TEST\n",
    "if \"test_hit_excl\" in base.columns and MIN_TEST_HIT_EXCL is not None:\n",
    "    gate_expr = gate_expr & (pl.col(\"test_hit_excl\").fill_null(-1.0) >= pl.lit(MIN_TEST_HIT_EXCL))\n",
    "\n",
    "# PF en TEST\n",
    "if \"test_pf\" in base.columns and MIN_TEST_PF is not None:\n",
    "    gate_expr = gate_expr & (pl.col(\"test_pf\").fill_null(-1.0) >= pl.lit(MIN_TEST_PF))\n",
    "\n",
    "# Ratios\n",
    "if \"ratio_pf\" in base.columns and MIN_RATIO_PF is not None:\n",
    "    gate_expr = gate_expr & (pl.col(\"ratio_pf\").fill_null(-1.0) >= pl.lit(MIN_RATIO_PF))\n",
    "\n",
    "if \"ratio_hit_excl\" in base.columns and MIN_RATIO_HIT_EXCL is not None:\n",
    "    gate_expr = gate_expr & (pl.col(\"ratio_hit_excl\").fill_null(-1.0) >= pl.lit(MIN_RATIO_HIT_EXCL))\n",
    "\n",
    "# |Î”ret_mean| absoluto\n",
    "if (\"delta_ret_mean\" in base.columns) and (MAX_DELTA_RET_MEAN_ABS is not None):\n",
    "    gate_expr = gate_expr & (pl.col(\"delta_ret_mean\").abs().fill_null(1e9) <= pl.lit(float(MAX_DELTA_RET_MEAN_ABS)))\n",
    "\n",
    "# DS gate (si se exige)\n",
    "if GATE_REQUIRE_DS_OK:\n",
    "    gate_expr = gate_expr & (pl.col(\"ds_ok\").fill_null(False) == pl.lit(True))\n",
    "\n",
    "# PBO gate global\n",
    "gate_expr = gate_expr & (pl.col(\"pbo_ok\") == pl.lit(True))\n",
    "\n",
    "scored = base.with_columns(gate_expr.alias(\"gate_pass\"))\n",
    "\n",
    "n_pass = int(scored.filter(pl.col(\"gate_pass\")==True).height)\n",
    "_print_kv(\"passed_gate_rows\", n_pass)\n",
    "\n",
    "# -------------------------------------- D) Campeones por ventana --------------------------------------\n",
    "print(\"ğŸ† Seleccionando campeones por ventana ...\")\n",
    "\n",
    "# Orden de ranking (desc) para mÃ©tricas; rank_in_window asc como Ãºltimo criterio\n",
    "rank_primary = RANK_METRIC if RANK_METRIC in scored.columns else (\"test_pf\" if \"test_pf\" in scored.columns else \"test_hit_excl\")\n",
    "sort_cols = [c for c in [rank_primary, \"ratio_pf\", \"test_hit_excl\"] if c in scored.columns]\n",
    "sort_desc = [True] * len(sort_cols)\n",
    "\n",
    "# Filtramos aprobados y ordenamos\n",
    "approved = (\n",
    "    scored\n",
    "    .filter(pl.col(\"gate_pass\")==True)\n",
    "    .sort([\"wf_id\"] + sort_cols + ([\"rank_in_window\"] if \"rank_in_window\" in scored.columns else []),\n",
    "          descending=[False] + sort_desc + ([False] if \"rank_in_window\" in scored.columns else []))\n",
    ")\n",
    "\n",
    "# Tomar hasta K por wf_id\n",
    "# Nota: group_by(\"wf_id\").head(K) respeta el orden de apariciÃ³n\n",
    "if approved.height > 0:\n",
    "    champs = approved.group_by(\"wf_id\").head(MAX_CHAMPIONS_PER_WF)\n",
    "else:\n",
    "    champs = pl.DataFrame({\"wf_id\": pl.Series([], dtype=pl.Utf8)})\n",
    "\n",
    "_print_kv(\"wf_champions_rows\", champs.height)\n",
    "_print_kv(\"wf_champions_cols\", champs.width)\n",
    "\n",
    "# Persistimos campeones por ventana\n",
    "print(\"ğŸ’¾ Guardando wf_champions ...\")\n",
    "champs.write_parquet(WF_CHAMPS_PARQ)\n",
    "WF_CHAMPS_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rank_metric\": rank_primary,\n",
    "    \"max_champions_per_window\": int(MAX_CHAMPIONS_PER_WF),\n",
    "    \"gates\": {\n",
    "        \"require_ds_ok\": bool(GATE_REQUIRE_DS_OK),\n",
    "        \"max_pbo\": float(GATE_MAX_PBO),\n",
    "        \"min_test_n\": int(MIN_TEST_N),\n",
    "        \"min_test_hit_excl\": float(MIN_TEST_HIT_EXCL),\n",
    "        \"min_test_pf\": float(MIN_TEST_PF),\n",
    "        \"min_ratio_pf\": float(MIN_RATIO_PF),\n",
    "        \"min_ratio_hit_excl\": float(MIN_RATIO_HIT_EXCL),\n",
    "        \"max_delta_ret_mean_abs\": (float(MAX_DELTA_RET_MEAN_ABS) if MAX_DELTA_RET_MEAN_ABS is not None else None),\n",
    "    },\n",
    "    \"rows\": champs.to_dicts()[:200]\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"WF_CHAMPS_PARQ\", WF_CHAMPS_PARQ); _print_kv(\"WF_CHAMPS_JSON\", WF_CHAMPS_JSON)\n",
    "\n",
    "# -------------------------------------- E) Resumen global & Manifest --------------------------------------\n",
    "print(\"ğŸ“ˆ Construyendo resumen global y manifest ...\")\n",
    "\n",
    "if champs.height > 0:\n",
    "    # Agregados por entry_col sobre campeones\n",
    "    agg_cols = [c for c in [\"test_pf\",\"test_hit_excl\",\"ratio_pf\",\"ratio_hit_excl\",\"delta_ret_mean\",\"test_n\"] if c in champs.columns]\n",
    "    global_tbl = (\n",
    "        champs\n",
    "        .group_by(\"entry_col\")\n",
    "        .agg(\n",
    "            pl.len().alias(\"windows_pass\"),\n",
    "            *([pl.col(\"test_pf\").mean().alias(\"avg_test_pf\")] if \"test_pf\" in champs.columns else []),\n",
    "            *([pl.col(\"test_hit_excl\").mean().alias(\"avg_test_hit_excl\")] if \"test_hit_excl\" in champs.columns else []),\n",
    "            *([pl.col(\"ratio_pf\").mean().alias(\"avg_ratio_pf\")] if \"ratio_pf\" in champs.columns else []),\n",
    "            *([pl.col(\"ratio_hit_excl\").mean().alias(\"avg_ratio_hit_excl\")] if \"ratio_hit_excl\" in champs.columns else []),\n",
    "            *([pl.col(\"delta_ret_mean\").abs().mean().alias(\"avg_abs_delta_ret_mean\")] if \"delta_ret_mean\" in champs.columns else []),\n",
    "            *([pl.col(\"test_n\").sum().alias(\"sum_test_n\")] if \"test_n\" in champs.columns else []),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (\n",
    "                (\n",
    "                    (pl.col(\"avg_ratio_pf\") if \"avg_ratio_pf\" in champs.group_by(\"entry_col\").agg(pl.len()).columns else pl.lit(0.0))\n",
    "                    +\n",
    "                    (pl.col(\"avg_ratio_hit_excl\") if \"avg_ratio_hit_excl\" in champs.group_by(\"entry_col\").agg(pl.len()).columns else pl.lit(0.0))\n",
    "                ) / 2.0\n",
    "                - (pl.col(\"avg_abs_delta_ret_mean\") if \"avg_abs_delta_ret_mean\" in champs.group_by(\"entry_col\").agg(pl.len()).columns else pl.lit(0.0))\n",
    "            ).alias(\"score_global\")\n",
    "        )\n",
    "        .sort([\"windows_pass\",\"score_global\"], descending=[True, True])\n",
    "    )\n",
    "else:\n",
    "    global_tbl = pl.DataFrame({\"entry_col\": pl.Series([], dtype=pl.Utf8)})\n",
    "\n",
    "topN_preview = global_tbl.head(10).to_dicts() if global_tbl.height > 0 else []\n",
    "\n",
    "GLOBAL_JSON.write_text(json.dumps({\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "    \"rank_metric\": rank_primary,\n",
    "    \"summary_top10\": topN_preview\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"GLOBAL_JSON\", GLOBAL_JSON)\n",
    "\n",
    "# Manifest de producciÃ³n: combos con >= MIN_WINDOWS_PASS_GLOBAL apariciones como campeÃ³n\n",
    "if global_tbl.height > 0:\n",
    "    manifest_df = global_tbl.filter(pl.col(\"windows_pass\") >= pl.lit(int(MIN_WINDOWS_PASS_GLOBAL))).select(\"entry_col\",\"windows_pass\",\"score_global\")\n",
    "    manifest = {\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "        \"min_windows_pass\": int(MIN_WINDOWS_PASS_GLOBAL),\n",
    "        \"pbo_ok\": bool(pbo_ok),\n",
    "        \"pbo_value\": pbo_val,\n",
    "        \"require_ds_ok\": bool(GATE_REQUIRE_DS_OK),\n",
    "        \"entries\": manifest_df.to_dicts()\n",
    "    }\n",
    "else:\n",
    "    manifest = {\n",
    "        \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"symbol\": SYMBOL, \"timeframe\": TF,\n",
    "        \"min_windows_pass\": int(MIN_WINDOWS_PASS_GLOBAL),\n",
    "        \"pbo_ok\": bool(pbo_ok),\n",
    "        \"pbo_value\": pbo_val,\n",
    "        \"require_ds_ok\": bool(GATE_REQUIRE_DS_OK),\n",
    "        \"entries\": []\n",
    "    }\n",
    "\n",
    "MANIFEST_JSON.write_text(json.dumps(manifest, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "_print_kv(\"MANIFEST_JSON\", MANIFEST_JSON)\n",
    "\n",
    "print(\"âœ… Celda 10 finalizada sin errores.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa59b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Leyendo configuraciÃ³n de reportes ...\n",
      "   - KPI_PARQ                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\kpi_BTCUSDT_15m.parquet\n",
      "   - WF_EVAL_PARQ                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_select_eval_BTCUSDT_15m.parquet\n",
      "   - WF_DECAY_PARQ                 : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_decay_BTCUSDT_15m.parquet\n",
      "   - WF_CHAMPS_PARQ                : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_champions_BTCUSDT_15m.parquet\n",
      "   - PBO_JSON                      : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\wf_pbo_BTCUSDT_15m.json\n",
      "   - QC_JSON                       : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\surfaces_qc_summary_BTCUSDT_15m.json\n",
      "   - Styling (jinja2)              : False\n",
      "ğŸ§© Construyendo HTML global ...\n",
      "   - HTML_GLOBAL                   : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\report_global_BTCUSDT_15m.html\n",
      "ğŸ§© Construyendo HTML por ventana ...\n",
      "   - HTML_WINDOWS                  : C:\\Quant\\TWF\\outputs\\BTCUSDT\\15m\\reports\\report_windows_BTCUSDT_15m.html\n",
      "âœ… Celda 11 finalizada sin errores (con fallback de estilo si falta jinja2).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 11 (reportes HTML v1.1): fallback sin jinja2 (HTML manual con degradado inline)\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Entradas: kpi_*.parquet, wf_select_eval_*.parquet, wf_decay_*.parquet/json, wf_champions_*.parquet,\n",
    "#           wf_pbo_*.json, states_*.parquet, surfaces_* y surfaces_qc_summary_*.json\n",
    "# Salidas : report_global_{SYMBOL}_{TF}.html  y  report_windows_{SYMBOL}_{TF}.html\n",
    "# Naturaleza: 100% estadÃ­stico (sin mÃ©tricas de backtest). No modifica artefactos previos.\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, math, importlib.util\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "HAS_J2 = importlib.util.find_spec(\"jinja2\") is not None\n",
    "\n",
    "def _print_kv(k, v): \n",
    "    print(f\"   - {k:<30}: {v}\")\n",
    "\n",
    "def _read_json_safe(p: Path, default=None):\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return default\n",
    "    return default\n",
    "\n",
    "# -------- helpers de render --------\n",
    "\n",
    "def _is_numeric_dtype(ser):\n",
    "    try:\n",
    "        import numpy as np\n",
    "        return pd.api.types.is_numeric_dtype(ser)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _df_to_html_manual(pdf: pd.DataFrame, caption: str = \"\", cmap_columns=None, precision=6):\n",
    "    \"\"\"Render HTML manual con degradado por-columna vÃ­a HSL (sin Styler).\"\"\"\n",
    "    if pdf.shape[0] == 0:\n",
    "        return f'<h3>{caption}</h3><p><i>(sin filas)</i></p>'\n",
    "    pdf = pdf.copy()\n",
    "\n",
    "    if cmap_columns is None:\n",
    "        cmap_columns = [c for c in pdf.columns if _is_numeric_dtype(pdf[c])]\n",
    "\n",
    "    # rangos por columna (evitar divisiÃ³n por cero)\n",
    "    col_minmax = {}\n",
    "    for c in cmap_columns:\n",
    "        ser = pd.to_numeric(pdf[c], errors=\"coerce\")\n",
    "        vmin = float(pd.Series(ser).min(skipna=True)) if ser.notna().any() else 0.0\n",
    "        vmax = float(pd.Series(ser).max(skipna=True)) if ser.notna().any() else 1.0\n",
    "        if math.isfinite(vmin) and math.isfinite(vmax) and vmax - vmin > 1e-12:\n",
    "            col_minmax[c] = (vmin, vmax)\n",
    "        else:\n",
    "            col_minmax[c] = (0.0, 1.0)\n",
    "\n",
    "    def color_for(val, vmin, vmax):\n",
    "        try:\n",
    "            x = float(val)\n",
    "            if not math.isfinite(x):\n",
    "                return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "        t = (x - vmin) / (vmax - vmin + 1e-12)\n",
    "        t = 0.0 if t < 0 else 1.0 if t > 1 else t\n",
    "        # 0=rojo (h=0), 1=verde (h=120)\n",
    "        hue = 120.0 * t\n",
    "        return f\"background-color:hsl({hue:.0f},70%,65%);\"\n",
    "\n",
    "    # construir tabla\n",
    "    html = []\n",
    "    html.append(\"<table>\")\n",
    "    if caption:\n",
    "        html.append(f\"<caption style='caption-side:top;font-weight:bold;font-size:14px'>{caption}</caption>\")\n",
    "    # header\n",
    "    html.append(\"<thead><tr>\")\n",
    "    for c in pdf.columns:\n",
    "        html.append(f\"<th style='text-align:center;font-size:12px'>{c}</th>\")\n",
    "    html.append(\"</tr></thead>\")\n",
    "    # body\n",
    "    html.append(\"<tbody>\")\n",
    "    for _, row in pdf.iterrows():\n",
    "        html.append(\"<tr>\")\n",
    "        for c in pdf.columns:\n",
    "            val = row[c]\n",
    "            styles = \"\"\n",
    "            if c in col_minmax and _is_numeric_dtype(pdf[c]):\n",
    "                vmin, vmax = col_minmax[c]\n",
    "                styles = color_for(val, vmin, vmax)\n",
    "            # formateo\n",
    "            if isinstance(val, float):\n",
    "                sval = f\"{val:.{precision}f}\"\n",
    "            else:\n",
    "                sval = str(val)\n",
    "            html.append(f\"<td style='border:1px solid #ddd;padding:4px 6px;{styles}'>{sval}</td>\")\n",
    "        html.append(\"</tr>\")\n",
    "    html.append(\"</tbody></table>\")\n",
    "    return \"\".join(html)\n",
    "\n",
    "def _df_to_html(df: pl.DataFrame, caption: str = \"\", cmap_columns=None, precision=6):\n",
    "    \"\"\"Usa pandas Styler si hay jinja2, si no usa motor manual con degradado inline.\"\"\"\n",
    "    if df.is_empty():\n",
    "        return f'<h3>{caption}</h3><p><i>(sin filas)</i></p>'\n",
    "    pdf = df.to_pandas()\n",
    "    # columnas numÃ©ricas por defecto\n",
    "    if cmap_columns is None:\n",
    "        cmap_columns = [c for c in pdf.columns if _is_numeric_dtype(pdf[c])]\n",
    "\n",
    "    if HAS_J2:\n",
    "        try:\n",
    "            sty = (pdf.style\n",
    "                   .format(precision=precision)\n",
    "                   .background_gradient(subset=cmap_columns, cmap=\"RdYlGn\")\n",
    "                   .set_table_styles([\n",
    "                       {'selector': 'th', 'props': [('font-size', '12px'), ('text-align', 'center')]},\n",
    "                       {'selector': 'td', 'props': [('font-size', '12px')]},\n",
    "                       {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-weight', 'bold'), ('font-size', '14px')]}\n",
    "                   ])\n",
    "                   .set_caption(caption)\n",
    "                   .hide(axis=\"index\"))\n",
    "            return sty.to_html()\n",
    "        except Exception:\n",
    "            # fallback manual si falla Styler por cualquier razÃ³n\n",
    "            pass\n",
    "    return _df_to_html_manual(pdf, caption=caption, cmap_columns=cmap_columns, precision=precision)\n",
    "\n",
    "def _matrix_to_html(path_parq: Path, title: str, value_label: str = \"valor\", precision=6):\n",
    "    \"\"\"Carga un wide y lo convierte en HTML con degradado; fallback manual sin jinja2.\"\"\"\n",
    "    if not path_parq.exists():\n",
    "        return \"\"\n",
    "    w = pl.read_parquet(path_parq)\n",
    "    if w.is_empty():\n",
    "        return \"\"\n",
    "    pdf = w.to_pandas().set_index(\"entry_col\")\n",
    "    if HAS_J2:\n",
    "        try:\n",
    "            sty = (pdf.style\n",
    "                   .format(precision=precision)\n",
    "                   .background_gradient(cmap=\"RdYlGn\", axis=None)\n",
    "                   .set_table_styles([\n",
    "                       {'selector': 'th', 'props': [('font-size', '12px'), ('text-align', 'center')]},\n",
    "                       {'selector': 'td', 'props': [('font-size', '12px')]},\n",
    "                       {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-weight', 'bold'), ('font-size', '14px')]}\n",
    "                   ])\n",
    "                   .set_caption(f\"{title} â€” {value_label}\"))\n",
    "            return sty.to_html()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # manual\n",
    "    pdf = pdf.reset_index()\n",
    "    return _df_to_html_manual(pdf, caption=f\"{title} â€” {value_label}\", cmap_columns=[c for c in pdf.columns if c != \"entry_col\"], precision=precision)\n",
    "\n",
    "# -------------------- A) Config y rutas --------------------\n",
    "print(\"ğŸ“– Leyendo configuraciÃ³n de reportes ...\")\n",
    "need_env = (\"TWF_SYMBOL\",\"TWF_TF\",\"TWF_START_DATE\",\"TWF_END_DATE\",\"TWF_SEED_BASE\")\n",
    "assert all(k in os.environ for k in need_env), \"Faltan TWF_*; ejecuta celdas previas.\"\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]; TF = os.environ[\"TWF_TF\"]\n",
    "\n",
    "PROJ_ROOT   = Path(os.environ.get(\"TWF_BASE\", Path.cwd())).resolve()\n",
    "CFG_PATH    = PROJ_ROOT / \"outputs\" / SYMBOL / TF / \"logs\" / f\"config_{SYMBOL}_{TF}.json\"\n",
    "CFG         = json.loads(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "PATHS       = {k: Path(v) for k, v in CFG[\"paths\"].items()}\n",
    "REPORTS_DIR = PATHS[\"reports\"]; REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Entradas esperadas\n",
    "KPI_PARQ        = REPORTS_DIR / f\"kpi_{SYMBOL}_{TF}.parquet\"\n",
    "WF_EVAL_PARQ    = REPORTS_DIR / f\"wf_select_eval_{SYMBOL}_{TF}.parquet\"\n",
    "WF_DECAY_PARQ   = REPORTS_DIR / f\"wf_decay_{SYMBOL}_{TF}.parquet\"\n",
    "WF_DECAY_SUM    = REPORTS_DIR / f\"wf_decay_summary_{SYMBOL}_{TF}.json\"\n",
    "WF_CHAMPS_PARQ  = REPORTS_DIR / f\"wf_champions_{SYMBOL}_{TF}.parquet\"\n",
    "PBO_JSON        = REPORTS_DIR / f\"wf_pbo_{SYMBOL}_{TF}.json\"\n",
    "QC_JSON         = REPORTS_DIR / f\"surfaces_qc_summary_{SYMBOL}_{TF}.json\"\n",
    "STATES_PARQ     = REPORTS_DIR / f\"states_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "# Salidas\n",
    "HTML_GLOBAL     = REPORTS_DIR / f\"report_global_{SYMBOL}_{TF}.html\"\n",
    "HTML_WINDOWS    = REPORTS_DIR / f\"report_windows_{SYMBOL}_{TF}.html\"\n",
    "\n",
    "_print_kv(\"KPI_PARQ\", KPI_PARQ)\n",
    "_print_kv(\"WF_EVAL_PARQ\", WF_EVAL_PARQ)\n",
    "_print_kv(\"WF_DECAY_PARQ\", WF_DECAY_PARQ)\n",
    "_print_kv(\"WF_CHAMPS_PARQ\", WF_CHAMPS_PARQ)\n",
    "_print_kv(\"PBO_JSON\", PBO_JSON)\n",
    "_print_kv(\"QC_JSON\", QC_JSON)\n",
    "_print_kv(\"Styling (jinja2)\", HAS_J2)\n",
    "\n",
    "# -------------------- B) Cargar artefactos -----------------\n",
    "kpi      = pl.read_parquet(KPI_PARQ)          if KPI_PARQ.exists() else pl.DataFrame()\n",
    "eval_df  = pl.read_parquet(WF_EVAL_PARQ)      if WF_EVAL_PARQ.exists() else pl.DataFrame()\n",
    "decay    = pl.read_parquet(WF_DECAY_PARQ)     if WF_DECAY_PARQ.exists() else pl.DataFrame()\n",
    "decay_js = _read_json_safe(WF_DECAY_SUM, default={})\n",
    "champs   = pl.read_parquet(WF_CHAMPS_PARQ)    if WF_CHAMPS_PARQ.exists() else pl.DataFrame()\n",
    "pbo_js   = _read_json_safe(PBO_JSON, default={})\n",
    "qc_js    = _read_json_safe(QC_JSON, default={\"summary\": []})\n",
    "\n",
    "# Para detectar state_types presentes (para heatmaps)\n",
    "state_types = []\n",
    "if STATES_PARQ.exists():\n",
    "    st = pl.read_parquet(STATES_PARQ).select(\"state_type\").unique()\n",
    "    state_types = [r[0] for r in st.iter_rows()]\n",
    "\n",
    "# -------------------- C) Secciones HTML (Global) -----------\n",
    "print(\"ğŸ§© Construyendo HTML global ...\")\n",
    "sections = []\n",
    "\n",
    "# Header\n",
    "header_html = f\"\"\"\n",
    "<h1>Reporte EstadÃ­stico â€” {SYMBOL} {TF}</h1>\n",
    "<p>Generado UTC: {datetime.now(timezone.utc).isoformat(timespec=\"seconds\")}</p>\n",
    "<hr/>\n",
    "\"\"\"\n",
    "sections.append(header_html)\n",
    "\n",
    "# PBO\n",
    "if pbo_js:\n",
    "    pbo_val = pbo_js.get(\"PBO\", None)\n",
    "    usable  = pbo_js.get(\"windows_usable\", 0)\n",
    "    if pbo_val is not None:\n",
    "        pbo_html = f\"\"\"\n",
    "        <h2>PBO (Overfit Probability)</h2>\n",
    "        <p><b>PBO</b>: {float(pbo_val):.4f} &nbsp; | &nbsp; <b>Ventanas evaluadas</b>: {int(usable)}</p>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        pbo_html = \"<h2>PBO (Overfit Probability)</h2><p><i>No disponible</i></p>\"\n",
    "    # Detalle (limitado)\n",
    "    details = pbo_js.get(\"details\", [])[:50]\n",
    "    if details:\n",
    "        det_df = pl.DataFrame(details)\n",
    "        pbo_html += _df_to_html(det_df, caption=\"Detalle PBO (primeras filas)\")\n",
    "    sections.append(pbo_html)\n",
    "\n",
    "# Campeones\n",
    "if not champs.is_empty():\n",
    "    if all(c in champs.columns for c in [\"test_ret_mean\",\"test_hit_excl\",\"test_pf\",\"delta_ret_mean\",\"ratio_hit_excl\",\"ratio_pf\",\"n_total_seen\"]):\n",
    "        champs_view = champs.select([\n",
    "            \"wf_id\",\"entry_col\",\"rank_in_window\",\n",
    "            \"test_ret_mean\",\"test_hit_excl\",\"test_pf\",\n",
    "            \"delta_ret_mean\",\"ratio_hit_excl\",\"ratio_pf\",\"n_total_seen\"\n",
    "        ])\n",
    "    else:\n",
    "        champs_view = champs\n",
    "    sections.append(_df_to_html(\n",
    "        champs_view, caption=\"Campeones por ventana\",\n",
    "        cmap_columns=[c for c in champs_view.columns if c not in (\"wf_id\",\"entry_col\",\"rank_in_window\")]\n",
    "    ))\n",
    "\n",
    "# Decay\n",
    "if not decay.is_empty():\n",
    "    cols = [c for c in [\n",
    "        \"wf_id\",\"entry_col\",\"train_n\",\"test_n\",\n",
    "        \"train_ret_mean\",\"test_ret_mean\",\"delta_ret_mean\",\"ratio_ret_mean_abs\",\n",
    "        \"train_hit_excl\",\"test_hit_excl\",\"delta_hit_excl\",\"ratio_hit_excl\",\n",
    "        \"train_pf\",\"test_pf\",\"delta_pf\",\"ratio_pf\"\n",
    "    ] if c in decay.columns]\n",
    "    decay_view = decay.select(cols).sort([\"wf_id\",\"entry_col\"])\n",
    "    sections.append(_df_to_html(\n",
    "        decay_view, caption=\"Decay TRAINâ†’TEST (seleccionados)\",\n",
    "        cmap_columns=[c for c in cols if c not in (\"wf_id\",\"entry_col\",\"train_n\",\"test_n\")]\n",
    "    ))\n",
    "    if decay_js:\n",
    "        top5 = pl.DataFrame(decay_js.get(\"top5_by_robustness\", []))\n",
    "        bot5 = pl.DataFrame(decay_js.get(\"bottom5_by_robustness\", []))\n",
    "        if not top5.is_empty():\n",
    "            sections.append(_df_to_html(top5, caption=\"Top-5 robustez (segÃºn resumen)\", cmap_columns=None))\n",
    "        if not bot5.is_empty():\n",
    "            sections.append(_df_to_html(bot5, caption=\"Bottom-5 robustez (segÃºn resumen)\", cmap_columns=None))\n",
    "\n",
    "# KPIs globales (Celda 5)\n",
    "if not kpi.is_empty():\n",
    "    cols = [c for c in [\n",
    "        \"entry_col\",\"n_signals\",\"tp\",\"sl\",\"timeout\",\n",
    "        \"ret_mean\",\"ret_median\",\"ret_std\",\n",
    "        \"hit_rate_tp_incl_to\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\",\n",
    "        \"ret_mean_ci95_lo\",\"ret_mean_ci95_hi\",\n",
    "        \"hit_rate_excl_to_ci95_lo\",\"hit_rate_excl_to_ci95_hi\"\n",
    "    ] if c in kpi.columns]\n",
    "    kpi_view = kpi.select(cols).sort(\"entry_col\")\n",
    "    sections.append(_df_to_html(\n",
    "        kpi_view, caption=\"KPIs globales por entry_col (forward por-seÃ±al)\",\n",
    "        cmap_columns=[c for c in cols if c not in (\"entry_col\",\"tp\",\"sl\",\"timeout\",\"n_signals\")]\n",
    "    ))\n",
    "\n",
    "# QC de surfaces (resumen)\n",
    "if qc_js and qc_js.get(\"summary\"):\n",
    "    qc_df = pl.DataFrame(qc_js[\"summary\"])\n",
    "    sections.append(_df_to_html(\n",
    "        qc_df, caption=\"QC de surfaces por estado (resumen)\",\n",
    "        cmap_columns=[c for c in qc_df.columns if c not in (\"state_type\",\"flag_name\")]\n",
    "    ))\n",
    "\n",
    "# Heatmaps de surfaces (ret_mean)\n",
    "heatmap_blocks = []\n",
    "for s in state_types:\n",
    "    w_ret = REPORTS_DIR / f\"surfaces_wide_ret_mean_{SYMBOL}_{TF}_{s}.parquet\"\n",
    "    if w_ret.exists():\n",
    "        heatmap_blocks.append(_matrix_to_html(w_ret, title=f\"Surface â€” {s}\", value_label=\"ret_mean\", precision=6))\n",
    "if heatmap_blocks:\n",
    "    sections.append(\"<h2>Surfaces (ret_mean)</h2>\" + \"\".join(heatmap_blocks))\n",
    "\n",
    "# Ensamblar y guardar GLOBAL\n",
    "GLOBAL_HTML = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>Reporte EstadÃ­stico â€” {SYMBOL} {TF}</title>\n",
    "<style>\n",
    "body {{ font-family: Arial, sans-serif; margin: 16px; }}\n",
    "h1, h2, h3 {{ margin: 6px 0; }}\n",
    "table {{ border-collapse: collapse; margin: 8px 0; }}\n",
    "th, td {{ border: 1px solid #ddd; padding: 4px 6px; font-size: 12px; }}\n",
    "caption {{ caption-side: top; font-weight: bold; font-size: 14px; }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "{''.join(sections)}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "HTML_GLOBAL.write_text(GLOBAL_HTML, encoding=\"utf-8\")\n",
    "_print_kv(\"HTML_GLOBAL\", HTML_GLOBAL)\n",
    "\n",
    "# -------------------- D) Reporte por ventana ----------------\n",
    "print(\"ğŸ§© Construyendo HTML por ventana ...\")\n",
    "win_sections = [f\"\"\"\n",
    "<h1>Reporte por Ventana â€” {SYMBOL} {TF}</h1>\n",
    "<p>Generado UTC: {datetime.now(timezone.utc).isoformat(timespec=\"seconds\")}</p>\n",
    "<hr/>\"\"\"]\n",
    "\n",
    "if not eval_df.is_empty():\n",
    "    # Lista de ventanas en eval\n",
    "    win_ids = eval_df.select(\"wf_id\").unique().to_series().to_list()\n",
    "    for wid in win_ids:\n",
    "        sub = eval_df.filter(pl.col(\"wf_id\")==wid).sort([\"rank_in_window\",\"entry_col\"])\n",
    "        title = f\"Ventana {wid} â€” Seleccionados (TEST)\"\n",
    "        cols = [c for c in [\n",
    "            \"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\",\"n_signals\",\n",
    "            \"ret_mean\",\"hit_rate_tp_excl_to\",\"profit_factor_stat\",\n",
    "            \"ret_mean_ci95_lo\",\"ret_mean_ci95_hi\",\n",
    "            \"hit_rate_excl_to_ci95_lo\",\"hit_rate_excl_to_ci95_hi\"\n",
    "        ] if c in sub.columns]\n",
    "        win_sections.append(_df_to_html(\n",
    "            sub.select(cols), caption=title,\n",
    "            cmap_columns=[c for c in cols if c not in (\"wf_id\",\"entry_col\",\"rank_in_window\",\"cluster_id\",\"cluster_size\",\"n_signals\")]\n",
    "        ))\n",
    "else:\n",
    "    win_sections.append(\"<p><i>No hay evaluaciÃ³n TEST para mostrar.</i></p>\")\n",
    "\n",
    "WINDOWS_HTML = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>Reporte por Ventana â€” {SYMBOL} {TF}</title>\n",
    "<style>\n",
    "body {{ font-family: Arial, sans-serif; margin: 16px; }}\n",
    "h1, h2, h3 {{ margin: 6px 0; }}\n",
    "table {{ border-collapse: collapse; margin: 8px 0; }}\n",
    "th, td {{ border: 1px solid #ddd; padding: 4px 6px; font-size: 12px; }}\n",
    "caption {{ caption-side: top; font-weight: bold; font-size: 14px; }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "{''.join(win_sections)}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "HTML_WINDOWS.write_text(WINDOWS_HTML, encoding=\"utf-8\")\n",
    "_print_kv(\"HTML_WINDOWS\", HTML_WINDOWS)\n",
    "\n",
    "print(\"âœ… Celda 11 finalizada sin errores (con fallback de estilo si falta jinja2).\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
