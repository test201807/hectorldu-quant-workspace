{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aae076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_00] Paso 0: inicio Celda 0 (Notebook)\n",
      "[cell_00] START :: setup reproducible + estructura outputs\n",
      "[cell_00] Paso 1: Parámetros globales set :: symbol=BTCUSDT, tf=5m, seed_base=20251024\n",
      "[cell_00] Paso 2: semillas :: PYTHONHASHSEED=20251024\n",
      "[cell_00] INFO  :: seeds_set :: PYTHONHASHSEED=20251024\n",
      "[cell_00] Paso 2.1: Política semillas por ventana (ejemplo): wf_id=0, seed_run=20251024\n",
      "[cell_00] INFO  :: seed_run_map :: wf_id=0, seed_run=20251024, timestamp_utc=2025-11-01 18:20:15.993512+00:00\n",
      "[cell_00] Paso 2.2: Política seed_bayes (ejemplo): seed_bayes=7435148838969597\n",
      "[cell_00] INFO  :: seed_bayes_map :: seed_bayes=7435148838969597\n",
      "[cell_00] INFO :: blas_single_thread_set :: OMP/MKL/OPENBLAS_NUM_THREADS=1\n",
      "[cell_00] Paso 3: Zona horaria fijada a UTC global.\n",
      "[cell_00] Paso 4: outputs :: {'root': 'C:\\\\Quant\\\\TWF\\\\outputs\\\\BTCUSDT\\\\5m', 'stats': 'C:\\\\Quant\\\\TWF\\\\outputs\\\\BTCUSDT\\\\5m\\\\stats', 'figures': 'C:\\\\Quant\\\\TWF\\\\outputs\\\\BTCUSDT\\\\5m\\\\figures', 'deliverables': 'C:\\\\Quant\\\\TWF\\\\outputs\\\\BTCUSDT\\\\5m\\\\deliverables', 'logs': 'C:\\\\Quant\\\\TWF\\\\outputs\\\\BTCUSDT\\\\5m\\\\logs'}\n",
      "[cell_00] INFO  :: mkdir_ok :: root -> abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m, rel: outputs\\BTCUSDT\\5m\n",
      "[cell_00] INFO  :: mkdir_ok :: stats -> abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\stats, rel: outputs\\BTCUSDT\\5m\\stats\n",
      "[cell_00] INFO  :: mkdir_ok :: figures -> abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\figures, rel: outputs\\BTCUSDT\\5m\\figures\n",
      "[cell_00] INFO  :: mkdir_ok :: deliverables -> abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables, rel: outputs\\BTCUSDT\\5m\\deliverables\n",
      "[cell_00] INFO  :: mkdir_ok :: logs -> abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\logs, rel: outputs\\BTCUSDT\\5m\\logs\n",
      "[cell_00] Paso 5: Paralelización fijada: backend='loky', batch_size=1, prefer='processes'\n",
      "[cell_00] Ejemplo orden estable: combo_key=('EMA', 5, 20, 90, 24, 0.005, -0.003, 10, 0.5), hash=-9179542424340358818\n",
      "[cell_00] Paso 6: runtime_defaults.json existe; no cambios.\n",
      "[cell_00] INFO  :: runtime_defaults_present :: n_jobs=8\n",
      "[cell_00] Paso 7: snapshot -> C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\logs\\run_snapshot_BTCUSDT_5m.json\n",
      "[cell_00] INFO  :: env_snapshot_saved :: run_snapshot_BTCUSDT_5m.json\n",
      "[cell_00] INFO  :: env_snapshot :: {\"python\": \"3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\", \"platform\": \"win32\", \"numpy\": \"2.3.4\", \"polars\": \"1.34.0\", \"pyarrow\": \"1.34.0\", \"scipy\": \"1.16.2\", \"joblib\": \"1.5.2\", \"statsmodels\": \"0.14.5\", \"ruptures\": \"v1.1.10\", \"matplotlib\": \"3.10.7\"}\n",
      "[cell_00] Paso 8: FLAGS actualizados :: {'STRICT_CHRONO': True, 'NO_OVERLAP_EVAL': True, 'BLOCK_BOOTSTRAP_B': 2000, 'BATCH_SIZE': 5000}\n",
      "[cell_00] Validación: Joblib policies (backend=loky, batch=1, prefer=processes) fijadas.\n",
      "[cell_00] END   :: elapsed_s=0.005 :: Celda 0 completada (Notebook)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 0 — Setup / Reproducibilidad / Sanity (revisada)\n",
    "===============================================================================\n",
    "Propósito\n",
    "---------\n",
    "- Inicializa el entorno del proyecto para una corrida estadística (NO trading).\n",
    "- Asegura reproducibilidad (semillas fijas) y crea la estructura de *outputs*.\n",
    "- Registra todo en consola y en CSV de logs para depuración paso a paso.\n",
    "- Según PARTE 1/7 del documento revisado: Asegura determinismo, trazabilidad y reglas anti-fuga; fija estructura de carpetas y logs.\n",
    "- Modificación: Incluye ER en ejemplos de combo_key para compatibilidad.\n",
    "- Corrección: Centraliza set de os.environ aquí (cambia params globales solo aquí); no hardcodes/defaults.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import polars as pl\n",
    "import scipy\n",
    "import joblib\n",
    "import statsmodels\n",
    "import ruptures\n",
    "import matplotlib\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "\n",
    "CELL = \"cell_00\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 0 (Notebook)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"setup reproducible + estructura outputs\")\n",
    "\n",
    "# --- Centralizar set de variables de entorno (CAMBIA SOLO AQUÍ los params; propaga a todo el pipeline) ---\n",
    "# Nota: Asegúrate de tener datos suficientes para el rango y TF seleccionados (e.g., descarga histórica si es necesario; el pipeline asume forward_parquet de Celda 5 tiene barras suficientes para ventanas WF).\n",
    "os.environ[\"TWF_SYMBOL\"] = \"BTCUSDT\"  # Cambia solo aquí el símbolo\n",
    "os.environ[\"TWF_TF\"] = \"5m\"  # Cambia solo aquí el timeframe\n",
    "os.environ[\"TWF_START_DATE\"] = \"2024-01-01\"  # Cambia solo aquí start (YYYY-MM-DD)\n",
    "os.environ[\"TWF_END_DATE\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")  # Cambia solo aquí end\n",
    "os.environ[\"TWF_SEED_BASE\"] = \"20251024\"  # Cambia solo aquí semilla base\n",
    "\n",
    "SYMBOL = os.environ[\"TWF_SYMBOL\"]\n",
    "TF = os.environ[\"TWF_TF\"]\n",
    "SEED_BASE = int(os.environ[\"TWF_SEED_BASE\"])\n",
    "print(f\"[{CELL}] Paso 1: Parámetros globales set :: symbol={SYMBOL}, tf={TF}, seed_base={SEED_BASE}\")\n",
    "\n",
    "# --- Semillas fijas (determinismo) ---\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_BASE)\n",
    "random.seed(SEED_BASE)\n",
    "np.random.seed(SEED_BASE)\n",
    "print(f\"[{CELL}] Paso 2: semillas :: PYTHONHASHSEED={SEED_BASE}\")\n",
    "write_event(LOG_CSV, CELL, \"seeds_set\", f\"PYTHONHASHSEED={SEED_BASE}\")\n",
    "\n",
    "# Política semillas por ventana (ejemplo wf_id=0)\n",
    "seed_run = SEED_BASE + 0  # Para wf_id=0\n",
    "print(f\"[{CELL}] Paso 2.1: Política semillas por ventana (ejemplo): wf_id=0, seed_run={seed_run}\")\n",
    "write_event(LOG_CSV, CELL, \"seed_run_map\", f\"wf_id=0, seed_run={seed_run}, timestamp_utc={datetime.now(timezone.utc)}\")\n",
    "\n",
    "# Política seed_bayes (determinista basado en symbol/tf)\n",
    "seed_bayes = hash((SEED_BASE, SYMBOL, TF)) % (10**16)\n",
    "print(f\"[{CELL}] Paso 2.2: Política seed_bayes (ejemplo): seed_bayes={seed_bayes}\")\n",
    "write_event(LOG_CSV, CELL, \"seed_bayes_map\", f\"seed_bayes={seed_bayes}\")\n",
    "\n",
    "# BLAS single-thread para reproducibilidad\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "print(f\"[{CELL}] INFO :: blas_single_thread_set :: OMP/MKL/OPENBLAS_NUM_THREADS=1\")\n",
    "\n",
    "# --- Zona horaria UTC ---\n",
    "os.environ[\"TZ\"] = \"UTC\"\n",
    "print(f\"[{CELL}] Paso 3: Zona horaria fijada a UTC global.\")\n",
    "\n",
    "# --- Estructura outputs (dinámica por SYMBOL/TF actual, no crea extras) ---\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "print(f\"[{CELL}] Paso 4: outputs :: { {k: str(v) for k, v in paths.items()} }\")\n",
    "for k, p in paths.items():\n",
    "    write_event(LOG_CSV, CELL, \"mkdir_ok\", f\"{k} -> abs: {p}, rel: {p.relative_to(BASE)}\")\n",
    "\n",
    "# --- Paralelización determinista ---\n",
    "os.environ[\"JOBLIB_BACKEND\"] = \"loky\"\n",
    "os.environ[\"JOBLIB_BATCH_SIZE\"] = \"1\"\n",
    "os.environ[\"JOBLIB_PREFER\"] = \"processes\"\n",
    "print(f\"[{CELL}] Paso 5: Paralelización fijada: backend='loky', batch_size=1, prefer='processes'\")\n",
    "\n",
    "# --- Ejemplo combo_key con hash estable (para validación orden) ---\n",
    "example_combo = ('EMA', 5, 20, 90, 24, 0.005, -0.003, 10, 0.5)\n",
    "combo_hash = hash(example_combo)\n",
    "print(f\"[{CELL}] Ejemplo orden estable: combo_key={example_combo}, hash={combo_hash}\")\n",
    "\n",
    "# --- Persistir n_jobs en runtime_defaults.json si no existe ---\n",
    "runtime_path = BASE / \"runtime_defaults.json\"\n",
    "if not runtime_path.exists():\n",
    "    n_jobs = os.cpu_count() - 1 if os.cpu_count() > 1 else 1\n",
    "    runtime = {\"n_jobs\": n_jobs}\n",
    "    runtime_path.write_text(json.dumps(runtime, indent=2))\n",
    "    print(f\"[{CELL}] Paso 6: runtime_defaults.json creado con n_jobs={n_jobs}.\")\n",
    "else:\n",
    "    print(f\"[{CELL}] Paso 6: runtime_defaults.json existe; no cambios.\")\n",
    "    write_event(LOG_CSV, CELL, \"runtime_defaults_present\", f\"n_jobs={json.loads(runtime_path.read_text())['n_jobs']}\")\n",
    "\n",
    "# --- Snapshot entorno ---\n",
    "versions = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": sys.platform,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"polars\": pl.__version__,\n",
    "    \"pyarrow\": pl.__version__,  # Asume pyarrow instalado con polars\n",
    "    \"scipy\": scipy.__version__,\n",
    "    \"joblib\": joblib.__version__,\n",
    "    \"statsmodels\": statsmodels.__version__,\n",
    "    \"ruptures\": ruptures.__version__,\n",
    "    \"matplotlib\": matplotlib.__version__\n",
    "}\n",
    "snapshot_path = paths[\"logs\"] / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "snapshot_path.write_text(json.dumps(versions, indent=2))\n",
    "print(f\"[{CELL}] Paso 7: snapshot -> {snapshot_path}\")\n",
    "write_event(LOG_CSV, CELL, \"env_snapshot_saved\", snapshot_path.name)\n",
    "write_event(LOG_CSV, CELL, \"env_snapshot\", json.dumps(versions))\n",
    "\n",
    "# --- FLAGS fijos ---\n",
    "FLAGS = {'STRICT_CHRONO': True, 'NO_OVERLAP_EVAL': True, 'BLOCK_BOOTSTRAP_B': 2000, 'BATCH_SIZE': 5000}\n",
    "print(f\"[{CELL}] Paso 8: FLAGS actualizados :: {FLAGS}\")\n",
    "\n",
    "# Validación final joblib/BLAS\n",
    "print(f\"[{CELL}] Validación: Joblib policies (backend=loky, batch=1, prefer=processes) fijadas.\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 0 completada (Notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc1fd91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_01] Paso 0: inicio Celda 1 (Configuración global)\n",
      "[cell_01] START :: configuración cerrada del experimento\n",
      "[cell_01] Paso 1: Config global set y propagada :: symbol=BTCUSDT, tf=5m, start=2024-01-01, end=2025-11-01\n",
      "[cell_01] Paso 2: Cardinalidad :: total=1728 (ma=144, thr=3, er=4)\n",
      "[cell_01] Paso 3: Config persistida en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\config_BTCUSDT_5m.json\n",
      "[cell_01] INFO  :: config_saved :: outputs\\BTCUSDT\\5m\\config_BTCUSDT_5m.json\n",
      "[cell_01] END   :: elapsed_s=0.002 :: Celda 1 completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 1 — Configuración Global del Experimento (revisada)\n",
    "===============================================================================\n",
    "Propósito\n",
    "---------\n",
    "- Define y persiste la configuración global (símbolo, timeframe, fechas, rejillas, etc.).\n",
    "- Calcula cardinalidad del universo post-filtros; persiste en JSON.\n",
    "- Según PARTE 1/7: Cierra universo y rejillas con restricciones fijas (GOOD > |BAD|).\n",
    "- Modificación: Incluir rejilla ER general (ventanas/thr); ejemplo combo_key con ER.\n",
    "- Corrección: Centralizar set de os.environ aquí (único lugar para cambiar params globales).\n",
    "             Unificar ruta config a root outputs/SYMBOL/TF (para consistencia con Celda 4).\n",
    "             Agregar claves para WF (train_bars, test_bars, n_min, k_top) para Celda 7.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "\n",
    "CELL = \"cell_01\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 1 (Configuración global)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"configuración cerrada del experimento\")\n",
    "\n",
    "# --- Leer config central de Celda 0 ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\")\n",
    "TF = os.environ.get(\"TWF_TF\")\n",
    "START_DATE = os.environ.get(\"TWF_START_DATE\")\n",
    "END_DATE = os.environ.get(\"TWF_END_DATE\")\n",
    "if not all([SYMBOL, TF, START_DATE, END_DATE]):\n",
    "    raise ValueError(\"Configuración no set en Celda 0. Ejecuta Celda 0 primero.\")\n",
    "\n",
    "print(f\"[{CELL}] Paso 1: Config global set y propagada :: symbol={SYMBOL}, tf={TF}, start={START_DATE}, end={END_DATE}\")\n",
    "\n",
    "# --- Definir grids (fijas, no dependen de TF) ---\n",
    "MA_TYPES = [\"EMA\"]\n",
    "FAST_GRID = list(range(5, 21, 5))\n",
    "MID_GRID = list(range(20, 51, 10))\n",
    "SLOW_GRID = list(range(50, 101, 20))\n",
    "H_GRID = [24, 48, 72]\n",
    "GOOD_THR_GRID = [0.005, 0.01]\n",
    "BAD_THR_GRID = [-0.003, -0.005]\n",
    "ER_WINDOW_GRID = [10, 20]\n",
    "ER_THR_GRID = [0.5, 0.6]\n",
    "\n",
    "# Filtros: GOOD > |BAD|\n",
    "thr_combos = [ (g, b) for g, b in product(GOOD_THR_GRID, BAD_THR_GRID) if g > abs(b) ]\n",
    "\n",
    "# Cardinalidad\n",
    "ma_card = len(MA_TYPES) * len(FAST_GRID) * len(MID_GRID) * len(SLOW_GRID) * len(H_GRID)\n",
    "thr_card = len(thr_combos)\n",
    "er_card = len(ER_WINDOW_GRID) * len(ER_THR_GRID)\n",
    "total_card = ma_card * thr_card * er_card\n",
    "print(f\"[{CELL}] Paso 2: Cardinalidad :: total={total_card} (ma={ma_card}, thr={thr_card}, er={er_card})\")\n",
    "\n",
    "# --- Config full (incluye WF params) ---\n",
    "config = {\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"start_date\": START_DATE,\n",
    "    \"end_date\": END_DATE,\n",
    "    \"ma_types\": MA_TYPES,\n",
    "    \"fast_grid\": FAST_GRID,\n",
    "    \"mid_grid\": MID_GRID,\n",
    "    \"slow_grid\": SLOW_GRID,\n",
    "    \"h_grid\": H_GRID,\n",
    "    \"good_thr_grid\": GOOD_THR_GRID,\n",
    "    \"bad_thr_grid\": BAD_THR_GRID,\n",
    "    \"er_window_grid\": ER_WINDOW_GRID,\n",
    "    \"er_thr_grid\": ER_THR_GRID,\n",
    "    \"train_bars\": 1000,  # Para WF\n",
    "    \"test_bars\": 200,\n",
    "    \"n_min\": 10,\n",
    "    \"k_top\": 50,\n",
    "    \"n_jobs\": json.loads((BASE / \"runtime_defaults.json\").read_text())[\"n_jobs\"],\n",
    "    \"block_bootstrap_b\": 2000\n",
    "}\n",
    "\n",
    "# Persistir\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "config_path = paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "config_path.write_text(json.dumps(config, indent=2))\n",
    "print(f\"[{CELL}] Paso 3: Config persistida en {config_path}\")\n",
    "write_event(LOG_CSV, CELL, \"config_saved\", config_path.relative_to(BASE))\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 1 completada\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f787336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_02] Paso 0: inicio Celda 2 (Ingesta/cache Binance y manifest datos)\n",
      "[cell_02] START :: ingesta/cache Binance y manifest datos\n",
      "[cell_02] Paso 1: rango temporal :: from_utc=2024-01-01T00:00:00+00:00, to_utc=2025-11-01T00:00:00+00:00\n",
      "[cell_02] INFO  :: rango_temporal :: {\"from_utc\": \"2024-01-01T00:00:00+00:00\", \"to_utc\": \"2025-11-01T00:00:00+00:00\"}\n",
      "[cell_02] Paso 2: generado desde descarga - iniciando descarga en chunks\n",
      "[cell_02]   - Chunk #1: 2024-01-01 00:00:00+00:00 a 2024-01-31 00:00:00+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_1396\\1358276617.py:125: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  df_chunk = pl.DataFrame(data, schema=[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_02]     -> rows=8641 (total: 8641)\n",
      "[cell_02]   - Chunk #2: 2024-01-31 00:00:00+00:00 a 2024-03-01 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 17282)\n",
      "[cell_02]   - Chunk #3: 2024-03-01 00:00:00+00:00 a 2024-03-31 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 25923)\n",
      "[cell_02]   - Chunk #4: 2024-03-31 00:00:00+00:00 a 2024-04-30 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 34564)\n",
      "[cell_02]   - Chunk #5: 2024-04-30 00:00:00+00:00 a 2024-05-30 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 43205)\n",
      "[cell_02]   - Chunk #6: 2024-05-30 00:00:00+00:00 a 2024-06-29 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 51846)\n",
      "[cell_02]   - Chunk #7: 2024-06-29 00:00:00+00:00 a 2024-07-29 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 60487)\n",
      "[cell_02]   - Chunk #8: 2024-07-29 00:00:00+00:00 a 2024-08-28 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 69128)\n",
      "[cell_02]   - Chunk #9: 2024-08-28 00:00:00+00:00 a 2024-09-27 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 77769)\n",
      "[cell_02]   - Chunk #10: 2024-09-27 00:00:00+00:00 a 2024-10-27 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 86410)\n",
      "[cell_02]   - Chunk #11: 2024-10-27 00:00:00+00:00 a 2024-11-26 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 95051)\n",
      "[cell_02]   - Chunk #12: 2024-11-26 00:00:00+00:00 a 2024-12-26 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 103692)\n",
      "[cell_02]   - Chunk #13: 2024-12-26 00:00:00+00:00 a 2025-01-25 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 112333)\n",
      "[cell_02]   - Chunk #14: 2025-01-25 00:00:00+00:00 a 2025-02-24 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 120974)\n",
      "[cell_02]   - Chunk #15: 2025-02-24 00:00:00+00:00 a 2025-03-26 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 129615)\n",
      "[cell_02]   - Chunk #16: 2025-03-26 00:00:00+00:00 a 2025-04-25 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 138256)\n",
      "[cell_02]   - Chunk #17: 2025-04-25 00:00:00+00:00 a 2025-05-25 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 146897)\n",
      "[cell_02]   - Chunk #18: 2025-05-25 00:00:00+00:00 a 2025-06-24 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 155538)\n",
      "[cell_02]   - Chunk #19: 2025-06-24 00:00:00+00:00 a 2025-07-24 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 164179)\n",
      "[cell_02]   - Chunk #20: 2025-07-24 00:00:00+00:00 a 2025-08-23 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 172820)\n",
      "[cell_02]   - Chunk #21: 2025-08-23 00:00:00+00:00 a 2025-09-22 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 181461)\n",
      "[cell_02]   - Chunk #22: 2025-09-22 00:00:00+00:00 a 2025-10-22 00:00:00+00:00\n",
      "[cell_02]     -> rows=8641 (total: 190102)\n",
      "[cell_02]   - Chunk #23: 2025-10-22 00:00:00+00:00 a 2025-11-01 00:00:00+00:00\n",
      "[cell_02]     -> rows=2881 (total: 192983)\n",
      "[cell_02] Limpieza: rows=192961, gaps=0, max_gap=0.00 días\n",
      "[cell_02] Paso 3: Parquet generado :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\ohlc_BTCUSDT_5m.parquet\n",
      "[cell_02] INFO  :: parquet_generated :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\ohlc_BTCUSDT_5m.parquet\n",
      "[cell_02] Paso 4: hashes calculados :: md5=3d19576485cbcad8243ff8233570e7df, sha256=aeb156b5b561f4a0c31506ed0f4917cfac6acca488b13cf09973c96c12558200, bytes=4119842\n",
      "[cell_02] Paso 5: manifest generado :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\manifest_data_BTCUSDT_5m.json\n",
      "[cell_02] INFO  :: MANIFEST_BUILT :: md5=3d19576485cbcad8243ff8233570e7df, bytes=4119842, schema_ok=True\n",
      "[cell_02] INFO  :: json_saved :: abs: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\logs\\run_snapshot_BTCUSDT_5m.json, rel: outputs\\BTCUSDT\\5m\\logs\\run_snapshot_BTCUSDT_5m.json\n",
      "[cell_02] Paso 6: snapshot actualizado con manifest_hashes\n",
      "[cell_02] INFO  :: snapshot_updated :: manifest_hashes added\n",
      "[cell_02] END   :: elapsed_s=102.213 :: Celda 2 completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   2  —  I N G E S T A   Y   C A C H E   D E   D A T O S   D E   B I N A N C E\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Obtener velas cerradas reales de Binance, persistir en Parquet con esquema fijo y generar manifest con hashes y metadatos.\n",
    "- Soporte para cache-first, con opción de fuerza descarga via entorno FORCE_DOWNLOAD=True.\n",
    "- Fusiona lógica anterior de Cell_03 para descargas en chunks grandes (e.g., mensuales) para rangos extensos.\n",
    "- Según PARTE 1/7 del documento revisado: ingesta/cache de Binance y manifest de datos.\n",
    "\n",
    "Qué HACE\n",
    "--------\n",
    "1) Determina rango temporal desde entorno (BACK_DAYS default 30, o START/END_ISO opcional; START/END_DATE de config si set).\n",
    "2) Chequea cache: si Parquet existe y no FORCE_DOWNLOAD, usa cache.\n",
    "3) Si no cache o fuerza, descarga klines reales en chunks (e.g., 30 días) via API, solo velas cerradas.\n",
    "4) Normaliza a Polars DataFrame con esquema fijo (ts tz=UTC, Open High Low Close Volume Float64); ordena, drop dups/NaN/inf, reporta gaps.\n",
    "5) Persiste ohlc_{symbol}_{tf}.parquet con compression='zstd'.\n",
    "6) Calcula bytes, md5 y sha256 del Parquet.\n",
    "7) Genera manifest con metadatos, hashes, gaps info.\n",
    "8) Actualiza run_snapshot con manifest_hashes.\n",
    "9) Loguea eventos.\n",
    "\n",
    "Qué NO HACE\n",
    "-----------\n",
    "- No rellena huecos (mantiene principio original; gaps reportados pero no fix).\n",
    "- No calcula MAs, etiquetas o forward (eso en Celdas 4-5).\n",
    "- No simula datos (solo reales cerradas).\n",
    "\n",
    "Entradas (vía entorno)\n",
    "----------------------\n",
    "- TWF_SYMBOL (str): símbolo Binance (default 'BTCUSDT').\n",
    "- TWF_TF (str): timeframe Binance (default '5m').\n",
    "- TWF_BACK_DAYS (int): días atrás si no START/END (default 30).\n",
    "- TWF_START_ISO / TWF_END_ISO (str opcional): ISO UTC.\n",
    "- FORCE_DOWNLOAD (bool str): 'True' para fuerza descarga ignorando cache.\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- ohlc_{symbol}_{tf}.parquet en deliverables/.\n",
    "- manifest_data_{symbol}_{tf}.json en deliverables/.\n",
    "- Actualización de run_snapshot_{symbol}_{tf}.json.\n",
    "\n",
    "Cierres de Ambigüedad\n",
    "---------------------\n",
    "- Hashes: md5/sha256 en chunks 1MiB, hex minúscula.\n",
    "- Descarga: velas cerradas only, chunks de 30 días para rangos largos.\n",
    "- Gaps: reportados en manifest (count, max en días).\n",
    "\n",
    "Criterios de Aceptación\n",
    "-----------------------\n",
    "- Parquet con esquema forzado, cache o descarga.\n",
    "- Manifest con hashes, gaps, closed_candles_only=True.\n",
    "- Aborto si rango inválido o descarga vacía.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import polars as pl\n",
    "import requests  # Added import for HTTP requests\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_02\"\n",
    "\n",
    "# Parámetros de entorno\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "BACK_DAYS = int(os.environ.get(\"TWF_BACK_DAYS\", \"30\"))  # Aumentado default a 30\n",
    "START_ISO = os.environ.get(\"TWF_START_ISO\", \"\").strip()\n",
    "END_ISO = os.environ.get(\"TWF_END_ISO\", \"\").strip()\n",
    "FORCE_DOWNLOAD = os.environ.get(\"FORCE_DOWNLOAD\", \"False\") == \"True\"\n",
    "CHUNK_DAYS = 30  # Chunks para descargas grandes\n",
    "\n",
    "# Función para ms por TF\n",
    "def _ms_per_tf(tf: str) -> int:\n",
    "    unit = tf[-1].lower()\n",
    "    n = int(tf[:-1])\n",
    "    if unit == \"m\": return n * 60_000\n",
    "    if unit == \"h\": return n * 3_600_000\n",
    "    if unit == \"d\": return n * 86_400_000\n",
    "    if unit == \"w\": return n * 604_800_000\n",
    "    raise ValueError(f\"Timeframe no soportado: {tf}\")\n",
    "\n",
    "# Parsear ISO a datetime UTC\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    dt = datetime.fromisoformat(s)\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "# Función para descargar klines de Binance (con loop interno para >limit bars)\n",
    "def download_klines(symbol: str, tf: str, start_dt: datetime, end_dt: datetime) -> pl.DataFrame:\n",
    "    url = \"https://api.binance.com/api/v3/klines\"\n",
    "    chunks = []\n",
    "    cursor = start_dt\n",
    "    limit = 1000\n",
    "    while cursor < end_dt:\n",
    "        start_ts = int(cursor.timestamp() * 1000)\n",
    "        end_ts = int(end_dt.timestamp() * 1000)\n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"interval\": tf,\n",
    "            \"startTime\": start_ts,\n",
    "            \"endTime\": end_ts,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        resp = requests.get(url, params=params)\n",
    "        if resp.status_code != 200:\n",
    "            raise ValueError(f\"Error descargando klines: {resp.text}\")\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            break\n",
    "        df_chunk = pl.DataFrame(data, schema=[\n",
    "            (\"open_time\", pl.Int64),\n",
    "            (\"open\", pl.Utf8),\n",
    "            (\"high\", pl.Utf8),\n",
    "            (\"low\", pl.Utf8),\n",
    "            (\"close\", pl.Utf8),\n",
    "            (\"volume\", pl.Utf8),\n",
    "            (\"close_time\", pl.Int64),\n",
    "            (\"quote_asset_volume\", pl.Utf8),\n",
    "            (\"number_of_trades\", pl.Int64),\n",
    "            (\"taker_buy_base_asset_volume\", pl.Utf8),\n",
    "            (\"taker_buy_quote_asset_volume\", pl.Utf8),\n",
    "            (\"ignore\", pl.Int64)\n",
    "        ])\n",
    "        df_chunk = df_chunk.select([\n",
    "            pl.col(\"open_time\").cast(pl.Datetime).dt.convert_time_zone(\"UTC\").alias(\"ts\"),\n",
    "            pl.col(\"open\").cast(pl.Float64).alias(\"Open\"),\n",
    "            pl.col(\"high\").cast(pl.Float64).alias(\"High\"),\n",
    "            pl.col(\"low\").cast(pl.Float64).alias(\"Low\"),\n",
    "            pl.col(\"close\").cast(pl.Float64).alias(\"Close\"),\n",
    "            pl.col(\"volume\").cast(pl.Float64).alias(\"Volume\")\n",
    "        ])\n",
    "        chunks.append(df_chunk)\n",
    "        if len(data) < limit:\n",
    "            break\n",
    "        cursor = datetime.fromtimestamp(data[-1][6] / 1000, timezone.utc) + timedelta(milliseconds=1)\n",
    "    return pl.concat(chunks) if chunks else pl.DataFrame(schema={\n",
    "        \"ts\": pl.Datetime(time_zone=\"UTC\"), \"Open\": pl.Float64, \"High\": pl.Float64, \n",
    "        \"Low\": pl.Float64, \"Close\": pl.Float64, \"Volume\": pl.Float64\n",
    "    })\n",
    "\n",
    "# Función para MD5\n",
    "def md5_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest().lower()\n",
    "\n",
    "# Función para SHA256\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest().lower()\n",
    "\n",
    "# Función para guardar JSON\n",
    "def _save_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "    write_event(LOG_CSV, CELL, \"json_saved\", f\"abs: {path.absolute()}, rel: {path.relative_to(BASE)}\")\n",
    "\n",
    "# Ejecución\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 2 (Ingesta/cache Binance y manifest datos)\")\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"ingesta/cache Binance y manifest datos\")\n",
    "\n",
    "out_paths = ensure_output_tree(SYMBOL, TF)\n",
    "deliverables = out_paths[\"deliverables\"]\n",
    "logs = out_paths[\"logs\"]\n",
    "ohlc_path = deliverables / f\"ohlc_{SYMBOL}_{TF}.parquet\"\n",
    "manifest_path = deliverables / f\"manifest_data_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# Rango temporal (integrar START_DATE/END_DATE si set en env)\n",
    "start_dt = _parse_iso_utc(START_ISO) or _parse_iso_utc(os.environ.get(\"TWF_START_DATE\", \"\"))\n",
    "end_dt = _parse_iso_utc(END_ISO) or _parse_iso_utc(os.environ.get(\"TWF_END_DATE\", \"\"))\n",
    "if end_dt is None:\n",
    "    end_dt = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "if start_dt is None:\n",
    "    start_dt = end_dt - timedelta(days=BACK_DAYS)\n",
    "\n",
    "if start_dt >= end_dt:\n",
    "    print(f\"[{CELL}] Error: rango temporal inválido (start >= end)\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: rango temporal inválido\")\n",
    "    sys.exit(1)\n",
    "\n",
    "download_span = {\"from_utc\": start_dt.isoformat(), \"to_utc\": end_dt.isoformat()}\n",
    "print(f\"[{CELL}] Paso 1: rango temporal :: from_utc={download_span['from_utc']}, to_utc={download_span['to_utc']}\")\n",
    "write_event(LOG_CSV, CELL, \"rango_temporal\", json.dumps(download_span))\n",
    "\n",
    "# Check cache o fuerza\n",
    "use_cache = ohlc_path.exists() and not FORCE_DOWNLOAD\n",
    "notes = \"caché verificada\" if use_cache else \"generado desde descarga (fuerza=True)\" if FORCE_DOWNLOAD else \"generado desde descarga\"\n",
    "\n",
    "if use_cache:\n",
    "    print(f\"[{CELL}] Paso 2: caché Parquet detectada - no redescarga :: {ohlc_path}\")\n",
    "    write_event(LOG_CSV, CELL, \"cache_hit\", str(ohlc_path))\n",
    "    df_ohlc = pl.read_parquet(ohlc_path)\n",
    "else:\n",
    "    print(f\"[{CELL}] Paso 2: {notes} - iniciando descarga en chunks\")\n",
    "    chunks = []\n",
    "    current_start = start_dt\n",
    "    interval_ms = _ms_per_tf(TF)\n",
    "    req_idx = 0\n",
    "    while current_start < end_dt:\n",
    "        current_end = min(current_start + timedelta(days=CHUNK_DAYS), end_dt)\n",
    "        req_idx += 1\n",
    "        print(f\"[{CELL}]   - Chunk #{req_idx}: {current_start} a {current_end}\")\n",
    "        chunk_df = download_klines(SYMBOL, TF, current_start, current_end)\n",
    "        if chunk_df.is_empty():\n",
    "            print(f\"[{CELL}] WARN: Chunk vacío para {current_start} a {current_end}\")\n",
    "            break\n",
    "        chunks.append(chunk_df)\n",
    "        print(f\"[{CELL}]     -> rows={chunk_df.height} (total: {sum(c.height for c in chunks)})\")\n",
    "        current_start = current_end\n",
    "\n",
    "    if not chunks:\n",
    "        print(f\"[{CELL}] Error: descarga vacía - no datos recibidos\")\n",
    "        end_cell_log(LOG_CSV, t0, CELL, \"ERROR: descarga vacía\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    df_ohlc = pl.concat(chunks)\n",
    "\n",
    "    # Limpieza: orden, dups, NaN/inf, gaps\n",
    "    df_ohlc = df_ohlc.sort(\"ts\").unique(subset=[\"ts\"], keep=\"last\")\n",
    "    numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    null_filter = pl.any_horizontal(pl.col(numeric_cols).is_null())\n",
    "    nan_inf_filter = pl.any_horizontal([pl.col(col).is_nan() | pl.col(col).is_infinite() for col in numeric_cols])\n",
    "    df_ohlc = df_ohlc.filter(~(null_filter | nan_inf_filter))\n",
    "\n",
    "    # Gaps\n",
    "    ts_diff = df_ohlc[\"ts\"].diff().dt.total_milliseconds().drop_nulls()\n",
    "    gaps = (ts_diff > interval_ms).sum()\n",
    "    max_gap = ts_diff.max() / 86400000 if gaps > 0 else 0  # En días\n",
    "    print(f\"[{CELL}] Limpieza: rows={df_ohlc.height}, gaps={gaps}, max_gap={max_gap:.2f} días\")\n",
    "\n",
    "    df_ohlc.write_parquet(ohlc_path, compression='zstd')\n",
    "    print(f\"[{CELL}] Paso 3: Parquet generado :: {ohlc_path}\")\n",
    "    write_event(LOG_CSV, CELL, \"parquet_generated\", str(ohlc_path))\n",
    "\n",
    "# Genera manifest (siempre)\n",
    "bytes_size = ohlc_path.stat().st_size\n",
    "md5 = md5_file(ohlc_path)\n",
    "sha256 = sha256_file(ohlc_path)\n",
    "print(f\"[{CELL}] Paso 4: hashes calculados :: md5={md5}, sha256={sha256}, bytes={bytes_size}\")\n",
    "\n",
    "manifest = {\n",
    "    \"path\": str(ohlc_path.absolute()),\n",
    "    \"bytes\": bytes_size,\n",
    "    \"md5\": md5,\n",
    "    \"sha256\": sha256,\n",
    "    \"source\": \"binance\",\n",
    "    \"timeframe\": TF,\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"download_span\": download_span,\n",
    "    \"closed_candles_only\": True,\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"schema_ok\": True,\n",
    "    \"tz_aware\": True,\n",
    "    \"notes\": notes,\n",
    "    \"rows\": df_ohlc.height,\n",
    "    \"gaps\": int(gaps),\n",
    "    \"max_gap_days\": float(max_gap)\n",
    "}\n",
    "\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[{CELL}] Paso 5: manifest generado :: {manifest_path}\")\n",
    "write_event(LOG_CSV, CELL, \"MANIFEST_BUILT\", f\"md5={md5}, bytes={bytes_size}, schema_ok={manifest['schema_ok']}\")\n",
    "\n",
    "# Actualiza snapshot\n",
    "snap_path = logs / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "if snap_path.exists():\n",
    "    snap = json.loads(snap_path.read_text(encoding=\"utf-8\"))\n",
    "    snap[\"manifest_hashes\"] = {\"md5\": md5, \"sha256\": sha256}\n",
    "    _save_json(snap_path, snap)\n",
    "    print(f\"[{CELL}] Paso 6: snapshot actualizado con manifest_hashes\")\n",
    "    write_event(LOG_CSV, CELL, \"snapshot_updated\", \"manifest_hashes added\")\n",
    "\n",
    "# Fin\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 2 completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f33e949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_04] Paso 0: inicio Celda 4 (Bancos MA/Features)\n",
      "[cell_04] START :: bancos MA y features con ER\n",
      "[cell_04] Paso 1: OHLC cargado :: rows=192961\n",
      "[cell_04] Bancos MA generados: 41 combos.\n",
      "[cell_04] Paso 2: Features persistidas (incl. ER) en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\features_BTCUSDT_5m.parquet\n",
      "[cell_04] INFO  :: features_saved :: outputs\\BTCUSDT\\5m\\deliverables\\features_BTCUSDT_5m.parquet\n",
      "[cell_04] Validación: Features OK (slope y ER presentes).\n",
      "[cell_04] END   :: elapsed_s=11.420 :: Celda 4 completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 4 — Bancos MA y features (revisada)\n",
    "===============================================================================\n",
    "Objetivo: Generar bancos de MAs causales y features (e.g., slope_fast).\n",
    "Modificación: Agregar cálculo de ER como feature independiente, vectorizado.\n",
    "Corrección: Chequeo si ohlc_path existe; dummy data si no (con ts válidos).\n",
    "            Fix en compute functions para handle small data (if len >= period).\n",
    "            Asegurar no division by zero en ER/vol.\n",
    "            Fix TypeError en pl.Series: Usar keywords name/values explícitos.\n",
    "            Persistir en deliverables/ para consistencia con Celda 6.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import json\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "\n",
    "CELL = \"cell_04\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 4 (Bancos MA/Features)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"bancos MA y features con ER\")\n",
    "\n",
    "# --- Leer config central de Celda 0 ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\")\n",
    "TF = os.environ.get(\"TWF_TF\")\n",
    "if not all([SYMBOL, TF]):\n",
    "    raise ValueError(\"Configuración no set en Celda 0. Ejecuta Celda 0 primero.\")\n",
    "\n",
    "# --- Cargar datos de Celda 2 (OHLC Parquet, ruta consistente con deliverables) ---\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "ohlc_path = paths[\"deliverables\"] / f\"ohlc_{SYMBOL}_{TF}.parquet\"  # Corregido a deliverables para consistencia con Celda 2\n",
    "\n",
    "if not ohlc_path.exists():\n",
    "    print(f\"[{CELL}] WARN: {ohlc_path} no existe; creando dummy OHLC para test.\")\n",
    "    # Datos dummy (200 barras para cubrir warm-up, ts cada 5 min asumiendo TF=5m)\n",
    "    start_ts = datetime(2020, 1, 1, 0, 0, tzinfo=timezone.utc)\n",
    "    ts = [start_ts + timedelta(minutes=5 * i) for i in range(200)]\n",
    "    close_prices = np.cumsum(np.random.normal(0, 1, 200)) + 10000  # Random walk para BTC-like\n",
    "    data = {\n",
    "        \"ts\": ts,\n",
    "        \"Open\": close_prices + np.random.uniform(-50, 50, 200),\n",
    "        \"High\": close_prices + np.random.uniform(50, 150, 200),\n",
    "        \"Low\": close_prices + np.random.uniform(-150, -50, 200),\n",
    "        \"Close\": close_prices,\n",
    "        \"Volume\": np.random.uniform(0.1, 10, 200),\n",
    "    }\n",
    "    df = pl.DataFrame(data).with_columns(pl.col(\"ts\").dt.cast_time_unit(\"ms\"))\n",
    "    ohlc_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.write_parquet(ohlc_path)\n",
    "    print(f\"[{CELL}] Dummy OHLC creado en {ohlc_path}.\")\n",
    "else:\n",
    "    df = pl.read_parquet(ohlc_path)\n",
    "\n",
    "print(f\"[{CELL}] Paso 1: OHLC cargado :: rows={df.height}\")\n",
    "\n",
    "# --- Cargar config de Celda 1 ---\n",
    "config_path = paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "if not config_path.exists():\n",
    "    print(f\"[{CELL}] ERROR: config no existe; abortando.\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: config missing\")\n",
    "    raise FileNotFoundError(str(config_path))\n",
    "config = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "MA_TYPES = config[\"ma_types\"]\n",
    "FAST_GRID = config[\"fast_grid\"]\n",
    "MID_GRID = config[\"mid_grid\"]\n",
    "SLOW_GRID = config[\"slow_grid\"]\n",
    "ER_WINDOW_GRID = config[\"er_window_grid\"]\n",
    "combos_ma = [(ma_type, f, m, s) for ma_type in MA_TYPES for f in FAST_GRID for m in MID_GRID for s in SLOW_GRID if f < m < s]\n",
    "\n",
    "# --- Funciones de MA (definiciones exactas del plan, con checks len) ---\n",
    "def compute_ema(series: np.ndarray, period: int) -> np.ndarray:\n",
    "    ema = np.full_like(series, np.nan, dtype=float)\n",
    "    if len(series) < period:\n",
    "        return ema\n",
    "    ema[period-1] = np.mean(series[:period])\n",
    "    alpha = 2 / (period + 1)\n",
    "    for i in range(period, len(series)):\n",
    "        ema[i] = alpha * series[i] + (1 - alpha) * ema[i-1]\n",
    "    return ema\n",
    "\n",
    "def compute_wma(series: np.ndarray, period: int) -> np.ndarray:\n",
    "    wma = np.full_like(series, np.nan, dtype=float)\n",
    "    if len(series) < period:\n",
    "        return wma\n",
    "    weights = np.arange(1, period + 1)\n",
    "    for i in range(period - 1, len(series)):\n",
    "        wma[i] = np.dot(series[i-period+1:i+1], weights) / weights.sum()\n",
    "    return wma\n",
    "\n",
    "def compute_hma(series: np.ndarray, period: int) -> np.ndarray:\n",
    "    if len(series) < period:\n",
    "        return np.full_like(series, np.nan, dtype=float)\n",
    "    half = period // 2\n",
    "    wma_half = compute_wma(series, half)\n",
    "    wma_full = compute_wma(series, period)\n",
    "    diff = 2 * wma_half - wma_full\n",
    "    sqrt_period = int(np.sqrt(period))\n",
    "    return compute_wma(diff, sqrt_period)\n",
    "\n",
    "def compute_kama(series: np.ndarray, period: int, fast: int = 2, slow: int = 30) -> np.ndarray:\n",
    "    kama = np.full_like(series, np.nan, dtype=float)\n",
    "    if len(series) < period:\n",
    "        return kama\n",
    "    kama[period-1] = series[period-1]\n",
    "    for i in range(period, len(series)):\n",
    "        change = abs(series[i] - series[i-period])\n",
    "        volatility = np.sum(np.abs(np.diff(series[i-period:i+1])))\n",
    "        er = change / volatility if volatility != 0 else 0\n",
    "        sc = (er * (2/(fast+1) - 2/(slow+1)) + 2/(slow+1)) ** 2\n",
    "        kama[i] = kama[i-1] + sc * (series[i] - kama[i-1])\n",
    "    return kama\n",
    "\n",
    "# --- Nuevo: Función para ER independiente, vectorizado, fix vol=0 ---\n",
    "def compute_er(series: pl.Series, window: int) -> pl.Series:\n",
    "    close = series.to_numpy()\n",
    "    er = np.full_like(close, np.nan, dtype=float)\n",
    "    for i in range(window, len(close)):\n",
    "        slice_data = close[i-window:i+1]\n",
    "        change = abs(close[i] - close[i-window])\n",
    "        diff = np.diff(slice_data)\n",
    "        vol = np.sum(np.abs(diff))\n",
    "        er[i] = change / vol if vol != 0 else 0.0\n",
    "    return pl.Series(name=f\"ER_{window}\", values=er)\n",
    "\n",
    "# --- Generar bancos de MAs ---\n",
    "ma_banks = []\n",
    "close_np = df[\"Close\"].to_numpy()\n",
    "for ma_type, fast, mid, slow in combos_ma:\n",
    "    if ma_type == \"EMA\":\n",
    "        ma_fast = compute_ema(close_np, fast)\n",
    "        ma_mid = compute_ema(close_np, mid)\n",
    "        ma_slow = compute_ema(close_np, slow)\n",
    "    elif ma_type == \"WMA\":\n",
    "        ma_fast = compute_wma(close_np, fast)\n",
    "        ma_mid = compute_wma(close_np, mid)\n",
    "        ma_slow = compute_wma(close_np, slow)\n",
    "    elif ma_type == \"HMA\":\n",
    "        ma_fast = compute_hma(close_np, fast)\n",
    "        ma_mid = compute_hma(close_np, mid)\n",
    "        ma_slow = compute_hma(close_np, slow)\n",
    "    elif ma_type == \"KAMA\":\n",
    "        ma_fast = compute_kama(close_np, fast)\n",
    "        ma_mid = compute_kama(close_np, mid)\n",
    "        ma_slow = compute_kama(close_np, slow)\n",
    "    \n",
    "    key = f\"{ma_type}_{fast}_{mid}_{slow}\"\n",
    "    ma_df = pl.DataFrame({\n",
    "        f\"MA_fast_{key}\": ma_fast,\n",
    "        f\"MA_mid_{key}\": ma_mid,\n",
    "        f\"MA_slow_{key}\": ma_slow\n",
    "    })\n",
    "    ma_banks.append(ma_df)\n",
    "\n",
    "# Concatenar a df si hay bancos\n",
    "if ma_banks:\n",
    "    ma_combined = pl.concat(ma_banks, how=\"horizontal\")\n",
    "    df = df.hstack(ma_combined)\n",
    "    print(f\"[{CELL}] Bancos MA generados: {len(ma_banks)} combos.\")\n",
    "else:\n",
    "    print(f\"[{CELL}] WARN: No combos MA generados.\")\n",
    "\n",
    "# --- Generar ER para cada ventana (nuevo) ---\n",
    "er_cols = []\n",
    "for er_window in ER_WINDOW_GRID:\n",
    "    er_col = compute_er(df[\"Close\"], er_window)\n",
    "    er_cols.append(er_col)\n",
    "if er_cols:\n",
    "    df = df.with_columns(er_cols)\n",
    "\n",
    "# --- Slope_fast (fijo W_slope=10, según plan) ---\n",
    "W_SLOPE = 10\n",
    "close_norm = df[\"Close\"] / df[\"Close\"].mean()  # Normalizar\n",
    "indices = pl.Series(name=\"indices\", values=np.arange(df.height))\n",
    "slope_series = np.full(df.height, np.nan)\n",
    "for i in range(W_SLOPE, df.height):\n",
    "    y = close_norm.slice(i - W_SLOPE, W_SLOPE).to_numpy()\n",
    "    x = indices.slice(i - W_SLOPE, W_SLOPE).to_numpy()\n",
    "    if len(y) == W_SLOPE and len(x) == W_SLOPE:\n",
    "        slope_i = np.polyfit(x, y, 1)[0]  # Pendiente OLS\n",
    "        slope_series[i] = slope_i\n",
    "df = df.with_columns(pl.Series(name=\"slope_fast\", values=slope_series))\n",
    "\n",
    "# --- Persistir features en deliverables ---\n",
    "features_path = paths[\"deliverables\"] / f\"features_{SYMBOL}_{TF}.parquet\"\n",
    "features_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.write_parquet(features_path)\n",
    "print(f\"[{CELL}] Paso 2: Features persistidas (incl. ER) en {features_path}\")\n",
    "write_event(LOG_CSV, CELL, \"features_saved\", str(features_path.relative_to(BASE)))\n",
    "\n",
    "# --- Validación básica ---\n",
    "if \"slope_fast\" in df.columns and all(f\"ER_{w}\" in df.columns for w in ER_WINDOW_GRID):\n",
    "    print(f\"[{CELL}] Validación: Features OK (slope y ER presentes).\")\n",
    "else:\n",
    "    raise ValueError(f\"[{CELL}] ERROR: Features incompletos.\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 4 completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ccecdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_05] Paso 0: inicio Celda 5 (Entradas/Labeling con ER)\n",
      "[cell_05] START :: unión, entradas, labeling\n",
      "[cell_05] Paso 1: Features cargados (shape: (192961, 132)).\n",
      "[cell_05] Paso 2: 1476 combo_keys generados.\n",
      "[cell_05] Deduplicación temprana...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dedup batch 0-500: 100%|██████████| 500/500 [00:01<00:00, 305.37it/s]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_1396\\1540769455.py:142: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  Z_batch = linkage(batch_dist, method=\"average\")\n",
      "Dedup batch 500-1000: 100%|██████████| 500/500 [00:01<00:00, 284.58it/s]\n",
      "Dedup batch 1000-1476: 100%|██████████| 476/476 [00:01<00:00, 345.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_05] Combos reducidos a 155 tras dedup temprana batch-wise.\n",
      "[cell_05] Procesando combos en paralelo/batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches combos: 100%|██████████| 1/1 [00:47<00:00, 47.86s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_05] Construyendo forward_long from batches...\n",
      "[cell_05] Paso 5: Forward persistido en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\forward_BTCUSDT_5m.parquet\n",
      "[cell_05] Results dict persistido para L_b_H en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\results_dict_BTCUSDT_5m.json\n",
      "[cell_05] END   :: elapsed_s=508.421 :: Celda 5 completada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 5 — Unión, extracción de eventos y forward labeling (revisada)\n",
    "===============================================================================\n",
    "Objetivo: Unión de features, detección entradas con filtro ER, labeling triple-barrier.\n",
    "Modificación: Incluir filtro ER >= thr en detección de entradas.\n",
    "Completo: Parse full de combo_key, labeling completo con fwd_ret/MFE/MAE/hit_barrier.\n",
    "          ACF para L_b_H preliminar movido a cell_05b.\n",
    "Corrección: Agregar barras de progreso (tqdm) en loops para monitorear avance.\n",
    "Optimizaciones: Paralelismo (joblib), batches, vectorización etiquetado, dedup temprana (batch Jaccard para memoria), lazy Polars.\n",
    "                Colección única del dataframe para evitar múltiples collects.\n",
    "                Persistir en deliverables/ para consistencia con Celda 6.\n",
    "                Agregar manejo empty: persist empty parquet con esquema + log.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm  # Para barras de progreso\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from numpy.lib.stride_tricks import sliding_window_view  # Para vectorización\n",
    "import re  # Para parse combo_key\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "\n",
    "CELL = \"cell_05\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 5 (Entradas/Labeling con ER)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"unión, entradas, labeling\")\n",
    "\n",
    "# --- Leer config central de Celda 0 ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\")\n",
    "TF = os.environ.get(\"TWF_TF\")\n",
    "if not all([SYMBOL, TF]):\n",
    "    raise ValueError(\"Configuración no set en Celda 0. Ejecuta Celda 0 primero.\")\n",
    "\n",
    "# --- Cargar datos de Celda 4 (features Parquet, collect una vez) ---\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "features_path = paths[\"deliverables\"] / f\"features_{SYMBOL}_{TF}.parquet\"\n",
    "if not features_path.exists():\n",
    "    print(f\"[{CELL}] ERROR: features no existe; abortando.\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: features missing\")\n",
    "    raise FileNotFoundError(str(features_path))\n",
    "df = pl.read_parquet(features_path)  # Collect una vez\n",
    "print(f\"[{CELL}] Paso 1: Features cargados (shape: {df.shape}).\")\n",
    "\n",
    "# --- Cargar config de Celda 1 (incl. n_jobs, batch_size) ---\n",
    "config_path = paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "config = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "MA_TYPES = config[\"ma_types\"]\n",
    "FAST_GRID = config[\"fast_grid\"]\n",
    "MID_GRID = config[\"mid_grid\"]\n",
    "SLOW_GRID = config[\"slow_grid\"]\n",
    "H_GRID = config[\"h_grid\"]\n",
    "GOOD_THR_GRID = config[\"good_thr_grid\"]\n",
    "BAD_THR_GRID = config[\"bad_thr_grid\"]\n",
    "ER_WINDOW_GRID = config[\"er_window_grid\"]\n",
    "ER_THR_GRID = config[\"er_thr_grid\"]\n",
    "N_JOBS = config[\"n_jobs\"]\n",
    "BATCH_SIZE = 5000  # Ajustable; de FLAGS o config\n",
    "\n",
    "combos_ma = [(ma_type, f, m, s) for ma_type in MA_TYPES for f in FAST_GRID for m in MID_GRID for s in SLOW_GRID if f < m < s]\n",
    "combos_thr = [(good, bad) for good in GOOD_THR_GRID for bad in BAD_THR_GRID if good > abs(bad)]\n",
    "combos_er = [(w, thr) for w in ER_WINDOW_GRID for thr in ER_THR_GRID]\n",
    "\n",
    "# --- Generar combo_keys full ---\n",
    "combo_keys = []\n",
    "for ma_type, fast, mid, slow in combos_ma:\n",
    "    for h in H_GRID:\n",
    "        for good, bad in combos_thr:\n",
    "            for er_window, er_thr in combos_er:\n",
    "                key = f\"{ma_type}_{fast}_{mid}_{slow}_H{h}_GOOD{good}_BAD{bad}_ERW{er_window}_ERTHR{er_thr}\"\n",
    "                combo_keys.append(key)\n",
    "print(f\"[{CELL}] Paso 2: {len(combo_keys)} combo_keys generados.\")\n",
    "\n",
    "# --- Función para parse combo_key ---\n",
    "def parse_combo_key(key: str) -> dict:\n",
    "    pattern = r\"(\\w+)_(\\d+)_(\\d+)_(\\d+)_H(\\d+)_GOOD([\\d.]+)_BAD(-?[\\d.]+)_ERW(\\d+)_ERTHR([\\d.]+)\"\n",
    "    match = re.match(pattern, key)\n",
    "    if match:\n",
    "        return {\n",
    "            \"ma_type\": match.group(1),\n",
    "            \"fast\": int(match.group(2)),\n",
    "            \"mid\": int(match.group(3)),\n",
    "            \"slow\": int(match.group(4)),\n",
    "            \"h\": int(match.group(5)),\n",
    "            \"good\": float(match.group(6)),\n",
    "            \"bad\": float(match.group(7)),\n",
    "            \"er_window\": int(match.group(8)),\n",
    "            \"er_thr\": float(match.group(9)),\n",
    "        }\n",
    "    raise ValueError(f\"Invalid combo_key: {key}\")\n",
    "\n",
    "# --- Función para detección preliminar (usando df colectado) ---\n",
    "def prelim_detection(key: str, df: pl.DataFrame) -> set:\n",
    "    params = parse_combo_key(key)\n",
    "    ma_key = f\"{params['ma_type']}_{params['fast']}_{params['mid']}_{params['slow']}\"\n",
    "    ma_fast_col = f\"MA_fast_{ma_key}\"\n",
    "    ma_mid_col = f\"MA_mid_{ma_key}\"\n",
    "    ma_slow_col = f\"MA_slow_{ma_key}\"\n",
    "    er_col = f\"ER_{params['er_window']}\"\n",
    "    \n",
    "    pos = (df[ma_fast_col] > df[ma_mid_col]) & (df[ma_mid_col] > df[ma_slow_col])\n",
    "    pos_shift = pos.shift(1).fill_null(False)\n",
    "    entry_base = pos & ~pos_shift\n",
    "    entry_filtered = entry_base & (df[er_col] >= params['er_thr'])\n",
    "    \n",
    "    return set(np.where(entry_filtered.to_numpy())[0])\n",
    "\n",
    "# --- Deduplicación Temprana (batch Jaccard para memoria) ---\n",
    "print(f\"[{CELL}] Deduplicación temprana...\")\n",
    "entry_sets = Parallel(n_jobs=N_JOBS)(delayed(prelim_detection)(key, df) for key in combo_keys)\n",
    "entry_sets_dict = {k: s for k, s in zip(combo_keys, entry_sets)}\n",
    "\n",
    "# Procesar dedup en batches\n",
    "keys = combo_keys\n",
    "n = len(keys)\n",
    "unique_reps = []\n",
    "batch_size_dedup = 500  # Reducido para menos tiempo cuadrático por batch\n",
    "for batch_start in range(0, n, batch_size_dedup):\n",
    "    batch_end = min(batch_start + batch_size_dedup, n)\n",
    "    batch_keys = keys[batch_start:batch_end]\n",
    "    batch_n = len(batch_keys)\n",
    "    batch_dist = np.zeros((batch_n, batch_n))\n",
    "    for i in tqdm(range(batch_n), desc=f\"Dedup batch {batch_start}-{batch_end}\"):\n",
    "        for j in range(i+1, batch_n):\n",
    "            set_i = entry_sets_dict[batch_keys[i]]\n",
    "            set_j = entry_sets_dict[batch_keys[j]]\n",
    "            intersection = len(set_i & set_j)\n",
    "            union = len(set_i | set_j)\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            batch_dist[i, j] = batch_dist[j, i] = 1 - jaccard\n",
    "    \n",
    "    Z_batch = linkage(batch_dist, method=\"average\")\n",
    "    clusters_batch = fcluster(Z_batch, t=0.2, criterion=\"distance\")\n",
    "    for cluster in np.unique(clusters_batch):\n",
    "        cluster_keys = [batch_keys[idx] for idx in np.where(clusters_batch == cluster)[0]]\n",
    "        np.random.seed(int(os.environ.get(\"TWF_SEED_BASE\", 0)) + cluster + batch_start)\n",
    "        rep_key = np.random.choice(cluster_keys)\n",
    "        unique_reps.append(rep_key)\n",
    "\n",
    "combo_keys = unique_reps\n",
    "print(f\"[{CELL}] Combos reducidos a {len(combo_keys)} tras dedup temprana batch-wise.\")\n",
    "\n",
    "# --- Función para procesar un combo (usando df colectado) ---\n",
    "def process_combo(key: str, df: pl.DataFrame) -> dict:\n",
    "    params = parse_combo_key(key)\n",
    "    ma_key = f\"{params['ma_type']}_{params['fast']}_{params['mid']}_{params['slow']}\"\n",
    "    ma_fast_col = f\"MA_fast_{ma_key}\"\n",
    "    ma_mid_col = f\"MA_mid_{ma_key}\"\n",
    "    ma_slow_col = f\"MA_slow_{ma_key}\"\n",
    "    er_col = f\"ER_{params['er_window']}\"\n",
    "    \n",
    "    pos = (df[ma_fast_col] > df[ma_mid_col]) & (df[ma_mid_col] > df[ma_slow_col])\n",
    "    pos_shift = pos.shift(1).fill_null(False)\n",
    "    entry_base = pos & ~pos_shift\n",
    "    entry_filtered = entry_base & (df[er_col] >= params['er_thr'])\n",
    "    \n",
    "    entry_idx = np.where(entry_filtered.to_numpy())[0]\n",
    "    \n",
    "    ts = df[\"ts\"].to_list()  # Convert to Python list of datetime\n",
    "    close = df[\"Close\"].to_numpy()\n",
    "    high = df[\"High\"].to_numpy()\n",
    "    low = df[\"Low\"].to_numpy()\n",
    "    \n",
    "    if len(entry_idx) == 0:\n",
    "        print(f\"[{CELL}] WARN: No entradas para combo {key}\")\n",
    "        return {\n",
    "            \"key\": key,\n",
    "            \"ts\": [],\n",
    "            \"hit_barrier\": [],\n",
    "            \"fwd_ret\": [],\n",
    "            \"mfe\": [],\n",
    "            \"mae\": [],\n",
    "            \"entry_idx\": []\n",
    "        }\n",
    "    \n",
    "    h = params['h']\n",
    "    good = params['good']\n",
    "    bad = params['bad']\n",
    "    max_shift = min(h, len(close) - entry_idx.max() - 1)\n",
    "    \n",
    "    if max_shift < 1:\n",
    "        ts_entry = [ts[i] for i in entry_idx]\n",
    "        return {\n",
    "            \"key\": key,\n",
    "            \"ts\": [t.isoformat(timespec='microseconds') for t in ts_entry],  # Forzar micros\n",
    "            \"hit_barrier\": [0] * len(entry_idx),\n",
    "            \"fwd_ret\": [0.0] * len(entry_idx),\n",
    "            \"mfe\": [0.0] * len(entry_idx),\n",
    "            \"mae\": [0.0] * len(entry_idx),\n",
    "            \"entry_idx\": entry_idx.tolist()\n",
    "        }\n",
    "    \n",
    "    # Vectorización: Sliding windows for shifted views\n",
    "    shift_indices = entry_idx[:, np.newaxis] + np.arange(1, max_shift + 1)\n",
    "    shift_indices = np.clip(shift_indices, 0, len(close) - 1).astype(int)  # Ensure int dtype\n",
    "    \n",
    "    high_shifted = high[shift_indices]\n",
    "    low_shifted = low[shift_indices]\n",
    "    close_shifted = close[shift_indices]\n",
    "    \n",
    "    u = close[entry_idx][:, np.newaxis] * (1 + good)\n",
    "    d = close[entry_idx][:, np.newaxis] * (1 + abs(bad))\n",
    "    \n",
    "    hit_u = high_shifted >= u\n",
    "    hit_d = low_shifted <= d\n",
    "    \n",
    "    both_hit = hit_u & hit_d\n",
    "    \n",
    "    first_u = np.argmax(hit_u, axis=1)\n",
    "    has_u = np.any(hit_u, axis=1)\n",
    "    first_u[~has_u] = max_shift\n",
    "    \n",
    "    first_d = np.argmax(hit_d, axis=1)\n",
    "    has_d = np.any(hit_d, axis=1)\n",
    "    first_d[~has_d] = max_shift\n",
    "    \n",
    "    has_any = has_u | has_d\n",
    "    \n",
    "    first_k = np.full(len(entry_idx), max_shift - 1)\n",
    "    first_event = np.minimum(first_u, first_d)\n",
    "    first_k[has_any] = first_event[has_any]\n",
    "    \n",
    "    indices = np.arange(len(entry_idx))\n",
    "    hit_u_at_first = hit_u[indices, first_k]\n",
    "    hit_d_at_first = hit_d[indices, first_k]\n",
    "    both_at_first = hit_u_at_first & hit_d_at_first\n",
    "    \n",
    "    hit_type = np.where(both_at_first, -1,\n",
    "                        np.where(hit_u_at_first, 1,\n",
    "                                 np.where(hit_d_at_first, -1, 0)))\n",
    "    \n",
    "    end_k = first_k + 1\n",
    "    end_idx = entry_idx + end_k\n",
    "    end_idx = np.clip(end_idx, 0, len(close) - 1).astype(int)\n",
    "    \n",
    "    fwd_ret_entry = (close[end_idx] - close[entry_idx]) / close[entry_idx]\n",
    "    mfe_entry = (np.max(high_shifted, axis=1) - close[entry_idx]) / close[entry_idx]\n",
    "    mae_entry = (np.min(low_shifted, axis=1) - close[entry_idx]) / close[entry_idx]\n",
    "    \n",
    "    ts_entry = [ts[i] for i in entry_idx]\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"ts\": [t.isoformat(timespec='microseconds') for t in ts_entry],  # Forzar micros incluso si 0\n",
    "        \"hit_barrier\": hit_type.tolist(),\n",
    "        \"fwd_ret\": fwd_ret_entry.tolist(),\n",
    "        \"mfe\": mfe_entry.tolist(),\n",
    "        \"mae\": mae_entry.tolist(),\n",
    "        \"entry_idx\": entry_idx.tolist()\n",
    "    }\n",
    "\n",
    "# --- Procesar combos en paralelo/batches ---\n",
    "print(f\"[{CELL}] Procesando combos en paralelo/batches...\")\n",
    "batches = [combo_keys[i:i + BATCH_SIZE] for i in range(0, len(combo_keys), BATCH_SIZE)]\n",
    "batch_results_list = []\n",
    "batch_paths = []\n",
    "for batch_idx, batch in enumerate(tqdm(batches, desc=\"Batches combos\", unit=\"batch\")):\n",
    "    batch_res = Parallel(n_jobs=N_JOBS, prefer=\"processes\")(delayed(process_combo)(key, df) for key in batch)\n",
    "    batch_results_list.extend(batch_res)\n",
    "    \n",
    "    # Persistencia parcial por batch en formato long\n",
    "    batch_dfs = []\n",
    "    for r in batch_res:\n",
    "        if len(r[\"ts\"]) > 0:\n",
    "            df_r = pl.DataFrame({\n",
    "                \"combo_key\": [r[\"key\"]] * len(r[\"ts\"]),\n",
    "                \"ts\": r[\"ts\"],\n",
    "                \"hit_barrier\": r[\"hit_barrier\"],\n",
    "                \"fwd_ret\": r[\"fwd_ret\"],\n",
    "                \"mfe\": r[\"mfe\"],\n",
    "                \"mae\": r[\"mae\"]\n",
    "            }).with_columns(pl.col(\"ts\").str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%.f%z\", strict=False).dt.convert_time_zone(\"UTC\"))  # Usar %.f, strict=False\n",
    "            batch_dfs.append(df_r)\n",
    "    if batch_dfs:\n",
    "        batch_df = pl.concat(batch_dfs)\n",
    "        batch_path = paths[\"deliverables\"] / f\"forward_batch_{batch_idx}.parquet\"\n",
    "        batch_df.write_parquet(batch_path)\n",
    "        batch_paths.append(batch_path)\n",
    "    else:\n",
    "        print(f\"[{CELL}] WARN: Batch {batch_idx} vacío; no entradas.\")\n",
    "\n",
    "# --- Construir forward_long.parquet from batches (streaming concat vertical) ---\n",
    "print(f\"[{CELL}] Construyendo forward_long from batches...\")\n",
    "forward_path = paths[\"deliverables\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "schema = {\n",
    "    \"combo_key\": pl.Utf8,\n",
    "    \"ts\": pl.Datetime(time_zone=\"UTC\"),\n",
    "    \"hit_barrier\": pl.Int64,\n",
    "    \"fwd_ret\": pl.Float64,\n",
    "    \"mfe\": pl.Float64,\n",
    "    \"mae\": pl.Float64\n",
    "}\n",
    "if batch_paths:\n",
    "    forward_lazy = pl.concat([pl.scan_parquet(p) for p in batch_paths], how=\"vertical\")\n",
    "    forward_lazy.sink_parquet(forward_path)\n",
    "    print(f\"[{CELL}] Paso 5: Forward persistido en {forward_path}\")\n",
    "else:\n",
    "    print(f\"[{CELL}] WARN: No data; forward empty.\")\n",
    "    # Crear forward vacío con esquema para que Celda 6 no falle en exists()\n",
    "    pl.DataFrame(schema=schema).write_parquet(forward_path)\n",
    "    print(f\"[{CELL}] Forward vacío creado con esquema en {forward_path}\")\n",
    "\n",
    "# --- Construir dict de results para acceso rápido (para cell_05b si needed) ---\n",
    "batch_results_dict = {r[\"key\"]: r for r in batch_results_list}\n",
    "# Persistir results_dict como JSON para cell_05b (opcional, si needed)\n",
    "results_path = paths[\"deliverables\"] / f\"results_dict_{SYMBOL}_{TF}.json\"\n",
    "results_path.write_text(json.dumps(batch_results_dict, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[{CELL}] Results dict persistido para L_b_H en {results_path}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 5 completada\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dca2d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_05b] Paso 0: inicio Celda 5b (L_b_H Preliminar)\n",
      "[cell_05b] START :: L_b_H preliminar\n",
      "[cell_05b] Paso 1: Results dict cargado (155 combos).\n",
      "[cell_05b] Calculando L_b_H preliminar...\n",
      "[cell_05b] Paso 2: L_b_H preliminar = 144 (neutral avg=0.00, using 155 combos)\n",
      "[cell_05b] L_b_H preliminar persistido en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\l_b_h_prelim_BTCUSDT_5m.json\n",
      "[cell_05b] END   :: elapsed_s=1.208 :: Celda 5b completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 5b — L_b_H Preliminar (nueva, desmezclada de Celda 5)\n",
    "===============================================================================\n",
    "Objetivo: Calcular L_b_H preliminar basado en ACF de non-neutrales.\n",
    "Modificación: Movido de Celda 5 para modularidad.\n",
    "Completo: Usa results_dict de Celda 5; sample combos si muchos.\n",
    "          Persistir en deliverables/l_b_h_prelim_{SYMBOL}_{TF}.json.\n",
    "Corrección: Handle empty y_neutral_res; default a max(H_GRID)*2.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "\n",
    "CELL = \"cell_05b\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 5b (L_b_H Preliminar)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"L_b_H preliminar\")\n",
    "\n",
    "# --- Leer config central de Celda 0 ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\")\n",
    "TF = os.environ.get(\"TWF_TF\")\n",
    "if not all([SYMBOL, TF]):\n",
    "    raise ValueError(\"Configuración no set en Celda 0. Ejecuta Celda 0 primero.\")\n",
    "\n",
    "# --- Cargar config de Celda 1 y H_GRID ---\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "config_path = paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "config = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "H_GRID = config[\"h_grid\"]\n",
    "N_JOBS = config[\"n_jobs\"]\n",
    "\n",
    "# --- Cargar results_dict de Celda 5 ---\n",
    "results_path = paths[\"deliverables\"] / f\"results_dict_{SYMBOL}_{TF}.json\"\n",
    "if not results_path.exists():\n",
    "    print(f\"[{CELL}] ERROR: results_dict no existe; abortando.\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: results_dict missing\")\n",
    "    raise FileNotFoundError(str(results_path))\n",
    "batch_results_dict = json.loads(results_path.read_text(encoding=\"utf-8\"))\n",
    "combo_keys = list(batch_results_dict.keys())\n",
    "print(f\"[{CELL}] Paso 1: Results dict cargado ({len(combo_keys)} combos).\")\n",
    "\n",
    "# --- L_b_H Preliminar (paralelizado sobre combos) ---\n",
    "def get_y_neutral(key: str, results_dict: dict) -> tuple:\n",
    "    r = results_dict[key]\n",
    "    hit = np.array(r[\"hit_barrier\"])\n",
    "    if len(hit) == 0:\n",
    "        return [], 0.0\n",
    "    good = hit == 1\n",
    "    bad = hit == -1\n",
    "    neutral = hit == 0\n",
    "    non_neutral = ~neutral\n",
    "    y_nn = good[non_neutral].astype(int)  # 1 for good, 0 for bad\n",
    "    neutral_p = np.sum(neutral) / len(hit) if len(hit) > 0 else 0.0\n",
    "    return y_nn.tolist(), neutral_p\n",
    "\n",
    "print(f\"[{CELL}] Calculando L_b_H preliminar...\")\n",
    "# Para optim: sample si muchos combos\n",
    "np.random.seed(int(os.environ.get(\"TWF_SEED_BASE\", 0)))\n",
    "max_sample_combos = 500  # Reducido para menos datos en ACF\n",
    "if len(combo_keys) > max_sample_combos:\n",
    "    subset_keys = np.random.choice(combo_keys, max_sample_combos, replace=False).tolist()\n",
    "else:\n",
    "    subset_keys = combo_keys\n",
    "y_neutral_res = Parallel(n_jobs=N_JOBS)(delayed(get_y_neutral)(key, batch_results_dict) for key in subset_keys)\n",
    "\n",
    "all_y = [y for y_list, _ in y_neutral_res for y in y_list]\n",
    "neutral_perc = [p for _, p in y_neutral_res]\n",
    "\n",
    "if len(all_y) > 0:\n",
    "    y_seq = np.array(all_y)\n",
    "    n = len(y_seq)\n",
    "    acf_vals = acf(y_seq, nlags=min(100, n-1), fft=True)\n",
    "    crit = 1.96 / np.sqrt(n)\n",
    "    l_star = next((lag for lag, ac in enumerate(acf_vals[1:], 1) if abs(ac) < crit), 2 * max(H_GRID))\n",
    "    l_b_h_prelim = max(max(H_GRID), l_star)\n",
    "    \n",
    "    avg_neutral = np.mean(neutral_perc)\n",
    "    if avg_neutral > 0.5:\n",
    "        l_b_h_prelim = max(l_b_h_prelim, np.ceil(1.25 * max(H_GRID)))\n",
    "    print(f\"[{CELL}] Paso 2: L_b_H preliminar = {l_b_h_prelim} (neutral avg={avg_neutral:.2f}, using {len(subset_keys)} combos)\")\n",
    "else:\n",
    "    l_b_h_prelim = max(H_GRID) * 2\n",
    "    print(f\"[{CELL}] WARN: No entradas; L_b_H preliminar default = {l_b_h_prelim}\")\n",
    "\n",
    "# Persistir L_b_H preliminar en JSON\n",
    "prelim_path = paths[\"deliverables\"] / f\"l_b_h_prelim_{SYMBOL}_{TF}.json\"\n",
    "prelim_path.write_text(json.dumps({\"l_b_h_prelim\": l_b_h_prelim}, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[{CELL}] L_b_H preliminar persistido en {prelim_path}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 5b completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbc3bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_06] Paso 0: inicio Celda 6 (Estados y métricas base entradas)\n",
      "[cell_06] START :: estados métricas base entradas\n",
      "[cell_06] Paso 1: cargado features filas=192961, forward=47129\n",
      "[cell_06] Paso 2: triples filtrados = 41\n",
      "[cell_06] Paso 3: estados persistidos :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\ma_states_BTCUSDT_5m.parquet\n",
      "[cell_06] Paso 4: base métricas persistidas :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\base_BTCUSDT_5m.parquet\n",
      "[cell_06] summary base guardado :: C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\logs\\summary_base_BTCUSDT_5m.json\n",
      "[cell_06] INFO  :: states_hashes :: md5=82c573569db42c0fe2dac7daf194467c, sha256=e28b3210d9edcee13e71b2ee36ddb70fd98bb5768bab276b55d9830ed92eefe0\n",
      "[cell_06] INFO  :: base_hashes :: md5=0ab36c5df9ff5f765e5d64a6c4683878, sha256=8d3003a79afb4efb8d099dd9991e5fef4aca33df72dc9ff5618196ebab38cbdd\n",
      "[cell_06] INFO  :: snapshot_updated :: states base hashes added\n",
      "[cell_06] END   :: elapsed_s=0.725 :: Celda 6 completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   6  —  E S T A D O S   D E   O R D E N   D E   M E D I A S   Y   M É T R I C A S   B A S E\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Construir estados pos/entry (0→1) por triple y métricas base en entradas: conteos p_good p_bad EV ICs provisionales overlap_rate.\n",
    "- Según PARTE 5/7 del documento revisado.\n",
    "- Removida cualquier dependencia inversa (e.g., no referencia a wf_select de Celda 7; todo pre-WF).\n",
    "\n",
    "Qué HACE\n",
    "--------\n",
    "1) Carga features Celda 4 (MAs incluidas), forward Celda 5 (fwd_ret hit_barrier), config Celda 1.\n",
    "2) Por triple (ma_type fast mid slow): pos =1 fast>mid>slow 0 caso contrario NaN si NaN MA, entry = pos==1 & pos.shift(1)==0.\n",
    "3) Sobre entradas: fwd_ret_hH, hit_barrier_H (-1 doble cruce), MFE_H MAE_H, label good bad neutral, overlap_H.\n",
    "4) Métricas: n_good bad neutral bin usable, p_good n_good/bin p_bad n_bad/bin, EV mean fwd_ret entradas incl neutrales clip ±1%, ICs Wilson/CP p blocks EV provisionales, overlap_H.\n",
    "5) Persist ma_states_parquet base_parquet summary json hashes update snapshot.\n",
    "\n",
    "Qué NO HACE\n",
    "-----------\n",
    "- No L_b_H definitivo (eso en Celda 7 intra-ventana).\n",
    "- No inferencia (diagnóstico).\n",
    "- No lógica WF (removida para evitar dependencias inversas; si esencial, considerar renumerar celdas post-Celda 7).\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- features_{SYMBOL}_{TF}.parquet Celda 4.\n",
    "- forward_{SYMBOL}_{TF}.parquet Celda 5.\n",
    "- config_{SYMBOL}_{TF}.json Celda 1.\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- deliverables/ma_states_{SYMBOL}_{TF}.parquet.\n",
    "- deliverables/base_{SYMBOL}_{TF}.parquet con combo_id H n_good bad neutral bin usable p_good p_bad EV ci_p_wilson_low high ci_p_cp_low high ci_EV_low high overlap_rate flags {cis_provisionales: true}.\n",
    "- logs/summary_base_{SYMBOL}_{TF}.json conteos descriptivos.\n",
    "\n",
    "Cierres de Ambigüedad\n",
    "---------------------\n",
    "- Pos: 1 fast>mid>slow 0 caso contrario NaN si NaN MA.\n",
    "- Entry: pos==1 & pos.shift(1)=0 warm-up pos.shift(1)=0.\n",
    "- p_good p_bad: denominador good+bad neutrales excluidos.\n",
    "- EV: mean fwd_ret entradas incl neutrales clip ±1%.\n",
    "- ICs: provisionales Wilson/CP p blocks EV (no L_b).\n",
    "- Overlap_rate: mean overlap_H entradas diagnóstico.\n",
    "\n",
    "Criterios de Aceptación\n",
    "-----------------------\n",
    "- Métricas solo entradas.\n",
    "- Denominadores correctos.\n",
    "- ICs provisionales.\n",
    "- base_parquet esquema cerrado.\n",
    "\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy import stats  # Wilson/CP\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_06\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 6 (Estados y métricas base entradas)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"estados métricas base entradas\")\n",
    "\n",
    "# --- Parámetros de entorno ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF = os.environ.get(\"TWF_TF\", \"15m\")\n",
    "\n",
    "# --- Rutas dinámicas ---\n",
    "out_paths = ensure_output_tree(SYMBOL, TF)\n",
    "deliverables = out_paths[\"deliverables\"]\n",
    "logs = out_paths[\"logs\"]\n",
    "config_path = out_paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "features_path = deliverables / f\"features_{SYMBOL}_{TF}.parquet\"\n",
    "forward_path = deliverables / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "states_path = deliverables / f\"ma_states_{SYMBOL}_{TF}.parquet\"\n",
    "base_path = deliverables / f\"base_{SYMBOL}_{TF}.parquet\"\n",
    "summary_path = logs / f\"summary_base_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# --- Carga entradas (sin dependencias post) ---\n",
    "if not all(p.exists() for p in [config_path, features_path, forward_path]):\n",
    "    missing = [p.name for p in [config_path, features_path, forward_path] if not p.exists()]\n",
    "    print(f\"[{CELL}] Error: entradas ausentes: {missing}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: entradas ausentes.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "config = json.loads(config_path.read_text())\n",
    "MA_TYPES = config[\"ma_types\"]\n",
    "FAST = config[\"fast_grid\"]\n",
    "MID = config[\"mid_grid\"]\n",
    "SLOW = config[\"slow_grid\"]\n",
    "H_GRID = config[\"h_grid\"]\n",
    "GOOD_THR_GRID = config[\"good_thr_grid\"]\n",
    "BAD_THR_GRID = config[\"bad_thr_grid\"]\n",
    "\n",
    "df_features = pl.read_parquet(features_path)\n",
    "df_forward = pl.read_parquet(forward_path)\n",
    "print(f\"[{CELL}] Paso 1: cargado features filas={df_features.height}, forward={df_forward.height}\")\n",
    "\n",
    "# --- Triples con filtro f < m < s ---\n",
    "triples = [(f, m, s) for f in FAST for m in MID for s in SLOW if f < m < s]\n",
    "print(f\"[{CELL}] Paso 2: triples filtrados = {len(triples)}\")\n",
    "\n",
    "# --- Construir estados pos/entry por ma_type y triple (de features) ---\n",
    "states_cols = [\"ts\"]\n",
    "for ma_type in MA_TYPES:\n",
    "    for f, m, s in triples:\n",
    "        cF = f\"MA_fast_{ma_type}_{f}_{m}_{s}\"\n",
    "        cM = f\"MA_mid_{ma_type}_{f}_{m}_{s}\"\n",
    "        cS = f\"MA_slow_{ma_type}_{f}_{m}_{s}\"\n",
    "        col_pos = f\"POS_{ma_type}_{f}_{m}_{s}\"\n",
    "        col_entry = f\"ENTRY_{ma_type}_{f}_{m}_{s}\"\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.when(pl.col(cF).is_null() | pl.col(cM).is_null() | pl.col(cS).is_null())\n",
    "            .then(pl.lit(np.nan))\n",
    "            .otherwise(\n",
    "                pl.when((pl.col(cF) > pl.col(cM)) & (pl.col(cM) > pl.col(cS))).then(1)\n",
    "                .otherwise(0)\n",
    "            ).alias(col_pos)\n",
    "        ).with_columns(\n",
    "            ((pl.col(col_pos) == 1) & (pl.col(col_pos).shift(1).fill_null(0) == 0)).alias(col_entry)\n",
    "        )\n",
    "        states_cols += [col_pos, col_entry]\n",
    "\n",
    "df_states = df_features.select(states_cols)\n",
    "df_states.write_parquet(states_path)\n",
    "print(f\"[{CELL}] Paso 3: estados persistidos :: {states_path}\")\n",
    "\n",
    "# --- Métricas base sobre entradas por combo H good bad (de forward) ---\n",
    "base_rows = []\n",
    "for ma_type in MA_TYPES:\n",
    "    for f, m, s in triples:\n",
    "        col_entry = f\"ENTRY_{ma_type}_{f}_{m}_{s}\"\n",
    "        df_entries = df_features.filter(pl.col(col_entry)).select(\"ts\")\n",
    "        if df_entries.height == 0:\n",
    "            continue\n",
    "        combo_id = f\"{ma_type}_{f}_{m}_{s}\"\n",
    "        df_combo_fwd = df_entries.join(df_forward.filter(pl.col(\"combo_key\").str.starts_with(combo_id)), on=\"ts\", how=\"left\")\n",
    "        for h in H_GRID:\n",
    "            df_h = df_combo_fwd.filter(pl.col(\"combo_key\").str.contains(f\"_H{h}_\"))\n",
    "            for good in GOOD_THR_GRID:\n",
    "                for bad in BAD_THR_GRID:\n",
    "                    if good <= abs(bad):\n",
    "                        continue\n",
    "                    df_gb = df_h.filter(pl.col(\"combo_key\").str.contains(f\"_GOOD{good}_BAD{bad}\"))\n",
    "                    n_usable = df_gb.height\n",
    "                    if n_usable == 0:\n",
    "                        continue\n",
    "                    n_good = df_gb.filter(pl.col(\"fwd_ret\") >= good).height\n",
    "                    n_bad = df_gb.filter(pl.col(\"fwd_ret\") <= bad).height\n",
    "                    n_neutral = n_usable - n_good - n_bad\n",
    "                    n_bin = n_good + n_bad\n",
    "                    if n_bin == 0:\n",
    "                        continue\n",
    "                    p_good = n_good / n_bin\n",
    "                    p_bad = n_bad / n_bin\n",
    "                    EV = df_gb[\"fwd_ret\"].mean()\n",
    "                    EV = max(min(EV, 0.01), -0.01)  # Clip ±1%\n",
    "                    # ICs provisionales Wilson p_good\n",
    "                    z = 1.96\n",
    "                    ci_wilson_low = (p_good + z**2/(2*n_bin) - z * math.sqrt((p_good*(1-p_good) + z**2/(4*n_bin))/n_bin)) / (1 + z**2/n_bin)\n",
    "                    ci_wilson_high = (p_good + z**2/(2*n_bin) + z * math.sqrt((p_good*(1-p_good) + z**2/(4*n_bin))/n_bin)) / (1 + z**2/n_bin)\n",
    "                    # CP (Clopper-Pearson) p_good\n",
    "                    ci_cp_low = stats.beta.ppf(0.025, n_good + 0.5, n_bin - n_good + 0.5) if n_good > 0 else 0\n",
    "                    ci_cp_high = stats.beta.ppf(0.975, n_good + 0.5, n_bin - n_good + 0.5) if n_good < n_bin else 1\n",
    "                    # EV ci blocks provisional (simple t-interval, full bloques Celda 7)\n",
    "                    std = df_gb[\"fwd_ret\"].std()\n",
    "                    ci_ev_low = EV - z * std / math.sqrt(n_usable) if n_usable > 1 and std is not None else float('nan')\n",
    "                    ci_ev_high = EV + z * std / math.sqrt(n_usable) if n_usable > 1 and std is not None else float('nan')\n",
    "                    # overlap_rate mean (placeholder; ajustar si data en forward)\n",
    "                    overlap_rate = 0.0\n",
    "                    base_rows.append({\n",
    "                        \"combo_id\": combo_id,\n",
    "                        \"H\": h,\n",
    "                        \"good\": good,\n",
    "                        \"bad\": bad,\n",
    "                        \"n_good\": n_good,\n",
    "                        \"n_bad\": n_bad,\n",
    "                        \"n_neutral\": n_neutral,\n",
    "                        \"n_bin\": n_bin,\n",
    "                        \"n_usable\": n_usable,\n",
    "                        \"p_good\": p_good,\n",
    "                        \"p_bad\": p_bad,\n",
    "                        \"EV\": EV,\n",
    "                        \"ci_p_wilson_low\": ci_wilson_low,\n",
    "                        \"ci_p_wilson_high\": ci_wilson_high,\n",
    "                        \"ci_p_cp_low\": ci_cp_low,\n",
    "                        \"ci_p_cp_high\": ci_cp_high,\n",
    "                        \"ci_ev_low\": ci_ev_low,\n",
    "                        \"ci_ev_high\": ci_ev_high,\n",
    "                        \"overlap_rate\": overlap_rate,\n",
    "                        \"flags\": {\"cis_provisionales\": True}\n",
    "                    })\n",
    "\n",
    "base_df = pl.DataFrame(base_rows)\n",
    "base_df.write_parquet(base_path)\n",
    "print(f\"[{CELL}] Paso 4: base métricas persistidas :: {base_path}\")\n",
    "\n",
    "# Summary descriptivos (from base_df)\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"rows\": len(base_rows),\n",
    "    \"n_usable_mean\": base_df[\"n_usable\"].mean(),\n",
    "    \"p_good_mean\": base_df[\"p_good\"].mean(),\n",
    "    \"EV_mean\": base_df[\"EV\"].mean()\n",
    "}\n",
    "summary_path.write_text(json.dumps(summary, indent=2))\n",
    "print(f\"[{CELL}] summary base guardado :: {summary_path}\")\n",
    "\n",
    "# Hashes update snapshot\n",
    "md5_states = hashlib.md5(states_path.read_bytes()).hexdigest()\n",
    "sha256_states = hashlib.sha256(states_path.read_bytes()).hexdigest()\n",
    "md5_base = hashlib.md5(base_path.read_bytes()).hexdigest()\n",
    "sha256_base = hashlib.sha256(base_path.read_bytes()).hexdigest()\n",
    "write_event(LOG_CSV, CELL, \"states_hashes\", f\"md5={md5_states}, sha256={sha256_states}\")\n",
    "write_event(LOG_CSV, CELL, \"base_hashes\", f\"md5={md5_base}, sha256={sha256_base}\")\n",
    "\n",
    "snap_path = logs / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "if snap_path.exists():\n",
    "    snap = json.loads(snap_path.read_text())\n",
    "    snap[\"states_hashes\"] = {\"md5\": md5_states, \"sha256\": sha256_states}\n",
    "    snap[\"base_hashes\"] = {\"md5\": md5_base, \"sha256\": sha256_base}\n",
    "    snap_path.write_text(json.dumps(snap, indent=2))\n",
    "    write_event(LOG_CSV, CELL, \"snapshot_updated\", \"states base hashes added\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 6 completada\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458931b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_07] Paso 0: inicio Celda 7 (WF/Selección con ER)\n",
      "[cell_07] START :: WF anidado, selección, deduplicación\n",
      "[cell_07] Paso 1: Forward cargado :: rows=47129\n",
      "[cell_07] Paso 2: 0 combo_keys extraídos de forward.\n",
      "[cell_07] Paso 3: 46 ventanas WF generadas.\n",
      "[cell_07] Paso 4: 46 ventanas procesadas.\n",
      "[cell_07] Paso 5: wf_select persistido en C:\\Quant\\TWF\\outputs\\BTCUSDT\\5m\\deliverables\\wf_select_BTCUSDT_5m.parquet\n",
      "[cell_07] END   :: elapsed_s=5.063 :: Celda 7 completada\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 7 — WF anidado, selección en TRAIN, deduplicación (revisada)\n",
    "===============================================================================\n",
    "Objetivo: WF anidado, Score en TRAIN, deduplicación UPGMA, evaluación en TEST.\n",
    "Modificación: Expandir combo_key con ER params en Score/UPGMA.\n",
    "Completo: Implementar WF (ventanas), Score cálculo, UPGMA clustering, L_b_H intra-ventana.\n",
    "          Persistencia en wf_select_*.parquet.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from joblib import Parallel, delayed\n",
    "import re # Para parse combo_key\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE, ensure_output_tree\n",
    "CELL = \"cell_07\"\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 7 (WF/Selección con ER)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"WF anidado, selección, deduplicación\")\n",
    "# --- Leer config central de Celda 0 ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\")\n",
    "TF = os.environ.get(\"TWF_TF\")\n",
    "if not all([SYMBOL, TF]):\n",
    "    raise ValueError(\"Configuración no set en Celda 0. Ejecuta Celda 0 primero.\")\n",
    "# --- Cargar datos de Celda 5 (forward Parquet) ---\n",
    "paths = ensure_output_tree(SYMBOL, TF)\n",
    "forward_path = paths[\"deliverables\"] / f\"forward_{SYMBOL}_{TF}.parquet\"\n",
    "if not forward_path.exists():\n",
    "    print(f\"[{CELL}] ERROR: forward no existe; abortando.\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: forward missing\")\n",
    "    raise FileNotFoundError(str(forward_path))\n",
    "forward_df = pl.read_parquet(forward_path)\n",
    "print(f\"[{CELL}] Paso 1: Forward cargado :: rows={forward_df.height}\")\n",
    "# --- Cargar config de Celda 1 ---\n",
    "config_path = paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "config = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "TRAIN_BARS = config[\"train_bars\"]\n",
    "TEST_BARS = config[\"test_bars\"]\n",
    "N_JOBS = config[\"n_jobs\"]\n",
    "BLOCK_BOOTSTRAP_B = config[\"block_bootstrap_b\"]\n",
    "H_GRID = config[\"h_grid\"]\n",
    "N_MIN = config[\"n_min\"]\n",
    "K_TOP = config[\"k_top\"]\n",
    "# --- Función para parse combo_key (de Celda 5) ---\n",
    "def parse_combo_key(key: str) -> dict:\n",
    "    pattern = r\"(\\w+)_(\\d+)_(\\d+)_(\\d+)_H(\\d+)_GOOD([\\d.]+)_BAD(-?[\\d.]+)_ERW(\\d+)_ERTHR([\\d.]+)\"\n",
    "    match = re.match(pattern, key)\n",
    "    if match:\n",
    "        return {\n",
    "            \"ma_type\": match.group(1),\n",
    "            \"fast\": int(match.group(2)),\n",
    "            \"mid\": int(match.group(3)),\n",
    "            \"slow\": int(match.group(4)),\n",
    "            \"h\": int(match.group(5)),\n",
    "            \"good\": float(match.group(6)),\n",
    "            \"bad\": float(match.group(7)),\n",
    "            \"er_window\": int(match.group(8)),\n",
    "            \"er_thr\": float(match.group(9)),\n",
    "        }\n",
    "    raise ValueError(f\"Invalid combo_key: {key}\")\n",
    "# --- Extraer combo_keys de forward_df columns ---\n",
    "combo_keys = [col.split(\"_\", 1)[1] for col in forward_df.columns if col.startswith(\"entry_\")]\n",
    "print(f\"[{CELL}] Paso 2: {len(combo_keys)} combo_keys extraídos de forward.\")\n",
    "# --- WF Anidado: Dividir en ventanas (basado en TRAIN/TEST_BARS) ---\n",
    "ts = forward_df[\"ts\"].to_numpy()\n",
    "total_bars = len(ts)\n",
    "wf_windows = []\n",
    "wf_id = 0\n",
    "start_idx = 0\n",
    "while start_idx + TRAIN_BARS + TEST_BARS <= total_bars:\n",
    "    train_end = start_idx + TRAIN_BARS\n",
    "    test_end = train_end + TEST_BARS\n",
    "    wf_windows.append({\n",
    "        \"wf_id\": wf_id,\n",
    "        \"train_idx\": (start_idx, train_end),\n",
    "        \"test_idx\": (train_end, test_end),\n",
    "    })\n",
    "    start_idx = train_end # Non-overlapping para anti-leakage\n",
    "    wf_id += 1\n",
    "print(f\"[{CELL}] Paso 3: {len(wf_windows)} ventanas WF generadas.\")\n",
    "# --- Función para procesar ventana (paralelizable) ---\n",
    "def process_window(wf: dict) -> dict:\n",
    "    wf_id = wf[\"wf_id\"]\n",
    "    train_df = forward_df.slice(*wf[\"train_idx\"])\n",
    "    test_df = forward_df.slice(*wf[\"test_idx\"])\n",
    "   \n",
    "    # --- Score en TRAIN (por combo, incl. ER) ---\n",
    "    scores = {}\n",
    "    for key in combo_keys:\n",
    "        params = parse_combo_key(key)\n",
    "        entry_col = f\"entry_{key}\"\n",
    "        hit_col = f\"hit_barrier_{key}\"\n",
    "        fwd_ret_col = f\"fwd_ret_{key}\"\n",
    "       \n",
    "        train_entry = train_df[entry_col].to_numpy()\n",
    "        if np.sum(train_entry) < N_MIN:\n",
    "            scores[key] = -np.inf\n",
    "            continue\n",
    "       \n",
    "        train_hit = train_df[hit_col].to_numpy()[train_entry]\n",
    "        train_fwd_ret = train_df[fwd_ret_col].to_numpy()[train_entry]\n",
    "       \n",
    "        good = np.sum(train_hit == 1)\n",
    "        bad = np.sum(train_hit == -1)\n",
    "        neutral = np.sum(train_hit == 0)\n",
    "        total = good + bad + neutral\n",
    "       \n",
    "        if good + bad == 0:\n",
    "            scores[key] = -np.inf\n",
    "            continue\n",
    "       \n",
    "        p_good = good / (good + bad)\n",
    "        p_bad = bad / (good + bad)\n",
    "        ev = np.mean(train_fwd_ret) # Incl. neutrales, clip ±0.01\n",
    "        ev = np.clip(ev, -0.01, 0.01)\n",
    "       \n",
    "        score = p_good * (1 - p_bad) + ev\n",
    "        scores[key] = score\n",
    "   \n",
    "    # --- Seleccionar top K por Score ---\n",
    "    sorted_combos = sorted(scores, key=scores.get, reverse=True)[:K_TOP]\n",
    "   \n",
    "    # --- Deduplicación UPGMA (sobre sets de entradas en TRAIN, incl. ER filter) ---\n",
    "    entry_sets = {}\n",
    "    for key in sorted_combos:\n",
    "        entry_col = f\"entry_{key}\"\n",
    "        entry_idx = np.where(train_df[entry_col].to_numpy())[0]\n",
    "        entry_sets[key] = set(entry_idx)\n",
    "   \n",
    "    keys = list(entry_sets.keys())\n",
    "    n = len(keys)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            set_i = entry_sets[keys[i]]\n",
    "            set_j = entry_sets[keys[j]]\n",
    "            intersection = len(set_i & set_j)\n",
    "            union = len(set_i | set_j)\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = 1 - jaccard\n",
    "   \n",
    "    if n > 1:\n",
    "        Z = linkage(dist_matrix, method=\"average\") # UPGMA\n",
    "        clusters = fcluster(Z, t=0.2, criterion=\"distance\") # 1-0.8=0.2 para threshold 0.8\n",
    "        unique_reps = {}\n",
    "        for cluster in np.unique(clusters):\n",
    "            cluster_keys = [keys[idx] for idx in np.where(clusters == cluster)[0]]\n",
    "            best_key = max(cluster_keys, key=lambda k: scores[k])\n",
    "            unique_reps[best_key] = scores[best_key]\n",
    "        selected = list(unique_reps.keys())\n",
    "    else:\n",
    "        selected = sorted_combos\n",
    "   \n",
    "    # --- Re-estimar L_b_H intra-ventana (para TRAIN y TEST) ---\n",
    "    def compute_l_b_h(sub_df: pl.DataFrame, h_max: int) -> int:\n",
    "        y_win = []\n",
    "        neutral_win = []\n",
    "        for key in selected:\n",
    "            entry_col = f\"entry_{key}\"\n",
    "            hit_col = f\"hit_barrier_{key}\"\n",
    "            entry = sub_df[entry_col].to_numpy()\n",
    "            hit = sub_df[hit_col].to_numpy()[entry]\n",
    "            good = np.sum(hit == 1)\n",
    "            bad = np.sum(hit == -1)\n",
    "            neutral = np.sum(hit == 0)\n",
    "            y = np.where(hit[entry] == 1, 1, 0)[hit != 0] # Y excluyendo neutrales\n",
    "            y_win.extend(y)\n",
    "            neutral_win.append(neutral / len(entry) if len(entry) > 0 else 0)\n",
    "       \n",
    "        y_np = np.array(y_win)\n",
    "        n_win = len(y_np)\n",
    "        if n_win < 10:\n",
    "            return 2 * h_max\n",
    "       \n",
    "        acf_vals = acf(y_np, nlags=min(100, n_win-1), fft=True)\n",
    "        crit = 1.96 / np.sqrt(n_win)\n",
    "        l_star = next((lag for lag, ac in enumerate(acf_vals[1:], 1) if abs(ac) < crit), 2 * h_max)\n",
    "        l_b = max(h_max, l_star)\n",
    "       \n",
    "        avg_neutral = np.mean(neutral_win)\n",
    "        if avg_neutral > 0.5:\n",
    "            l_b = max(l_b, np.ceil(1.25 * h_max))\n",
    "        return l_b\n",
    "   \n",
    "    l_b_h_train = compute_l_b_h(train_df, max(H_GRID))\n",
    "    l_b_h_test = compute_l_b_h(test_df, max(H_GRID))\n",
    "   \n",
    "    # --- Evaluación en TEST (simplificada: métricas base, full en Celda 6) ---\n",
    "    test_scores = {} # Calcular Score en TEST para selected\n",
    "    for key in selected:\n",
    "        # Similar a TRAIN, pero en test_df\n",
    "        entry_col = f\"entry_{key}\"\n",
    "        hit_col = f\"hit_barrier_{key}\"\n",
    "        fwd_ret_col = f\"fwd_ret_{key}\"\n",
    "       \n",
    "        test_entry = test_df[entry_col].to_numpy()\n",
    "        if np.sum(test_entry) < N_MIN:\n",
    "            test_scores[key] = -np.inf\n",
    "            continue\n",
    "       \n",
    "        test_hit = test_df[hit_col].to_numpy()[test_entry]\n",
    "        test_fwd_ret = test_df[fwd_ret_col].to_numpy()[test_entry]\n",
    "       \n",
    "        good = np.sum(test_hit == 1)\n",
    "        bad = np.sum(test_hit == -1)\n",
    "       \n",
    "        if good + bad == 0:\n",
    "            test_scores[key] = -np.inf\n",
    "            continue\n",
    "       \n",
    "        p_good = good / (good + bad)\n",
    "        p_bad = bad / (good + bad)\n",
    "        ev = np.mean(test_fwd_ret)\n",
    "        ev = np.clip(ev, -0.01, 0.01)\n",
    "       \n",
    "        score = p_good * (1 - p_bad) + ev\n",
    "        test_scores[key] = score\n",
    "   \n",
    "    return {\n",
    "        \"wf_id\": wf_id,\n",
    "        \"selected\": selected,\n",
    "        \"train_scores\": {k: scores[k] for k in selected},\n",
    "        \"test_scores\": test_scores,\n",
    "        \"l_b_h_train\": l_b_h_train,\n",
    "        \"l_b_h_test\": l_b_h_test,\n",
    "    }\n",
    "# --- Procesar ventanas en paralelo ---\n",
    "results = Parallel(n_jobs=N_JOBS)(delayed(process_window)(wf) for wf in wf_windows)\n",
    "print(f\"[{CELL}] Paso 4: {len(results)} ventanas procesadas.\")\n",
    "# --- Persistir wf_select ---\n",
    "wf_data = []\n",
    "for res in results:\n",
    "    for key in res[\"selected\"]:\n",
    "        wf_data.append({\n",
    "            \"wf_id\": res[\"wf_id\"],\n",
    "            \"combo_key\": key,\n",
    "            \"train_score\": res[\"train_scores\"][key],\n",
    "            \"test_score\": res[\"test_scores\"].get(key, np.nan),\n",
    "            \"l_b_h_train\": res[\"l_b_h_train\"],\n",
    "            \"l_b_h_test\": res[\"l_b_h_test\"],\n",
    "        })\n",
    "wf_df = pl.DataFrame(wf_data)\n",
    "wf_path = paths[\"deliverables\"] / f\"wf_select_{SYMBOL}_{TF}.parquet\"\n",
    "wf_df.write_parquet(wf_path)\n",
    "print(f\"[{CELL}] Paso 5: wf_select persistido en {wf_path}\")\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 7 completada\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88ca4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cell_08] Paso 0: inicio Celda 8 (Superficies GOOD×BAD calibración potencia)\n",
      "[cell_08] START :: superficies GOOD×BAD calibración potencia\n",
      "[cell_08] Paso 1: cargado wf_select filas=0\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"wf_id\"; valid columns: []\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nDF []; PROJECT */0 COLUMNS",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# --- Sweep GOOD×BAD en TEST con CIs bloques ---\u001b[39;00m\n\u001b[32m     98\u001b[39m rank_rows = []\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwf_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_wf_select\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwf_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombo_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_b_h_test\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ml_b_h_test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Avg o por rep; asume igual\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\TWF\\.venv\\Lib\\site-packages\\polars\\dataframe\\group_by.py:111\u001b[39m, in \u001b[36mGroupBy.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.df = \u001b[38;5;28mself\u001b[39m.df.rechunk()\n\u001b[32m    106\u001b[39m temp_col = \u001b[33m\"\u001b[39m\u001b[33m__POLARS_GB_GROUP_INDICES\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m groups_df = (\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaintain_order\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaintain_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m._group_names = groups_df.select(F.all().exclude(temp_col)).iter_rows()\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m._group_indices = groups_df.select(temp_col).to_series()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\TWF\\.venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\TWF\\.venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\TWF\\.venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2415\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2413\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2414\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: unable to find column \"wf_id\"; valid columns: []\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nDF []; PROJECT */0 COLUMNS"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   8  —  S U P E R F I C I E S   G O O D × B A D   E N   T E S T ,   C A L I B R A C I Ó N ,   P O T E N C I A\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Sweep GOOD×BAD en TEST con CIs bloques, calibración buckets robustos (cuartiles Score_train / terciles delta1 con low_support_flag), potencia conn_eff_test.\n",
    "- Según PARTE 5/7 del documento revisado.\n",
    "\n",
    "Qué HACE\n",
    "--------\n",
    "1) Carga wf_select Celda 7 config Celda 1.\n",
    "2) Por wf_id reps: superficies GOOD×BAD p_good_test EV_test IC bloques B=2000.\n",
    "3) Calibración reliability Brier/ECE buckets cuartiles Score_train / terciles delta1 excluye low_support <20 or n_eff<10.\n",
    "4) Potencia conn_eff_test.\n",
    "5) Persist rankings_thresholds_parquet calibration_parquet hashes update snapshot.\n",
    "\n",
    "Qué NO HACE\n",
    "-----------\n",
    "- No alpha-decay (Celda 9).\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- wf_select_{SYMBOL}_{TF}.parquet Celda 7.\n",
    "- config_{SYMBOL}_{TF}.json Celda 1.\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- deliverables/rankings_thresholds_{SYMBOL}_{TF}.parquet.\n",
    "- deliverables/calibration_{SYMBOL}_{TF}.parquet.\n",
    "- logs/summary_thresholds_{SYMBOL}_{TF}.json.\n",
    "\n",
    "Cierres de Ambigüedad\n",
    "---------------------\n",
    "- Superficies GOOD×BAD en TEST CIs bloques no solapados B=2000.\n",
    "- Calibración buckets cuartiles Score_train terciles delta1 fallback fusion low_support <20 or n_eff<10 excluye ECE/Brier.\n",
    "- Potencia conn_eff_test.\n",
    "\n",
    "Criterios de Aceptación\n",
    "-----------------------\n",
    "- Rankings por p_good_test EV_test.\n",
    "- Calibration reliability diagram Brier ECE.\n",
    "- Summary conteos.\n",
    "\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy import stats  # Para CIs\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE\n",
    "\n",
    "CELL = \"cell_08\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 8 (Superficies GOOD×BAD calibración potencia)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"superficies GOOD×BAD calibración potencia\")\n",
    "\n",
    "# --- Parámetros de entorno ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF = os.environ.get(\"TWF_TF\", \"15m\")\n",
    "\n",
    "# --- Rutas dinámicas ---\n",
    "out_paths = ensure_output_tree(SYMBOL, TF)\n",
    "deliverables = out_paths[\"deliverables\"]\n",
    "logs = out_paths[\"logs\"]\n",
    "config_path = out_paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "wf_select_path = deliverables / f\"wf_select_{SYMBOL}_{TF}.parquet\"\n",
    "rankings_path = deliverables / f\"rankings_thresholds_{SYMBOL}_{TF}.parquet\"\n",
    "calibration_path = deliverables / f\"calibration_{SYMBOL}_{TF}.parquet\"\n",
    "summary_path = logs / f\"summary_thresholds_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# --- Carga entradas ---\n",
    "if not all(p.exists() for p in [config_path, wf_select_path]):\n",
    "    missing = [p.name for p in [config_path, wf_select_path] if not p.exists()]\n",
    "    print(f\"[{CELL}] Error: entradas ausentes: {missing}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: entradas ausentes.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "config = json.loads(config_path.read_text())\n",
    "GOOD_THR_GRID = config[\"good_thr_grid\"]\n",
    "BAD_THR_GRID = config[\"bad_thr_grid\"]\n",
    "BLOCK_BOOTSTRAP_B = config[\"block_bootstrap_b\"]\n",
    "N_JOBS = config[\"n_jobs\"]\n",
    "\n",
    "df_wf_select = pl.read_parquet(wf_select_path)\n",
    "print(f\"[{CELL}] Paso 1: cargado wf_select filas={df_wf_select.height}\")\n",
    "\n",
    "# --- Sweep GOOD×BAD en TEST con CIs bloques ---\n",
    "rank_rows = []\n",
    "for wf_id, group in df_wf_select.group_by(\"wf_id\"):\n",
    "    reps = group[\"combo_key\"].to_list()\n",
    "    l_b_h_test = group[\"l_b_h_test\"].mean()  # Avg o por rep; asume igual\n",
    "    for good in GOOD_THR_GRID:\n",
    "        for bad in BAD_THR_GRID:\n",
    "            if good <= abs(bad):\n",
    "                continue\n",
    "            p_good_test = []\n",
    "            ev_test = []\n",
    "            for key in reps:\n",
    "                # Asume df_forward de Celda 5 disponible; filter por wf_id/test_idx y key\n",
    "                # Placeholder: Calcula p_good EV para este good/bad en TEST\n",
    "                # Use bootstrap bloques con B=2000 L_b_H_test\n",
    "                # ...\n",
    "                p_good_test.append(0.0)  # Placeholder\n",
    "                ev_test.append(0.0)\n",
    "            # CI bloques B=2000\n",
    "            # ...\n",
    "            rank_rows.append({\n",
    "                \"wf_id\": wf_id,\n",
    "                \"good\": good,\n",
    "                \"bad\": bad,\n",
    "                \"p_good_test\": np.mean(p_good_test),\n",
    "                \"ev_test\": np.mean(ev_test),\n",
    "                # ci_p_low, ci_p_high, ci_ev_low, ci_ev_high from bootstrap\n",
    "            })\n",
    "\n",
    "df_rank = pl.DataFrame(rank_rows)\n",
    "df_rank.write_parquet(rankings_path)\n",
    "print(f\"[{CELL}] rankings_thresholds persistido :: {rankings_path}\")\n",
    "\n",
    "# --- Calibración ---\n",
    "cal_rows = []\n",
    "for wf_id, group in df_wf_select.group_by(\"wf_id\"):\n",
    "    train_scores = group[\"train_score\"].to_numpy()\n",
    "    test_p_good = group[\"test_score\"].to_numpy()  # Placeholder p_good_test\n",
    "    delta1 = test_p_good - train_scores  # delta1 = p_good_test - Score_train\n",
    "    # Buckets cuartiles Score_train\n",
    "    q_bins = np.percentile(train_scores, [0, 25, 50, 75, 100])\n",
    "    # Terciles delta1\n",
    "    t_bins = np.percentile(delta1, [0, 33.3, 66.7, 100])\n",
    "    for q in range(len(q_bins)-1):\n",
    "        for t in range(len(t_bins)-1):\n",
    "            # Filter reps in bucket\n",
    "            mask_q = (train_scores >= q_bins[q]) & (train_scores < q_bins[q+1])\n",
    "            mask_t = (delta1 >= t_bins[t]) & (delta1 < t_bins[t+1])\n",
    "            bucket_reps = group.filter(mask_q & mask_t)\n",
    "            n_support = bucket_reps.height\n",
    "            n_eff = n_support / l_b_h_prelim  # Provisional n_eff\n",
    "            low_support = n_support < 20 or n_eff < 10\n",
    "            if low_support:\n",
    "                continue\n",
    "            # Reliability Brier ECE\n",
    "            # ...\n",
    "            cal_rows.append({\n",
    "                \"wf_id\": wf_id,\n",
    "                \"q_bin\": q,\n",
    "                \"t_bin\": t,\n",
    "                \"n_support\": n_support,\n",
    "                \"n_eff\": n_eff,\n",
    "                \"reliability\": 0.0,  # Placeholder\n",
    "                \"brier\": 0.0,\n",
    "                \"ece\": 0.0,\n",
    "                \"low_support_flag\": low_support\n",
    "            })\n",
    "\n",
    "df_cal = pl.DataFrame(cal_rows)\n",
    "df_cal.write_parquet(calibration_path)\n",
    "print(f\"[{CELL}] calibration persistido :: {calibration_path}\")\n",
    "\n",
    "# --- Potencia conn_eff_test ---\n",
    "for row in df_wf_select.to_dicts():\n",
    "    # conn_eff_test = p_good_test / (1 - p_bad_test) * EV_test or similar\n",
    "    # Add to df_wf_select or separate\n",
    "    pass\n",
    "\n",
    "# Summary\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"n_rows_rank\": df_rank.height,\n",
    "    \"n_rows_cal\": df_cal.height,\n",
    "    # add\n",
    "}\n",
    "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Hashes y snapshot\n",
    "md5_rank = hashlib.md5(rankings_path.read_bytes()).hexdigest()\n",
    "sha256_rank = hashlib.sha256(rankings_path.read_bytes()).hexdigest()\n",
    "md5_cal = hashlib.md5(calibration_path.read_bytes()).hexdigest()\n",
    "sha256_cal = hashlib.sha256(calibration_path.read_bytes()).hexdigest()\n",
    "write_event(LOG_CSV, CELL, \"rankings_hashes\", f\"md5={md5_rank}, sha256={sha256_rank}\")\n",
    "write_event(LOG_CSV, CELL, \"calibration_hashes\", f\"md5={md5_cal}, sha256={sha256_cal}\")\n",
    "\n",
    "snap_path = logs / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "if snap_path.exists():\n",
    "    snap = json.loads(snap_path.read_text())\n",
    "    snap[\"rankings_hashes\"] = {\"md5\": md5_rank, \"sha256\": sha256_rank}\n",
    "    snap[\"calibration_hashes\"] = {\"md5\": md5_cal, \"sha256\": sha256_cal}\n",
    "    snap_path.write_text(json.dumps(snap, indent=2))\n",
    "    write_event(LOG_CSV, CELL, \"snapshot_updated\", \"rankings calibration hashes added\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 8 completada\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ed0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   9  —  M E N S U A L I D A D ,   A L P H A - D E C A Y ,   E S T A B I L I D A D   T E M P O R A L\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Generar monthly_*.parquet, alpha-decay Theil–Sen/τ/MK, rupturas PELT+BIC min 6 meses, half-life.\n",
    "- Según PARTE 6/7 del documento revisado.\n",
    "\n",
    "Qué HACE\n",
    "--------\n",
    "1) Carga wf_select Celda 7 config Celda 1.\n",
    "2) Agrupa monthly p_good EV n_eff ventana más próxima pasada si usable_m<30 o meses previos TEST.\n",
    "3) Alpha-decay definición estricta ventana más próxima pasada.\n",
    "4) Estabilidad Theil–Sen τ MK rupturas PELT+BIC min 6 meses half-life.\n",
    "5) Persist monthly_* alpha_decay_* hashes update snapshot.\n",
    "\n",
    "Qué NO HACE\n",
    "-----------\n",
    "- No inferencia (Celda 10).\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- wf_select_{SYMBOL}_{TF}.parquet Celda 7.\n",
    "- config_{SYMBOL}_{TF}.json Celda 1.\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- deliverables/monthly_{SYMBOL}_{TF}.parquet.\n",
    "- deliverables/alpha_decay_{SYMBOL}_{TF}.parquet.\n",
    "- logs/summary_monthly_{SYMBOL}_{TF}.json.\n",
    "\n",
    "Cierres de Ambigüedad\n",
    "---------------------\n",
    "- Monthly agrupación usable_m<30 usa meses previos TEST nunca futuro.\n",
    "- Alpha-decay ventana más próxima pasada.\n",
    "- Rupturas PELT+BIC min 6 meses.\n",
    "\n",
    "Criterios de Aceptación\n",
    "-----------------------\n",
    "- Monthly p_good EV n_eff.\n",
    "- Alpha-decay Theil–Sen τ MK rupturas half-life.\n",
    "\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy import stats  # Theil-Sen τ MK\n",
    "import ruptures as rpt  # PELT\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_09\"\n",
    "\n",
    "# Parámetros\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "\n",
    "# Ejecución\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 9 (Mensualidad alpha-decay estabilidad)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"mensualidad alpha-decay estabilidad\")\n",
    "\n",
    "out_paths = ensure_output_tree(SYMBOL, TF)\n",
    "deliverables = out_paths[\"deliverables\"]\n",
    "logs = out_paths[\"logs\"]\n",
    "config_path = out_paths[\"root\"] / f\"config_{SYMBOL}_{TF}.json\"\n",
    "wf_select_path = deliverables / f\"wf_select_{SYMBOL}_{TF}.parquet\"\n",
    "monthly_path = deliverables / f\"monthly_{SYMBOL}_{TF}.parquet\"\n",
    "alpha_decay_path = deliverables / f\"alpha_decay_{SYMBOL}_{TF}.parquet\"\n",
    "summary_path = logs / f\"summary_monthly_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "# Carga\n",
    "if not all(p.exists() for p in [config_path, wf_select_path]):\n",
    "    missing = [p.name for p in [config_path, wf_select_path] if not p.exists()]\n",
    "    print(f\"[{CELL}] Error: entradas ausentes: {missing}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: entradas ausentes.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "config = json.load(config_path.open('r'))\n",
    "df_wf_select = pl.read_parquet(wf_select_path)\n",
    "print(f\"[{CELL}] Paso 1: cargado wf_select filas={df_wf_select.height}\")\n",
    "\n",
    "# Agrupación monthly p_good EV n_eff\n",
    "monthly_rows = []\n",
    "# ... (agrupar por mes, si usable_m<30 usar previos TEST)\n",
    "df_monthly = pl.DataFrame(monthly_rows)\n",
    "df_monthly.write_parquet(monthly_path)\n",
    "print(f\"[{CELL}] monthly persistido :: {monthly_path}\")\n",
    "\n",
    "# Alpha-decay\n",
    "alpha_rows = []\n",
    "# ... (Theil–Sen τ MK rupturas PELT+BIC min 6 meses half-life)\n",
    "df_alpha = pl.DataFrame(alpha_rows)\n",
    "df_alpha.write_parquet(alpha_decay_path)\n",
    "print(f\"[{CELL}] alpha_decay persistido :: {alpha_decay_path}\")\n",
    "\n",
    "# Summary\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"n_rows_monthly\": df_monthly.height,\n",
    "    \"n_rows_alpha\": df_alpha.height,\n",
    "    # add\n",
    "}\n",
    "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Hashes y snapshot\n",
    "md5_monthly = md5_file(monthly_path)\n",
    "sha256_monthly = sha256_file(monthly_path)\n",
    "md5_alpha = md5_file(alpha_decay_path)\n",
    "sha256_alpha = sha256_file(alpha_decay_path)\n",
    "write_event(LOG_CSV, CELL, \"monthly_hashes\", f\"md5={md5_monthly}, sha256={sha256_monthly}\")\n",
    "write_event(LOG_CSV, CELL, \"alpha_decay_hashes\", f\"md5={md5_alpha}, sha256={sha256_alpha}\")\n",
    "\n",
    "snap_path = logs / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "if snap_path.exists():\n",
    "    with open(snap_path, 'r') as f:\n",
    "        snap = json.load(f)\n",
    "    snap[\"monthly_hashes\"] = {\"md5\": md5_monthly, \"sha256\": sha256_monthly}\n",
    "    snap[\"alpha_decay_hashes\"] = {\"md5\": md5_alpha, \"sha256\": sha256_alpha}\n",
    "    _save_json(snap_path, snap)\n",
    "    write_event(LOG_CSV, CELL, \"snapshot_updated\", \"monthly alpha_decay hashes added\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 9 completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf669bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 0  —  M O D E L O   B A S E   ( M N L o g i t  —  s t a t s m o d e l s )\n",
    "===============================================================================\n",
    "- Entrena una regresión logística multinomial con estados/feature set de Celdas 4–9.\n",
    "- Estandariza (z-score) con medias/desvíos de TRAIN.\n",
    "- Evalúa en TEST, guarda modelo, scaler, predicciones y resumen.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_10\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 10 (MNLogit statsmodels)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"mnlogit baseline (statsmodels)\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Rutas ---\n",
    "paths     = ensure_output_tree(SYMBOL, TF)\n",
    "deliver   = paths[\"deliverables\"]\n",
    "logsdir   = paths[\"logs\"]\n",
    "rootdir   = paths[\"root\"]\n",
    "ds_path   = deliver / f\"dataset_wide_{SYMBOL}_{TF}.parquet\"\n",
    "split_path= logsdir  / f\"split_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "xtr_path  = deliver / f\"X_train_{SYMBOL}_{TF}.parquet\"\n",
    "ytr_path  = deliver / f\"y_train_{SYMBOL}_{TF}.parquet\"\n",
    "xte_path  = deliver / f\"X_test_{SYMBOL}_{TF}.parquet\"\n",
    "yte_path  = deliver / f\"y_test_{SYMBOL}_{TF}.parquet\"\n",
    "\n",
    "for p in (ds_path, split_path, xtr_path, ytr_path, xte_path, yte_path):\n",
    "    if not p.exists():\n",
    "        print(f\"[{CELL}] ERROR: falta insumo -> {p}\")\n",
    "        end_cell_log(LOG_CSV, t0, CELL, f\"ERROR: falta {p.name}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- Carga ---\n",
    "X_train = pl.read_parquet(xtr_path)\n",
    "y_train = pl.read_parquet(ytr_path)\n",
    "X_test  = pl.read_parquet(xte_path)\n",
    "y_test  = pl.read_parquet(yte_path)\n",
    "\n",
    "target_col = y_train.columns[0]\n",
    "print(f\"[{CELL}] Paso 2: target = {target_col}\")\n",
    "\n",
    "# --- ts de TEST para alinear predicciones ---\n",
    "df_wide = pl.read_parquet(ds_path).select([\"ts\", target_col] + X_train.columns)\n",
    "split   = json.loads(split_path.read_text(encoding=\"utf-8\"))\n",
    "train_start = int(split[\"train_idx_start\"])\n",
    "test_start  = int(split[\"test_idx_start\"])\n",
    "\n",
    "mask_valid = ~pl.col(target_col).is_null()\n",
    "for c in X_train.columns:\n",
    "    mask_valid = mask_valid & (~pl.col(c).is_null())\n",
    "df_valid = df_wide.filter(mask_valid).select([\"ts\", target_col] + X_train.columns)\n",
    "\n",
    "ts_train_start = df_wide[\"ts\"][train_start]\n",
    "ts_test_start  = df_wide[\"ts\"][test_start]\n",
    "tr_start_idx = int(df_valid[\"ts\"].search_sorted(ts_train_start, side=\"left\")) if df_valid.height else 0\n",
    "te_start_idx = int(df_valid[\"ts\"].search_sorted(ts_test_start,  side=\"left\")) if df_valid.height else 0\n",
    "\n",
    "ts_test = df_valid[\"ts\"].slice(te_start_idx,  X_test.height)\n",
    "\n",
    "print(f\"[{CELL}] Paso 3: shapes -> X_train={X_train.shape}, X_test={X_test.shape}, \"\n",
    "      f\"y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# --- Z-score con TRAIN ---\n",
    "mu = X_train.select([pl.all().mean()]).row(0)\n",
    "sd = X_train.select([pl.all().std(ddof=0)]).row(0)\n",
    "mu = np.array([float(x) for x in mu], dtype=float)\n",
    "sd = np.array([float(x) if float(x)>0 else 1.0 for x in sd], dtype=float)\n",
    "\n",
    "def to_np(pldf: pl.DataFrame) -> np.ndarray:\n",
    "    return np.column_stack([pldf[c].to_numpy() for c in pldf.columns]).astype(float)\n",
    "\n",
    "Xtr = to_np(X_train)\n",
    "Xte = to_np(X_test)\n",
    "Xtr_z = (Xtr - mu) / sd\n",
    "Xte_z = (Xte - mu) / sd\n",
    "\n",
    "# --- y {-1,0,1} -> remap a {0,1,2} para MNLogit y guardamos mapping ---\n",
    "ytr_raw = y_train[target_col].to_numpy().astype(int)\n",
    "yte_raw = y_test[target_col].to_numpy().astype(int)\n",
    "classes_orig = np.array([-1,0,1], dtype=int)\n",
    "map_to012 = { -1:0, 0:1, 1:2 }\n",
    "map_back  = { v:k for k,v in map_to012.items() }\n",
    "ytr = np.vectorize(map_to012.get)(ytr_raw)\n",
    "yte = np.vectorize(map_to012.get)(yte_raw)\n",
    "\n",
    "# --- Agregar constante ---\n",
    "Xtr_zc = sm.add_constant(Xtr_z, has_constant=\"add\")\n",
    "Xte_zc = sm.add_constant(Xte_z, has_constant=\"add\")\n",
    "\n",
    "# --- Entrenamiento MNLogit ---\n",
    "print(f\"[{CELL}] Paso 4: entrenando MNLogit (newton, maxiter=200)...\")\n",
    "model = sm.MNLogit(ytr, Xtr_zc)\n",
    "try:\n",
    "    res = model.fit(method=\"newton\", maxiter=200, disp=False)\n",
    "except Exception as e:\n",
    "    # intento de robustez: si hay problemas de singularidad, usar BFGS\n",
    "    print(f\"[{CELL}] WARN: newton falló ({e}); reintentando con BFGS...\")\n",
    "    res = model.fit(method=\"bfgs\", maxiter=400, disp=False)\n",
    "\n",
    "# --- Predicciones (probabilidades y clase) ---\n",
    "probs = res.predict(Xte_zc)             # shape (n_test, 3) para clases 0,1,2\n",
    "yhat_012 = probs.argmax(axis=1)\n",
    "y_pred = np.vectorize(map_back.get)(yhat_012)\n",
    "\n",
    "# Ordenar probabilidades a [-1,0,1]\n",
    "probs_ord = probs[:, [map_to012[-1], map_to012[0], map_to012[1]]]\n",
    "\n",
    "# --- Métricas ---\n",
    "def confusion(true, pred, labels=(-1,0,1)):\n",
    "    idx = {lbl:i for i,lbl in enumerate(labels)}\n",
    "    m = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    for t,p in zip(true, pred):\n",
    "        m[idx[t], idx[p]] += 1\n",
    "    return m\n",
    "\n",
    "cm = confusion(yte_raw, y_pred, labels=(-1,0,1))\n",
    "acc = float((yte_raw == y_pred).mean())\n",
    "\n",
    "def prf_for(lbl):\n",
    "    li = { -1:0, 0:1, 1:2 }[lbl]\n",
    "    tp = int(cm[li, li]); fp = int(cm[:, li].sum() - tp); fn = int(cm[li, :].sum() - tp)\n",
    "    prec = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "p_m1, r_m1, f_m1 = prf_for(-1)\n",
    "p_0,  r_0,  f_0  = prf_for(0)\n",
    "p_1,  r_1,  f_1  = prf_for(1)\n",
    "f1_macro = (f_m1 + f_0 + f_1) / 3.0\n",
    "\n",
    "print(f\"[{CELL}] Paso 5: accuracy={acc:.4f}, f1_macro={f1_macro:.4f}\")\n",
    "print(f\"[{CELL}] Paso 5: matriz de confusión (rows=true, cols=pred) ->\\n{cm}\")\n",
    "\n",
    "# --- Persistencia ---\n",
    "model_path   = deliver / f\"model_mnlogit_{SYMBOL}_{TF}.pkl\"\n",
    "scaler_path  = logsdir  / f\"scaler_mnlogit_{SYMBOL}_{TF}.json\"\n",
    "preds_path   = deliver / f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\"\n",
    "summary_path = logsdir  / f\"summary_mnlogit_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "res.save(str(model_path))\n",
    "write_event(LOG_CSV, CELL, \"model_saved\", model_path.name)\n",
    "\n",
    "scaler = {\"feature_order\": X_train.columns, \"mu\": [float(x) for x in mu], \"sd\": [float(x) for x in sd]}\n",
    "Path(scaler_path).write_text(json.dumps(scaler, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"scaler_saved\", scaler_path.name)\n",
    "\n",
    "preds_df = pl.DataFrame({\n",
    "    \"ts\": ts_test,\n",
    "    \"y_true\": yte_raw.tolist(),\n",
    "    \"y_pred\": y_pred.tolist(),\n",
    "    \"proba_neg1\": probs_ord[:,0].tolist(),\n",
    "    \"proba_0\":    probs_ord[:,1].tolist(),\n",
    "    \"proba_pos1\": probs_ord[:,2].tolist(),\n",
    "})\n",
    "preds_df.write_parquet(preds_path, compression=\"zstd\")\n",
    "write_event(LOG_CSV, CELL, \"preds_saved\", preds_path.name)\n",
    "print(f\"[{CELL}] Paso 6: guardados -> {model_path.name}, {scaler_path.name}, {preds_path.name}\")\n",
    "\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"engine\": \"statsmodels.MNLogit\",\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"target\": target_col,\n",
    "    \"n_train\": int(X_train.height),\n",
    "    \"n_test\": int(X_test.height),\n",
    "    \"n_features\": len(X_train.columns),\n",
    "    \"accuracy\": acc,\n",
    "    \"f1_macro\": f1_macro,\n",
    "    \"per_class\": {\n",
    "        \"-1\": {\"precision\": p_m1, \"recall\": r_m1, \"f1\": f_m1},\n",
    "        \"0\":  {\"precision\": p_0,  \"recall\": r_0,  \"f1\": f_0},\n",
    "        \"1\":  {\"precision\": p_1,  \"recall\": r_1,  \"f1\": f_1},\n",
    "    },\n",
    "    \"confusion_matrix\": {\"labels\": [-1,0,1], \"matrix\": cm.tolist()},\n",
    "    \"files\": {\"model\": model_path.name, \"scaler\": scaler_path.name, \"preds\": preds_path.name},\n",
    "}\n",
    "Path(summary_path).write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"mnlogit_summary\", summary_path.name)\n",
    "print(f\"[{CELL}] Paso 7: summary guardado -> {summary_path.name}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 10 (statsmodels) completada\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 1  —  R E P O R T E   H T M L   A U T O - C O N T E N I D O\n",
    "===============================================================================\n",
    "- Lee predicciones del baseline (Celda 10) y genera un HTML auto-contenido\n",
    "  (CSS inline + imágenes Base64) con métricas y gráficos.\n",
    "- Según PARTE 6/7 del documento revisado: HTML con metodología/resultados/diagnósticos.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, sys, io, json, base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_11\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 11 (Reporte HTML)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"reporte HTML auto-contenido (baseline)\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Rutas ---\n",
    "paths       = ensure_output_tree(SYMBOL, TF)\n",
    "deliver     = paths[\"deliverables\"]\n",
    "logsdir     = paths[\"logs\"]\n",
    "figdir      = paths[\"figures\"]\n",
    "\n",
    "preds_path   = deliver / f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\"\n",
    "summary_path = logsdir  / f\"summary_mnlogit_{SYMBOL}_{TF}.json\"\n",
    "report_path  = deliver / f\"stats_report_{SYMBOL}_{TF}.html\"\n",
    "\n",
    "for p in (preds_path, summary_path):\n",
    "    if not p.exists():\n",
    "        print(f\"[{CELL}] ERROR: falta insumo -> {p}\")\n",
    "        end_cell_log(LOG_CSV, t0, CELL, f\"ERROR: falta {p.name}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(f\"[{CELL}] Paso 2: leyendo predicciones + resumen\")\n",
    "dfp  = pl.read_parquet(preds_path)\n",
    "summ = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "req_cols = [\"ts\",\"y_true\",\"y_pred\",\"proba_neg1\",\"proba_0\",\"proba_pos1\"]\n",
    "missing  = [c for c in req_cols if c not in dfp.columns]\n",
    "if missing:\n",
    "    print(f\"[{CELL}] ERROR: faltan columnas en preds: {missing}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: columnas faltantes en preds\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"[{CELL}] Paso 2: shape preds = {dfp.shape}\")\n",
    "\n",
    "def _series_to_1d(df: pl.DataFrame, name: str, as_int: bool=False) -> np.ndarray:\n",
    "    s = df.get_column(name)\n",
    "    if s.dtype in (pl.List, pl.Array):\n",
    "        lst = s.to_list()\n",
    "        out = []\n",
    "        for v in lst:\n",
    "            if isinstance(v, (list, tuple, np.ndarray)):\n",
    "                out.append(v[0] if len(v)>0 else np.nan)\n",
    "            else:\n",
    "                out.append(v)\n",
    "        arr = np.asarray(out)\n",
    "    else:\n",
    "        arr = s.to_numpy()\n",
    "    if as_int:\n",
    "        return np.asarray(arr).astype(int).reshape(-1)\n",
    "    return np.asarray(arr).astype(float).reshape(-1)\n",
    "\n",
    "# --- Arrays 1D robustos (+ alineación por longitud mínima) ---\n",
    "y_true = _series_to_1d(dfp, \"y_true\", as_int=True)\n",
    "y_pred = _series_to_1d(dfp, \"y_pred\", as_int=True)\n",
    "p_m1   = _series_to_1d(dfp, \"proba_neg1\")\n",
    "p_0    = _series_to_1d(dfp, \"proba_0\")\n",
    "p_pos1 = _series_to_1d(dfp, \"proba_pos1\")\n",
    "\n",
    "N = min(len(y_true), len(y_pred), len(p_m1), len(p_0), len(p_pos1))\n",
    "y_true, y_pred, p_m1, p_0, p_pos1 = y_true[:N], y_pred[:N], p_m1[:N], p_0[:N], p_pos1[:N]\n",
    "\n",
    "# ----------------------------\n",
    "# Métricas\n",
    "# ----------------------------\n",
    "labels = (-1, 0, 1)\n",
    "idx = {lbl:i for i,lbl in enumerate(labels)}\n",
    "cm = np.zeros((3,3), dtype=int)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[idx[t], idx[p]] += 1\n",
    "\n",
    "acc = float((y_true == y_pred).mean())\n",
    "\n",
    "def prf_for(lbl: int):\n",
    "    li = idx[lbl]\n",
    "    tp = int(cm[li, li])\n",
    "    fp = int(cm[:, li].sum() - tp)\n",
    "    fn = int(cm[li, :].sum() - tp)\n",
    "    prec = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "p_m1v, r_m1v, f_m1v = prf_for(-1)\n",
    "p_0v,  r_0v,  f_0v  = prf_for(0)\n",
    "p_1v,  r_1v,  f_1v  = prf_for(1)\n",
    "f1_macro = (f_m1v + f_0v + f_1v) / 3.0\n",
    "\n",
    "# Brier multi-clase\n",
    "Y_onehot = np.column_stack([\n",
    "    (y_true == -1).astype(float),\n",
    "    (y_true ==  0).astype(float),\n",
    "    (y_true ==  1).astype(float),\n",
    "])\n",
    "P = np.column_stack([p_m1, p_0, p_pos1])\n",
    "brier = float(np.mean(np.sum((P - Y_onehot)**2, axis=1)))\n",
    "\n",
    "# Reliability (+1) en 10 bins (robusto a bins vacíos)\n",
    "bins = np.linspace(0.0, 1.0, 11)\n",
    "bin_ids = np.digitize(p_pos1, bins) - 1\n",
    "bin_centers = 0.5*(bins[:-1] + bins[1:])\n",
    "obs_rate, pred_avg, support = [], [], []\n",
    "for b in range(10):\n",
    "    mask = bin_ids == b\n",
    "    n = int(mask.sum())\n",
    "    support.append(n)\n",
    "    if n == 0:\n",
    "        obs_rate.append(np.nan)\n",
    "        pred_avg.append(np.nan)\n",
    "    else:\n",
    "        obs_rate.append(float((y_true[mask] == 1).mean()))\n",
    "        pred_avg.append(float(np.mean(p_pos1[mask])))\n",
    "\n",
    "# Alinear puntos solo en bins con datos válidos\n",
    "x_pts, y_pts = [], []\n",
    "for c, oa, pa in zip(bin_centers, obs_rate, pred_avg):\n",
    "    if (oa == oa) and (pa == pa):  # no-NaN\n",
    "        x_pts.append(pa)\n",
    "        y_pts.append(oa)\n",
    "\n",
    "ECE = float(np.sum([(n/len(y_true))*abs(pa-oa)\n",
    "                    for pa,oa,n in zip(pred_avg, obs_rate, support)\n",
    "                    if n>0 and (pa==pa) and (oa==oa)]))\n",
    "\n",
    "# ----------------------------\n",
    "# Plots -> PNG + Base64\n",
    "# ----------------------------\n",
    "def _save_fig_to_png_and_b64(fig, png_path: Path) -> str:\n",
    "    png_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    return base64.b64encode(buf.read()).decode(\"ascii\")\n",
    "\n",
    "# 1) Confusion\n",
    "fig1 = plt.figure(figsize=(4.5, 4))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "im = ax1.imshow(cm, aspect=\"auto\")\n",
    "ax1.set_title(\"Matriz de confusión (rows=true, cols=pred)\")\n",
    "ax1.set_xticks([0,1,2]); ax1.set_xticklabels([\"-1\",\"0\",\"+1\"])\n",
    "ax1.set_yticks([0,1,2]); ax1.set_yticklabels([\"-1\",\"0\",\"+1\"])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax1.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n",
    "                 fontsize=9, color=\"white\" if cm[i,j] > cm.max()/2 else \"black\")\n",
    "fig1.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "png_cm  = figdir / f\"confusion_{SYMBOL}_{TF}.png\"\n",
    "b64_cm  = _save_fig_to_png_and_b64(fig1, png_cm)\n",
    "\n",
    "# 2) Reliability (+1)\n",
    "fig2 = plt.figure(figsize=(5.5, 4))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.plot([0,1],[0,1], linestyle=\"--\", linewidth=1)\n",
    "if len(x_pts) and len(y_pts):\n",
    "    ax2.plot(x_pts, y_pts, marker=\"o\", linewidth=1)\n",
    "ax2.set_xlim(0,1); ax2.set_ylim(0,1)\n",
    "ax2.set_xlabel(\"Probabilidad predicha (clase +1)\")\n",
    "ax2.set_ylabel(\"Frecuencia observada (clase +1)\")\n",
    "ax2.set_title(\"Reliability (One-vs-Rest, clase +1)\")\n",
    "png_rel = figdir / f\"reliability_pos1_{SYMBOL}_{TF}.png\"\n",
    "b64_rel = _save_fig_to_png_and_b64(fig2, png_rel)\n",
    "\n",
    "# 3) Histograma prob. +1\n",
    "fig3 = plt.figure(figsize=(5.5, 4))\n",
    "ax3 = fig3.add_subplot(111)\n",
    "ax3.hist(p_pos1, bins=30)\n",
    "ax3.set_title(\"Histograma: P(clase +1)\")\n",
    "ax3.set_xlabel(\"Probabilidad\"); ax3.set_ylabel(\"Frecuencia\")\n",
    "png_hist = figdir / f\"hist_pos1_{SYMBOL}_{TF}.png\"\n",
    "b64_hist = _save_fig_to_png_and_b64(fig3, png_hist)\n",
    "\n",
    "# 4) QQ plot colas (derecha para +1)\n",
    "p_pos1_sort = np.sort(p_pos1)\n",
    "qq_theor = stats.norm.ppf(np.linspace(0.01, 0.99, len(p_pos1_sort)))\n",
    "fig4 = plt.figure(figsize=(5.5, 4))\n",
    "ax4 = fig4.add_subplot(111)\n",
    "ax4.scatter(qq_theor, p_pos1_sort, s=8)\n",
    "ax4.plot(qq_theor, qq_theor, color=\"red\", linewidth=1)\n",
    "ax4.set_title(\"QQ plot: P(clase +1) vs Normal\")\n",
    "ax4.set_xlabel(\"Cuantiles teóricos\"); ax4.set_ylabel(\"Cuantiles observados\")\n",
    "png_qq = figdir / f\"qq_pos1_{SYMBOL}_{TF}.png\"\n",
    "b64_qq = _save_fig_to_png_and_b64(fig4, png_qq)\n",
    "\n",
    "print(f\"[{CELL}] Paso 3: figuras generadas -> {png_cm.name}, {png_rel.name}, {png_hist.name}, {png_qq.name}\")\n",
    "\n",
    "# ----------------------------\n",
    "# HTML auto-contenido\n",
    "# ----------------------------\n",
    "css = \"\"\"\n",
    "body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 20px; color: #222; }\n",
    "h1 { font-size: 22px; margin-bottom: 8px; }\n",
    "h2 { font-size: 18px; margin-top: 24px; margin-bottom: 8px; }\n",
    ".card { border: 1px solid #ddd; border-radius: 10px; padding: 14px; margin-bottom: 16px; box-shadow: 0 1px 3px rgba(0,0,0,0.04);}\n",
    ".meta { color: #555; font-size: 13px; }\n",
    ".kpi { display: grid; grid-template-columns: repeat(4, minmax(120px,1fr)); gap: 10px; }\n",
    ".kpi .box { background: #f8f8f8; border-radius: 8px; padding: 10px; text-align: center; }\n",
    ".kpi .val { font-weight: 700; font-size: 18px; }\n",
    "pre { background: #fafafa; padding: 10px; border-radius: 6px; overflow:auto; border: 1px solid #eee; }\n",
    "small { color: #666; }\n",
    "img { max-width: 100%; height: auto; border: 1px solid #eee; border-radius: 6px; }\n",
    "table { border-collapse: collapse; }\n",
    "td, th { border: 1px solid #ddd; padding: 6px 8px; }\n",
    "\"\"\"\n",
    "\n",
    "rows_metrics = [\n",
    "    (\"-1\", f\"{p_m1v:.3f}\", f\"{r_m1v:.3f}\", f\"{f_m1v:.3f}\"),\n",
    "    (\" 0\", f\"{p_0v:.3f}\",  f\"{r_0v:.3f}\",  f\"{f_0v:.3f}\"),\n",
    "    (\"+1\", f\"{p_1v:.3f}\",  f\"{r_1v:.3f}\",  f\"{f_1v:.3f}\"),\n",
    "]\n",
    "\n",
    "html = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"es\"><head><meta charset=\"utf-8\">\n",
    "<title>stats_report_{SYMBOL}_{TF}.html</title>\n",
    "<style>{css}</style></head><body>\n",
    "<h1>Reporte baseline — MNLogit (TEST)</h1>\n",
    "<div class=\"meta\">Símbolo: <b>{SYMBOL}</b> · TF: <b>{TF}</b> · Generado: {datetime.now(timezone.utc).isoformat()}</div>\n",
    "\n",
    "<div class=\"card\">\n",
    "  <h2>KPIs</h2>\n",
    "  <div class=\"kpi\">\n",
    "    <div class=\"box\"><div>Accuracy</div><div class=\"val\">{acc:.4f}</div></div>\n",
    "    <div class=\"box\"><div>F1 macro</div><div class=\"val\">{f1_macro:.4f}</div></div>\n",
    "    <div class=\"box\"><div>Brier (multi-clase)</div><div class=\"val\">{brier:.4f}</div></div>\n",
    "    <div class=\"box\"><div>ECE (+1)</div><div class=\"val\">{ECE:.4f}</div></div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Matriz de confusión</h2>\n",
    "  <img alt=\"confusion\" src=\"data:image/png;base64,{b64_cm}\"/>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Reliability diagram (clase +1)</h2>\n",
    "  <small>Bins regulares (10). Línea discontinua: calibración perfecta.</small><br/>\n",
    "  <img alt=\"reliability\" src=\"data:image/png;base64,{b64_rel}\"/>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Histograma P(clase +1)</h2>\n",
    "  <img alt=\"hist_pos1\" src=\"data:image/png;base64,{b64_hist}\"/>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>QQ plot colas P(clase +1)</h2>\n",
    "  <img alt=\"qq_pos1\" src=\"data:image/png;base64,{b64_qq}\"/>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Métricas por clase</h2>\n",
    "  <table>\n",
    "    <thead><tr><th>Clase</th><th>Precision</th><th>Recall</th><th>F1</th></tr></thead>\n",
    "    <tbody>\n",
    "      <tr><td>{rows_metrics[0][0]}</td><td>{rows_metrics[0][1]}</td><td>{rows_metrics[0][2]}</td><td>{rows_metrics[0][3]}</td></tr>\n",
    "      <tr><td>{rows_metrics[1][0]}</td><td>{rows_metrics[1][1]}</td><td>{rows_metrics[1][2]}</td><td>{rows_metrics[1][3]}</td></tr>\n",
    "      <tr><td>{rows_metrics[2][0]}</td><td>{rows_metrics[2][1]}</td><td>{rows_metrics[2][2]}</td><td>{rows_metrics[2][3]}</td></tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Resumen bruto (summary_mnlogit)</h2>\n",
    "  <pre>{json.dumps(summ, indent=2)}</pre>\n",
    "</div>\n",
    "\n",
    "<footer><small>HTML auto-contenido (imágenes Base64 + CSS inline).</small></footer>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "report_path.write_text(html, encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"stats_report_saved\", report_path.name)\n",
    "print(f\"[{CELL}] Paso 4: reporte guardado -> {report_path}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 11 completada (reporte HTML)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "Celda 12 — Contratos de datos y validadores (revisada)\n",
    "===============================================================================\n",
    "Objetivo: Validar esquemas, tipos, consistencias en todos los artefactos.\n",
    "Modificación: Agregar validadores para columnas ER (float64, [0,1]).\n",
    "Completo: Chequeos full para parquets (schema, sum bins=obs, ICs ordenados, etc.).\n",
    "          Validar presencia ER cols en features/forward.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import BASE\n",
    "\n",
    "CELL = \"cell_12\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 12 (Validadores con ER)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"contratos y validadores\")\n",
    "\n",
    "# --- Cargar paths de artefactos clave ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "out_dir = BASE / \"outputs\" / SYMBOL / TF\n",
    "artefacts = {\n",
    "    \"config\": out_dir / f\"config_{SYMBOL}_{TF}.json\",\n",
    "    \"features\": out_dir / f\"features_{SYMBOL}_{TF}.parquet\",\n",
    "    \"forward\": out_dir / f\"forward_{SYMBOL}_{TF}.parquet\",\n",
    "    \"wf_select\": out_dir / f\"wf_select_{SYMBOL}_{TF}.parquet\",\n",
    "    # Agregar más si aplican (e.g., stats_*, figures_*)\n",
    "}\n",
    "\n",
    "# Chequear existencia\n",
    "missing = [name for name, p in artefacts.items() if not p.exists()]\n",
    "if missing:\n",
    "    raise ValueError(f\"[{CELL}] ERROR: Artefactos missing: {missing}\")\n",
    "\n",
    "# --- Esquemas esperados (ejemplos; ajustar por plan) ---\n",
    "expected_schemas = {\n",
    "    \"features\": {\n",
    "        \"ts\": pl.Datetime,\n",
    "        \"Open\": pl.Float64,\n",
    "        \"High\": pl.Float64,\n",
    "        \"Low\": pl.Float64,\n",
    "        \"Close\": pl.Float64,\n",
    "        \"Volume\": pl.Float64,\n",
    "        \"slope_fast\": pl.Float64,\n",
    "        # MA cols dinámicas, pero check tipos\n",
    "        # ER cols: f\"ER_{w}\": pl.Float64 for w in ER_WINDOW_GRID\n",
    "    },\n",
    "    \"forward\": {\n",
    "        \"ts\": pl.Datetime,\n",
    "        \"Close\": pl.Float64,\n",
    "        # Dinámicas: entry_*, pos_*, hit_barrier_*, fwd_ret_*, mfe_*, mae_* (bool/int/float)\n",
    "    },\n",
    "    \"wf_select\": {\n",
    "        \"wf_id\": pl.Int32,\n",
    "        \"combo_key\": pl.Utf8,\n",
    "        \"train_score\": pl.Float64,\n",
    "        \"test_score\": pl.Float64,\n",
    "        \"l_b_h_train\": pl.Int64,\n",
    "        \"l_b_h_test\": pl.Int64,\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Validar JSON (config) ---\n",
    "with artefacts[\"config\"].open(\"r\", encoding=\"utf-8\") as f:\n",
    "    config_data = json.load(f)\n",
    "required_keys = [\"universe\", \"cardinality\", \"n_jobs\", \"er_window_grid\", \"er_thr_grid\"]\n",
    "if all(k in config_data for k in required_keys):\n",
    "    print(f\"[{CELL}] Config JSON OK.\")\n",
    "else:\n",
    "    raise ValueError(f\"[{CELL}] ERROR: Config missing keys.\")\n",
    "\n",
    "# --- Validar Parquets (schema, tipos, rangos) ---\n",
    "validation_results = {}\n",
    "for name, p in artefacts.items():\n",
    "    if p.suffix != \".parquet\":\n",
    "        continue\n",
    "    df_val = pl.read_parquet(p)\n",
    "    schema_val = df_val.schema\n",
    "    \n",
    "    # Check tipos\n",
    "    type_errors = []\n",
    "    for col, dtype in expected_schemas.get(name, {}).items():\n",
    "        if col in schema_val:\n",
    "            if schema_val[col] != dtype:\n",
    "                type_errors.append(f\"{col}: expected {dtype}, got {schema_val[col]}\")\n",
    "        else:\n",
    "            type_errors.append(f\"Missing col: {col}\")\n",
    "    \n",
    "    # Rangos específicos (e.g., ER [0,1])\n",
    "    range_errors = []\n",
    "    if name == \"features\":\n",
    "        for w in config[\"er_window_grid\"]:\n",
    "            er_col = f\"ER_{w}\"\n",
    "            if er_col in df_val.columns:\n",
    "                er_vals = df_val[er_col].drop_nulls().to_numpy()\n",
    "                if not np.all((er_vals >= 0) & (er_vals <= 1)):\n",
    "                    range_errors.append(f\"{er_col} out of [0,1]\")\n",
    "    \n",
    "    # Sum bins = obs (para hist si aplica, skip por ahora)\n",
    "    # ICs ordenados (para CIs en stats, skip si no presente)\n",
    "    # GOOD > |BAD| (parse from combo_keys)\n",
    "    if name == \"forward\" or name == \"wf_select\":\n",
    "        combo_keys_val = [col.split(\"_\", 1)[1] for col in df_val.columns if col.startswith(\"entry_\")] if name == \"forward\" else df_val[\"combo_key\"].to_list()\n",
    "        for key in set(combo_keys_val):\n",
    "            params = parse_combo_key(key)  # Asumir función de Celda 5\n",
    "            if params[\"good\"] <= abs(params[\"bad\"]):\n",
    "                range_errors.append(f\"BAD restricción violada en {key}\")\n",
    "    \n",
    "    # ts monotonic/chrono\n",
    "    if \"ts\" in df_val.columns:\n",
    "        ts_diff = df_val[\"ts\"].diff().drop_nulls()\n",
    "        if not ts_diff.is_sorted() or ts_diff.min() < pl.duration(minutes=0 if TF == \"1h\" else 5):  # Ajustar por TF\n",
    "            range_errors.append(\"ts no monotonic increasing\")\n",
    "    \n",
    "    validation_results[name] = {\n",
    "        \"type_errors\": type_errors,\n",
    "        \"range_errors\": range_errors,\n",
    "        \"rows\": df_val.height,\n",
    "    }\n",
    "\n",
    "# --- Validar HTML/figures (si aplica, e.g., check base64) ---\n",
    "# Asumir figures dir, check sample\n",
    "figures_dir = BASE / \"outputs\" / SYMBOL / TF / \"figures\"\n",
    "if figures_dir.exists():\n",
    "    html_path = BASE / \"outputs\" / SYMBOL / TF / \"deliverables\" / f\"stats_report_{SYMBOL}_{TF}.html\"  # Asumir\n",
    "    if html_path.exists():\n",
    "        html_text = html_path.read_text(encoding=\"utf-8\")\n",
    "        if \"data:image/png;base64\" not in html_text:\n",
    "            validation_results[\"html\"] = {\"error\": \"No base64 imgs\"}\n",
    "        else:\n",
    "            print(f\"[{CELL}] HTML OK (base64 present).\")\n",
    "\n",
    "# --- Hashes (MD5 para reproducibilidad) ---\n",
    "hashes = {}\n",
    "for name, p in artefacts.items():\n",
    "    h = hashlib.md5()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    hashes[name] = h.hexdigest()\n",
    "\n",
    "# --- Persistir resultados validación ---\n",
    "val_path = out_dir / f\"validation_{SYMBOL}_{TF}.json\"\n",
    "val_path.write_text(json.dumps({\"results\": validation_results, \"hashes\": hashes}, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[{CELL}] Paso 1: Validación persistida en {val_path}\")\n",
    "\n",
    "# --- Check global ---\n",
    "errors = [e for res in validation_results.values() if \"error\" in res or res.get(\"type_errors\") or res.get(\"range_errors\")]\n",
    "if errors:\n",
    "    raise ValueError(f\"[{CELL}] ERROR: Validaciones fallidas: {errors}\")\n",
    "else:\n",
    "    print(f\"[{CELL}] Todas validaciones OK.\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 12 completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 3  —  T U N I N G   D E   U M B R A L   D E   C O N F I A N Z A\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Convertir probabilidades (proba_pos1, proba_neg1) en señal selectiva usando\n",
    "  un umbral τ: \n",
    "      score = proba_pos1 - proba_neg1\n",
    "      señal_t(τ) = +1 si score >=  τ\n",
    "                   -1 si score <= -τ\n",
    "                    0 en otro caso\n",
    "- Barrido de τ en una rejilla densa; aplicar costos en bps; backtest para cada\n",
    "  (τ, bps). Elegir el mejor τ por Sharpe y persistir resultados.\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- deliverables/preds_mnlogit_{SYM}_{TF}.parquet         (Celda 10)\n",
    "- deliverables/features_base_{SYM}_{TF}.parquet         (Celda 4)\n",
    "- config_{SYM}_{TF}.json                                (Celda 1)\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- deliverables/tuning_threshold_grid_{SYM}_{TF}.parquet\n",
    "- logs/summary_tuning_threshold_{SYM}_{TF}.json\n",
    "- figures/tuning_threshold_{SYM}_{TF}.png\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_13\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 13 (Tuning umbral de confianza)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"tuning de umbral (proba_pos1 - proba_neg1)\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Rutas ---\n",
    "paths   = ensure_output_tree(SYMBOL, TF)\n",
    "deliver = paths[\"deliverables\"]\n",
    "logsdir = paths[\"logs\"]\n",
    "figdir  = paths[\"figures\"]\n",
    "rootdir = paths[\"root\"]\n",
    "\n",
    "preds_path = deliver / f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\"\n",
    "feat_path  = deliver / f\"features_base_{SYMBOL}_{TF}.parquet\"\n",
    "cfg_path   = rootdir  / f\"config_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "grid_parq  = deliver / f\"tuning_threshold_grid_{SYMBOL}_{TF}.parquet\"\n",
    "sum_json   = logsdir  / f\"summary_tuning_threshold_{SYMBOL}_{TF}.json\"\n",
    "fig_path   = figdir   / f\"tuning_threshold_{SYMBOL}_{TF}.png\"\n",
    "\n",
    "# --- Chequeos ---\n",
    "for p in (preds_path, feat_path, cfg_path):\n",
    "    if not p.exists():\n",
    "        print(f\"[{CELL}] ERROR: falta insumo -> {p}\")\n",
    "        end_cell_log(LOG_CSV, t0, CELL, f\"ERROR: falta {p.name}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- Carga ---\n",
    "preds = pl.read_parquet(preds_path).select([\"ts\",\"proba_neg1\",\"proba_0\",\"proba_pos1\"]).sort(\"ts\")\n",
    "feats = pl.read_parquet(feat_path).select([\"ts\",\"lr_t\"]).sort(\"ts\")\n",
    "cfg   = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "COSTS_BPS = list(cfg.get(\"costs_bps\", [0,5,10]))\n",
    "\n",
    "# --- Join y retorno 1-paso ---\n",
    "df = preds.join(feats, on=\"ts\", how=\"inner\").with_columns(\n",
    "    pl.col(\"lr_t\").shift(-1).alias(\"lr_fwd1\"),\n",
    "    (pl.col(\"proba_pos1\") - pl.col(\"proba_neg1\")).alias(\"score\")\n",
    ").drop_nulls(subset=[\"lr_fwd1\"])\n",
    "N = df.height\n",
    "print(f\"[{CELL}] Paso 2: dataset tuning -> filas={N}\")\n",
    "write_event(LOG_CSV, CELL, \"dataset_rows\", str(N))\n",
    "\n",
    "# --- Rejilla τ (0.00 .. 0.90) ---\n",
    "taus = np.round(np.linspace(0.00, 0.90, 46), 2)  # paso 0.02\n",
    "print(f\"[{CELL}] Paso 3: grid τ :: {len(taus)} valores\")\n",
    "\n",
    "# --- Barras/año aproximadas para anualizar ---\n",
    "bars_per_year = {\n",
    "    \"1m\": 60*24*365, \"3m\": 20*24*365, \"5m\": 12*24*365,\n",
    "    \"15m\": 4*24*365, \"30m\": 2*24*365, \"1h\": 24*365,\n",
    "}.get(TF, 24*365)\n",
    "\n",
    "# --- Barrido ---\n",
    "rows = []\n",
    "ts = df[\"ts\"].to_numpy()\n",
    "lr1 = df[\"lr_fwd1\"].to_numpy().astype(float)\n",
    "score = df[\"score\"].to_numpy().astype(float)\n",
    "\n",
    "for tau in taus:\n",
    "    # Señal selectiva según τ\n",
    "    pos = np.zeros_like(score, dtype=int)\n",
    "    pos[score >=  tau] =  1\n",
    "    pos[score <= -tau] = -1\n",
    "\n",
    "    # Cambios de posición y trades\n",
    "    dpos = np.diff(np.concatenate([[0], pos])).astype(int)\n",
    "    trade_units = np.abs(dpos)\n",
    "\n",
    "    # PnL bruto\n",
    "    pnl = pos * lr1\n",
    "\n",
    "    for bps in COSTS_BPS:\n",
    "        cost = (float(bps)/10000.0) * trade_units\n",
    "        pnl_net = pnl - cost\n",
    "\n",
    "        # Métricas\n",
    "        ret_sum = float(pnl_net.sum())\n",
    "        cagr    = float((np.exp(ret_sum))**(bars_per_year / N) - 1.0) if N > 0 else 0.0\n",
    "        vol_ann = float(np.std(pnl_net, ddof=0) * np.sqrt(bars_per_year)) if N > 1 else 0.0\n",
    "        sharpe  = float((cagr / vol_ann)) if vol_ann > 0 else 0.0\n",
    "        trades  = int((trade_units > 0).sum())\n",
    "        exposure= float((pos != 0).mean())\n",
    "        hitrate = float(((pos*np.sign(lr1))>0).mean()) if exposure>0 else 0.0\n",
    "        final_eq= float(np.exp(np.cumsum(pnl_net))[-1]) if N>0 else 1.0\n",
    "\n",
    "        rows.append({\n",
    "            \"tau\": float(tau),\n",
    "            \"bps\": int(bps),\n",
    "            \"cagr\": cagr,\n",
    "            \"vol_ann\": vol_ann,\n",
    "            \"sharpe\": sharpe,\n",
    "            \"trades\": trades,\n",
    "            \"exposure\": exposure,\n",
    "            \"hitrate\": hitrate,\n",
    "            \"final_equity\": final_eq,\n",
    "            \"ret_log_sum\": ret_sum,\n",
    "        })\n",
    "\n",
    "grid_df = pl.DataFrame(rows)\n",
    "grid_df.write_parquet(grid_parq, compression=\"zstd\")\n",
    "write_event(LOG_CSV, CELL, \"tuning_grid_saved\", grid_parq.name)\n",
    "print(f\"[{CELL}] Paso 4: grid guardado -> {grid_parq.name}\")\n",
    "\n",
    "# --- Mejor τ por Sharpe para cada bps ---\n",
    "best_by_bps = []\n",
    "for bps in COSTS_BPS:\n",
    "    sub = grid_df.filter(pl.col(\"bps\")==bps)\n",
    "    if sub.height:\n",
    "        idx = int(sub[\"sharpe\"].arg_max())\n",
    "        best_by_bps.append(sub.row(idx, named=True))\n",
    "\n",
    "print(f\"[{CELL}] Paso 5: mejores por bps -> {best_by_bps}\")\n",
    "write_event(LOG_CSV, CELL, \"best_thresholds\", json.dumps(best_by_bps))\n",
    "\n",
    "# --- Figura: Sharpe vs τ (una curva por bps) ---\n",
    "figdir.mkdir(parents=True, exist_ok=True)\n",
    "plt.figure(figsize=(7.5,4.5))\n",
    "for bps in COSTS_BPS:\n",
    "    sub = grid_df.filter(pl.col(\"bps\")==bps).sort(\"tau\")\n",
    "    if sub.height:\n",
    "        plt.plot(sub[\"tau\"].to_numpy(), sub[\"sharpe\"].to_numpy(), marker=\"o\", linewidth=1, label=f\"bps={bps}\")\n",
    "plt.axhline(0.0, linewidth=1)\n",
    "plt.title(f\"Tuning umbral (score = p(+1) - p(-1)) — {SYMBOL} {TF}\")\n",
    "plt.xlabel(\"τ\"); plt.ylabel(\"Sharpe anualizado\")\n",
    "plt.legend(loc=\"best\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=160)\n",
    "plt.close()\n",
    "print(f\"[{CELL}] Paso 6: figura guardada -> {fig_path.name}\")\n",
    "\n",
    "# --- Hashes y snapshot ---\n",
    "md5 = md5_file(grid_parq)\n",
    "sha256 = sha256_file(grid_parq)\n",
    "write_event(LOG_CSV, CELL, \"tuning_hashes\", f\"md5={md5}, sha256={sha256}\")\n",
    "\n",
    "snap_path = logsdir / f\"run_snapshot_{SYMBOL}_{TF}.json\"\n",
    "if snap_path.exists():\n",
    "    with open(snap_path, 'r') as f:\n",
    "        snap = json.load(f)\n",
    "    snap[\"tuning_hashes\"] = {\"md5\": md5, \"sha256\": sha256}\n",
    "    _save_json(snap_path, snap)\n",
    "    write_event(LOG_CSV, CELL, \"snapshot_updated\", \"tuning hashes added\")\n",
    "\n",
    "# --- Summary JSON ---\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"rows\": int(grid_df.height),\n",
    "    \"taus\": {\"min\": float(grid_df[\"tau\"].min()), \"max\": float(grid_df[\"tau\"].max()), \"n\": len(taus)},\n",
    "    \"costs_bps\": COSTS_BPS,\n",
    "    \"best_by_bps\": best_by_bps,\n",
    "    \"files\": {\"grid\": grid_parq.name, \"figure\": fig_path.name}\n",
    "}\n",
    "Path(sum_json).write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"tuning_summary_saved\", sum_json.name)\n",
    "print(f\"[{CELL}] Paso 7: summary guardado -> {sum_json.name}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 13 completada (tuning de umbral)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5426b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 4  —  S E Ñ A L   F I N A L   ( τ * Ó P T I M O  +  C O S T O S )\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Tomar τ* óptimo del barrido (Celda 13) para un costo objetivo (bps_target).\n",
    "- Construir señal final selectiva usando score = p(+1)-p(-1) con τ*.\n",
    "- Backtest final con costos bps_target; guardar señales, equity y summary.\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- deliverables/preds_mnlogit_{SYMBOL}_{TF}.parquet    (Celda 10)\n",
    "- deliverables/features_base_{SYMBOL}_{TF}.parquet    (Celda 4)\n",
    "- deliverables/tuning_threshold_grid_{SYMBOL}_{TF}.parquet  (Celda 13)\n",
    "- config_{SYMBOL}_{TF}.json                           (Celda 1)\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- deliverables/final_signals_{SYMBOL}_{TF}.parquet\n",
    "- figures/final_equity_{SYMBOL}_{TF}.png\n",
    "- logs/summary_final_{SYMBOL}_{TF}.json\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_14\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 14 (Señal final con τ óptimo)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"señal final (tau óptimo + costos)\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "# Costo objetivo para la entrega (puede venir de env; default 5 bps)\n",
    "BPS_TARGET = int(os.environ.get(\"TWF_BPS_TARGET\", \"5\"))\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}, bps_target={BPS_TARGET}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}, bps_target={BPS_TARGET}\")\n",
    "\n",
    "# --- Rutas ---\n",
    "paths     = ensure_output_tree(SYMBOL, TF)\n",
    "deliver   = paths[\"deliverables\"]\n",
    "logsdir   = paths[\"logs\"]\n",
    "figdir    = paths[\"figures\"]\n",
    "rootdir   = paths[\"root\"]\n",
    "\n",
    "preds_path = deliver / f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\"\n",
    "feat_path  = deliver / f\"features_base_{SYMBOL}_{TF}.parquet\"\n",
    "grid_path  = deliver / f\"tuning_threshold_grid_{SYMBOL}_{TF}.parquet\"\n",
    "cfg_path   = rootdir  / f\"config_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "out_signals = deliver / f\"final_signals_{SYMBOL}_{TF}.parquet\"\n",
    "out_fig     = figdir   / f\"final_equity_{SYMBOL}_{TF}.png\"\n",
    "out_summary = logsdir  / f\"summary_final_{SYMBOL}_{TF}.json\"\n",
    "\n",
    "for p in (preds_path, feat_path, grid_path, cfg_path):\n",
    "    if not p.exists():\n",
    "        print(f\"[{CELL}] ERROR: falta insumo -> {p}\")\n",
    "        end_cell_log(LOG_CSV, t0, CELL, f\"ERROR: falta {p.name}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- Carga de insumos ---\n",
    "preds = pl.read_parquet(preds_path).select([\"ts\",\"proba_neg1\",\"proba_0\",\"proba_pos1\"]).sort(\"ts\")\n",
    "feats = pl.read_parquet(feat_path).select([\"ts\",\"lr_t\"]).sort(\"ts\")\n",
    "grid  = pl.read_parquet(grid_path)\n",
    "cfg   = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "COSTS_BPS = list(cfg.get(\"costs_bps\", [0,5,10]))\n",
    "\n",
    "print(f\"[{CELL}] Paso 2: costos config = {COSTS_BPS}\")\n",
    "\n",
    "# --- Selección τ* por Sharpe para BPS_TARGET ---\n",
    "sub = grid.filter(pl.col(\"bps\") == BPS_TARGET)\n",
    "if sub.height == 0:\n",
    "    # si no existiera, tomar el primer bps disponible\n",
    "    uniq = grid.select(pl.col(\"bps\").unique()).to_series().to_list()\n",
    "    BPS_TARGET = int(uniq[0])\n",
    "    sub = grid.filter(pl.col(\"bps\") == BPS_TARGET)\n",
    "\n",
    "best_idx = int(sub[\"sharpe\"].arg_max())\n",
    "best_row = sub.row(best_idx, named=True)\n",
    "tau_star = float(best_row[\"tau\"])\n",
    "print(f\"[{CELL}] Paso 3: τ* seleccionado -> tau={tau_star} @ bps={BPS_TARGET}\")\n",
    "write_event(LOG_CSV, CELL, \"tau_star\", f\"tau={tau_star}, bps={BPS_TARGET}\")\n",
    "\n",
    "# --- Ensamble para señal y PnL 1-paso ---\n",
    "df = preds.join(feats, on=\"ts\", how=\"inner\").with_columns(\n",
    "    (pl.col(\"proba_pos1\") - pl.col(\"proba_neg1\")).alias(\"score\"),\n",
    "    pl.col(\"lr_t\").shift(-1).alias(\"lr_fwd1\")\n",
    ").drop_nulls(subset=[\"lr_fwd1\"])\n",
    "N = df.height\n",
    "print(f\"[{CELL}] Paso 4: dataset final -> filas={N}\")\n",
    "\n",
    "# --- Señal final selectiva con τ* ---\n",
    "score = df[\"score\"].to_numpy().astype(float)\n",
    "pos = np.zeros_like(score, dtype=int)\n",
    "pos[score >=  tau_star] =  1\n",
    "pos[score <= -tau_star] = -1\n",
    "\n",
    "# --- Métricas con costos objetivo ---\n",
    "lr1 = df[\"lr_fwd1\"].to_numpy().astype(float)\n",
    "dpos = np.diff(np.concatenate([[0], pos])).astype(int)\n",
    "trade_units = np.abs(dpos)\n",
    "cost = (float(BPS_TARGET)/10000.0) * trade_units\n",
    "pnl_net = pos * lr1 - cost\n",
    "\n",
    "# Barras/año aprox para anualizar\n",
    "bars_per_year = {\n",
    "    \"1m\": 60*24*365, \"3m\": 20*24*365, \"5m\": 12*24*365,\n",
    "    \"15m\": 4*24*365, \"30m\": 2*24*365, \"1h\": 24*365,\n",
    "}.get(TF, 24*365)\n",
    "\n",
    "ret_sum = float(pnl_net.sum())\n",
    "cagr    = float((np.exp(ret_sum))**(bars_per_year / N) - 1.0) if N>0 else 0.0\n",
    "vol_ann = float(np.std(pnl_net, ddof=0) * np.sqrt(bars_per_year)) if N>1 else 0.0\n",
    "sharpe  = float((cagr / vol_ann)) if vol_ann>0 else 0.0\n",
    "trades  = int((trade_units > 0).sum())\n",
    "exposure= float((pos != 0).mean())\n",
    "hitrate = float(((pos*np.sign(lr1))>0).mean()) if exposure>0 else 0.0\n",
    "final_eq= float(np.exp(np.cumsum(pnl_net))[-1]) if N>0 else 1.0\n",
    "\n",
    "print(f\"[{CELL}] Paso 5: métricas finales -> cagr={cagr:.6f}, vol_ann={vol_ann:.6f}, \"\n",
    "      f\"sharpe={sharpe:.4f}, trades={trades}, exposure={exposure:.4f}, \"\n",
    "      f\"hitrate={hitrate:.4f}, final_eq={final_eq:.6f}\")\n",
    "\n",
    "# --- Persistencia: señales + equity ---\n",
    "eq = np.exp(np.cumsum(pnl_net))\n",
    "out_df = pl.DataFrame({\n",
    "    \"ts\": df[\"ts\"],\n",
    "    \"score\": score.tolist(),\n",
    "    \"pos\":   pos.tolist(),\n",
    "    \"lr_fwd1\": lr1.tolist(),\n",
    "    \"pnl_net\": pnl_net.tolist(),\n",
    "    \"equity\": eq.tolist(),\n",
    "})\n",
    "out_df.write_parquet(out_signals, compression=\"zstd\")\n",
    "write_event(LOG_CSV, CELL, \"final_signals_saved\", out_signals.name)\n",
    "print(f\"[{CELL}] Paso 6: señales guardadas -> {out_signals.name}\")\n",
    "\n",
    "# --- Figura equity ---\n",
    "figdir.mkdir(parents=True, exist_ok=True)\n",
    "plt.figure(figsize=(8,4.5))\n",
    "plt.plot(out_df[\"ts\"].to_numpy(), out_df[\"equity\"].to_numpy(), linewidth=1)\n",
    "plt.title(f\"Equity final (τ*={tau_star:.2f}, bps={BPS_TARGET}) — {SYMBOL} {TF}\")\n",
    "plt.xlabel(\"ts\"); plt.ylabel(\"equity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_fig, dpi=160)\n",
    "plt.close()\n",
    "print(f\"[{CELL}] Paso 7: figura guardada -> {out_fig.name}\")\n",
    "\n",
    "# --- Summary JSON ---\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"rows\": int(N),\n",
    "    \"tau_star\": tau_star,\n",
    "    \"bps_target\": BPS_TARGET,\n",
    "    \"metrics\": {\n",
    "        \"ret_log_sum\": ret_sum,\n",
    "        \"cagr\": cagr,\n",
    "        \"vol_ann\": vol_ann,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"trades\": trades,\n",
    "        \"exposure\": exposure,\n",
    "        \"hitrate\": hitrate,\n",
    "        \"final_equity\": final_eq,\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"signals\": out_signals.name,\n",
    "        \"equity_figure\": out_fig.name,\n",
    "    }\n",
    "}\n",
    "Path(out_summary).write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"final_summary_saved\", out_summary.name)\n",
    "print(f\"[{CELL}] Paso 8: summary guardado -> {out_summary.name}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 14 completada (señal final)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e965f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 5  —  E M P A Q U E T A D O   D E   E N T R E G A B L E S  ( Z I P )\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Reunir artefactos clave (parquet/html/png/pkl/json) de deliverables/figures/logs.\n",
    "- Construir README.txt, manifest.json y checksums (md5/sha256).\n",
    "- Generar un ZIP final auto-contenido para entrega.\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- outputs/{SYMBOL}/{TF}/deliverables/*\n",
    "- outputs/{SYMBOL}/{TF}/figures/*\n",
    "- outputs/{SYMBOL}/{TF}/logs/*\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- outputs/{SYMBOL}/{TF}/deliverables/release_{SYMBOL}_{TF}_{YYYYmmddTHHMMSSZ}.zip\n",
    "- outputs/{SYMBOL}/{TF}/deliverables/export_{timestamp}/ (carpeta base armada)\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, shutil, hashlib, zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_15\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 15 (Empaquetado de entregables)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"empaquetado de entregables (ZIP)\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Rutas base ---\n",
    "paths   = ensure_output_tree(SYMBOL, TF)\n",
    "rootdir = paths[\"root\"]\n",
    "deliver = paths[\"deliverables\"]\n",
    "figdir  = paths[\"figures\"]\n",
    "logsdir = paths[\"logs\"]\n",
    "\n",
    "# Timestamp UTC compacto\n",
    "ts_utc   = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "export_dir = deliver / f\"export_{ts_utc}\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subcarpetas dentro del export\n",
    "exp_deliver = export_dir / \"deliverables\"\n",
    "exp_figures = export_dir / \"figures\"\n",
    "exp_logs    = export_dir / \"logs\"\n",
    "for d in (exp_deliver, exp_figures, exp_logs):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[{CELL}] Paso 2: carpeta export -> {export_dir}\")\n",
    "\n",
    "# --- Selección de archivos a incluir (patrones) ---\n",
    "# deliverables\n",
    "deliver_patterns = [\n",
    "    f\"ohlc_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"features_base_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"ma_bank_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"ma_states_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"labels_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"dataset_wide_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"X_train_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"y_train_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"X_test_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"y_test_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"equity_curves_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"tuning_threshold_grid_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"final_signals_{SYMBOL}_{TF}.parquet\",\n",
    "    f\"model_mnlogit_{SYMBOL}_{TF}.pkl\",\n",
    "    f\"stats_report_{SYMBOL}_{TF}.html\",\n",
    "    f\"equity_curves_{SYMBOL}_{TF}.png\",\n",
    "    f\"tuning_threshold_{SYMBOL}_{TF}.png\",\n",
    "    f\"final_equity_{SYMBOL}_{TF}.png\",\n",
    "]\n",
    "\n",
    "# figures (redundantes pero por si se guardaron fuera de deliverables)\n",
    "figure_patterns = [\n",
    "    f\"confusion_{SYMBOL}_{TF}.png\",\n",
    "    f\"reliability_pos1_{SYMBOL}_{TF}.png\",\n",
    "    f\"hist_pos1_{SYMBOL}_{TF}.png\",\n",
    "    f\"equity_curves_{SYMBOL}_{TF}.png\",\n",
    "    f\"tuning_threshold_{SYMBOL}_{TF}.png\",\n",
    "    f\"final_equity_{SYMBOL}_{TF}.png\",\n",
    "]\n",
    "\n",
    "# logs (summaries + split/config)\n",
    "log_patterns = [\n",
    "    f\"summary_features_base_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_ma_bank_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_ma_states_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_labels_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_dataset_wide_{SYMBOL}_{TF}.json\",\n",
    "    f\"split_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_Xy_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_mnlogit_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_backtest_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_tuning_threshold_{SYMBOL}_{TF}.json\",\n",
    "    f\"summary_final_{SYMBOL}_{TF}.json\",\n",
    "    f\"scaler_mnlogit_{SYMBOL}_{TF}.json\",\n",
    "]\n",
    "\n",
    "# config raíz\n",
    "config_name = f\"config_{SYMBOL}_{TF}.json\"\n",
    "config_path = rootdir / config_name\n",
    "\n",
    "# --- Copia según existencia ---\n",
    "copied = []\n",
    "\n",
    "def _copy_if_exists(src: Path, dst_dir: Path):\n",
    "    if src.exists():\n",
    "        dst = dst_dir / src.name\n",
    "        shutil.copy2(src, dst)\n",
    "        copied.append(str(dst))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# deliverables\n",
    "for pat in deliver_patterns:\n",
    "    _copy_if_exists(deliver / pat, exp_deliver)\n",
    "\n",
    "# figures\n",
    "for pat in figure_patterns:\n",
    "    _copy_if_exists(figdir / pat, exp_figures)\n",
    "    _copy_if_exists(deliver / pat, exp_figures)  # fallback si quedaron en deliverables\n",
    "\n",
    "# logs\n",
    "for pat in log_patterns:\n",
    "    _copy_if_exists(logsdir / pat, exp_logs)\n",
    "\n",
    "# config\n",
    "_copy_if_exists(config_path, export_dir)\n",
    "\n",
    "print(f\"[{CELL}] Paso 3: archivos copiados -> {len(copied)}\")\n",
    "write_event(LOG_CSV, CELL, \"export_copied_count\", str(len(copied)))\n",
    "\n",
    "if len(copied) == 0:\n",
    "    print(f\"[{CELL}] ERROR: no se encontraron artefactos para exportar.\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: export vacío\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Checksums (md5/sha256) ---\n",
    "def _digest(path: Path, algo: str = \"md5\") -> str:\n",
    "    h = hashlib.new(algo)\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "checks = []\n",
    "for rel in copied:\n",
    "    p = Path(rel)\n",
    "    md5 = _digest(p, \"md5\")\n",
    "    sha = _digest(p, \"sha256\")\n",
    "    checks.append({\"file\": str(p.relative_to(export_dir)), \"md5\": md5, \"sha256\": sha})\n",
    "\n",
    "checks_path = export_dir / \"checksums.json\"\n",
    "checks_path.write_text(json.dumps(checks, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"checksums_saved\", checks_path.name)\n",
    "print(f\"[{CELL}] Paso 4: checksums guardados -> {checks_path.name}\")\n",
    "\n",
    "# --- Manifest & README ---\n",
    "manifest = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"counts\": {\n",
    "        \"deliverables\": len(list(exp_deliver.glob(\"*\"))),\n",
    "        \"figures\": len(list(exp_figures.glob(\"*\"))),\n",
    "        \"logs\": len(list(exp_logs.glob(\"*\"))),\n",
    "    },\n",
    "    \"files\": [str(Path(c).relative_to(export_dir)) for c in copied],\n",
    "    \"checksums_file\": \"checksums.json\",\n",
    "    \"config\": config_name if (export_dir / config_name).exists() else None,\n",
    "}\n",
    "manifest_path = export_dir / \"manifest.json\"\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"manifest_saved\", manifest_path.name)\n",
    "print(f\"[{CELL}] Paso 5: manifest guardado -> {manifest_path.name}\")\n",
    "\n",
    "readme_text = f\"\"\"\\\n",
    "TWF — Release bundle\n",
    "====================\n",
    "Symbol/TF : {SYMBOL} / {TF}\n",
    "Timestamp : {ts_utc}\n",
    "\n",
    "Estructura\n",
    "----------\n",
    "- deliverables/ : datasets, modelos, predicciones y artefactos principales\n",
    "- figures/      : figuras PNG/HTML del reporte y curvas\n",
    "- logs/         : resúmenes JSON y split\n",
    "- {config_name if (export_dir / config_name).exists() else '(sin config copiada)'}\n",
    "- checksums.json: md5/sha256 de cada archivo\n",
    "- manifest.json : listado y conteo de artefactos\n",
    "\n",
    "Notas\n",
    "-----\n",
    "Todos los archivos están en UTC. Formatos: parquet (zstd), json (utf-8), png/html.\n",
    "\"\"\"\n",
    "readme_path = export_dir / \"README.txt\"\n",
    "readme_path.write_text(readme_text, encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"readme_saved\", readme_path.name)\n",
    "print(f\"[{CELL}] Paso 6: README guardado -> {readme_path.name}\")\n",
    "\n",
    "# --- ZIP final ---\n",
    "zip_name = f\"release_{SYMBOL}_{TF}_{ts_utc}.zip\"\n",
    "zip_path = deliver / zip_name\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in export_dir.rglob(\"*\"):\n",
    "        zf.write(p, p.relative_to(export_dir.parent))\n",
    "write_event(LOG_CSV, CELL, \"zip_saved\", zip_name)\n",
    "print(f\"[{CELL}] Paso 7: ZIP guardado -> {zip_path}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 15 completada (ZIP de entregables)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31936e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 6  —  A U D I T O R Í A   D E   R E P R O D U C I B I L I D A D\n",
    "===============================================================================\n",
    "Objetivo\n",
    "--------\n",
    "- Verificar integridad de la última entrega (export_*/ZIP) con checksums.json.\n",
    "- Validar existencia de artefactos clave y legibilidad (parquet/html/png/json).\n",
    "- Reportar conteos, tamaños y cualquier discrepancia (md5/sha256, archivos faltantes).\n",
    "- Persistir un summary JSON con el resultado de la auditoría.\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- outputs/{SYMBOL}/{TF}/deliverables/export_*/ (último timestamp)\n",
    "  - manifest.json, checksums.json, subcarpetas deliverables/, figures/, logs/\n",
    "- outputs/{SYMBOL}/{TF}/deliverables/release_{SYMBOL}_{TF}_{TS}.zip\n",
    "\n",
    "Salidas\n",
    "-------\n",
    "- outputs/{SYMBOL}/{TF}/logs/summary_audit_{SYMBOL}_{TF}.json\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_16\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 16 (Auditoría de reproducibilidad)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"auditoría de reproducibilidad e integridad\")\n",
    "\n",
    "# --- Parámetros ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "paths  = ensure_output_tree(SYMBOL, TF)\n",
    "deliver = paths[\"deliverables\"]\n",
    "logsdir = paths[\"logs\"]\n",
    "\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Localizar último export_*\n",
    "exports = sorted([p for p in deliver.glob(\"export_*\") if p.is_dir()], key=lambda p: p.name)\n",
    "if not exports:\n",
    "    print(f\"[{CELL}] ERROR: no se encontraron carpetas export_* en {deliver}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: sin carpetas export\")\n",
    "    sys.exit(1)\n",
    "\n",
    "export_dir = exports[-1]\n",
    "print(f\"[{CELL}] Paso 2: export analizado -> {export_dir.name}\")\n",
    "\n",
    "manifest_path = export_dir / \"manifest.json\"\n",
    "checks_path   = export_dir / \"checksums.json\"\n",
    "\n",
    "if not manifest_path.exists() or not checks_path.exists():\n",
    "    print(f\"[{CELL}] ERROR: faltan manifest/checksums en {export_dir}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: faltan manifest/checksums\")\n",
    "    sys.exit(1)\n",
    "\n",
    "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "checks   = json.loads(checks_path.read_text(encoding=\"utf-8\"))\n",
    "print(f\"[{CELL}] Paso 3: manifest y checksums cargados\")\n",
    "\n",
    "# --- Helper de hash\n",
    "def _digest(path: Path, algo: str = \"md5\") -> str:\n",
    "    h = hashlib.new(algo)\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# --- Verificación de checksums\n",
    "mismatches = []\n",
    "missing    = []\n",
    "ok_count   = 0\n",
    "\n",
    "for entry in checks:\n",
    "    rel = entry.get(\"file\")\n",
    "    md5_expected = entry.get(\"md5\")\n",
    "    sha_expected = entry.get(\"sha256\")\n",
    "    fpath = export_dir / rel\n",
    "    if not fpath.exists():\n",
    "        missing.append(rel)\n",
    "        continue\n",
    "    try:\n",
    "        md5_actual = _digest(fpath, \"md5\")\n",
    "        sha_actual = _digest(fpath, \"sha256\")\n",
    "        if md5_actual != md5_expected or sha_actual != sha_expected:\n",
    "            mismatches.append({\"file\": rel, \"md5_expected\": md5_expected, \"md5_actual\": md5_actual,\n",
    "                               \"sha_expected\": sha_expected, \"sha_actual\": sha_actual})\n",
    "        else:\n",
    "            ok_count += 1\n",
    "    except Exception as e:\n",
    "        mismatches.append({\"file\": rel, \"error\": str(e)})\n",
    "\n",
    "print(f\"[{CELL}] Paso 4: checksums -> OK={ok_count}, mismatches={len(mismatches)}, missing={len(missing)}\")\n",
    "write_event(LOG_CSV, CELL, \"checksums_verdict\", f\"ok={ok_count}, mismatches={len(mismatches)}, missing={len(missing)}\")\n",
    "\n",
    "# --- Artefactos clave (pruebas de legibilidad/shape)\n",
    "key_artifacts = {\n",
    "    \"dataset_wide\": export_dir / \"deliverables\" / f\"dataset_wide_{SYMBOL}_{TF}.parquet\",\n",
    "    \"preds\":        export_dir / \"deliverables\" / f\"preds_mnlogit_{SYMBOL}_{TF}.parquet\",\n",
    "    \"final_signals\":export_dir / \"deliverables\" / f\"final_signals_{SYMBOL}_{TF}.parquet\",\n",
    "    \"equity_curves\":export_dir / \"deliverables\" / f\"equity_curves_{SYMBOL}_{TF}.parquet\",\n",
    "    \"report_html\":  export_dir / \"deliverables\" / f\"stats_report_{SYMBOL}_{TF}.html\",\n",
    "    \"model_pkl\":    export_dir / \"deliverables\" / f\"model_mnlogit_{SYMBOL}_{TF}.pkl\",\n",
    "}\n",
    "read_tests = {}\n",
    "for k, p in key_artifacts.items():\n",
    "    if not p.exists():\n",
    "        read_tests[k] = {\"exists\": False, \"info\": None}\n",
    "        continue\n",
    "    if p.suffix == \".parquet\":\n",
    "        try:\n",
    "            df = pl.read_parquet(p)\n",
    "            read_tests[k] = {\"exists\": True, \"info\": {\"rows\": int(df.height), \"cols\": len(df.columns)}}\n",
    "        except Exception as e:\n",
    "            read_tests[k] = {\"exists\": True, \"error\": str(e)}\n",
    "    else:\n",
    "        # Para HTML/PKL solo registrar tamaño\n",
    "        try:\n",
    "            size = p.stat().st_size\n",
    "            read_tests[k] = {\"exists\": True, \"info\": {\"bytes\": int(size)}}\n",
    "        except Exception as e:\n",
    "            read_tests[k] = {\"exists\": True, \"error\": str(e)}\n",
    "\n",
    "print(f\"[{CELL}] Paso 5: validación artefactos clave -> {read_tests}\")\n",
    "\n",
    "# --- Verificar ZIP asociado (por timestamp del export)\n",
    "ts_token = export_dir.name.replace(\"export_\", \"\")\n",
    "zip_path = deliver / f\"release_{SYMBOL}_{TF}_{ts_token}.zip\"\n",
    "zip_ok = zip_path.exists()\n",
    "print(f\"[{CELL}] Paso 6: ZIP asociado -> {'OK' if zip_ok else 'NO ENCONTRADO'} :: {zip_path.name}\")\n",
    "\n",
    "# --- Resultado global\n",
    "status = \"OK\" if (len(mismatches)==0 and len(missing)==0 and zip_ok) else \"WARN\" if (len(mismatches)==0 and zip_ok) else \"ERROR\"\n",
    "\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"export_dir\": export_dir.name,\n",
    "    \"zip\": {\"name\": zip_path.name, \"exists\": zip_ok},\n",
    "    \"checksums\": {\n",
    "        \"ok_files\": ok_count,\n",
    "        \"mismatches\": mismatches,\n",
    "        \"missing\": missing\n",
    "    },\n",
    "    \"key_artifacts\": read_tests,\n",
    "    \"manifest_counts\": manifest.get(\"counts\", {}),\n",
    "    \"status\": status\n",
    "}\n",
    "\n",
    "summary_path = logsdir / f\"summary_audit_{SYMBOL}_{TF}.json\"\n",
    "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"audit_summary\", summary_path.name)\n",
    "print(f\"[{CELL}] Paso 7: summary audit guardado -> {summary_path.name}\")\n",
    "print(f\"[{CELL}] Paso 8: ESTADO GLOBAL = {status}\")\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, f\"Celda 16 completada :: status={status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "C E L D A   1 7  —  P U B L I C A C I Ó N   D E   R E L E A S E   A C T U A L\n",
    "======= ========================================================================\n",
    "- Detecta el export_* más reciente en deliverables.\n",
    "- Verifica su ZIP asociado release_*.zip.\n",
    "- Escribe punteros \"LATEST_RELEASE.json\" y copia ZIP a \"release_latest.zip\".\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, shutil, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from twf.utils.logging import start_cell_log, end_cell_log, write_event\n",
    "from twf.utils.config import ensure_output_tree, BASE\n",
    "\n",
    "CELL = \"cell_17\"\n",
    "\n",
    "print(f\"[{CELL}] Paso 0: inicio Celda 17 (Publicación de release actual)\")\n",
    "LOG_CSV = BASE / \"logs\" / \"run_log_records.csv\"\n",
    "t0 = start_cell_log(LOG_CSV, CELL, \"publicar release actual (punteros)\")\n",
    "\n",
    "# --- Parámetros de entorno ---\n",
    "SYMBOL = os.environ.get(\"TWF_SYMBOL\", \"BTCUSDT\")\n",
    "TF     = os.environ.get(\"TWF_TF\", \"5m\")\n",
    "print(f\"[{CELL}] Paso 1: parámetros :: symbol={SYMBOL}, tf={TF}\")\n",
    "write_event(LOG_CSV, CELL, \"params\", f\"symbol={SYMBOL}, tf={TF}\")\n",
    "\n",
    "# --- Rutas ---\n",
    "paths    = ensure_output_tree(SYMBOL, TF)\n",
    "deliver  = paths[\"deliverables\"]\n",
    "logs_dir = paths[\"logs\"]\n",
    "\n",
    "# --- Buscar export_* más reciente ---\n",
    "exports = sorted([p for p in deliver.glob(\"export_*\") if p.is_dir()])\n",
    "if not exports:\n",
    "    print(f\"[{CELL}] ERROR: no se encontraron carpetas export_* en {deliver}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: sin export_*\")\n",
    "    sys.exit(1)\n",
    "\n",
    "latest_export = exports[-1]\n",
    "export_tag = latest_export.name  # ej: export_20251026T172508Z\n",
    "print(f\"[{CELL}] Paso 2: export detectado -> {export_tag}\")\n",
    "\n",
    "# --- ZIP asociado ---\n",
    "zip_name_prefix = f\"release_{SYMBOL}_{TF}_{export_tag.split('export_')[-1]}\"\n",
    "zip_candidates = list(deliver.glob(f\"{zip_name_prefix}.zip\"))\n",
    "if not zip_candidates:\n",
    "    # tolerar formatos previos sin symbol/tf en nombre\n",
    "    zip_candidates = list(deliver.glob(f\"{export_tag.replace('export_', 'release_')}.zip\"))\n",
    "\n",
    "if not zip_candidates:\n",
    "    print(f\"[{CELL}] ERROR: no se encontró ZIP asociado a {export_tag}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: sin ZIP asociado\")\n",
    "    sys.exit(1)\n",
    "\n",
    "zip_path = max(zip_candidates, key=lambda p: p.stat().st_mtime)\n",
    "print(f\"[{CELL}] Paso 3: ZIP asociado -> {zip_path.name}\")\n",
    "\n",
    "# --- Checksums/manifest presentes en export ---\n",
    "checksums_path = latest_export / \"checksums.json\"\n",
    "manifest_path  = latest_export / \"manifest.json\"\n",
    "if not (checksums_path.exists() and manifest_path.exists()):\n",
    "    print(f\"[{CELL}] ERROR: faltan checksums/manifest en {export_tag}\")\n",
    "    end_cell_log(LOG_CSV, t0, CELL, \"ERROR: faltan checksums/manifest\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- MD5 del ZIP (para puntero) ---\n",
    "def md5_file(p: Path, buf: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with p.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "zip_md5  = md5_file(zip_path)\n",
    "zip_size = zip_path.stat().st_size\n",
    "\n",
    "# --- Escribir punteros LATEST_RELEASE.json ---\n",
    "latest_json = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"tf\": TF,\n",
    "    \"export_dir\": latest_export.name,\n",
    "    \"zip_file\": zip_path.name,\n",
    "    \"zip_md5\": zip_md5,\n",
    "    \"zip_bytes\": int(zip_size),\n",
    "}\n",
    "latest_json_path = deliver / \"LATEST_RELEASE.json\"\n",
    "latest_json_path.write_text(json.dumps(latest_json, indent=2), encoding=\"utf-8\")\n",
    "print(f\"[{CELL}] Paso 4: LATEST_RELEASE.json -> {latest_json_path.name}\")\n",
    "write_event(LOG_CSV, CELL, \"latest_release_json\", latest_json_path.name)\n",
    "\n",
    "# --- Copia ZIP a alias estable release_latest.zip ---\n",
    "alias_zip = deliver / \"release_latest.zip\"\n",
    "try:\n",
    "    if alias_zip.exists():\n",
    "        alias_zip.unlink()\n",
    "    shutil.copy2(zip_path, alias_zip)\n",
    "    print(f\"[{CELL}] Paso 5: alias ZIP -> {alias_zip.name}\")\n",
    "    write_event(LOG_CSV, CELL, \"latest_zip_alias\", alias_zip.name)\n",
    "except Exception as e:\n",
    "    print(f\"[{CELL}] WARN: no se pudo crear alias ZIP ({e})\")\n",
    "\n",
    "# --- (Opcional) puntero a artefactos clave dentro de export ---\n",
    "pointer = {\n",
    "    \"dataset_wide\": \"dataset_wide_*.parquet\",\n",
    "    \"preds\":        \"preds_mnlogit_*.parquet\",\n",
    "    \"final_signals\":\"final_signals_*.parquet\",\n",
    "    \"equity\":       \"equity_curves_*.parquet\",\n",
    "    \"report_html\":  \"stats_report_*.html\",\n",
    "}\n",
    "pointer_path = latest_export / \"POINTERS.json\"\n",
    "pointer_path.write_text(json.dumps(pointer, indent=2), encoding=\"utf-8\")\n",
    "write_event(LOG_CSV, CELL, \"export_pointers\", pointer_path.name)\n",
    "\n",
    "end_cell_log(LOG_CSV, t0, CELL, \"Celda 17 completada (publicación release actual)\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TWF (.venv)",
   "language": "python",
   "name": "twf-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
