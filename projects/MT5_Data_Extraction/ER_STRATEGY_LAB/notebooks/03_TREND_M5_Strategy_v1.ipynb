{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2506be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 00 :: Run Context + Freeze (NB3 TREND, M5)\n",
      "[Celda 00] ER_STRATEGY_LAB_ROOT = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\n",
      "[Celda 00] RUN_DIR             = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\run_meta.json (OK)\n",
      ">>> Celda 00 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 00 â€” Run Context + Freeze (NB3 TREND, M5) =====================\n",
    "# OBJETIVO:\n",
    "#   1) Definir el contexto reproducible del run (RUN_ID, paths, versiones).\n",
    "#   2) Crear carpeta de run y snapshots (configs/docs).\n",
    "#   3) Inicializar GLOBAL_STATE (contrato base de todo el notebook).\n",
    "#\n",
    "# NOTA OPERATIVA:\n",
    "#   - Esta celda NO debe depender de paths hardcodeados.\n",
    "#   - Si no detecta tu estructura, define ER_STRATEGY_LAB_ROOT o MT5_PROJECT_ROOT como variable de entorno.\n",
    "#\n",
    "# GATES (debe cumplir para continuar):\n",
    "#   - Se resuelve ER_STRATEGY_LAB_ROOT existente\n",
    "#   - Se crea RUN_DIR\n",
    "#   - Se escribe run_meta.json\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 00 :: Run Context + Freeze (NB3 TREND, M5)\")\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _resolve_er_strategy_lab_root() -> Path:\n",
    "    \"\"\"\n",
    "    Resuelve el root de ER_STRATEGY_LAB de forma robusta:\n",
    "      1) Env var ER_STRATEGY_LAB_ROOT / ER_STRATEGY_LAB\n",
    "      2) Env var MT5_PROJECT_ROOT / MT5_DE_PROJECT_ROOT (se asume <root>/ER_STRATEGY_LAB)\n",
    "      3) BÃºsqueda hacia arriba desde cwd\n",
    "      4) Fallback tÃ­pico Windows\n",
    "    \"\"\"\n",
    "    # 1) Directo a ER_STRATEGY_LAB\n",
    "    env_direct = os.getenv(\"ER_STRATEGY_LAB_ROOT\") or os.getenv(\"ER_STRATEGY_LAB\")\n",
    "    if env_direct:\n",
    "        p = Path(env_direct).expanduser().resolve()\n",
    "        return p\n",
    "\n",
    "    # 2) Project root -> <root>/ER_STRATEGY_LAB\n",
    "    env_proj = os.getenv(\"MT5_PROJECT_ROOT\") or os.getenv(\"MT5_DE_PROJECT_ROOT\")\n",
    "    if env_proj:\n",
    "        pr = Path(env_proj).expanduser().resolve()\n",
    "        cand = pr / \"ER_STRATEGY_LAB\"\n",
    "        return cand if cand.exists() else pr\n",
    "\n",
    "    # 3) Buscar en cwd y padres\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if p.name.upper() == \"ER_STRATEGY_LAB\" and p.exists():\n",
    "            return p\n",
    "        cand = p / \"ER_STRATEGY_LAB\"\n",
    "        if cand.exists():\n",
    "            return cand.resolve()\n",
    "\n",
    "    # 4) Fallback tÃ­pico (si existe)\n",
    "    fallback = Path(r\"C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\")\n",
    "    if fallback.exists():\n",
    "        return fallback.resolve()\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 00] No se pudo resolver ER_STRATEGY_LAB_ROOT.\\n\"\n",
    "        \"SoluciÃ³n recomendada: define variable de entorno ER_STRATEGY_LAB_ROOT apuntando al folder ER_STRATEGY_LAB.\\n\"\n",
    "        \"Alternativa: define MT5_PROJECT_ROOT apuntando al root del proyecto (que contiene ER_STRATEGY_LAB/).\\n\"\n",
    "        f\"cwd actual: {Path.cwd()}\"\n",
    "    )\n",
    "\n",
    "def _safe_mkdir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _polars_version() -> str:\n",
    "    try:\n",
    "        return pl.__version__\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "# ========================= Resolver raÃ­z del laboratorio =========================\n",
    "ER_STRATEGY_LAB_ROOT = _resolve_er_strategy_lab_root().resolve()\n",
    "\n",
    "# Estructura esperada (si falta algo, lo creamos; lo importante es que el root sea correcto)\n",
    "PATH_NOTEBOOKS = ER_STRATEGY_LAB_ROOT / \"notebooks\"\n",
    "PATH_CONFIG    = ER_STRATEGY_LAB_ROOT / \"config\"\n",
    "PATH_INPUTS    = ER_STRATEGY_LAB_ROOT / \"inputs\"\n",
    "PATH_ARTIFACTS = ER_STRATEGY_LAB_ROOT / \"artifacts\"\n",
    "PATH_LOGS      = ER_STRATEGY_LAB_ROOT / \"research_logs\" / \"runs\"\n",
    "\n",
    "for p in (PATH_NOTEBOOKS, PATH_CONFIG, PATH_INPUTS, PATH_ARTIFACTS, PATH_LOGS):\n",
    "    _safe_mkdir(p)\n",
    "\n",
    "# RUN_ID UTC (formato consistente)\n",
    "NOW_UTC = datetime.now(timezone.utc)\n",
    "RUN_ID = NOW_UTC.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "RUN_DIR      = PATH_LOGS / RUN_ID\n",
    "RUN_SNAP_DIR = RUN_DIR / \"snapshots\"\n",
    "RUN_OUT_DIR  = RUN_DIR / \"outputs\"\n",
    "RUN_TMP_DIR  = RUN_DIR / \"tmp\"\n",
    "\n",
    "for p in (RUN_DIR, RUN_SNAP_DIR, RUN_OUT_DIR, RUN_TMP_DIR):\n",
    "    _safe_mkdir(p)\n",
    "\n",
    "# ========================= GLOBAL_STATE (contrato) =========================\n",
    "GLOBAL_STATE = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\n",
    "        \"lab_root\": str(ER_STRATEGY_LAB_ROOT),\n",
    "        \"notebooks\": str(PATH_NOTEBOOKS),\n",
    "        \"config\": str(PATH_CONFIG),\n",
    "        \"inputs\": str(PATH_INPUTS),\n",
    "        \"artifacts\": str(PATH_ARTIFACTS),\n",
    "        \"logs_root\": str(PATH_LOGS),\n",
    "        \"run_dir\": str(RUN_DIR),\n",
    "        \"run_snapshots\": str(RUN_SNAP_DIR),\n",
    "        \"run_outputs\": str(RUN_OUT_DIR),\n",
    "        \"run_tmp\": str(RUN_TMP_DIR),\n",
    "    },\n",
    "    \"nb2\": {},      # se completa en Celda 01\n",
    "    \"universe\": {}, # se completa en Celda 01\n",
    "    \"data\": {},     # se completa en Celda 02+\n",
    "    \"config\": {},   # reservado\n",
    "    \"metrics\": {},  # reservado\n",
    "}\n",
    "\n",
    "# ========================= Snapshot meta del run =========================\n",
    "meta = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"ts_utc\": NOW_UTC.isoformat(),\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": {\n",
    "        \"system\": platform.system(),\n",
    "        \"release\": platform.release(),\n",
    "        \"version\": platform.version(),\n",
    "        \"machine\": platform.machine(),\n",
    "    },\n",
    "    \"libs\": {\n",
    "        \"polars\": _polars_version(),\n",
    "    },\n",
    "    \"paths\": GLOBAL_STATE[\"paths\"],\n",
    "}\n",
    "\n",
    "meta_path = RUN_DIR / \"run_meta.json\"\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# (Opcional) snapshot de config si existe algo\n",
    "try:\n",
    "    cfg_snap = RUN_SNAP_DIR / \"config_snapshot\"\n",
    "    if PATH_CONFIG.exists() and any(PATH_CONFIG.iterdir()):\n",
    "        if cfg_snap.exists():\n",
    "            shutil.rmtree(cfg_snap)\n",
    "        shutil.copytree(PATH_CONFIG, cfg_snap)\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 00] WARN: no se pudo snapshotear config/: {e}\")\n",
    "\n",
    "print(f\"[Celda 00] ER_STRATEGY_LAB_ROOT = {ER_STRATEGY_LAB_ROOT}\")\n",
    "print(f\"[Celda 00] RUN_DIR             = {RUN_DIR}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {meta_path} (OK)\")\n",
    "print(\">>> Celda 00 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae86d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 00C v1.0 :: State Recovery (GLOBAL_STATE) [WFO-safe]\n",
      "[Celda 00C] OK: GLOBAL_STATE['backtest_engine'] reconstruido.\n",
      "  trades_path = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\trades_engine_v10_overlay_v16.parquet\n",
      "  summary_path = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\summary_engine_v10_overlay_v16.parquet\n",
      ">>> Celda 00C v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 00C v1.0 â€” State Recovery (GLOBAL_STATE) [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "print(\">>> Celda 00C v1.0 :: State Recovery (GLOBAL_STATE) [WFO-safe]\")\n",
    "\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n",
    "\n",
    "def _pick_latest(files: list[Path]) -> Path | None:\n",
    "    files2 = [p for p in files if p.exists()]\n",
    "    if not files2:\n",
    "        return None\n",
    "    return sorted(files2, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "def _find_latest_by_glob(root: Path, pattern: str) -> Path | None:\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    hits = list(root.rglob(pattern))\n",
    "    return _pick_latest(hits)\n",
    "\n",
    "def ensure_global_state() -> None:\n",
    "    global GLOBAL_STATE\n",
    "\n",
    "    if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "        GLOBAL_STATE = {}\n",
    "\n",
    "    # Paths mÃ­nimos\n",
    "    if \"paths\" not in GLOBAL_STATE or not isinstance(GLOBAL_STATE[\"paths\"], dict):\n",
    "        # Fallback: intenta inferir un artifacts local relativo al cwd\n",
    "        cwd = Path.cwd().resolve()\n",
    "        GLOBAL_STATE[\"paths\"] = {\n",
    "            \"artifacts\": str((cwd / \"artifacts\").resolve()),\n",
    "            \"run_snapshots\": str((cwd / \"snapshots\").resolve()),\n",
    "        }\n",
    "\n",
    "    paths = GLOBAL_STATE[\"paths\"]\n",
    "    artifacts = Path(paths.get(\"artifacts\", \"\")).resolve()\n",
    "    snap_dir = Path(paths.get(\"run_snapshots\", \"\")).resolve()\n",
    "\n",
    "    # Si ya existe, no pisar\n",
    "    if \"backtest_engine\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"backtest_engine\"], dict):\n",
    "        bt = GLOBAL_STATE[\"backtest_engine\"]\n",
    "        tp = bt.get(\"trades_path\"); sp = bt.get(\"summary_path\")\n",
    "        if tp and Path(tp).exists() and sp and Path(sp).exists():\n",
    "            print(\"[Celda 00C] backtest_engine ya estÃ¡ OK en GLOBAL_STATE.\")\n",
    "            return\n",
    "\n",
    "    # Intentar reconstruir desde snapshots conocidos (prioriza overlay)\n",
    "    cand_snapshots = []\n",
    "    if snap_dir.exists():\n",
    "        cand_snapshots += [\n",
    "            snap_dir / \"overlay_engine_v16_snapshot.json\",\n",
    "            snap_dir / \"backtest_engine_v10_snapshot.json\",\n",
    "        ]\n",
    "    # TambiÃ©n buscar dentro de artifacts (por si guardas snapshots ahÃ­)\n",
    "    if artifacts.exists():\n",
    "        cand_snapshots += [\n",
    "            artifacts / \"snapshots\" / \"overlay_engine_v16_snapshot.json\",\n",
    "            artifacts / \"snapshots\" / \"backtest_engine_v10_snapshot.json\",\n",
    "        ]\n",
    "\n",
    "    snap_file = _pick_latest(cand_snapshots)\n",
    "    bt_dict = {}\n",
    "\n",
    "    if snap_file and snap_file.exists():\n",
    "        try:\n",
    "            j = json.loads(snap_file.read_text(encoding=\"utf-8\"))\n",
    "            # No asumimos schema exacto; buscamos paths tÃ­picos\n",
    "            # 1) outputs directos\n",
    "            outs = j.get(\"outputs\", {}) if isinstance(j.get(\"outputs\", {}), dict) else {}\n",
    "            tp = outs.get(\"trades_path\") or outs.get(\"trades\") or outs.get(\"trades_parquet\")\n",
    "            sp = outs.get(\"summary_path\") or outs.get(\"summary\") or outs.get(\"summary_parquet\")\n",
    "\n",
    "            # 2) inputs puede traerlo en algunos snapshots\n",
    "            ins = j.get(\"inputs\", {}) if isinstance(j.get(\"inputs\", {}), dict) else {}\n",
    "            tp = tp or ins.get(\"trades_path\")\n",
    "            sp = sp or ins.get(\"summary_path\")\n",
    "\n",
    "            # 3) si snapshot no trae, inferir por glob\n",
    "            if not tp or not Path(str(tp)).exists():\n",
    "                tp2 = _find_latest_by_glob(artifacts, \"trades_engine_v10*_overlay_*.parquet\") \\\n",
    "                      or _find_latest_by_glob(artifacts, \"trades_engine_v10*.parquet\")\n",
    "                tp = str(tp2) if tp2 else None\n",
    "\n",
    "            if not sp or not Path(str(sp)).exists():\n",
    "                sp2 = _find_latest_by_glob(artifacts, \"summary_engine_v10*_overlay_*.parquet\") \\\n",
    "                      or _find_latest_by_glob(artifacts, \"summary_engine_v10*.parquet\")\n",
    "                sp = str(sp2) if sp2 else None\n",
    "\n",
    "            bt_dict = {\n",
    "                \"trades_path\": str(tp) if tp else None,\n",
    "                \"summary_path\": str(sp) if sp else None,\n",
    "                \"params\": j.get(\"params\", {}) if isinstance(j.get(\"params\", {}), dict) else {},\n",
    "                \"recovered_from\": str(snap_file),\n",
    "                \"recovered_utc\": _utc_now_iso(),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[Celda 00C] WARN: no pude parsear snapshot {snap_file}: {e}\")\n",
    "\n",
    "    # Si no hubo snapshot o fallÃ³, inferir desde artifacts directo\n",
    "    if not bt_dict.get(\"trades_path\") or not Path(bt_dict[\"trades_path\"]).exists():\n",
    "        tp2 = _find_latest_by_glob(artifacts, \"trades_engine_v10*_overlay_*.parquet\") \\\n",
    "              or _find_latest_by_glob(artifacts, \"trades_engine_v10*.parquet\")\n",
    "        bt_dict[\"trades_path\"] = str(tp2) if tp2 else None\n",
    "\n",
    "    if not bt_dict.get(\"summary_path\") or not Path(bt_dict[\"summary_path\"]).exists():\n",
    "        sp2 = _find_latest_by_glob(artifacts, \"summary_engine_v10*_overlay_*.parquet\") \\\n",
    "              or _find_latest_by_glob(artifacts, \"summary_engine_v10*.parquet\")\n",
    "        bt_dict[\"summary_path\"] = str(sp2) if sp2 else None\n",
    "\n",
    "    # Gate final\n",
    "    if not bt_dict.get(\"trades_path\") or not Path(bt_dict[\"trades_path\"]).exists():\n",
    "        raise RuntimeError(f\"[Celda 00C] ERROR: no pude reconstruir trades_path. artifacts={artifacts}\")\n",
    "\n",
    "    if not bt_dict.get(\"summary_path\") or not Path(bt_dict[\"summary_path\"]).exists():\n",
    "        raise RuntimeError(f\"[Celda 00C] ERROR: no pude reconstruir summary_path. artifacts={artifacts}\")\n",
    "\n",
    "    GLOBAL_STATE[\"backtest_engine\"] = bt_dict\n",
    "    print(\"[Celda 00C] OK: GLOBAL_STATE['backtest_engine'] reconstruido.\")\n",
    "    print(f\"  trades_path = {bt_dict['trades_path']}\")\n",
    "    print(f\"  summary_path = {bt_dict['summary_path']}\")\n",
    "    if bt_dict.get(\"recovered_from\"):\n",
    "        print(f\"  recovered_from = {bt_dict['recovered_from']}\")\n",
    "\n",
    "ensure_global_state()\n",
    "print(\">>> Celda 00C v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd6aa90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 00B v1.0 :: Recover BACKTEST_ENGINE pointer (disk) [WFO-safe]\n",
      "[Celda 00B] OK :: backtest_engine.mode = OVERLAY_V16\n",
      "[Celda 00B] OK :: trades_path = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\trades_engine_v10_overlay_v16.parquet\n",
      "[Celda 00B] OK :: summary_path = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\summary_engine_v10_overlay_v16.parquet\n",
      "[Celda 00B] SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\backtest_engine_pointer_snapshot.json\n",
      ">>> Celda 00B v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 00B v1.0 â€” Recover BACKTEST_ENGINE pointer (disk) [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "print(\">>> Celda 00B v1.0 :: Recover BACKTEST_ENGINE pointer (disk) [WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 00B] ERROR: GLOBAL_STATE no existe o no es dict. Ejecuta tu Celda 00 (init).\")\n",
    "\n",
    "if \"paths\" not in GLOBAL_STATE or not isinstance(GLOBAL_STATE[\"paths\"], dict):\n",
    "    raise RuntimeError(\"[Celda 00B] ERROR: GLOBAL_STATE['paths'] no existe o no es dict. Ejecuta tu Celda 00 (init).\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "if \"artifacts\" not in paths:\n",
    "    raise RuntimeError(\"[Celda 00B] ERROR: paths['artifacts'] no existe. Revisa tu Celda 00 (init paths).\")\n",
    "\n",
    "ART_DIR = Path(paths[\"artifacts\"]).resolve()\n",
    "ENGINE_ROOT = ART_DIR / \"backtests\" / \"backtest_engine_v10\"\n",
    "OVERLAY_DIR = ENGINE_ROOT / \"overlay_engine_v16\"\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (ART_DIR / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_PTR = SNAP_DIR / \"backtest_engine_pointer_snapshot.json\"\n",
    "\n",
    "def _pick_existing_pair(cands: list[tuple[str, Path, Path]]) -> tuple[str, Path, Path]:\n",
    "    for tag, t, s in cands:\n",
    "        if t.exists() and s.exists():\n",
    "            return tag, t, s\n",
    "    # diagnÃ³stico\n",
    "    dbg = [{\"tag\": tag, \"trades\": str(t), \"trades_exists\": t.exists(), \"summary\": str(s), \"summary_exists\": s.exists()}\n",
    "           for tag, t, s in cands]\n",
    "    raise RuntimeError(f\"[Celda 00B] ERROR: no encuentro pares trades/summary vÃ¡lidos. debug={dbg}\")\n",
    "\n",
    "def _load_params_from_snapshots() -> dict:\n",
    "    # buscamos snapshots tÃ­picos (preferir run_snapshots)\n",
    "    cand_files = []\n",
    "\n",
    "    if \"run_snapshots\" in paths and paths[\"run_snapshots\"]:\n",
    "        sd = Path(paths[\"run_snapshots\"]).resolve()\n",
    "        cand_files += [\n",
    "            sd / \"overlay_engine_v16_snapshot.json\",\n",
    "            sd / \"backtest_engine_v10_snapshot.json\",\n",
    "        ]\n",
    "\n",
    "    # fallback: artifacts/snapshots\n",
    "    cand_files += [\n",
    "        (ART_DIR / \"snapshots\" / \"overlay_engine_v16_snapshot.json\"),\n",
    "        (ART_DIR / \"snapshots\" / \"backtest_engine_v10_snapshot.json\"),\n",
    "    ]\n",
    "\n",
    "    for fp in cand_files:\n",
    "        if fp.exists():\n",
    "            try:\n",
    "                js = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "                # soportar estructuras comunes\n",
    "                if isinstance(js, dict):\n",
    "                    if \"params\" in js and isinstance(js[\"params\"], dict):\n",
    "                        return js[\"params\"]\n",
    "                    if \"backtest_engine\" in js and isinstance(js[\"backtest_engine\"], dict):\n",
    "                        p = js[\"backtest_engine\"].get(\"params\")\n",
    "                        if isinstance(p, dict):\n",
    "                            return p\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {}\n",
    "\n",
    "# ========================= SelecciÃ³n preferente: OVERLAY si existe =========================\n",
    "candidates = []\n",
    "candidates.append((\"OVERLAY_V16\", OVERLAY_DIR / \"trades_engine_v10_overlay_v16.parquet\",\n",
    "                   OVERLAY_DIR / \"summary_engine_v10_overlay_v16.parquet\"))\n",
    "candidates.append((\"ENGINE_V10\", ENGINE_ROOT / \"trades_engine_v10.parquet\",\n",
    "                   ENGINE_ROOT / \"summary_engine_v10.parquet\"))\n",
    "\n",
    "mode, trades_fp, summary_fp = _pick_existing_pair(candidates)\n",
    "params = _load_params_from_snapshots()\n",
    "\n",
    "bt = {\n",
    "    \"mode\": mode,\n",
    "    \"trades_path\": str(trades_fp),\n",
    "    \"summary_path\": str(summary_fp),\n",
    "    \"params\": params,\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "}\n",
    "\n",
    "GLOBAL_STATE[\"backtest_engine\"] = bt\n",
    "\n",
    "SNAP_PTR.write_text(json.dumps({\n",
    "    \"created_utc\": bt[\"created_utc\"],\n",
    "    \"mode\": mode,\n",
    "    \"trades_path\": bt[\"trades_path\"],\n",
    "    \"summary_path\": bt[\"summary_path\"],\n",
    "    \"params_keys\": sorted(list((params or {}).keys())),\n",
    "}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[Celda 00B] OK :: backtest_engine.mode = {mode}\")\n",
    "print(f\"[Celda 00B] OK :: trades_path = {bt['trades_path']}\")\n",
    "print(f\"[Celda 00B] OK :: summary_path = {bt['summary_path']}\")\n",
    "print(f\"[Celda 00B] SNAPSHOT â†’ {SNAP_PTR}\")\n",
    "print(\">>> Celda 00B v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af04ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 01 v1.3 :: Universe Resolver (NOTEBOOK 2 basket) [HARD-SOURCE-OF-TRUTH + NO MISMATCH]\n",
      "[Celda 01] basket_source = FALLBACK_HARDCODED_FROM_SHARED_REPORT\n",
      "[Celda 01] report_path   = NONE (fallback)\n",
      "[Celda 01] run_dir       = NONE (fallback)\n",
      "[Celda 01] BASKET(TREND/CORE) size=4 => ['BNBUSD', 'XAUAUD', 'BTCUSD', 'LVMH']\n",
      "[Celda 01] SELECTED size=4 => ['BNBUSD', 'XAUAUD', 'BTCUSD', 'LVMH']\n",
      "[Celda 01] data_quality.final_symbols FORZADO => ['BNBUSD', 'XAUAUD', 'BTCUSD', 'LVMH']\n",
      ">>> Celda 01 v1.3 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================\n",
    "# Celda 01 v1.3 :: Universe Resolver (NOTEBOOK 2 basket) [HARD-SOURCE-OF-TRUTH + NO MISMATCH]\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# GARANTÃA:\n",
    "# - El universo de ESTE notebook (TREND M5) sale SOLO de la cesta del Notebook 2.\n",
    "# - Si NB2 no estÃ¡ accesible por ruta, usa fallback explÃ­cito (auditado) extraÃ­do del HTML report.\n",
    "# - BLOQUEA silencios: si aparece un sÃ­mbolo fuera del basket => error.\n",
    "#\n",
    "# EFECTO (soluciÃ³n de fondo):\n",
    "# - Escribe SIEMPRE:\n",
    "#   GLOBAL_STATE[\"universe\"][\"selected_symbols_TREND\"]\n",
    "#   GLOBAL_STATE[\"universe\"][\"basket_TREND\"]\n",
    "# - Y PARA EVITAR QUE CELDAS MAL ESCRITAS METAN SÃMBOLOS:\n",
    "#   Fuerza tambiÃ©n GLOBAL_STATE[\"data_quality\"][\"final_symbols\"] = selected\n",
    "#   (quedando trazabilidad del override)\n",
    "# ==========================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 01 v1.3 :: Universe Resolver (NOTEBOOK 2 basket) [HARD-SOURCE-OF-TRUTH + NO MISMATCH]\")\n",
    "\n",
    "# =========================\n",
    "# Config mÃ­nimo (editable)\n",
    "# =========================\n",
    "TARGET_FAMILY = \"TREND\"   # ESTE notebook es TREND\n",
    "TARGET_PRESET = \"CORE\"    # segÃºn tu report: CORE\n",
    "SELECT_TOP_N  = 0         # 0 => usar TODA la cesta; >0 => top N por score\n",
    "SELECT_SYMBOLS_MANUAL: list[str] = []  # opcional (se valida contra basket)\n",
    "\n",
    "# Overrides directos (si quieres control absoluto)\n",
    "NB2_REPORT_PATH_OVERRIDE = \"\"  # ej: r\"C:\\Quant\\MT5_Data_Extraction\\ER_FILTER_5M_V1\\research_logs\\runs\\YYYYMMDD_HHMMSS\\regimen_selector_report.html\"\n",
    "NB2_RUN_DIR_OVERRIDE     = \"\"  # ej: r\"C:\\Quant\\MT5_Data_Extraction\\ER_FILTER_5M_V1\\research_logs\\runs\\YYYYMMDD_HHMMSS\"\n",
    "\n",
    "# Fallback auditado (extraÃ­do del HTML que compartiste)\n",
    "FALLBACK_BASKETS = {\n",
    "    (\"TREND\",\"CORE\"): [\"BNBUSD\", \"XAUAUD\", \"BTCUSD\", \"LVMH\"],\n",
    "    (\"RANGE\",\"CORE\"): [\"ETHUSD\", \"XAUUSD\"],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Validaciones GLOBAL_STATE\n",
    "# =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 01] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "if \"artifacts\" not in paths:\n",
    "    raise RuntimeError(\"[Celda 01] ERROR: falta GLOBAL_STATE['paths']['artifacts'].\")\n",
    "\n",
    "lab_root = Path(paths.get(\"lab_root\") or Path(paths[\"artifacts\"]).resolve().parent).resolve()\n",
    "base_guess = lab_root.parent  # tÃ­pico: C:\\Quant\\MT5_Data_Extraction\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _is_run_dirname(name: str) -> bool:\n",
    "    return bool(re.match(r\"^\\d{8}_\\d{6}$\", name.strip()))\n",
    "\n",
    "def _latest_run_dir(runs_root: Path) -> Path | None:\n",
    "    if not runs_root.exists():\n",
    "        return None\n",
    "    runs = [p for p in runs_root.iterdir() if p.is_dir() and _is_run_dirname(p.name)]\n",
    "    if not runs:\n",
    "        return None\n",
    "    runs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return runs[0]\n",
    "\n",
    "def _find_report_in_run(run_dir: Path) -> Path | None:\n",
    "    if not run_dir.exists():\n",
    "        return None\n",
    "    c1 = run_dir / \"regimen_selector_report.html\"\n",
    "    c2 = run_dir / \"regime_selector_report.html\"\n",
    "    if c1.exists(): return c1.resolve()\n",
    "    if c2.exists(): return c2.resolve()\n",
    "    # fallback: buscar por patrÃ³n en nivel 1\n",
    "    for p in run_dir.glob(\"*selector*report*.html\"):\n",
    "        if p.is_file():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def _find_nb2_runs_roots(base_dir: Path) -> list[Path]:\n",
    "    roots: list[Path] = []\n",
    "    if not base_dir.exists():\n",
    "        return roots\n",
    "    for d in base_dir.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        name_u = d.name.upper()\n",
    "        # mÃ¡s laxo que \"ER_FILTER*\" para evitar misses\n",
    "        if (\"ER_FILTER\" in name_u) or (\"FILTER\" in name_u and \"ER_\" in name_u):\n",
    "            rr = d / \"research_logs\" / \"runs\"\n",
    "            if rr.exists():\n",
    "                roots.append(rr.resolve())\n",
    "    return roots\n",
    "\n",
    "def _read_basket_from_html(html_path: Path) -> pl.DataFrame:\n",
    "    try:\n",
    "        import pandas as pd  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Celda 01] ERROR: falta pandas para leer HTML: {e}\")\n",
    "\n",
    "    tables = pd.read_html(str(html_path))\n",
    "    best = None\n",
    "    for t in tables:\n",
    "        cols = [str(c).strip().lower() for c in list(t.columns)]\n",
    "        if (\"symbol\" in cols) and (\"family\" in cols) and (\"preset\" in cols):\n",
    "            best = t.copy()\n",
    "            break\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"[Celda 01] ERROR: HTML no contiene tabla con columnas (symbol,family,preset).\")\n",
    "\n",
    "    best.columns = [str(c).strip() for c in best.columns]\n",
    "    low = {c: c.lower() for c in best.columns}\n",
    "    best.rename(columns=low, inplace=True)\n",
    "\n",
    "    score_col = None\n",
    "    for c in [\"score_final\", \"score_v2\", \"score\"]:\n",
    "        if c in best.columns:\n",
    "            score_col = c\n",
    "            break\n",
    "\n",
    "    df = pl.from_pandas(best)\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"family\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"family\"),\n",
    "        pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"preset\"),\n",
    "    ])\n",
    "    if score_col:\n",
    "        df = df.with_columns(pl.col(score_col).cast(pl.Float64, strict=False).alias(\"score_key\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).cast(pl.Float64).alias(\"score_key\"))\n",
    "\n",
    "    return df.select([\"symbol\",\"family\",\"preset\",\"score_key\"]).unique(subset=[\"symbol\"], keep=\"first\")\n",
    "\n",
    "# =========================\n",
    "# Resolver report HTML (prioridad absoluta)\n",
    "# =========================\n",
    "picked_report: Path | None = None\n",
    "picked_run_dir: Path | None = None\n",
    "picked_source = None\n",
    "\n",
    "# 1) override directo por report path\n",
    "if NB2_REPORT_PATH_OVERRIDE.strip():\n",
    "    p = Path(NB2_REPORT_PATH_OVERRIDE.strip()).expanduser().resolve()\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"[Celda 01] ERROR: NB2_REPORT_PATH_OVERRIDE no existe: {p}\")\n",
    "    picked_report = p\n",
    "    picked_run_dir = p.parent\n",
    "    picked_source = \"OVERRIDE_REPORT_PATH\"\n",
    "\n",
    "# 2) override por run dir\n",
    "if picked_report is None and NB2_RUN_DIR_OVERRIDE.strip():\n",
    "    rd = Path(NB2_RUN_DIR_OVERRIDE.strip()).expanduser().resolve()\n",
    "    rp = _find_report_in_run(rd)\n",
    "    if rp is None:\n",
    "        raise RuntimeError(f\"[Celda 01] ERROR: no encontrÃ© *selector*report*.html dentro de NB2_RUN_DIR_OVERRIDE: {rd}\")\n",
    "    picked_report = rp\n",
    "    picked_run_dir = rd\n",
    "    picked_source = \"OVERRIDE_RUN_DIR\"\n",
    "\n",
    "# 3) auto-discovery: buscar NB2 roots en base_guess\n",
    "if picked_report is None:\n",
    "    roots = _find_nb2_runs_roots(base_guess)\n",
    "    best = None\n",
    "    best_mtime = -1.0\n",
    "    best_run = None\n",
    "    for rr in roots:\n",
    "        lr = _latest_run_dir(rr)\n",
    "        if lr is None:\n",
    "            continue\n",
    "        rp = _find_report_in_run(lr)\n",
    "        if rp is None:\n",
    "            continue\n",
    "        mt = rp.stat().st_mtime\n",
    "        if mt > best_mtime:\n",
    "            best_mtime = mt\n",
    "            best = rp\n",
    "            best_run = lr\n",
    "    if best is not None:\n",
    "        picked_report = best\n",
    "        picked_run_dir = best_run\n",
    "        picked_source = \"AUTO_NB2_LATEST_REPORT\"\n",
    "\n",
    "# 4) auto-discovery local: buscar report dentro de ER_STRATEGY_LAB (rÃ¡pido)\n",
    "if picked_report is None:\n",
    "    # buscar en rutas tÃ­picas del lab\n",
    "    candidates = [\n",
    "        lab_root / \"research_logs\" / \"runs\",\n",
    "        lab_root / \"research_logs\",\n",
    "        lab_root,\n",
    "    ]\n",
    "    found = []\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            for p in c.rglob(\"regimen_selector_report.html\"):\n",
    "                if p.is_file():\n",
    "                    found.append(p.resolve())\n",
    "            for p in c.rglob(\"regime_selector_report.html\"):\n",
    "                if p.is_file():\n",
    "                    found.append(p.resolve())\n",
    "    if found:\n",
    "        found.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        picked_report = found[0]\n",
    "        picked_run_dir = picked_report.parent\n",
    "        picked_source = \"AUTO_LOCAL_LATEST_REPORT\"\n",
    "\n",
    "# =========================\n",
    "# Leer basket (HTML -> fallback hardcode)\n",
    "# =========================\n",
    "df_basket = None\n",
    "basket_source = None\n",
    "\n",
    "if picked_report is not None:\n",
    "    try:\n",
    "        df_basket = _read_basket_from_html(picked_report)\n",
    "        basket_source = f\"HTML::{picked_source}\"\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 01] WARNING: fallÃ³ lectura HTML ({picked_report.name}). UsarÃ© fallback hardcode. err={e}\")\n",
    "\n",
    "if df_basket is None:\n",
    "    # fallback hardcode\n",
    "    rows = []\n",
    "    for (fam, pre), syms in FALLBACK_BASKETS.items():\n",
    "        for s in syms:\n",
    "            rows.append({\"symbol\": s, \"family\": fam, \"preset\": pre, \"score_key\": None})\n",
    "    df_basket = pl.DataFrame(rows)\n",
    "    basket_source = \"FALLBACK_HARDCODED_FROM_SHARED_REPORT\"\n",
    "    picked_report = None\n",
    "    picked_run_dir = None\n",
    "\n",
    "# =========================\n",
    "# Filtrar target family/preset\n",
    "# =========================\n",
    "df_target = df_basket.filter((pl.col(\"family\") == TARGET_FAMILY) & (pl.col(\"preset\") == TARGET_PRESET))\n",
    "if df_target.height == 0:\n",
    "    # relajar preset si no existe\n",
    "    df_target = df_basket.filter(pl.col(\"family\") == TARGET_FAMILY)\n",
    "\n",
    "symbols_basket = df_target.select(pl.col(\"symbol\")).to_series().to_list()\n",
    "if not symbols_basket:\n",
    "    raise RuntimeError(f\"[Celda 01] ERROR: basket vacÃ­o para family={TARGET_FAMILY} preset={TARGET_PRESET} (source={basket_source}).\")\n",
    "\n",
    "# =========================\n",
    "# SelecciÃ³n final (sin silencios)\n",
    "# =========================\n",
    "basket_set = set(symbols_basket)\n",
    "\n",
    "if SELECT_SYMBOLS_MANUAL:\n",
    "    selected = [str(s).upper().strip() for s in SELECT_SYMBOLS_MANUAL if str(s).strip()]\n",
    "    bad = [s for s in selected if s not in basket_set]\n",
    "    if bad:\n",
    "        raise RuntimeError(f\"[Celda 01] ERROR: SELECT_SYMBOLS_MANUAL fuera del basket NB2: bad={bad} basket_sample={symbols_basket[:20]}\")\n",
    "else:\n",
    "    # ranking si score_key existe y no es todo null\n",
    "    has_score = df_target.select(pl.col(\"score_key\").drop_nulls().count()).item() > 0\n",
    "    df_rank = df_target.sort(\"score_key\", descending=True) if has_score else df_target\n",
    "    if int(SELECT_TOP_N) > 0:\n",
    "        selected = df_rank.head(int(SELECT_TOP_N)).select(pl.col(\"symbol\")).to_series().to_list()\n",
    "    else:\n",
    "        selected = df_rank.select(pl.col(\"symbol\")).to_series().to_list()\n",
    "\n",
    "# guardrail definitivo\n",
    "bad2 = [s for s in selected if s not in basket_set]\n",
    "if bad2:\n",
    "    raise RuntimeError(f\"[Celda 01] ERROR: selecciÃ³n final fuera del basket: {bad2}\")\n",
    "\n",
    "# =========================\n",
    "# Persistir a GLOBAL_STATE (y forzar source-of-truth)\n",
    "# =========================\n",
    "GLOBAL_STATE.setdefault(\"universe\", {})\n",
    "GLOBAL_STATE[\"universe\"].update({\n",
    "    \"source\": \"NOTEBOOK2_BASKET_LOCKED\",\n",
    "    \"basket_source\": basket_source,\n",
    "    \"target_family\": TARGET_FAMILY,\n",
    "    \"target_preset\": TARGET_PRESET,\n",
    "    \"nb2_report_path\": str(picked_report) if picked_report else None,\n",
    "    \"nb2_run_dir\": str(picked_run_dir) if picked_run_dir else None,\n",
    "    \"basket_TREND\": symbols_basket if TARGET_FAMILY == \"TREND\" else GLOBAL_STATE[\"universe\"].get(\"basket_TREND\", []),\n",
    "    \"selected_symbols_TREND\": selected if TARGET_FAMILY == \"TREND\" else GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\", []),\n",
    "    \"resolved_utc\": datetime.utcnow().isoformat(),\n",
    "})\n",
    "\n",
    "# FORZAR: evitar que celdas posteriores usen listas equivocadas\n",
    "GLOBAL_STATE.setdefault(\"data_quality\", {})\n",
    "prev = GLOBAL_STATE[\"data_quality\"].get(\"final_symbols\")\n",
    "GLOBAL_STATE[\"data_quality\"][\"final_symbols_prev\"] = prev\n",
    "GLOBAL_STATE[\"data_quality\"][\"final_symbols\"] = selected\n",
    "GLOBAL_STATE[\"data_quality\"][\"final_symbols_source\"] = \"FORCED_FROM_NB2_BASKET_LOCKED\"\n",
    "GLOBAL_STATE[\"data_quality\"][\"final_symbols_forced_utc\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "# =========================\n",
    "# Prints auditables\n",
    "# =========================\n",
    "print(f\"[Celda 01] basket_source = {basket_source}\")\n",
    "print(f\"[Celda 01] report_path   = {str(picked_report) if picked_report else 'NONE (fallback)'}\")\n",
    "print(f\"[Celda 01] run_dir       = {str(picked_run_dir) if picked_run_dir else 'NONE (fallback)'}\")\n",
    "print(f\"[Celda 01] BASKET({TARGET_FAMILY}/{TARGET_PRESET}) size={len(symbols_basket)} => {symbols_basket}\")\n",
    "print(f\"[Celda 01] SELECTED size={len(selected)} => {selected}\")\n",
    "print(f\"[Celda 01] data_quality.final_symbols FORZADO => {GLOBAL_STATE['data_quality']['final_symbols']}\")\n",
    "\n",
    "# Killer guard: si aparece GBPNZD en selected, esto debe explotar (no deberÃ­a pasar)\n",
    "if \"GBPNZD\" in set(selected):\n",
    "    raise RuntimeError(\"[Celda 01] ERROR: GBPNZD apareciÃ³ en SELECTED pero NO pertenece al basket NB2. STOP.\")\n",
    "\n",
    "print(\">>> Celda 01 v1.3 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a80de4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 02 v2.2 :: OHLCV M5 + QA (robust time inference + polars engine)\n",
      "[Celda 02] selected_symbols_TREND = ['BNBUSD', 'XAUAUD', 'BTCUSD', 'LVMH']\n",
      "[Celda 02] DATA_ROOT = C:\\Quant\\MT5_Data_Extraction\n",
      "[Celda 02] Roots rates existentes:\n",
      "   - C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "   - C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\\m5_raw\n",
      "   - C:\\Quant\\MT5_Data_Extraction\\bulk_data\\rates_5m\n",
      "[Celda 02] Found dataset for BNBUSD in root: C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 02] WARN: BNBUSD max_gap_sec=125100s (> 7200s)\n",
      "[Celda 02] OK: BNBUSD â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\m5_ohlcv_clean\\BNBUSD_M5_OHLCV_clean.parquet | rows=409320\n",
      "[Celda 02] Found dataset for XAUAUD in root: C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 02] WARN: XAUAUD max_gap_sec=264000s (> 7200s)\n",
      "[Celda 02] WARN: XAUAUD coverage_7d=65.08% (< 95%)\n",
      "[Celda 02] OK: XAUAUD â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\m5_ohlcv_clean\\XAUAUD_M5_OHLCV_clean.parquet | rows=283032\n",
      "[Celda 02] Found dataset for BTCUSD in root: C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 02] WARN: BTCUSD max_gap_sec=172800s (> 7200s)\n",
      "[Celda 02] OK: BTCUSD â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\m5_ohlcv_clean\\BTCUSD_M5_OHLCV_clean.parquet | rows=337024\n",
      "[Celda 02] Found dataset for LVMH in root: C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 02] WARN: LVMH max_gap_sec=402000s (> 7200s)\n",
      "[Celda 02] WARN: LVMH coverage_7d=25.10% (< 95%)\n",
      "[Celda 02] OK: LVMH â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\m5_ohlcv_clean\\LVMH_M5_OHLCV_clean.parquet | rows=104240\n",
      "ðŸ’¾ QA_summary â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\m5_ohlcv_clean\\_QA_summary.parquet (OK)\n",
      ">>> Celda 02 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 02 v2.2 â€” Cargar OHLCV M5 + QA (robust time inference + polars engine) =====================\n",
    "# OBJETIVO:\n",
    "#   1) Cargar OHLCV M5 para sÃ­mbolos seleccionados.\n",
    "#   2) Inferir y normalizar columna de tiempo (incluye epoch int).\n",
    "#   3) Normalizar OHLCV; QA; persistir parquet limpio por sÃ­mbolo.\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 02 v2.2 :: OHLCV M5 + QA (robust time inference + polars engine)\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 02] ERROR: GLOBAL_STATE no existe.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "for k in (\"artifacts\", \"lab_root\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 02] ERROR: GLOBAL_STATE['paths'] incompleto; falta: {k}\")\n",
    "\n",
    "selected = (GLOBAL_STATE.get(\"universe\", {}) or {}).get(\"selected_symbols_TREND\", [])\n",
    "if not selected:\n",
    "    raise RuntimeError(\"[Celda 02] ERROR: no hay selected_symbols_TREND (ejecuta Celda 01).\")\n",
    "\n",
    "print(\"[Celda 02] selected_symbols_TREND =\", selected)\n",
    "\n",
    "# ========================= ParÃ¡metros QA =========================\n",
    "MAX_GAP_SEC = 2 * 3600\n",
    "MACRO_GAP_SEC = 12 * 3600\n",
    "MIN_COVERAGE_RECENT = 0.95\n",
    "\n",
    "FALLBACK_MAX_PARQUETS = 3000\n",
    "FALLBACK_MAX_MATCHES  = 50\n",
    "\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"features\" / \"m5_ohlcv_clean\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Roots =========================\n",
    "def _resolve_data_root(er_lab_root: Path) -> Path:\n",
    "    env = os.getenv(\"MT5_PROJECT_ROOT\") or os.getenv(\"MT5_DE_PROJECT_ROOT\") or os.getenv(\"DATA_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "\n",
    "    if er_lab_root.name.upper() == \"ER_STRATEGY_LAB\":\n",
    "        return er_lab_root.parent.resolve()\n",
    "\n",
    "    for p in [er_lab_root, *er_lab_root.parents]:\n",
    "        if (p / \"data\").exists():\n",
    "            return p.resolve()\n",
    "\n",
    "    common = Path(r\"C:\\Quant\\MT5_Data_Extraction\")\n",
    "    if common.exists():\n",
    "        return common.resolve()\n",
    "\n",
    "    raise RuntimeError(\"[Celda 02] No se pudo resolver DATA_ROOT. Define MT5_PROJECT_ROOT o DATA_ROOT.\")\n",
    "\n",
    "DATA_ROOT = _resolve_data_root(Path(paths[\"lab_root\"]))\n",
    "\n",
    "RATES_ROOT_CANDIDATES = [\n",
    "    DATA_ROOT / \"data\" / \"historical_data\" / \"m5_clean\",\n",
    "    DATA_ROOT / \"data\" / \"historical_data\" / \"rates_5m\",\n",
    "    DATA_ROOT / \"data\" / \"rates_5m\",\n",
    "    DATA_ROOT / \"data\" / \"bulk_data\" / \"m5_raw\",\n",
    "    DATA_ROOT / \"bulk_data\" / \"rates_5m\",\n",
    "]\n",
    "\n",
    "RATES_ROOT_EXISTING = [p for p in RATES_ROOT_CANDIDATES if p.exists() and p.is_dir()]\n",
    "if not RATES_ROOT_EXISTING:\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 02] ERROR: no existe ningÃºn root de rates M5.\\n\"\n",
    "        \"ProbÃ©:\\n  \" + \"\\n  \".join(map(str, RATES_ROOT_CANDIDATES)) + \"\\n\"\n",
    "        f\"DATA_ROOT={DATA_ROOT}\"\n",
    "    )\n",
    "\n",
    "print(\"[Celda 02] DATA_ROOT =\", DATA_ROOT)\n",
    "print(\"[Celda 02] Roots rates existentes:\")\n",
    "for p in RATES_ROOT_EXISTING:\n",
    "    print(\"   -\", p)\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _collect_compat(lf: pl.LazyFrame) -> pl.DataFrame:\n",
    "    # Polars >=1.25: streaming param deprecated; usar engine=\"streaming\"\n",
    "    try:\n",
    "        return lf.collect(engine=\"streaming\")\n",
    "    except TypeError:\n",
    "        # Polars viejo\n",
    "        return lf.collect(streaming=True)\n",
    "\n",
    "def _first_dir_with_parquets(dirs: list[Path]) -> Path | None:\n",
    "    for d in dirs:\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if any(d.glob(\"*.parquet\")) or any(d.glob(\"*/*.parquet\")) or any(d.glob(\"**/*.parquet\")):\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "def _scan_symbol_parquet(symbol: str, root: Path) -> pl.LazyFrame:\n",
    "    sym_u = symbol.upper().strip()\n",
    "\n",
    "    direct = root / sym_u\n",
    "    if direct.exists() and direct.is_dir():\n",
    "        return pl.scan_parquet(str(direct / \"**\" / \"*.parquet\"), hive_partitioning=True)\n",
    "\n",
    "    exact_dirs = [p for p in root.rglob(sym_u) if p.is_dir()]\n",
    "    chosen = _first_dir_with_parquets(sorted(exact_dirs, key=lambda x: len(str(x))))\n",
    "    if chosen is not None:\n",
    "        return pl.scan_parquet(str(chosen / \"**\" / \"*.parquet\"), hive_partitioning=True)\n",
    "\n",
    "    part_keys = (\"symbol=\", \"ticker=\", \"instrument=\", \"pair=\")\n",
    "    part_dirs = []\n",
    "    for p in root.rglob(f\"*{sym_u}*\"):\n",
    "        if p.is_dir():\n",
    "            nu = p.name.upper()\n",
    "            if any(k.upper() in nu for k in part_keys):\n",
    "                part_dirs.append(p)\n",
    "    chosen = _first_dir_with_parquets(sorted(part_dirs, key=lambda x: len(str(x))))\n",
    "    if chosen is not None:\n",
    "        return pl.scan_parquet(str(chosen / \"**\" / \"*.parquet\"), hive_partitioning=True)\n",
    "\n",
    "    hits = list(root.rglob(f\"*{sym_u}*.parquet\"))\n",
    "    if hits:\n",
    "        return pl.scan_parquet([str(p) for p in hits], hive_partitioning=True)\n",
    "\n",
    "    all_parqs = list(root.rglob(\"*.parquet\"))\n",
    "    if 0 < len(all_parqs) <= FALLBACK_MAX_PARQUETS:\n",
    "        lf_all = pl.scan_parquet([str(p) for p in all_parqs], hive_partitioning=True)\n",
    "        cols = lf_all.collect_schema().names()\n",
    "        if \"symbol\" in cols:\n",
    "            return lf_all.filter(pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase() == pl.lit(sym_u))\n",
    "        if \"ticker\" in cols:\n",
    "            return lf_all.filter(pl.col(\"ticker\").cast(pl.Utf8).str.to_uppercase() == pl.lit(sym_u))\n",
    "\n",
    "    raise RuntimeError(f\"[Celda 02] ERROR: no se encontrÃ³ dataset parquet para {sym_u} en root: {root}\")\n",
    "\n",
    "def _pick_time_col(df: pl.DataFrame) -> str:\n",
    "    # 1) nombres comunes (case-insensitive)\n",
    "    candidates = [\n",
    "        \"time_utc\",\"time\",\"datetime\",\"dt\",\"timestamp\",\n",
    "        \"Time\",\"TIME\",\"date\",\"Date\",\"DATE\",\"date_time\",\"DateTime\",\"datetime_utc\",\n",
    "        \"time_msc\",\"time_ms\",\"timestamp_ms\",\"timestamp_us\",\"timestamp_ns\",\n",
    "        \"ts\",\"ts_utc\",\"t\",\"open_time\",\"close_time\"\n",
    "    ]\n",
    "    cols = df.columns\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "\n",
    "    for c in candidates:\n",
    "        if c.lower() in cols_lower:\n",
    "            return cols_lower[c.lower()]\n",
    "\n",
    "    # 2) regex heurÃ­stica en nombre\n",
    "    rx = re.compile(r\"(time|date|datetime|timestamp|ts)\", re.IGNORECASE)\n",
    "    rx_hits = [c for c in cols if rx.search(c)]\n",
    "    if len(rx_hits) == 1:\n",
    "        return rx_hits[0]\n",
    "\n",
    "    # 3) por dtype: si hay exactamente 1 Datetime/Date, Ãºsala\n",
    "    schema = df.schema\n",
    "    dt_like = [c for c, t in schema.items() if isinstance(t, (pl.Datetime, pl.Date))]\n",
    "    if len(dt_like) == 1:\n",
    "        return dt_like[0]\n",
    "\n",
    "    # 4) falla: imprime diagnÃ³stico y corta\n",
    "    print(\"\\n[Celda 02][DIAG] Columnas disponibles (name: dtype):\")\n",
    "    for c, t in df.schema.items():\n",
    "        print(f\"  - {c}: {t}\")\n",
    "    if rx_hits:\n",
    "        print(\"\\n[Celda 02][DIAG] Candidatas por regex (time/date/ts...):\", rx_hits)\n",
    "    if dt_like:\n",
    "        print(\"\\n[Celda 02][DIAG] Candidatas por dtype Date/Datetime:\", dt_like)\n",
    "\n",
    "    raise RuntimeError(\"[Celda 02] ERROR: no encuentro columna de tiempo. Revisa [DIAG] arriba.\")\n",
    "\n",
    "def _to_time_utc(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    time_col = _pick_time_col(df)\n",
    "    out = df\n",
    "\n",
    "    if time_col != \"time_utc\":\n",
    "        out = out.rename({time_col: \"time_utc\"})\n",
    "\n",
    "    t = out.schema[\"time_utc\"]\n",
    "\n",
    "    # Si es string -> parse datetime\n",
    "    if t == pl.Utf8:\n",
    "        out = out.with_columns(pl.col(\"time_utc\").str.to_datetime(strict=False, time_zone=\"UTC\"))\n",
    "        return out\n",
    "\n",
    "    # Si es Date -> a Datetime UTC\n",
    "    if isinstance(t, pl.Date):\n",
    "        out = out.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(time_zone=\"UTC\")))\n",
    "        return out\n",
    "\n",
    "    # Si ya es Datetime -> asegurar UTC si no tiene tz\n",
    "    if isinstance(t, pl.Datetime):\n",
    "        try:\n",
    "            tz = out.schema[\"time_utc\"].time_zone\n",
    "            if tz is None:\n",
    "                out = out.with_columns(pl.col(\"time_utc\").dt.replace_time_zone(\"UTC\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return out\n",
    "\n",
    "    # Si es entero epoch -> inferir unidad por magnitud\n",
    "    if t in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Float64, pl.Float32):\n",
    "        # Tomar una muestra para inferir escala\n",
    "        sample = out.select(pl.col(\"time_utc\").drop_nulls().head(50)).to_series()\n",
    "        if sample.len() == 0:\n",
    "            raise RuntimeError(\"[Celda 02] ERROR: columna tiempo sin valores (todo null).\")\n",
    "\n",
    "        vmax = float(sample.max())\n",
    "        # heurÃ­stica:\n",
    "        # seconds ~ 1e9, ms ~ 1e12-1e13, us ~ 1e15, ns ~ 1e18\n",
    "        if vmax < 1e11:\n",
    "            unit = \"s\"\n",
    "        elif vmax < 1e14:\n",
    "            unit = \"ms\"\n",
    "        elif vmax < 1e17:\n",
    "            unit = \"us\"\n",
    "        else:\n",
    "            unit = \"ns\"\n",
    "\n",
    "        out = out.with_columns(\n",
    "            pl.from_epoch(pl.col(\"time_utc\").cast(pl.Int64, strict=False), time_unit=unit).dt.replace_time_zone(\"UTC\")\n",
    "        )\n",
    "        print(f\"[Celda 02] INFO: time_utc inferido desde epoch int con unidad='{unit}'\")\n",
    "        return out\n",
    "\n",
    "    # Ãºltimo intento genÃ©rico\n",
    "    out = out.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(time_zone=\"UTC\"), strict=False))\n",
    "    return out\n",
    "\n",
    "def _ensure_ohlcv_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    ren = {}\n",
    "    for c in df.columns:\n",
    "        cl = c.lower().strip()\n",
    "        if cl in (\"o\",\"open\"): ren[c] = \"open\"\n",
    "        if cl in (\"h\",\"high\"): ren[c] = \"high\"\n",
    "        if cl in (\"l\",\"low\"):  ren[c] = \"low\"\n",
    "        if cl in (\"c\",\"close\"): ren[c] = \"close\"\n",
    "        if cl in (\"vol\",\"volume\",\"tick_volume\"): ren[c] = \"volume\"\n",
    "        if cl in (\"spr\",\"spread\"): ren[c] = \"spread\"\n",
    "    out = df.rename(ren) if ren else df\n",
    "\n",
    "    for req in [\"open\",\"high\",\"low\",\"close\"]:\n",
    "        if req not in out.columns:\n",
    "            print(\"\\n[Celda 02][DIAG] Columnas disponibles:\", out.columns)\n",
    "            raise RuntimeError(f\"[Celda 02] ERROR: falta columna requerida OHLC: {req}\")\n",
    "\n",
    "    out = out.with_columns([\n",
    "        pl.col(\"open\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"high\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"low\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "    ])\n",
    "    if \"volume\" in out.columns:\n",
    "        out = out.with_columns(pl.col(\"volume\").cast(pl.Float64, strict=False))\n",
    "    if \"spread\" in out.columns:\n",
    "        out = out.with_columns(pl.col(\"spread\").cast(pl.Float64, strict=False))\n",
    "    return out\n",
    "\n",
    "def _qa_time_gaps(df: pl.DataFrame) -> dict:\n",
    "    t = df.get_column(\"time_utc\").to_list()\n",
    "    if len(t) < 3:\n",
    "        return {\"max_gap_sec\": None, \"macro_gaps\": 0}\n",
    "\n",
    "    gaps = []\n",
    "    for i in range(1, len(t)):\n",
    "        dt = (t[i] - t[i-1]).total_seconds()\n",
    "        if dt is not None and dt >= 0:\n",
    "            gaps.append(dt)\n",
    "    if not gaps:\n",
    "        return {\"max_gap_sec\": None, \"macro_gaps\": 0}\n",
    "\n",
    "    max_gap = max(gaps)\n",
    "    macro = sum(1 for g in gaps if g > MACRO_GAP_SEC)\n",
    "    return {\"max_gap_sec\": float(max_gap), \"macro_gaps\": int(macro)}\n",
    "\n",
    "def _coverage_recent(df: pl.DataFrame) -> float:\n",
    "    if df.height < 10:\n",
    "        return 0.0\n",
    "    tmax = df.select(pl.col(\"time_utc\").max()).item()\n",
    "    tmin = tmax - timedelta(days=7)\n",
    "    recent = df.filter(pl.col(\"time_utc\") >= pl.lit(tmin))\n",
    "    if recent.height < 10:\n",
    "        return 0.0\n",
    "    expected = 7 * 24 * 12\n",
    "    return min(1.0, recent.height / expected)\n",
    "\n",
    "# ========================= Main =========================\n",
    "clean_paths = {}\n",
    "qa_rows = []\n",
    "\n",
    "for sym in selected:\n",
    "    sym_u = sym.upper().strip()\n",
    "\n",
    "    lf = None\n",
    "    last_err = None\n",
    "    for root in RATES_ROOT_EXISTING:\n",
    "        try:\n",
    "            lf = _scan_symbol_parquet(sym_u, root)\n",
    "            print(f\"[Celda 02] Found dataset for {sym_u} in root: {root}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    if lf is None:\n",
    "        raise RuntimeError(f\"[Celda 02] ERROR: no pude cargar {sym_u}. Ãšltimo error: {last_err}\")\n",
    "\n",
    "    df = _collect_compat(lf)\n",
    "\n",
    "    # --- FIX central: inferir time correctamente ---\n",
    "    df = _to_time_utc(df)\n",
    "    df = _ensure_ohlcv_cols(df)\n",
    "\n",
    "    df = df.sort(\"time_utc\").unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "\n",
    "    qa = _qa_time_gaps(df)\n",
    "    cov = _coverage_recent(df)\n",
    "\n",
    "    qa_rows.append({\n",
    "        \"symbol\": sym_u,\n",
    "        \"n_rows\": int(df.height),\n",
    "        \"max_gap_sec\": qa[\"max_gap_sec\"],\n",
    "        \"macro_gaps\": qa[\"macro_gaps\"],\n",
    "        \"coverage_7d\": float(cov),\n",
    "        \"time_min\": df.select(pl.col(\"time_utc\").min()).item(),\n",
    "        \"time_max\": df.select(pl.col(\"time_utc\").max()).item(),\n",
    "    })\n",
    "\n",
    "    if qa[\"max_gap_sec\"] is not None and qa[\"max_gap_sec\"] > MAX_GAP_SEC:\n",
    "        print(f\"[Celda 02] WARN: {sym_u} max_gap_sec={qa['max_gap_sec']:.0f}s (> {MAX_GAP_SEC}s)\")\n",
    "    if cov < MIN_COVERAGE_RECENT:\n",
    "        print(f\"[Celda 02] WARN: {sym_u} coverage_7d={cov:.2%} (< {MIN_COVERAGE_RECENT:.0%})\")\n",
    "\n",
    "    out_path = OUT_DIR / f\"{sym_u}_M5_OHLCV_clean.parquet\"\n",
    "    df.write_parquet(str(out_path), compression=\"zstd\")\n",
    "    clean_paths[sym_u] = str(out_path)\n",
    "\n",
    "    print(f\"[Celda 02] OK: {sym_u} â†’ {out_path} | rows={df.height}\")\n",
    "\n",
    "qa_df = pl.DataFrame(qa_rows).sort(\"symbol\")\n",
    "qa_path = OUT_DIR / \"_QA_summary.parquet\"\n",
    "qa_df.write_parquet(str(qa_path), compression=\"zstd\")\n",
    "\n",
    "GLOBAL_STATE[\"data\"] = {\n",
    "    \"ohlcv_clean_dir\": str(OUT_DIR),\n",
    "    \"ohlcv_clean_paths\": clean_paths,\n",
    "    \"qa_summary_path\": str(qa_path),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ QA_summary â†’ {qa_path} (OK)\")\n",
    "print(\">>> Celda 02 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2497c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 02B v1.3 :: QA Gate INSTITUCIONAL (break-aware) + Coverage bounded + Timeframe sanity\n",
      "[Celda 02B] BNBUSD :: GO | m5_ok=True med_gap_small=300s | class=ALLDAY_24_7 win_h_med=23.9 wk_share=28.06% | active_days_14=14/12 cov_med=0.993 | gap_p90_excl=300s (raw=300s, day_max_p50=900s) | crop=True\n",
      "[Celda 02B] BTCUSD :: GO | m5_ok=True med_gap_small=300s | class=ALLDAY_24_7 win_h_med=23.9 wk_share=28.05% | active_days_14=15/12 cov_med=0.993 | gap_p90_excl=300s (raw=300s, day_max_p50=900s) | crop=True\n",
      "[Celda 02B] LVMH :: GO | m5_ok=True med_gap_small=300s | class=SESSION win_h_med=8.3 wk_share=0.00% | active_days_14=11/7 cov_med=1.000 | gap_p90_excl=300s (raw=300s, day_max_p50=56400s) | crop=True\n",
      "[Celda 02B] XAUAUD :: WARN | m5_ok=True med_gap_small=300s | class=WEEKDAY_24H win_h_med=23.9 wk_share=3.71% | active_days_14=12/8 cov_med=0.948 | gap_p90_excl=300s (raw=300s, day_max_p50=4800s) | crop=True\n",
      "\n",
      "ðŸ’¾ [Celda 02B] decisions -> C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\data_quality_decisions.parquet (OK)\n",
      "ðŸ’¾ [Celda 02B] snapshot  -> C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\data_quality_snapshot.json (OK)\n",
      "[Celda 02B] FINAL_SYMBOLS = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      ">>> Celda 02B v1.3 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 02B v1.3 â€” QA Gate INSTITUCIONAL (break-aware) + Coverage bounded + Timeframe sanity =====================\n",
    "# FIXES vs v1.2:\n",
    "# - coverage_day CLIP a [0,1] (nunca >1).\n",
    "# - gap_p90 se calcula EXCLUYENDO el gap mÃ¡ximo por dÃ­a (remueve break diario / weekend gap).\n",
    "# - sanity check de timeframe: median_gap_small ~300s para M5 (si no, NO_GO por dataset mal).\n",
    "# - diagnÃ³stico ampliado en print por sÃ­mbolo (para auditar sin celdas extra).\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 02B v1.3 :: QA Gate INSTITUCIONAL (break-aware) + Coverage bounded + Timeframe sanity\")\n",
    "\n",
    "# ------------------------- Requisitos -------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 02B] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "if \"run_snapshots\" not in paths:\n",
    "    raise RuntimeError(\"[Celda 02B] ERROR: falta GLOBAL_STATE['paths']['run_snapshots'] (ejecuta Celda 00).\")\n",
    "\n",
    "data_state = GLOBAL_STATE.get(\"data\", {}) or {}\n",
    "if \"ohlcv_clean_paths\" not in data_state:\n",
    "    raise RuntimeError(\"[Celda 02B] ERROR: falta GLOBAL_STATE['data']['ohlcv_clean_paths'] (ejecuta Celda 02).\")\n",
    "\n",
    "clean_paths = dict(data_state[\"ohlcv_clean_paths\"])\n",
    "symbols = sorted([str(s).upper().strip() for s in clean_paths.keys() if str(s).strip()])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 02B] ERROR: no hay sÃ­mbolos en ohlcv_clean_paths.\")\n",
    "\n",
    "snap_dir = Path(paths[\"run_snapshots\"]).resolve()\n",
    "snap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------- ParÃ¡metros institucionales -------------------------\n",
    "FAIL_ON_NO_GO = True\n",
    "REPLACE_SELECTED_WITH_FINAL = True\n",
    "\n",
    "AUTO_CROP_ALWAYS = True\n",
    "CROP_DAYS = 180\n",
    "MIN_DAYS_AFTER_CROP = 30\n",
    "\n",
    "LOOKBACK_DAYS_FOR_STATS = 60\n",
    "WINDOW_CAL_DAYS = 14\n",
    "\n",
    "MIN_BARS_DAY_ACTIVE = 30\n",
    "\n",
    "# Thresholds M5 intra-day (excluyendo gap mÃ¡ximo por dÃ­a):\n",
    "GAP_P90_EXCL_GO_SEC = 900\n",
    "GAP_P90_EXCL_WARN_SEC = 1800\n",
    "\n",
    "# Coverage (bounded) sobre dÃ­as activos:\n",
    "COV_MED_GO = 0.95\n",
    "COV_MED_WARN = 0.85\n",
    "\n",
    "# Timeframe sanity para M5:\n",
    "# mediana de gaps \"pequeÃ±os\" (filtrados <= 1800s) debe estar cerca de 300s\n",
    "M5_TARGET_SEC = 300\n",
    "M5_TOL_SEC = 30   # tolerancia amplia (broker quirks)\n",
    "MAX_SMALL_GAP_SEC = 1800  # para estimar periodicidad sin contaminar con breaks\n",
    "\n",
    "# Actividad esperada (14 dÃ­as) por tipo:\n",
    "MIN_ACTIVE_DAYS_SESSION = 7\n",
    "MIN_ACTIVE_DAYS_WEEKDAY_24H = 8\n",
    "MIN_ACTIVE_DAYS_24_7 = 12\n",
    "\n",
    "# Force-continue (research) por defecto apagado\n",
    "ALLOW_FORCE_CONTINUE = False\n",
    "FORCE_CONTINUE_SYMBOLS: list[str] = []\n",
    "\n",
    "# ------------------------- Helpers -------------------------\n",
    "def _save_json(p: Path, obj: dict) -> None:\n",
    "    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def _classify(window_hours_median: float, weekend_share: float) -> tuple[str, int]:\n",
    "    # SESSION: ventana <=12h\n",
    "    if window_hours_median <= 12.0:\n",
    "        return \"SESSION\", MIN_ACTIVE_DAYS_SESSION\n",
    "    # 24h\n",
    "    if weekend_share < 0.05:\n",
    "        return \"WEEKDAY_24H\", MIN_ACTIVE_DAYS_WEEKDAY_24H\n",
    "    return \"ALLDAY_24_7\", MIN_ACTIVE_DAYS_24_7\n",
    "\n",
    "def _decision(m5_ok: bool, active_days_14: int, min_active_days_14: int, cov_med: float, gap_p90_excl: float) -> str:\n",
    "    if not m5_ok:\n",
    "        return \"NO_GO\"\n",
    "    if active_days_14 < min_active_days_14:\n",
    "        return \"NO_GO\"\n",
    "    if (cov_med < COV_MED_WARN) or (gap_p90_excl > GAP_P90_EXCL_WARN_SEC):\n",
    "        return \"NO_GO\"\n",
    "    if (cov_med < COV_MED_GO) or (gap_p90_excl > GAP_P90_EXCL_GO_SEC):\n",
    "        return \"WARN\"\n",
    "    return \"GO\"\n",
    "\n",
    "# ------------------------- Main -------------------------\n",
    "decisions = []\n",
    "final_symbols = []\n",
    "\n",
    "for sym in symbols:\n",
    "    p = Path(clean_paths[sym]).resolve()\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"[Celda 02B] ERROR: no existe parquet para {sym}: {p}\")\n",
    "\n",
    "    df = pl.read_parquet(str(p))\n",
    "    if \"time_utc\" not in df.columns:\n",
    "        raise RuntimeError(f\"[Celda 02B] ERROR: {sym} no tiene time_utc (revisa Celda 02).\")\n",
    "\n",
    "    df = df.sort(\"time_utc\")\n",
    "\n",
    "    # ---- Crop (siempre, si habilitado) ----\n",
    "    crop_applied = False\n",
    "    crop_path = None\n",
    "    crop_days_effective = None\n",
    "\n",
    "    if AUTO_CROP_ALWAYS:\n",
    "        tmax0 = df.select(pl.col(\"time_utc\").max()).item()\n",
    "        tmin0 = tmax0 - timedelta(days=int(CROP_DAYS))\n",
    "        df_crop = df.filter(pl.col(\"time_utc\") >= pl.lit(tmin0))\n",
    "        if df_crop.height >= 10:\n",
    "            tmin_eff = df_crop.select(pl.col(\"time_utc\").min()).item()\n",
    "            crop_days_effective = int((tmax0 - tmin_eff).days)\n",
    "            if crop_days_effective >= MIN_DAYS_AFTER_CROP:\n",
    "                out_dir = Path(data_state.get(\"ohlcv_clean_dir\") or p.parent).resolve()\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                out_p = out_dir / f\"{sym}_M5_OHLCV_clean_CROP_{CROP_DAYS}d.parquet\"\n",
    "                df_crop.write_parquet(str(out_p), compression=\"zstd\")\n",
    "                crop_applied = True\n",
    "                crop_path = str(out_p)\n",
    "                clean_paths[sym] = crop_path\n",
    "                df = df_crop.sort(\"time_utc\")\n",
    "\n",
    "    # ---- lookback stats ----\n",
    "    tmax = df.select(pl.col(\"time_utc\").max()).item()\n",
    "    tmin_stats = tmax - timedelta(days=int(LOOKBACK_DAYS_FOR_STATS))\n",
    "    df_s = df.filter(pl.col(\"time_utc\") >= pl.lit(tmin_stats)).sort(\"time_utc\")\n",
    "\n",
    "    df_s = df_s.with_columns([\n",
    "        pl.col(\"time_utc\").dt.date().alias(\"_date\"),\n",
    "        pl.col(\"time_utc\").dt.weekday().alias(\"_wday\"),\n",
    "        pl.col(\"time_utc\").dt.hour().alias(\"_hour\"),\n",
    "        pl.col(\"time_utc\").dt.minute().alias(\"_minute\"),\n",
    "        pl.col(\"time_utc\").dt.epoch(\"s\").alias(\"_ts_s\"),\n",
    "    ]).with_columns([\n",
    "        (pl.col(\"_hour\") * 60 + pl.col(\"_minute\")).alias(\"_tod_min\"),\n",
    "        (pl.col(\"_wday\") >= 6).alias(\"_is_weekend\"),\n",
    "    ]).with_columns([\n",
    "        (pl.col(\"_ts_s\") - pl.col(\"_ts_s\").shift(1)).alias(\"_gap_s\"),\n",
    "    ]).with_columns([\n",
    "        # limpiar gaps invÃ¡lidos\n",
    "        pl.when((pl.col(\"_gap_s\") > 0) & (pl.col(\"_gap_s\") < 86400 * 10)).then(pl.col(\"_gap_s\")).otherwise(None).alias(\"_gap_s\"),\n",
    "    ])\n",
    "\n",
    "    # weekend share (por barras)\n",
    "    weekend_share = float(\n",
    "        df_s.select(\n",
    "            (pl.col(\"_is_weekend\").cast(pl.Int8).sum() / (pl.len() + 1e-12)).alias(\"wk_share\")\n",
    "        ).item()\n",
    "    )\n",
    "\n",
    "    # ---- Timeframe sanity (M5) ----\n",
    "    gaps_small = df_s.select(pl.col(\"_gap_s\")).drop_nulls().filter(pl.col(\"_gap_s\") <= MAX_SMALL_GAP_SEC)\n",
    "    med_gap_small = float(gaps_small.select(pl.col(\"_gap_s\").median()).item() or 0.0)\n",
    "    m5_ok = (abs(med_gap_small - M5_TARGET_SEC) <= M5_TOL_SEC) if med_gap_small > 0 else False\n",
    "\n",
    "    # ---- Daily stats (ventana observada + expected bars) ----\n",
    "    daily = (\n",
    "        df_s.group_by(\"_date\")\n",
    "        .agg([\n",
    "            pl.len().alias(\"bars\"),\n",
    "            pl.col(\"_ts_s\").min().alias(\"ts_min\"),\n",
    "            pl.col(\"_ts_s\").max().alias(\"ts_max\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"ts_max\") - pl.col(\"ts_min\")).alias(\"window_sec\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.when(pl.col(\"window_sec\") <= 0)\n",
    "               .then(1)\n",
    "               .otherwise((pl.col(\"window_sec\") / M5_TARGET_SEC).floor() + 1)\n",
    "             ).cast(pl.Int64).alias(\"expected_bars\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # coverage bounded\n",
    "            (pl.col(\"bars\") / (pl.col(\"expected_bars\") + 1e-12)).clip(0.0, 1.0).alias(\"coverage_day\"),\n",
    "            (pl.col(\"bars\") >= MIN_BARS_DAY_ACTIVE).alias(\"is_active_day\"),\n",
    "        ])\n",
    "        .sort(\"_date\")\n",
    "    )\n",
    "\n",
    "    # ventana (horas) sobre dÃ­as activos\n",
    "    daily_active_all = daily.filter(pl.col(\"is_active_day\"))\n",
    "    window_hours_median = float(\n",
    "        daily_active_all.select((pl.col(\"window_sec\").median() / 3600.0).alias(\"wh\")).item() or 0.0\n",
    "    )\n",
    "\n",
    "    cls_kind, min_active_days_14 = _classify(window_hours_median, weekend_share)\n",
    "\n",
    "    # ---- gap_p90 excluyendo el gap mÃ¡ximo por dÃ­a ----\n",
    "    # rank de gaps dentro del dÃ­a (donde el gap pertenece al dÃ­a del bar actual)\n",
    "    gaps = df_s.select([\"_date\", \"_gap_s\"]).drop_nulls()\n",
    "    if gaps.height > 0:\n",
    "        gaps = gaps.with_columns([\n",
    "            pl.col(\"_gap_s\").rank(method=\"dense\", descending=True).over(\"_date\").alias(\"_gap_rank\"),\n",
    "        ]).with_columns([\n",
    "            pl.when(pl.col(\"_gap_rank\") > 1).then(pl.col(\"_gap_s\")).otherwise(None).alias(\"_gap_excl_day_max\"),\n",
    "        ])\n",
    "        gap_p90_raw = float(gaps.select(pl.col(\"_gap_s\").quantile(0.90, interpolation=\"nearest\")).item() or 0.0)\n",
    "        gap_p90_excl = float(gaps.select(pl.col(\"_gap_excl_day_max\").drop_nulls().quantile(0.90, interpolation=\"nearest\")).item() or 0.0)\n",
    "\n",
    "        # si no hay suficientes gaps tras excluir max (p.ej. muy pocos por dÃ­a), fallback a raw\n",
    "        if gap_p90_excl <= 0:\n",
    "            gap_p90_excl = gap_p90_raw\n",
    "\n",
    "        # mediana del gap mÃ¡ximo diario (para auditar tamaÃ±o del break)\n",
    "        day_max_gap = gaps.group_by(\"_date\").agg(pl.col(\"_gap_s\").max().alias(\"day_max_gap\"))\n",
    "        day_max_gap_p50 = float(day_max_gap.select(pl.col(\"day_max_gap\").median()).item() or 0.0)\n",
    "    else:\n",
    "        gap_p90_raw = 0.0\n",
    "        gap_p90_excl = 0.0\n",
    "        day_max_gap_p50 = 0.0\n",
    "\n",
    "    # ---- ventana operativa 14 dÃ­as ----\n",
    "    tmin_win = tmax - timedelta(days=int(WINDOW_CAL_DAYS))\n",
    "    daily_w = daily.filter(pl.col(\"_date\") >= pl.lit(tmin_win.date()))\n",
    "    daily_active = daily_w.filter(pl.col(\"is_active_day\"))\n",
    "    active_days_14 = int(daily_active.height)\n",
    "\n",
    "    cov_med = float(daily_active.select(pl.col(\"coverage_day\").median()).item() or 0.0)\n",
    "\n",
    "    dec = _decision(m5_ok, active_days_14, min_active_days_14, cov_med, gap_p90_excl)\n",
    "\n",
    "    decisions.append({\n",
    "        \"symbol\": sym,\n",
    "        \"decision\": dec,\n",
    "        \"m5_ok\": bool(m5_ok),\n",
    "        \"med_gap_small_sec\": float(med_gap_small),\n",
    "        \"class_inferred\": cls_kind,\n",
    "        \"window_hours_median\": float(window_hours_median),\n",
    "        \"weekend_share\": float(weekend_share),\n",
    "        \"active_days_14\": int(active_days_14),\n",
    "        \"min_active_days_14\": int(min_active_days_14),\n",
    "        \"coverage_median_active\": float(cov_med),\n",
    "        \"gap_p90_raw_sec\": float(gap_p90_raw),\n",
    "        \"gap_p90_excl_day_max_sec\": float(gap_p90_excl),\n",
    "        \"day_max_gap_p50_sec\": float(day_max_gap_p50),\n",
    "        \"crop_applied\": bool(crop_applied),\n",
    "        \"crop_days\": int(CROP_DAYS) if crop_applied else None,\n",
    "        \"crop_days_effective\": crop_days_effective,\n",
    "        \"active_ohlcv_path\": str(clean_paths[sym]),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"[Celda 02B] {sym} :: {dec} | m5_ok={m5_ok} med_gap_small={med_gap_small:.0f}s \"\n",
    "        f\"| class={cls_kind} win_h_med={window_hours_median:.1f} wk_share={weekend_share:.2%} \"\n",
    "        f\"| active_days_14={active_days_14}/{min_active_days_14} cov_med={cov_med:.3f} \"\n",
    "        f\"| gap_p90_excl={gap_p90_excl:.0f}s (raw={gap_p90_raw:.0f}s, day_max_p50={day_max_gap_p50:.0f}s) \"\n",
    "        f\"| crop={crop_applied}\"\n",
    "    )\n",
    "\n",
    "    if dec in (\"GO\", \"WARN\"):\n",
    "        final_symbols.append(sym)\n",
    "\n",
    "# Force continue (research)\n",
    "if ALLOW_FORCE_CONTINUE and FORCE_CONTINUE_SYMBOLS:\n",
    "    force = [str(s).upper().strip() for s in FORCE_CONTINUE_SYMBOLS if str(s).strip()]\n",
    "    for s in force:\n",
    "        if s in symbols and s not in final_symbols:\n",
    "            final_symbols.append(s)\n",
    "\n",
    "final_symbols = sorted(final_symbols)\n",
    "\n",
    "# Persistir paths activos (incluye crop)\n",
    "GLOBAL_STATE[\"data\"][\"ohlcv_clean_paths\"] = clean_paths\n",
    "GLOBAL_STATE[\"data\"][\"m5_ohlcv_paths\"] = dict(clean_paths)  # alias directo\n",
    "\n",
    "GLOBAL_STATE[\"data_quality\"] = {\n",
    "    \"final_symbols\": final_symbols,\n",
    "    \"decisions\": decisions,\n",
    "    \"params\": {\n",
    "        \"fail_on_no_go\": FAIL_ON_NO_GO,\n",
    "        \"replace_selected_with_final\": REPLACE_SELECTED_WITH_FINAL,\n",
    "        \"auto_crop_always\": AUTO_CROP_ALWAYS,\n",
    "        \"crop_days\": CROP_DAYS,\n",
    "        \"min_days_after_crop\": MIN_DAYS_AFTER_CROP,\n",
    "        \"lookback_days_for_stats\": LOOKBACK_DAYS_FOR_STATS,\n",
    "        \"window_cal_days\": WINDOW_CAL_DAYS,\n",
    "        \"min_bars_day_active\": MIN_BARS_DAY_ACTIVE,\n",
    "        \"gap_p90_excl_go_sec\": GAP_P90_EXCL_GO_SEC,\n",
    "        \"gap_p90_excl_warn_sec\": GAP_P90_EXCL_WARN_SEC,\n",
    "        \"cov_med_go\": COV_MED_GO,\n",
    "        \"cov_med_warn\": COV_MED_WARN,\n",
    "        \"m5_target_sec\": M5_TARGET_SEC,\n",
    "        \"m5_tol_sec\": M5_TOL_SEC,\n",
    "        \"max_small_gap_sec\": MAX_SMALL_GAP_SEC,\n",
    "    }\n",
    "}\n",
    "\n",
    "if REPLACE_SELECTED_WITH_FINAL:\n",
    "    GLOBAL_STATE.setdefault(\"universe\", {})\n",
    "    GLOBAL_STATE[\"universe\"][\"selected_symbols_TREND\"] = final_symbols\n",
    "\n",
    "# Snapshots\n",
    "dec_df = pl.DataFrame(decisions).sort(\"symbol\")\n",
    "dec_path = snap_dir / \"data_quality_decisions.parquet\"\n",
    "dec_df.write_parquet(str(dec_path), compression=\"zstd\")\n",
    "\n",
    "snap = {\n",
    "    \"cell\": \"02B\",\n",
    "    \"version\": \"v1.3\",\n",
    "    \"final_symbols\": final_symbols,\n",
    "    \"decisions_path\": str(dec_path),\n",
    "    \"notes\": [\n",
    "        \"Coverage bounded a [0,1].\",\n",
    "        \"gap_p90 excluye el gap mÃ¡ximo por dÃ­a (break-aware).\",\n",
    "        \"Timeframe sanity: med_gap_small ~300s para validar M5.\",\n",
    "    ],\n",
    "}\n",
    "snap_path = snap_dir / \"data_quality_snapshot.json\"\n",
    "_save_json(snap_path, snap)\n",
    "\n",
    "print(f\"\\nðŸ’¾ [Celda 02B] decisions -> {dec_path} (OK)\")\n",
    "print(f\"ðŸ’¾ [Celda 02B] snapshot  -> {snap_path} (OK)\")\n",
    "print(f\"[Celda 02B] FINAL_SYMBOLS = {final_symbols}\")\n",
    "\n",
    "if not final_symbols and FAIL_ON_NO_GO:\n",
    "    raise RuntimeError(\"[Celda 02B] NO_GO: ningÃºn sÃ­mbolo pasÃ³ QA (revisa data_quality_decisions.parquet).\")\n",
    "\n",
    "print(\">>> Celda 02B v1.3 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "108f9100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 02C :: Compat aliases (m5_ohlcv_paths)\n",
      "[Celda 02C] OK: m5_ohlcv_paths apunta a tus parquets limpios/activos.\n",
      ">>> Celda 02C :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 02C â€” Compat: alias m5_ohlcv_paths â†’ ohlcv_clean_paths =====================\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "\n",
    "print(\">>> Celda 02C :: Compat aliases (m5_ohlcv_paths)\")\n",
    "\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 02C] ERROR: GLOBAL_STATE no existe.\")\n",
    "\n",
    "data_state = GLOBAL_STATE.get(\"data\", {}) or GLOBAL_STATE.get(\"data\", {})\n",
    "# En tu pipeline actual, Celda 02 v2.2 guardÃ³ en GLOBAL_STATE[\"data\"] (no en root).\n",
    "# Normalizamos:\n",
    "if \"data\" not in GLOBAL_STATE or not isinstance(GLOBAL_STATE.get(\"data\"), dict):\n",
    "    GLOBAL_STATE[\"data\"] = {}\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "\n",
    "# Fuente preferida: paths ya â€œactivosâ€ (incluye crop si 02B lo aplicÃ³)\n",
    "src_paths = data_state.get(\"ohlcv_clean_paths\") or data_state.get(\"ohlcv_clean_paths\".lower()) or {}\n",
    "src_dir   = data_state.get(\"ohlcv_clean_dir\") or \"\"\n",
    "\n",
    "if not src_paths:\n",
    "    raise RuntimeError(\"[Celda 02C] ERROR: no encuentro ohlcv_clean_paths (ejecuta Celda 02 v2.2).\")\n",
    "\n",
    "# Alias esperados por Celda 05/07:\n",
    "data_state[\"m5_ohlcv_paths\"] = dict(src_paths)\n",
    "data_state[\"m5_ohlcv_dir\"] = str(Path(src_dir).resolve()) if src_dir else str(Path(list(src_paths.values())[0]).resolve().parent)\n",
    "\n",
    "print(\"[Celda 02C] OK: m5_ohlcv_paths apunta a tus parquets limpios/activos.\")\n",
    "print(\">>> Celda 02C :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6aa6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 03 v1.3.3 :: Cost Model INSTRUMENT-AWARE (class + spread + coherent fallback)\n",
      "[Celda 03] selected_TREND = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 03] COST_DEFAULTS = {'min_cost_bps': 0.0, 'max_cost_bps': 500.0, 'cost_reported_is_roundtrip': False, 'stress_mult_if_missing': 1.8, 'slippage_extra_bps_base': 0.0, 'slippage_extra_bps_stress': 0.0}\n",
      "[Celda 03] BNBUSD (CRYPTO) :: BASE=8.000bps [fallback_by_class:CRYPTO] | STRESS=16.000bps [fallback_by_class:CRYPTO]\n",
      "[Celda 03] BTCUSD (CRYPTO) :: BASE=8.000bps [fallback_by_class:CRYPTO] | STRESS=16.000bps [fallback_by_class:CRYPTO]\n",
      "[Celda 03] LVMH (EQUITY) :: BASE=12.000bps [fallback_by_class:EQUITY] | STRESS=25.000bps [fallback_by_class:EQUITY]\n",
      "[Celda 03] XAUAUD (METAL) :: BASE=4.000bps [fallback_by_class:METAL] | STRESS=8.000bps [fallback_by_class:METAL]\n",
      "[Celda 03] Gate 03 :: sanity tests OK\n",
      "[Celda 03] SNAPSHOT -> C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\execution_and_cost_model.json (OK)\n",
      ">>> Celda 03 v1.3.3 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 03 v1.3.3 â€” Cost Model INSTRUMENT-AWARE (class + spread + coherent fallback) =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 03 v1.3.3 :: Cost Model INSTRUMENT-AWARE (class + spread + coherent fallback)\")\n",
    "\n",
    "# ========================= Requisitos previos =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 03] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "for k in (\"run_snapshots\",):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 03] ERROR: falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "univ = (GLOBAL_STATE.get(\"universe\", {}) or {})\n",
    "selected = univ.get(\"selected_symbols_TREND\", []) or []\n",
    "if not selected:\n",
    "    raise RuntimeError(\"[Celda 03] ERROR: no hay selected_symbols_TREND. Ejecuta Celda 01/02B.\")\n",
    "\n",
    "selected_u = [str(s).upper().strip() for s in selected if str(s).strip()]\n",
    "print(f\"[Celda 03] selected_TREND = {selected_u}\")\n",
    "\n",
    "data_state = (GLOBAL_STATE.get(\"data\", {}) or {})\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 03] ERROR: no hay m5_ohlcv_paths/ohlcv_clean_paths. Ejecuta Celda 02/02B/02C.\")\n",
    "\n",
    "# normaliza keys a UPPER\n",
    "m5_paths_u = {str(k).upper().strip(): str(v) for k, v in m5_paths.items() if k is not None and v is not None}\n",
    "\n",
    "snap_dir = Path(paths[\"run_snapshots\"]).resolve()\n",
    "snap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= ConvenciÃ³n de ejecuciÃ³n (freeze) =========================\n",
    "EXECUTION_CONVENTION = {\n",
    "    \"signal_timestamp\": \"close(t)\",\n",
    "    \"fill_timestamp\": \"t+1\",\n",
    "    \"entry_price_rule\": \"open(t+1) if exists else close(t+1)\",\n",
    "    \"exit_price_rule\": \"open(t+1) if exists else close(t+1)\",\n",
    "    \"notes\": [\n",
    "        \"ConvenciÃ³n fija para todo cÃ¡lculo downstream.\",\n",
    "        \"El modelo de costos se aplica como roundtrip (entry+exit) si el costo es por-lado.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ========================= Config defaults =========================\n",
    "cfg = GLOBAL_STATE.get(\"config\", {}) or {}\n",
    "costs_cfg = (cfg.get(\"costs\", {}) or {})\n",
    "\n",
    "def _cfg_float(d: dict, k: str, default: float) -> float:\n",
    "    try:\n",
    "        return float(d.get(k, default))\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _cfg_bool(d: dict, k: str, default: bool) -> bool:\n",
    "    try:\n",
    "        v = d.get(k, default)\n",
    "        if isinstance(v, bool):\n",
    "            return v\n",
    "        if isinstance(v, str):\n",
    "            return v.strip().lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
    "        return bool(v)\n",
    "    except Exception:\n",
    "        return bool(default)\n",
    "\n",
    "COST_DEFAULTS = {\n",
    "    \"min_cost_bps\": _cfg_float(costs_cfg, \"min_cost_bps\", 0.0),\n",
    "    \"max_cost_bps\": _cfg_float(costs_cfg, \"max_cost_bps\", 500.0),\n",
    "    \"cost_reported_is_roundtrip\": _cfg_bool(costs_cfg, \"cost_reported_is_roundtrip_default\", False),\n",
    "    \"stress_mult_if_missing\": _cfg_float(costs_cfg, \"stress_mult_if_missing\", 1.80),\n",
    "    \"slippage_extra_bps_base\": _cfg_float(costs_cfg, \"slippage_extra_bps_base\", 0.0),\n",
    "    \"slippage_extra_bps_stress\": _cfg_float(costs_cfg, \"slippage_extra_bps_stress\", 0.0),\n",
    "}\n",
    "\n",
    "print(f\"[Celda 03] COST_DEFAULTS = {COST_DEFAULTS}\")\n",
    "\n",
    "def _clip_bps(x: float) -> float:\n",
    "    return float(min(max(float(x), float(COST_DEFAULTS[\"min_cost_bps\"])), float(COST_DEFAULTS[\"max_cost_bps\"])))\n",
    "\n",
    "def _bps_to_dec(bps: float) -> float:\n",
    "    return float(bps) / 10000.0\n",
    "\n",
    "def cost_roundtrip_dec(cost_bps: float, cost_reported_is_roundtrip: bool) -> float:\n",
    "    c = _bps_to_dec(cost_bps)\n",
    "    return c if cost_reported_is_roundtrip else (2.0 * c)\n",
    "\n",
    "def apply_cost_to_gross_return(gross_ret_dec: float, cost_bps: float, cost_reported_is_roundtrip: bool) -> float:\n",
    "    return float(gross_ret_dec) - cost_roundtrip_dec(cost_bps, cost_reported_is_roundtrip)\n",
    "\n",
    "# ========================= ClasificaciÃ³n de instrumento =========================\n",
    "_CRYPTO_PREFIX = (\"BTC\",\"ETH\",\"BNB\",\"SOL\",\"XRP\",\"ADA\",\"DOGE\",\"LTC\",\"AVAX\",\"DOT\",\"MATIC\",\"LINK\",\"TRX\")\n",
    "_METAL_PREFIX  = (\"XAU\",\"XAG\",\"XPT\",\"XPD\")\n",
    "\n",
    "def infer_asset_class(sym_u: str) -> str:\n",
    "    s = sym_u.upper().strip()\n",
    "    if s.startswith(_METAL_PREFIX):\n",
    "        return \"METAL\"\n",
    "    if s.startswith(_CRYPTO_PREFIX) or (\"USD\" in s and any(s.startswith(p) for p in _CRYPTO_PREFIX)):\n",
    "        return \"CRYPTO\"\n",
    "    if re.fullmatch(r\"[A-Z]{6}\", s):\n",
    "        return \"FX\"\n",
    "    # fallback institucional: si no es FX/crypto/metal, tratar como equity/other\n",
    "    return \"EQUITY\"\n",
    "\n",
    "# Fallback coherente por clase (bps, base/stress)\n",
    "FALLBACK_BPS = {\n",
    "    \"CRYPTO\": (8.0, 16.0),\n",
    "    \"FX\":     (1.2, 2.5),\n",
    "    \"METAL\":  (4.0, 8.0),\n",
    "    \"EQUITY\": (12.0, 25.0),\n",
    "}\n",
    "\n",
    "# ========================= DerivaciÃ³n desde spread (si existe) =========================\n",
    "def _load_tail_close_spread(sym_u: str, max_rows: int = 200_000) -> pl.DataFrame:\n",
    "    if sym_u not in m5_paths_u:\n",
    "        raise KeyError(f\"[Celda 03] ERROR: sÃ­mbolo {sym_u} no existe en m5_paths. keys_sample={list(m5_paths_u)[:10]}\")\n",
    "    p = Path(m5_paths_u[sym_u]).resolve()\n",
    "    # leer solo columnas necesarias\n",
    "    df = pl.read_parquet(str(p), columns=[c for c in (\"time_utc\",\"close\",\"spread\") if c in pl.read_parquet(str(p), n_rows=1).columns])\n",
    "    if \"close\" not in df.columns:\n",
    "        raise RuntimeError(f\"[Celda 03] ERROR: falta close en {sym_u}.\")\n",
    "    df = df.sort(\"time_utc\") if \"time_utc\" in df.columns else df\n",
    "    if df.height > max_rows:\n",
    "        df = df.tail(max_rows)\n",
    "    return df\n",
    "\n",
    "def _derive_cost_from_spread_bps(sym_u: str) -> tuple[float | None, float | None, str | None, str | None]:\n",
    "    # Devuelve: base_bps, stress_bps, base_src, stress_src\n",
    "    try:\n",
    "        df = _load_tail_close_spread(sym_u)\n",
    "    except Exception:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if \"spread\" not in df.columns:\n",
    "        return None, None, None, None\n",
    "\n",
    "    d = (\n",
    "        df.select([\n",
    "            pl.col(\"close\").cast(pl.Float64, strict=False).alias(\"close\"),\n",
    "            pl.col(\"spread\").cast(pl.Float64, strict=False).alias(\"spread\"),\n",
    "        ])\n",
    "        .filter(pl.col(\"close\") > 0)\n",
    "        .filter(pl.col(\"spread\").is_not_null() & (pl.col(\"spread\") >= 0))\n",
    "        .with_columns((pl.col(\"spread\") / pl.col(\"close\") * 10000.0).alias(\"spread_bps\"))\n",
    "        .filter(pl.col(\"spread_bps\").is_not_null())\n",
    "        .filter(pl.col(\"spread_bps\") >= 0)\n",
    "        .filter(pl.col(\"spread_bps\") <= 500)   # guardrail institucional\n",
    "    )\n",
    "\n",
    "    if d.height < 500:\n",
    "        return None, None, None, None\n",
    "\n",
    "    med = float(d.select(pl.col(\"spread_bps\").median()).item())\n",
    "    p90 = float(d.select(pl.col(\"spread_bps\").quantile(0.90, interpolation=\"nearest\")).item())\n",
    "\n",
    "    if not (math.isfinite(med) and 0 <= med <= 500):\n",
    "        return None, None, None, None\n",
    "    if not (math.isfinite(p90) and 0 <= p90 <= 500):\n",
    "        p90 = med * float(COST_DEFAULTS[\"stress_mult_if_missing\"])\n",
    "\n",
    "    return med, max(p90, med), \"derived_from_m5_spread_median\", \"derived_from_m5_spread_p90\"\n",
    "\n",
    "# ========================= ConstrucciÃ³n por sÃ­mbolo =========================\n",
    "costs_by_symbol: dict[str, dict] = {}\n",
    "slip_base = float(COST_DEFAULTS[\"slippage_extra_bps_base\"])\n",
    "slip_strs = float(COST_DEFAULTS[\"slippage_extra_bps_stress\"])\n",
    "stress_mult = float(COST_DEFAULTS[\"stress_mult_if_missing\"])\n",
    "is_roundtrip = bool(COST_DEFAULTS[\"cost_reported_is_roundtrip\"])\n",
    "\n",
    "for su in selected_u:\n",
    "    asset_class = infer_asset_class(su)\n",
    "\n",
    "    base_bps = None\n",
    "    stress_bps = None\n",
    "    base_src = None\n",
    "    stress_src = None\n",
    "\n",
    "    # A) intentar derivar de spread (si existe y es razonable)\n",
    "    b, s, bs, ss = _derive_cost_from_spread_bps(su)\n",
    "    if b is not None:\n",
    "        base_bps, stress_bps, base_src, stress_src = float(b), float(s), bs, ss\n",
    "\n",
    "    # B) fallback coherente por clase\n",
    "    if base_bps is None:\n",
    "        fb = FALLBACK_BPS.get(asset_class, (10.0, 20.0))\n",
    "        base_bps = float(fb[0])\n",
    "        stress_bps = float(fb[1])\n",
    "        base_src = f\"fallback_by_class:{asset_class}\"\n",
    "        stress_src = f\"fallback_by_class:{asset_class}\"\n",
    "\n",
    "    # C) si stress faltara, derivar\n",
    "    if stress_bps is None:\n",
    "        stress_bps = float(base_bps) * stress_mult\n",
    "        stress_src = f\"derived_mult:{stress_mult:.2f}\"\n",
    "\n",
    "    # D) slippage extra + clip\n",
    "    base_bps = _clip_bps(float(base_bps) + slip_base)\n",
    "    stress_bps = _clip_bps(float(stress_bps) + slip_strs)\n",
    "\n",
    "    # Guardrail: nunca â€œcrypto fallbackâ€ en equity\n",
    "    if asset_class == \"EQUITY\" and isinstance(base_src, str) and \"fallback_by_class:CRYPTO\" in base_src:\n",
    "        raise RuntimeError(f\"[Celda 03] ERROR: Equity {su} quedÃ³ clasificado/valuado como CRYPTO. Revisa infer_asset_class().\")\n",
    "\n",
    "    costs_by_symbol[su] = {\n",
    "        \"ASSET_CLASS\": asset_class,\n",
    "        \"COST_BASE_BPS\": float(base_bps),\n",
    "        \"COST_STRESS_BPS\": float(stress_bps),\n",
    "        \"COST_BASE_SOURCE\": str(base_src),\n",
    "        \"COST_STRESS_SOURCE\": str(stress_src),\n",
    "        \"SLIPPAGE_EXTRA_BPS_BASE\": float(slip_base),\n",
    "        \"SLIPPAGE_EXTRA_BPS_STRESS\": float(slip_strs),\n",
    "    }\n",
    "\n",
    "    print(f\"[Celda 03] {su} ({asset_class}) :: BASE={base_bps:.3f}bps [{base_src}] | STRESS={stress_bps:.3f}bps [{stress_src}]\")\n",
    "\n",
    "# ========================= Gate 03 â€” Sanity tests =========================\n",
    "for su in selected_u:\n",
    "    # usar close t0,t1 para validar net<=gross y cost=0 preserva\n",
    "    p = Path(m5_paths_u[su]).resolve()\n",
    "    dfp = pl.read_parquet(str(p), columns=[c for c in (\"time_utc\",\"close\") if c in pl.read_parquet(str(p), n_rows=1).columns]).sort(\"time_utc\")\n",
    "    if dfp.height < 3:\n",
    "        raise RuntimeError(f\"[Celda 03] ERROR: muy pocas filas para sanity ({su}).\")\n",
    "\n",
    "    c0 = float(dfp[\"close\"][0]); c1 = float(dfp[\"close\"][1])\n",
    "    if c0 <= 0 or c1 <= 0:\n",
    "        raise RuntimeError(f\"[Celda 03] ERROR: close invÃ¡lido en sanity ({su}).\")\n",
    "\n",
    "    gross = (c1 / c0) - 1.0\n",
    "    net0 = apply_cost_to_gross_return(gross, cost_bps=0.0, cost_reported_is_roundtrip=is_roundtrip)\n",
    "    if abs(net0 - gross) > 1e-12:\n",
    "        raise RuntimeError(f\"[Celda 03] GATE FAIL: cost=0 no preserva gross ({su}).\")\n",
    "\n",
    "    cb = float(costs_by_symbol[su][\"COST_BASE_BPS\"])\n",
    "    netb = apply_cost_to_gross_return(gross, cost_bps=cb, cost_reported_is_roundtrip=is_roundtrip)\n",
    "    if netb > gross + 1e-12:\n",
    "        raise RuntimeError(f\"[Celda 03] GATE FAIL: net>gross con costo ({su}). gross={gross}, net={netb}, cost_bps={cb}\")\n",
    "\n",
    "print(\"[Celda 03] Gate 03 :: sanity tests OK\")\n",
    "\n",
    "# ========================= Snapshot + GLOBAL_STATE =========================\n",
    "snapshot = {\n",
    "    \"execution_convention\": EXECUTION_CONVENTION,\n",
    "    \"cost_defaults\": COST_DEFAULTS,\n",
    "    \"cost_reported_is_roundtrip\": bool(is_roundtrip),\n",
    "    \"fallback_bps_by_class\": FALLBACK_BPS,\n",
    "    \"costs_by_symbol\": costs_by_symbol,\n",
    "    \"symbols\": costs_by_symbol,  # compat\n",
    "}\n",
    "\n",
    "snap_path = snap_dir / \"execution_and_cost_model.json\"\n",
    "snap_path.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"execution\", {})\n",
    "GLOBAL_STATE[\"execution\"][\"convention\"] = EXECUTION_CONVENTION\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"cost_model\", {})\n",
    "GLOBAL_STATE[\"cost_model\"][\"cost_reported_is_roundtrip\"] = bool(is_roundtrip)\n",
    "GLOBAL_STATE[\"cost_model\"][\"costs_by_symbol\"] = costs_by_symbol\n",
    "GLOBAL_STATE[\"cost_model\"][\"symbols\"] = costs_by_symbol\n",
    "GLOBAL_STATE[\"cost_model\"][\"snapshot_path\"] = str(snap_path)\n",
    "\n",
    "print(f\"[Celda 03] SNAPSHOT -> {snap_path} (OK)\")\n",
    "print(\">>> Celda 03 v1.3.3 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2991020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 04 v1.3 :: WFO folds (hard-consistency + multi-symbol safe)\n",
      "[Celda 04] selected_TREND = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 04] m5_paths keys sample = ['BNBUSD', 'XAUAUD', 'BTCUSD', 'LVMH']\n",
      "[Celda 04] BNBUSD :: tmin=2025-06-05 05:50:00+00:00 | tmax=2025-12-02 05:50:00+00:00 | rows=49040\n",
      "[Celda 04] BTCUSD :: tmin=2025-06-05 23:50:00+00:00 | tmax=2025-12-02 23:50:00+00:00 | rows=48551\n",
      "[Celda 04] LVMH :: tmin=2025-06-04 18:25:00+00:00 | tmax=2025-12-01 18:25:00+00:00 | rows=12929\n",
      "[Celda 04] XAUAUD :: tmin=2025-06-05 05:50:00+00:00 | tmax=2025-12-02 05:50:00+00:00 | rows=34788\n",
      "[Celda 04] GLOBAL common_range :: tmin=2025-06-05 23:50:00+00:00 | tmax=2025-12-01 18:25:00+00:00 | span_daysâ‰ˆ178.8\n",
      "[Celda 04] GLOBAL mode=SINGLE_FOLD_AUTO folds=1 degraded_minimums=True\n",
      "  - F1 :: IS[2025-06-06T18:25:00+00:00 -> 2025-10-09T18:25:00+00:00] | OOS[2025-10-16T18:25:00+00:00 -> 2025-12-01T18:25:00+00:00] | embargo=7d\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\wfo\\folds_wfo.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\folds_wfo.json (OK)\n",
      ">>> Celda 04 v1.3 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 04 v1.3 â€” WFO folds (hard-consistency + multi-symbol safe) =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 04 v1.3 :: WFO folds (hard-consistency + multi-symbol safe)\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 04] ERROR: GLOBAL_STATE no existe.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "for k in (\"artifacts\", \"run_snapshots\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 04] ERROR: falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "univ = (GLOBAL_STATE.get(\"universe\", {}) or {})\n",
    "selected = [str(s).upper().strip() for s in (univ.get(\"selected_symbols_TREND\", []) or []) if str(s).strip()]\n",
    "if not selected:\n",
    "    raise RuntimeError(\"[Celda 04] ERROR: selected_symbols_TREND vacÃ­o. Ejecuta 02B (REPLACE_SELECTED_WITH_FINAL=True).\")\n",
    "\n",
    "dq = (GLOBAL_STATE.get(\"data_quality\", {}) or {})\n",
    "dq_final = [str(s).upper().strip() for s in (dq.get(\"final_symbols\", []) or []) if str(s).strip()]\n",
    "\n",
    "# Hard-consistency: si existe data_quality.final_symbols, debe coincidir con selected\n",
    "if dq_final:\n",
    "    if set(dq_final) != set(selected):\n",
    "        raise RuntimeError(\n",
    "            \"[Celda 04] ERROR: mismatch entre universe.selected_symbols_TREND y data_quality.final_symbols.\\n\"\n",
    "            f\"  selected={selected}\\n\"\n",
    "            f\"  dq_final={dq_final}\\n\"\n",
    "            \"SoluciÃ³n: re-ejecuta Celda 02B v1.3 (o revisa si ejecutaste celdas fuera de orden).\"\n",
    "        )\n",
    "\n",
    "data_state = (GLOBAL_STATE.get(\"data\", {}) or {})\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 04] ERROR: falta m5_ohlcv_paths/ohlcv_clean_paths. Ejecuta Celda 02B/02C.\")\n",
    "\n",
    "# normaliza keys\n",
    "m5_paths_u = {str(k).upper().strip(): str(v) for k, v in m5_paths.items() if k is not None and v is not None}\n",
    "\n",
    "print(f\"[Celda 04] selected_TREND = {selected}\")\n",
    "print(f\"[Celda 04] m5_paths keys sample = {list(m5_paths_u.keys())[:8]}\")\n",
    "\n",
    "OUT_WFO_DIR = Path(paths[\"artifacts\"]).resolve() / \"wfo\"\n",
    "OUT_WFO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "snap_dir = Path(paths[\"run_snapshots\"]).resolve()\n",
    "snap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= ParÃ¡metros base =========================\n",
    "PREF = {\"n_folds\": 4, \"is_days\": 365, \"oos_days\": 90, \"embargo_days\": 7}\n",
    "FALLBACK = {\"n_folds\": 3, \"is_days\": 180, \"oos_days\": 60, \"embargo_days\": 7}\n",
    "\n",
    "MIN_OOS_DAYS_HARD = 30\n",
    "MIN_IS_DAYS_HARD = 60\n",
    "MIN_OOS_DAYS_TARGET = 60\n",
    "MIN_IS_DAYS_TARGET = 180\n",
    "\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _read_time_range(sym_u: str) -> tuple[datetime, datetime, int]:\n",
    "    if sym_u not in m5_paths_u:\n",
    "        raise KeyError(f\"[Celda 04] ERROR: no hay path para {sym_u} en m5_paths.\")\n",
    "    p = Path(m5_paths_u[sym_u]).resolve()\n",
    "    lf = pl.scan_parquet(str(p)).select([\n",
    "        pl.col(\"time_utc\").min().alias(\"tmin\"),\n",
    "        pl.col(\"time_utc\").max().alias(\"tmax\"),\n",
    "        pl.len().alias(\"n\"),\n",
    "    ])\n",
    "    out = lf.collect()\n",
    "    tmin = out[\"tmin\"][0]; tmax = out[\"tmax\"][0]; n = int(out[\"n\"][0])\n",
    "    if tmin is None or tmax is None:\n",
    "        raise RuntimeError(f\"[Celda 04] ERROR: rango temporal invÃ¡lido ({sym_u}).\")\n",
    "    return (_ensure_utc(tmin), _ensure_utc(tmax), n)\n",
    "\n",
    "def _required_span(n_folds: int, is_days: int, oos_days: int, emb_days: int) -> float:\n",
    "    return float(n_folds) * (float(is_days) + float(oos_days) + float(emb_days))\n",
    "\n",
    "def _build_folds_for_range(tmin: datetime, tmax: datetime, n_folds: int, is_days: int, oos_days: int, emb_days: int) -> list[dict]:\n",
    "    folds = []\n",
    "    oos_end = tmax\n",
    "    for i in range(int(n_folds)):\n",
    "        oos_start = oos_end - timedelta(days=int(oos_days))\n",
    "        is_end = oos_start - timedelta(days=int(emb_days))\n",
    "        is_start = is_end - timedelta(days=int(is_days))\n",
    "        if is_start < tmin:\n",
    "            break\n",
    "        folds.append({\n",
    "            \"fold_id\": f\"F{i+1}\",\n",
    "            \"IS_start\": is_start.isoformat(),\n",
    "            \"IS_end\": is_end.isoformat(),\n",
    "            \"embargo_days\": int(emb_days),\n",
    "            \"OOS_start\": oos_start.isoformat(),\n",
    "            \"OOS_end\": oos_end.isoformat(),\n",
    "            \"is_days\": int(is_days),\n",
    "            \"oos_days\": int(oos_days),\n",
    "        })\n",
    "        oos_end = oos_start\n",
    "    return folds\n",
    "\n",
    "def _auto_params(span_days: float) -> dict:\n",
    "    emb = int(FALLBACK[\"embargo_days\"])\n",
    "    oos = int(max(MIN_OOS_DAYS_HARD, min(90, round(span_days * 0.20))))\n",
    "    is_guess = int(max(MIN_IS_DAYS_HARD, round(span_days * 0.70)))\n",
    "    is_days = int(min(is_guess, max(MIN_IS_DAYS_HARD, int(span_days) - oos - emb)))\n",
    "    oos_days = int(max(MIN_OOS_DAYS_HARD, int(span_days) - is_days - emb))\n",
    "    if oos_days < 10:\n",
    "        oos_days = 10\n",
    "        is_days = int(max(10, int(span_days) - oos_days - emb))\n",
    "    return {\"n_folds\": 1, \"is_days\": is_days, \"oos_days\": oos_days, \"embargo_days\": emb}\n",
    "\n",
    "def _gate_folds(folds: list[dict], label: str) -> dict:\n",
    "    degraded = False\n",
    "    for f in folds:\n",
    "        is_s = _ensure_utc(datetime.fromisoformat(f[\"IS_start\"]))\n",
    "        is_e = _ensure_utc(datetime.fromisoformat(f[\"IS_end\"]))\n",
    "        o_s  = _ensure_utc(datetime.fromisoformat(f[\"OOS_start\"]))\n",
    "        o_e  = _ensure_utc(datetime.fromisoformat(f[\"OOS_end\"]))\n",
    "        is_len = (is_e - is_s).total_seconds() / 86400.0\n",
    "        o_len  = (o_e - o_s).total_seconds() / 86400.0\n",
    "        if is_len + 1e-9 < MIN_IS_DAYS_HARD or o_len + 1e-9 < MIN_OOS_DAYS_HARD:\n",
    "            raise RuntimeError(f\"[Celda 04] GATE FAIL ({label}): fold demasiado corto. IS={is_len:.1f}d OOS={o_len:.1f}d\")\n",
    "        if is_len + 1e-9 < MIN_IS_DAYS_TARGET or o_len + 1e-9 < MIN_OOS_DAYS_TARGET:\n",
    "            degraded = True\n",
    "    return {\"degraded_minimums\": degraded}\n",
    "\n",
    "# ========================= Rango por sÃ­mbolo + folds por sÃ­mbolo =========================\n",
    "ranges = []\n",
    "for sym in selected:\n",
    "    tmin, tmax, n = _read_time_range(sym)\n",
    "    ranges.append((sym, tmin, tmax, n))\n",
    "    print(f\"[Celda 04] {sym} :: tmin={tmin} | tmax={tmax} | rows={n}\")\n",
    "\n",
    "folds_by_symbol: dict[str, list[dict]] = {}\n",
    "params_by_symbol: dict[str, dict] = {}\n",
    "flags_by_symbol: dict[str, dict] = {}\n",
    "\n",
    "for sym, tmin, tmax, _ in ranges:\n",
    "    span_days = (tmax - tmin).total_seconds() / 86400.0\n",
    "    pref_need = _required_span(PREF[\"n_folds\"], PREF[\"is_days\"], PREF[\"oos_days\"], PREF[\"embargo_days\"])\n",
    "    fb_need   = _required_span(FALLBACK[\"n_folds\"], FALLBACK[\"is_days\"], FALLBACK[\"oos_days\"], FALLBACK[\"embargo_days\"])\n",
    "\n",
    "    if span_days >= pref_need:\n",
    "        used = dict(PREF); mode = \"WFO_pref\"\n",
    "    elif span_days >= fb_need:\n",
    "        used = dict(FALLBACK); mode = \"WFO_fallback\"\n",
    "    else:\n",
    "        used = _auto_params(span_days); mode = \"SINGLE_FOLD_AUTO\"\n",
    "\n",
    "    folds = _build_folds_for_range(tmin, tmax, used[\"n_folds\"], used[\"is_days\"], used[\"oos_days\"], used[\"embargo_days\"])\n",
    "    if len(folds) < 1:\n",
    "        raise RuntimeError(f\"[Celda 04] GATE FAIL ({sym}): no se pudo construir ni 1 fold. span_daysâ‰ˆ{span_days:.1f}\")\n",
    "\n",
    "    q = _gate_folds(folds, label=sym)\n",
    "\n",
    "    folds_by_symbol[sym] = folds\n",
    "    params_by_symbol[sym] = {\"mode\": mode, \"params_used\": used, \"span_days\": float(span_days)}\n",
    "    flags_by_symbol[sym] = q\n",
    "\n",
    "# ========================= Folds globales (intersecciÃ³n) para compat =========================\n",
    "global_tmin = max(r[1] for r in ranges)\n",
    "global_tmax = min(r[2] for r in ranges)\n",
    "if global_tmax <= global_tmin:\n",
    "    raise RuntimeError(\"[Celda 04] ERROR: rango comÃºn invÃ¡lido (tmax<=tmin).\")\n",
    "\n",
    "span_days_g = (global_tmax - global_tmin).total_seconds() / 86400.0\n",
    "pref_need_g = _required_span(PREF[\"n_folds\"], PREF[\"is_days\"], PREF[\"oos_days\"], PREF[\"embargo_days\"])\n",
    "fb_need_g   = _required_span(FALLBACK[\"n_folds\"], FALLBACK[\"is_days\"], FALLBACK[\"oos_days\"], FALLBACK[\"embargo_days\"])\n",
    "\n",
    "if span_days_g >= pref_need_g:\n",
    "    used_g = dict(PREF); mode_g = \"WFO_pref\"\n",
    "elif span_days_g >= fb_need_g:\n",
    "    used_g = dict(FALLBACK); mode_g = \"WFO_fallback\"\n",
    "else:\n",
    "    used_g = _auto_params(span_days_g); mode_g = \"SINGLE_FOLD_AUTO\"\n",
    "\n",
    "folds_global = _build_folds_for_range(global_tmin, global_tmax, used_g[\"n_folds\"], used_g[\"is_days\"], used_g[\"oos_days\"], used_g[\"embargo_days\"])\n",
    "if len(folds_global) < 1:\n",
    "    raise RuntimeError(f\"[Celda 04] GATE FAIL (GLOBAL): no se pudo construir ni 1 fold. span_daysâ‰ˆ{span_days_g:.1f}\")\n",
    "\n",
    "qg = _gate_folds(folds_global, label=\"GLOBAL\")\n",
    "\n",
    "print(f\"[Celda 04] GLOBAL common_range :: tmin={global_tmin} | tmax={global_tmax} | span_daysâ‰ˆ{span_days_g:.1f}\")\n",
    "print(f\"[Celda 04] GLOBAL mode={mode_g} folds={len(folds_global)} degraded_minimums={qg['degraded_minimums']}\")\n",
    "for f in folds_global:\n",
    "    print(f\"  - {f['fold_id']} :: IS[{f['IS_start']} -> {f['IS_end']}] | OOS[{f['OOS_start']} -> {f['OOS_end']}] | embargo={f['embargo_days']}d\")\n",
    "\n",
    "# ========================= Persistencia =========================\n",
    "wfo_payload = {\n",
    "    \"symbols\": selected,\n",
    "    \"folds_global\": folds_global,\n",
    "    \"folds_by_symbol\": folds_by_symbol,\n",
    "    \"common_range_global\": {\"tmin\": global_tmin.isoformat(), \"tmax\": global_tmax.isoformat(), \"span_days\": span_days_g},\n",
    "    \"global_mode\": mode_g,\n",
    "    \"global_params_used\": used_g,\n",
    "    \"by_symbol_meta\": params_by_symbol,\n",
    "    \"by_symbol_quality\": flags_by_symbol,\n",
    "    \"rules\": [\n",
    "        \"Calibrar SOLO en IS.\",\n",
    "        \"Aplicar thresholds frozen en OOS.\",\n",
    "        \"No usar OOS en calibraciÃ³n.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "out_path = OUT_WFO_DIR / \"folds_wfo.json\"\n",
    "out_path.write_text(json.dumps(wfo_payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "snap_path = snap_dir / \"folds_wfo.json\"\n",
    "snap_path.write_text(json.dumps(wfo_payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"wfo\", {})\n",
    "GLOBAL_STATE[\"wfo\"][\"folds_path\"] = str(out_path)\n",
    "GLOBAL_STATE[\"wfo\"][\"folds\"] = folds_global                 # compat: lista\n",
    "GLOBAL_STATE[\"wfo\"][\"folds_by_symbol\"] = folds_by_symbol    # robust multi-asset\n",
    "GLOBAL_STATE[\"wfo\"][\"common_range\"] = wfo_payload[\"common_range_global\"]\n",
    "GLOBAL_STATE[\"wfo\"][\"params_used\"] = used_g\n",
    "GLOBAL_STATE[\"wfo\"][\"quality_flags\"] = {\"mode\": mode_g, **qg}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_path} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_path} (OK)\")\n",
    "print(\">>> Celda 04 v1.3 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58ac1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 05 v1.1 :: Features causales (TREND, M5) [OHLCV-aware + state robust + ATR]\n",
      "[Celda 05] selected_TREND          = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 05] data paths key used    = m5_ohlcv_paths\n",
      "[Celda 05] OUT_FEATURES_BASE      = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\features_base_v11\n",
      "[Celda 05] Gate 05 :: spot-check anti-lookahead OK (sin cambios en filas previas).\n",
      "[Celda 05] BNBUSD :: features saved â†’ BNBUSD_features_base.parquet | rows=49040 | cols=30\n",
      "[Celda 05] BTCUSD :: features saved â†’ BTCUSD_features_base.parquet | rows=48551 | cols=30\n",
      "[Celda 05] LVMH :: features saved â†’ LVMH_features_base.parquet | rows=12929 | cols=30\n",
      "[Celda 05] XAUAUD :: features saved â†’ XAUAUD_features_base.parquet | rows=34788 | cols=30\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\features\\features_base_v11 (n_files=4)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\features_base_v11_meta.json (OK)\n",
      ">>> Celda 05 v1.1 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 05 v1.1 â€” Features Causales (TREND, M5) [OHLCV-aware + state robust + ATR] =====================\n",
    "# OBJETIVO:\n",
    "#   1) Construir features base *causales* sobre series M5 limpias (Celda 02 v2.0).\n",
    "#   2) Features:\n",
    "#       - returns (logret, ret)\n",
    "#       - volatilidad rolling (std de logret)\n",
    "#       - momentum (ratio close/close.shift(L) - 1)\n",
    "#       - ER_kaufman + PD_kaufman (=1-ER)\n",
    "#       - ATR (Wilder) si hay OHLC\n",
    "#   3) Persistir features por sÃ­mbolo en artifacts/features/features_base_v11/\n",
    "#\n",
    "# CAUSALIDAD:\n",
    "#   - Ventanas rolling usan SOLO pasado (t y anteriores).\n",
    "#   - Nada de shift(-1) ni ventanas centradas.\n",
    "#   - DiseÃ±ado para seÃ±al al close(t) y ejecuciÃ³n t+1 (Celda 03).\n",
    "#\n",
    "# GATE 05:\n",
    "#   - Spot-check anti-lookahead: perturbar Ãºltimo close y verificar que filas previas no cambian.\n",
    "# ========================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 05 v1.1 :: Features causales (TREND, M5) [OHLCV-aware + state robust + ATR]\")\n",
    "\n",
    "# ========================= Validaciones de estado =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 05] ERROR: GLOBAL_STATE no existe.\")\n",
    "\n",
    "for k in (\"paths\", \"data\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 05] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "uni_state = GLOBAL_STATE[\"universe\"]\n",
    "\n",
    "selected = (uni_state.get(\"selected_symbols_TREND\") or [])\n",
    "if not selected:\n",
    "    raise RuntimeError(\"[Celda 05] ERROR: no hay selected_symbols_TREND. Ejecuta Celda 01.\")\n",
    "\n",
    "# Compatibilidad: aceptar m5_ohlcv_paths o m5_clean_paths\n",
    "m5_paths = data_state.get(\"m5_ohlcv_paths\") or data_state.get(\"m5_clean_paths\") or {}\n",
    "m5_key_used = \"m5_ohlcv_paths\" if data_state.get(\"m5_ohlcv_paths\") else (\"m5_clean_paths\" if data_state.get(\"m5_clean_paths\") else \"NONE\")\n",
    "if not isinstance(m5_paths, dict) or not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 05] ERROR: no hay m5_ohlcv_paths ni m5_clean_paths en GLOBAL_STATE. Ejecuta Celda 02 v2.0.\")\n",
    "\n",
    "# Output dir (no pisar v1.0)\n",
    "OUT_FEATURES_BASE = Path(paths[\"artifacts\"]).resolve() / \"features\" / \"features_base_v11\"\n",
    "OUT_FEATURES_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Celda 05] selected_TREND          = {selected}\")\n",
    "print(f\"[Celda 05] data paths key used    = {m5_key_used}\")\n",
    "print(f\"[Celda 05] OUT_FEATURES_BASE      = {OUT_FEATURES_BASE}\")\n",
    "\n",
    "# ========================= ParÃ¡metros =========================\n",
    "# Ventanas tÃ­picas M5:\n",
    "BARS_1H  = 12\n",
    "BARS_4H  = 48\n",
    "BARS_1D  = 288\n",
    "BARS_7D  = 2016\n",
    "\n",
    "# ER Kaufman\n",
    "ER_N_FAST = 10\n",
    "ER_N_MAIN = 20\n",
    "\n",
    "# Vol windows (std logret)\n",
    "VOL_W1 = BARS_4H\n",
    "VOL_W2 = BARS_1D\n",
    "VOL_W3 = BARS_7D\n",
    "\n",
    "# Momentum windows (ratio)\n",
    "MOM_W1 = BARS_4H\n",
    "MOM_W2 = BARS_1D\n",
    "MOM_W3 = BARS_7D\n",
    "\n",
    "# ATR (Wilder)\n",
    "ATR_N = 72  # ~6h\n",
    "\n",
    "# Spot-check anti-lookahead\n",
    "SPOTCHECK_ENABLE = True\n",
    "SPOTCHECK_MAX_SYMBOLS = 1\n",
    "SPOTCHECK_PERTURB_MULT = 1.25\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_contract(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Requeridos mÃ­nimos\n",
    "    if \"time_utc\" not in df.columns:\n",
    "        raise RuntimeError(\"[Celda 05] ERROR: falta time_utc en dataset.\")\n",
    "    if \"close\" not in df.columns:\n",
    "        raise RuntimeError(\"[Celda 05] ERROR: falta close en dataset.\")\n",
    "\n",
    "    # Tipos + orden + dedupe\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "        pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "    ])\n",
    "\n",
    "    # OHLC opcional pero si existe lo tipamos\n",
    "    for c in (\"open\", \"high\", \"low\"):\n",
    "        if c in df.columns:\n",
    "            df = df.with_columns(pl.col(c).cast(pl.Float64, strict=False))\n",
    "\n",
    "    df = df.sort(\"time_utc\").unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    return df\n",
    "\n",
    "def _alpha_wilder(n: int) -> float:\n",
    "    # Wilder smoothing alpha\n",
    "    return 1.0 / float(n)\n",
    "\n",
    "def _compute_atr_wilder(df: pl.DataFrame, n: int) -> pl.Series | None:\n",
    "    # ATR requiere OHLC\n",
    "    if not all(c in df.columns for c in (\"high\", \"low\", \"close\")):\n",
    "        return None\n",
    "\n",
    "    # True Range: max(high-low, abs(high-prev_close), abs(low-prev_close))\n",
    "    prev_close = pl.col(\"close\").shift(1)\n",
    "    tr = pl.max_horizontal([\n",
    "        (pl.col(\"high\") - pl.col(\"low\")).abs(),\n",
    "        (pl.col(\"high\") - prev_close).abs(),\n",
    "        (pl.col(\"low\") - prev_close).abs(),\n",
    "    ]).alias(\"__tr__\")\n",
    "\n",
    "    # Wilder ATR = ewm_mean(alpha=1/n, adjust=False) sobre TR\n",
    "    a = _alpha_wilder(n)\n",
    "    atr_expr = tr.ewm_mean(alpha=a, adjust=False).alias(f\"atr_{n}\")\n",
    "    out = df.select([atr_expr]).get_column(f\"atr_{n}\")\n",
    "    return out\n",
    "\n",
    "def _compute_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = _ensure_contract(df)\n",
    "\n",
    "    # Returns\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"close\") / pl.col(\"close\").shift(1)).log().alias(\"logret_1\"),\n",
    "        (pl.col(\"close\") / pl.col(\"close\").shift(1) - 1.0).alias(\"ret_1\"),\n",
    "    ])\n",
    "\n",
    "    # Volatilidad rolling (std) sobre logret (Polars: min_samples)\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"logret_1\").rolling_std(window_size=VOL_W1, min_samples=max(10, VOL_W1 // 5)).alias(f\"vol_logret_{VOL_W1}\"),\n",
    "        pl.col(\"logret_1\").rolling_std(window_size=VOL_W2, min_samples=max(20, VOL_W2 // 10)).alias(f\"vol_logret_{VOL_W2}\"),\n",
    "        pl.col(\"logret_1\").rolling_std(window_size=VOL_W3, min_samples=max(50, VOL_W3 // 20)).alias(f\"vol_logret_{VOL_W3}\"),\n",
    "    ])\n",
    "\n",
    "    # Momentum (ratio)\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"close\") / pl.col(\"close\").shift(MOM_W1) - 1.0).alias(f\"mom_{MOM_W1}\"),\n",
    "        (pl.col(\"close\") / pl.col(\"close\").shift(MOM_W2) - 1.0).alias(f\"mom_{MOM_W2}\"),\n",
    "        (pl.col(\"close\") / pl.col(\"close\").shift(MOM_W3) - 1.0).alias(f\"mom_{MOM_W3}\"),\n",
    "    ])\n",
    "\n",
    "    # Kaufman ER\n",
    "    abs_diff_1 = (pl.col(\"close\") - pl.col(\"close\").shift(1)).abs()\n",
    "\n",
    "    # fast\n",
    "    change_fast = (pl.col(\"close\") - pl.col(\"close\").shift(ER_N_FAST)).abs()\n",
    "    vol_fast = abs_diff_1.rolling_sum(window_size=ER_N_FAST, min_samples=ER_N_FAST).alias(\"__k_vol_fast__\")\n",
    "    df = df.with_columns([vol_fast])\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"__k_vol_fast__\") > 0.0)\n",
    "          .then((change_fast / pl.col(\"__k_vol_fast__\")).clip(0.0, 1.0))\n",
    "          .otherwise(None)\n",
    "          .alias(f\"ER_kaufman_{ER_N_FAST}\")\n",
    "    ]).with_columns([\n",
    "        (1.0 - pl.col(f\"ER_kaufman_{ER_N_FAST}\")).alias(f\"PD_kaufman_{ER_N_FAST}\")\n",
    "    ])\n",
    "\n",
    "    # main\n",
    "    change_main = (pl.col(\"close\") - pl.col(\"close\").shift(ER_N_MAIN)).abs()\n",
    "    vol_main = abs_diff_1.rolling_sum(window_size=ER_N_MAIN, min_samples=ER_N_MAIN).alias(\"__k_vol_main__\")\n",
    "    df = df.with_columns([vol_main])\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"__k_vol_main__\") > 0.0)\n",
    "          .then((change_main / pl.col(\"__k_vol_main__\")).clip(0.0, 1.0))\n",
    "          .otherwise(None)\n",
    "          .alias(f\"ER_kaufman_{ER_N_MAIN}\")\n",
    "    ]).with_columns([\n",
    "        (1.0 - pl.col(f\"ER_kaufman_{ER_N_MAIN}\")).alias(f\"PD_kaufman_{ER_N_MAIN}\")\n",
    "    ])\n",
    "\n",
    "    # Canon aliases\n",
    "    df = df.with_columns([\n",
    "        pl.col(f\"ER_kaufman_{ER_N_MAIN}\").alias(\"ER_kaufman\"),\n",
    "        pl.col(f\"PD_kaufman_{ER_N_MAIN}\").alias(\"PD_kaufman\"),\n",
    "    ])\n",
    "\n",
    "    # ATR (si OHLC)\n",
    "    atr_series = _compute_atr_wilder(df, ATR_N)\n",
    "    if atr_series is not None:\n",
    "        df = df.with_columns(pl.Series(name=f\"atr_{ATR_N}\", values=atr_series))\n",
    "        df = df.with_columns(pl.col(f\"atr_{ATR_N}\").alias(\"atr\"))\n",
    "\n",
    "    # Limpieza helpers\n",
    "    drop_cols = [c for c in (\"__k_vol_fast__\", \"__k_vol_main__\") if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(drop_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "def _spotcheck_no_lookahead(df_raw: pl.DataFrame) -> None:\n",
    "    df_a = _compute_features(df_raw)\n",
    "    if df_a.height < 1000:\n",
    "        print(\"[Celda 05][Gate] Spot-check omitido: muy pocas filas.\")\n",
    "        return\n",
    "\n",
    "    df_b = df_raw.clone()\n",
    "    last_close = df_b.select(pl.col(\"close\").tail(1)).item()\n",
    "    if last_close is None or not math.isfinite(float(last_close)):\n",
    "        print(\"[Celda 05][Gate] Spot-check omitido: last_close no finito.\")\n",
    "        return\n",
    "\n",
    "    # perturbar Ãºltimo close\n",
    "    n = df_b.height\n",
    "    df_b = df_b.with_columns([\n",
    "        pl.when(pl.arange(0, n) == (n - 1))\n",
    "          .then(pl.lit(float(last_close) * float(SPOTCHECK_PERTURB_MULT)))\n",
    "          .otherwise(pl.col(\"close\"))\n",
    "          .alias(\"close\")\n",
    "    ])\n",
    "\n",
    "    df_b = _compute_features(df_b)\n",
    "\n",
    "    # comparar todas las filas excepto la Ãºltima\n",
    "    df_a2 = df_a.slice(0, n - 1)\n",
    "    df_b2 = df_b.slice(0, n - 1)\n",
    "\n",
    "    feat_cols = [c for c in df_a2.columns if c not in (\"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"spread\")]\n",
    "    eps = 1e-12\n",
    "    diffs = []\n",
    "\n",
    "    for c in feat_cols:\n",
    "        a = df_a2.get_column(c)\n",
    "        b = df_b2.get_column(c)\n",
    "        if a.dtype in (pl.Float32, pl.Float64):\n",
    "            cnt = (\n",
    "                pl.DataFrame({\"a\": a, \"b\": b})\n",
    "                .with_columns((pl.col(\"a\") - pl.col(\"b\")).abs().alias(\"d\"))\n",
    "                .filter(pl.col(\"d\").is_not_null() & (pl.col(\"d\") > eps))\n",
    "                .height\n",
    "            )\n",
    "        else:\n",
    "            cnt = int((a != b).sum() or 0)\n",
    "\n",
    "        if cnt > 0:\n",
    "            diffs.append((c, int(cnt)))\n",
    "\n",
    "    if diffs:\n",
    "        msg = \"[Celda 05] GATE FAIL: posible lookahead (features cambiaron antes del Ãºltimo bar).\\n\"\n",
    "        msg += \"Columnas afectadas (top 10):\\n\" + \"\\n\".join([f\"  - {c}: {k} diffs\" for c, k in diffs[:10]])\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "    print(\"[Celda 05] Gate 05 :: spot-check anti-lookahead OK (sin cambios en filas previas).\")\n",
    "\n",
    "# ========================= ConstrucciÃ³n y persistencia =========================\n",
    "features_paths: dict[str, str] = {}\n",
    "features_meta: dict[str, dict] = {}\n",
    "\n",
    "for sym in selected:\n",
    "    sym_u = sym.upper().strip()\n",
    "    src_path = m5_paths.get(sym_u) or m5_paths.get(sym)  # tolerancia\n",
    "    if not src_path:\n",
    "        # fallback case-insensitive\n",
    "        keys = {k.upper(): k for k in m5_paths.keys()}\n",
    "        if sym_u in keys:\n",
    "            src_path = m5_paths[keys[sym_u]]\n",
    "        else:\n",
    "            raise RuntimeError(f\"[Celda 05] ERROR: no encuentro dataset para {sym_u} en m5_paths.\")\n",
    "\n",
    "    df_raw = pl.read_parquet(str(src_path))\n",
    "    df_raw = _ensure_contract(df_raw)\n",
    "\n",
    "    if SPOTCHECK_ENABLE and (len(features_paths) < SPOTCHECK_MAX_SYMBOLS):\n",
    "        _spotcheck_no_lookahead(df_raw)\n",
    "\n",
    "    df_feat = _compute_features(df_raw)\n",
    "\n",
    "    out_path = OUT_FEATURES_BASE / f\"{sym_u}_features_base.parquet\"\n",
    "    df_feat.write_parquet(str(out_path), compression=\"zstd\")\n",
    "    if (not out_path.exists()) or out_path.stat().st_size == 0:\n",
    "        raise RuntimeError(f\"[Celda 05] ERROR: no se pudo escribir {out_path}\")\n",
    "\n",
    "    features_paths[sym_u] = str(out_path)\n",
    "    features_meta[sym_u] = {\n",
    "        \"symbol\": sym_u,\n",
    "        \"source_m5_path\": str(src_path),\n",
    "        \"rows\": int(df_feat.height),\n",
    "        \"tmin\": str(df_feat.select(pl.col(\"time_utc\").min()).item()),\n",
    "        \"tmax\": str(df_feat.select(pl.col(\"time_utc\").max()).item()),\n",
    "        \"ER_N_FAST\": int(ER_N_FAST),\n",
    "        \"ER_N_MAIN\": int(ER_N_MAIN),\n",
    "        \"VOL_WINDOWS\": [int(VOL_W1), int(VOL_W2), int(VOL_W3)],\n",
    "        \"MOM_WINDOWS\": [int(MOM_W1), int(MOM_W2), int(MOM_W3)],\n",
    "        \"ATR_N\": int(ATR_N) if \"atr\" in df_feat.columns else None,\n",
    "        \"has_ohlc\": all(c in df_raw.columns for c in (\"open\",\"high\",\"low\",\"close\")),\n",
    "    }\n",
    "\n",
    "    print(f\"[Celda 05] {sym_u} :: features saved â†’ {out_path.name} | rows={df_feat.height} | cols={len(df_feat.columns)}\")\n",
    "\n",
    "# Snapshot\n",
    "snap_dir = Path(paths[\"run_snapshots\"]).resolve()\n",
    "snap_dir.mkdir(parents=True, exist_ok=True)\n",
    "meta_path = snap_dir / \"features_base_v11_meta.json\"\n",
    "meta_path.write_text(json.dumps(features_meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"features\", {})\n",
    "GLOBAL_STATE[\"features\"][\"features_base_dir\"] = str(OUT_FEATURES_BASE)\n",
    "GLOBAL_STATE[\"features\"][\"features_base_paths\"] = features_paths\n",
    "GLOBAL_STATE[\"features\"][\"features_base_meta_path\"] = str(meta_path)\n",
    "GLOBAL_STATE[\"features\"][\"features_base_version\"] = \"v1.1\"\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_FEATURES_BASE} (n_files={len(features_paths)})\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {meta_path} (OK)\")\n",
    "print(\">>> Celda 05 v1.1 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67e5f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 06 v1.0 :: Regime Gate por fold (TREND, M5) [IS-only calibration, no leakage]\n",
      "[Celda 06] symbols = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 06] folds   = ['F1']\n",
      "[Celda 06] BNBUSD F1 :: status=OK | scheme=BASE | cov_IS=0.166 cov_OOS=0.086 | thr_ER=0.24351851851851655 thr_MOM=0.007062234009745261 thr_VOL=0.0020915961985700417\n",
      "[Celda 06] BTCUSD F1 :: status=OK | scheme=BASE | cov_IS=0.165 cov_OOS=0.074 | thr_ER=0.22960119184047928 thr_MOM=0.0031787892661669925 thr_VOL=0.0013088198411497986\n",
      "[Celda 06] LVMH F1 :: status=OK | scheme=BASE | cov_IS=0.160 cov_OOS=0.153 | thr_ER=0.2665832290362983 thr_MOM=0.008020938872002725 thr_VOL=0.0022830000387667666\n",
      "[Celda 06] XAUAUD F1 :: status=OK | scheme=BASE | cov_IS=0.168 cov_OOS=0.067 | thr_ER=0.2361707501894501 thr_MOM=0.0024392239682979877 thr_VOL=0.0006660874714588537\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\regime_gates\\regime_gate_by_fold.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\regime_gate_by_fold.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\regime_gate_snapshot.json (OK)\n",
      ">>> Celda 06 v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 06 v1.0 â€” Regime Gate por Fold (TREND, M5) [IS-only calibration, no leakage] =====================\n",
    "# OBJETIVO:\n",
    "#   - Calibrar un \"regime gate\" para TREND usando SOLO el tramo IS de cada fold (WFO).\n",
    "#   - Gate basado en features causales ya calculadas (Celda 05): ER_kaufman, mom_1D, vol_logret_1D (y opcional ATR).\n",
    "#   - Medir cobertura (share de barras donde gate=True) en IS y OOS como diagnÃ³stico.\n",
    "#\n",
    "# PRODUCE:\n",
    "#   - artifacts/regime_gates/regime_gate_by_fold.parquet\n",
    "#   - snapshots/regime_gate_by_fold.parquet + .json\n",
    "#   - GLOBAL_STATE[\"regime_gate\"] con paths + thresholds por sÃ­mbolo/fold\n",
    "#\n",
    "# NOTA:\n",
    "#   - No mete sÃ­mbolos fuera del universo: usa data_quality.final_symbols si existe.\n",
    "#   - No usa datos futuros: todo threshold sale de IS, luego se evalÃºa en OOS.\n",
    "# =========================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 06 v1.0 :: Regime Gate por fold (TREND, M5) [IS-only calibration, no leakage]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 06] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"wfo\", \"features\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 06] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "wfo_state = GLOBAL_STATE[\"wfo\"]\n",
    "feat_state = GLOBAL_STATE[\"features\"]\n",
    "\n",
    "# sÃ­mbolos efectivos (preferir QA final)\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "final_symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not final_symbols:\n",
    "    raise RuntimeError(\"[Celda 06] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "\n",
    "symbols_u = [str(s).upper().strip() for s in final_symbols]\n",
    "\n",
    "# folds\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    # fallback: leer desde archivo si existe\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 06] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 06] ERROR: folds vacÃ­o (Celda 04 no produjo folds).\")\n",
    "\n",
    "# features paths\n",
    "feat_paths = feat_state.get(\"features_base_paths\") or {}\n",
    "feat_dir = feat_state.get(\"features_base_dir\")\n",
    "if (not feat_paths) and feat_dir:\n",
    "    # reconstruir mapping desde dir\n",
    "    pdir = Path(feat_dir).resolve()\n",
    "    if pdir.exists():\n",
    "        tmp = {}\n",
    "        for p in pdir.glob(\"*_features_base.parquet\"):\n",
    "            sym = p.name.split(\"_features_base.parquet\")[0].upper()\n",
    "            tmp[sym] = str(p.resolve())\n",
    "        feat_paths = tmp\n",
    "\n",
    "if not feat_paths:\n",
    "    raise RuntimeError(\"[Celda 06] ERROR: no encuentro features_base_paths/features_base_dir. Ejecuta Celda 05.\")\n",
    "\n",
    "# normalizar keys a upper\n",
    "feat_paths_u = {str(k).upper().strip(): str(v) for k, v in feat_paths.items()}\n",
    "\n",
    "# ========================= ParÃ¡metros institucionales =========================\n",
    "# Columnas esperadas (Celda 05)\n",
    "ER_COL = \"ER_kaufman\"\n",
    "VOL_COL = \"vol_logret_288\"   # 1D ~288 barras en M5\n",
    "MOM_COL = \"mom_288\"          # 1D momentum\n",
    "\n",
    "# quantiles base y reglas de ajuste para evitar gates degenerados\n",
    "Q_SCHEMES = [\n",
    "    {\"name\": \"BASE\",    \"q_er\": 0.60, \"q_mom\": 0.55, \"q_vol\": 0.90},\n",
    "    {\"name\": \"RELAX1\",  \"q_er\": 0.50, \"q_mom\": 0.50, \"q_vol\": 0.95},\n",
    "    {\"name\": \"RELAX2\",  \"q_er\": 0.40, \"q_mom\": 0.50, \"q_vol\": 0.99},\n",
    "    {\"name\": \"TIGHT1\",  \"q_er\": 0.70, \"q_mom\": 0.60, \"q_vol\": 0.85},\n",
    "]\n",
    "\n",
    "# Cobertura deseada del gate en IS (evitar â€œsiempre falsoâ€ o â€œsiempre trueâ€)\n",
    "COV_IS_MIN = 0.05\n",
    "COV_IS_MAX = 0.80\n",
    "\n",
    "# mÃ­nimos de filas Ãºtiles por tramo (post drop_nulls)\n",
    "MIN_IS_ROWS = 5_000\n",
    "MIN_OOS_ROWS = 2_000\n",
    "\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"regime_gates\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    dt = datetime.fromisoformat(s)\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _need_cols(df: pl.DataFrame, cols: list[str], sym: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 06] ERROR: {sym} no tiene columnas requeridas: {miss}. cols={df.columns}\")\n",
    "\n",
    "def _quantile_safe(s: pl.Series, q: float) -> float | None:\n",
    "    s2 = s.drop_nulls()\n",
    "    if s2.len() == 0:\n",
    "        return None\n",
    "    v = s2.quantile(q, interpolation=\"nearest\")\n",
    "    if v is None:\n",
    "        return None\n",
    "    try:\n",
    "        fv = float(v)\n",
    "        if not math.isfinite(fv):\n",
    "            return None\n",
    "        return fv\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _apply_gate(df: pl.DataFrame, thr_er: float, thr_mom: float, thr_vol: float) -> pl.Series:\n",
    "    # long-only TREND: momentum nunca por debajo de 0\n",
    "    thr_mom_eff = max(0.0, float(thr_mom))\n",
    "    return (\n",
    "        (pl.col(ER_COL) >= pl.lit(float(thr_er))) &\n",
    "        (pl.col(MOM_COL) >= pl.lit(float(thr_mom_eff))) &\n",
    "        (pl.col(VOL_COL) <= pl.lit(float(thr_vol)))\n",
    "    ).alias(\"__gate__\")\n",
    "\n",
    "def _calibrate_thresholds_is(df_is: pl.DataFrame) -> dict:\n",
    "    # devuelve thresholds + esquema usado + cobertura IS\n",
    "    er_s = df_is.get_column(ER_COL)\n",
    "    mom_s = df_is.get_column(MOM_COL)\n",
    "    vol_s = df_is.get_column(VOL_COL)\n",
    "\n",
    "    best = None\n",
    "\n",
    "    for sch in Q_SCHEMES:\n",
    "        thr_er = _quantile_safe(er_s, sch[\"q_er\"])\n",
    "        thr_mom = _quantile_safe(mom_s, sch[\"q_mom\"])\n",
    "        thr_vol = _quantile_safe(vol_s, sch[\"q_vol\"])\n",
    "\n",
    "        if thr_er is None or thr_mom is None or thr_vol is None:\n",
    "            continue\n",
    "\n",
    "        gate = df_is.select(_apply_gate(df_is, thr_er, thr_mom, thr_vol)).get_column(\"__gate__\")\n",
    "        cov = float(gate.mean()) if gate.len() > 0 else 0.0\n",
    "\n",
    "        payload = {\n",
    "            \"scheme\": sch[\"name\"],\n",
    "            \"q_er\": float(sch[\"q_er\"]),\n",
    "            \"q_mom\": float(sch[\"q_mom\"]),\n",
    "            \"q_vol\": float(sch[\"q_vol\"]),\n",
    "            \"thr_er\": float(thr_er),\n",
    "            \"thr_mom\": float(max(0.0, float(thr_mom))),\n",
    "            \"thr_vol\": float(thr_vol),\n",
    "            \"cov_is\": float(cov),\n",
    "        }\n",
    "\n",
    "        # elegir el primer esquema que cae en rango deseado\n",
    "        if COV_IS_MIN <= cov <= COV_IS_MAX:\n",
    "            return payload\n",
    "\n",
    "        # si ninguno cae en rango, guarda el â€œmenos maloâ€ por cercanÃ­a al centro\n",
    "        target = 0.25\n",
    "        score = abs(cov - target)\n",
    "        if best is None or score < best[\"score\"]:\n",
    "            best = {\"score\": score, \"payload\": payload}\n",
    "\n",
    "    if best is None:\n",
    "        # no se pudo calibrar por falta total de datos\n",
    "        return {\"scheme\": \"FAIL\", \"q_er\": None, \"q_mom\": None, \"q_vol\": None, \"thr_er\": None, \"thr_mom\": None, \"thr_vol\": None, \"cov_is\": 0.0}\n",
    "\n",
    "    return best[\"payload\"]\n",
    "\n",
    "# ========================= Main =========================\n",
    "rows = []\n",
    "thresholds_by_symbol_fold = {}\n",
    "\n",
    "print(f\"[Celda 06] symbols = {symbols_u}\")\n",
    "print(f\"[Celda 06] folds   = {[f.get('fold_id') for f in folds]}\")\n",
    "\n",
    "for sym in symbols_u:\n",
    "    if sym not in feat_paths_u:\n",
    "        raise RuntimeError(f\"[Celda 06] ERROR: no encuentro features parquet para {sym}. keys_sample={list(feat_paths_u)[:10]}\")\n",
    "\n",
    "    df = pl.read_parquet(feat_paths_u[sym])\n",
    "\n",
    "    # contrato mÃ­nimo\n",
    "    _need_cols(df, [\"time_utc\", ER_COL, MOM_COL, VOL_COL], sym)\n",
    "    df = df.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False)).sort(\"time_utc\")\n",
    "\n",
    "    for f in folds:\n",
    "        fid = str(f[\"fold_id\"])\n",
    "        is_s = _parse_iso_utc(f[\"IS_start\"])\n",
    "        is_e = _parse_iso_utc(f[\"IS_end\"])\n",
    "        o_s  = _parse_iso_utc(f[\"OOS_start\"])\n",
    "        o_e  = _parse_iso_utc(f[\"OOS_end\"])\n",
    "        emb  = int(f.get(\"embargo_days\", 0))\n",
    "\n",
    "        df_is = df.filter((pl.col(\"time_utc\") >= pl.lit(is_s)) & (pl.col(\"time_utc\") <= pl.lit(is_e)))\n",
    "        df_oos = df.filter((pl.col(\"time_utc\") >= pl.lit(o_s)) & (pl.col(\"time_utc\") <= pl.lit(o_e)))\n",
    "\n",
    "        # drop nulls en columnas clave\n",
    "        df_is_u = df_is.drop_nulls([ER_COL, MOM_COL, VOL_COL])\n",
    "        df_oos_u = df_oos.drop_nulls([ER_COL, MOM_COL, VOL_COL])\n",
    "\n",
    "        n_is = int(df_is_u.height)\n",
    "        n_oos = int(df_oos_u.height)\n",
    "\n",
    "        status = \"OK\"\n",
    "        if n_is < MIN_IS_ROWS:\n",
    "            status = \"FAIL_IS_TOO_SMALL\"\n",
    "        if n_oos < MIN_OOS_ROWS:\n",
    "            status = \"FAIL_OOS_TOO_SMALL\" if status == \"OK\" else status\n",
    "\n",
    "        calib = _calibrate_thresholds_is(df_is_u) if status.startswith(\"OK\") else {\"scheme\":\"SKIP\", \"q_er\":None,\"q_mom\":None,\"q_vol\":None,\"thr_er\":None,\"thr_mom\":None,\"thr_vol\":None,\"cov_is\":0.0}\n",
    "\n",
    "        thr_er = calib.get(\"thr_er\")\n",
    "        thr_mom = calib.get(\"thr_mom\")\n",
    "        thr_vol = calib.get(\"thr_vol\")\n",
    "\n",
    "        cov_is = float(calib.get(\"cov_is\") or 0.0)\n",
    "        cov_oos = 0.0\n",
    "        n_is_pass = 0\n",
    "        n_oos_pass = 0\n",
    "\n",
    "        if status == \"OK\" and thr_er is not None and thr_mom is not None and thr_vol is not None:\n",
    "            gate_is = df_is_u.select(_apply_gate(df_is_u, thr_er, thr_mom, thr_vol)).get_column(\"__gate__\")\n",
    "            gate_oos = df_oos_u.select(_apply_gate(df_oos_u, thr_er, thr_mom, thr_vol)).get_column(\"__gate__\")\n",
    "\n",
    "            cov_is = float(gate_is.mean()) if gate_is.len() > 0 else 0.0\n",
    "            cov_oos = float(gate_oos.mean()) if gate_oos.len() > 0 else 0.0\n",
    "            n_is_pass = int(gate_is.sum())\n",
    "            n_oos_pass = int(gate_oos.sum())\n",
    "\n",
    "            # marcar warnings si el gate quedÃ³ degenerado\n",
    "            if cov_is < 0.02 or cov_is > 0.90:\n",
    "                status = \"WARN_DEGENERATE_COVERAGE_IS\"\n",
    "\n",
    "        row = {\n",
    "            \"symbol\": sym,\n",
    "            \"fold_id\": fid,\n",
    "            \"IS_start\": is_s.isoformat(),\n",
    "            \"IS_end\": is_e.isoformat(),\n",
    "            \"OOS_start\": o_s.isoformat(),\n",
    "            \"OOS_end\": o_e.isoformat(),\n",
    "            \"embargo_days\": emb,\n",
    "            \"scheme\": calib.get(\"scheme\"),\n",
    "            \"q_er\": calib.get(\"q_er\"),\n",
    "            \"q_mom\": calib.get(\"q_mom\"),\n",
    "            \"q_vol\": calib.get(\"q_vol\"),\n",
    "            \"thr_er\": thr_er,\n",
    "            \"thr_mom\": thr_mom,\n",
    "            \"thr_vol\": thr_vol,\n",
    "            \"n_is\": n_is,\n",
    "            \"n_oos\": n_oos,\n",
    "            \"n_is_pass\": n_is_pass,\n",
    "            \"n_oos_pass\": n_oos_pass,\n",
    "            \"cov_is\": float(cov_is),\n",
    "            \"cov_oos\": float(cov_oos),\n",
    "            \"status\": status,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        thresholds_by_symbol_fold.setdefault(sym, {})[fid] = row\n",
    "\n",
    "        print(f\"[Celda 06] {sym} {fid} :: status={status} | scheme={row['scheme']} | cov_IS={row['cov_is']:.3f} cov_OOS={row['cov_oos']:.3f} | thr_ER={row['thr_er']} thr_MOM={row['thr_mom']} thr_VOL={row['thr_vol']}\")\n",
    "\n",
    "gate_df = pl.DataFrame(rows).sort([\"symbol\", \"fold_id\"])\n",
    "\n",
    "out_parq = OUT_DIR / \"regime_gate_by_fold.parquet\"\n",
    "gate_df.write_parquet(str(out_parq), compression=\"zstd\")\n",
    "\n",
    "snap_parq = SNAP_DIR / \"regime_gate_by_fold.parquet\"\n",
    "gate_df.write_parquet(str(snap_parq), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": [f.get(\"fold_id\") for f in folds],\n",
    "    \"params\": {\n",
    "        \"ER_COL\": ER_COL,\n",
    "        \"MOM_COL\": MOM_COL,\n",
    "        \"VOL_COL\": VOL_COL,\n",
    "        \"COV_IS_MIN\": COV_IS_MIN,\n",
    "        \"COV_IS_MAX\": COV_IS_MAX,\n",
    "        \"MIN_IS_ROWS\": MIN_IS_ROWS,\n",
    "        \"MIN_OOS_ROWS\": MIN_OOS_ROWS,\n",
    "        \"Q_SCHEMES\": Q_SCHEMES,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"artifacts_parquet\": str(out_parq),\n",
    "        \"snapshot_parquet\": str(snap_parq),\n",
    "    }\n",
    "}\n",
    "snap_json = SNAP_DIR / \"regime_gate_snapshot.json\"\n",
    "snap_json.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"regime_gate\"] = {\n",
    "    \"gate_table_path\": str(out_parq),\n",
    "    \"gate_table_snapshot_path\": str(snap_parq),\n",
    "    \"snapshot_json\": str(snap_json),\n",
    "    \"thresholds_by_symbol_fold\": thresholds_by_symbol_fold,\n",
    "    \"params\": snapshot[\"params\"],\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_parq} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_parq} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json} (OK)\")\n",
    "print(\">>> Celda 06 v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81279265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07 v1.0.2 :: SeÃ±al TREND + EjecuciÃ³n t+1 + Costos (BASE/STRESS) [WFO-safe]\n",
      "[Celda 07] symbols = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 07] folds   = ['F1']\n",
      "[Celda 07] BNBUSD F1 :: trades=6685 | IS/OOS split OK | cost_base_bps=8.0 cost_stress_bps=16.0\n",
      "[Celda 07] BTCUSD F1 :: trades=6408 | IS/OOS split OK | cost_base_bps=8.0 cost_stress_bps=16.0\n",
      "[Celda 07] LVMH F1 :: trades=1919 | IS/OOS split OK | cost_base_bps=12.0 cost_stress_bps=25.0\n",
      "[Celda 07] XAUAUD F1 :: trades=4640 | IS/OOS split OK | cost_base_bps=4.0 cost_stress_bps=8.0\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\strategy_signals\\trades_m5_all.parquet (OK) | rows=19652\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\strategy_signals\\summary_by_symbol_fold.parquet (OK) | rows=8\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\trades_m5_all.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\summary_by_symbol_fold.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\signals_snapshot.json (OK)\n",
      ">>> Celda 07 v1.0.2 :: OK\n"
     ]
    }
   ],
   "source": [
    " # ===================== Celda 07 v1.0.2 â€” SeÃ±al TREND + EjecuciÃ³n t+1 + Costos (BASE/STRESS) [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 07 v1.0.2 :: SeÃ±al TREND + EjecuciÃ³n t+1 + Costos (BASE/STRESS) [WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"data\", \"features\", \"wfo\", \"regime_gate\", \"cost_model\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 07] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "feat_state = GLOBAL_STATE[\"features\"]\n",
    "wfo_state = GLOBAL_STATE[\"wfo\"]\n",
    "gate_state = GLOBAL_STATE[\"regime_gate\"]\n",
    "cost_state = GLOBAL_STATE[\"cost_model\"]\n",
    "\n",
    "# sÃ­mbolos efectivos (preferir QA final)\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "symbols_u = [str(s).upper().strip() for s in symbols]\n",
    "\n",
    "# folds\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 07] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: folds vacÃ­o (Celda 04 no produjo folds).\")\n",
    "\n",
    "# m5 paths\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"m5_clean_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not isinstance(m5_paths, dict) or not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: no hay m5_ohlcv_paths/m5_clean_paths/ohlcv_clean_paths. Ejecuta Celda 02/02C.\")\n",
    "\n",
    "# features paths\n",
    "feat_paths = feat_state.get(\"features_base_paths\") or {}\n",
    "if not feat_paths:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: no hay features_base_paths. Ejecuta Celda 05.\")\n",
    "\n",
    "# gate table\n",
    "gate_path = gate_state.get(\"gate_table_path\")\n",
    "if not gate_path:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: no hay regime_gate.gate_table_path. Ejecuta Celda 06.\")\n",
    "gate_df = pl.read_parquet(str(gate_path))\n",
    "\n",
    "# costs\n",
    "costs_by_symbol = cost_state.get(\"costs_by_symbol\") or cost_state.get(\"symbols\") or {}\n",
    "if not costs_by_symbol:\n",
    "    raise RuntimeError(\"[Celda 07] ERROR: no hay cost_model.costs_by_symbol. Ejecuta Celda 03.\")\n",
    "\n",
    "cost_reported_is_roundtrip = bool(cost_state.get(\"cost_reported_is_roundtrip\", False))\n",
    "\n",
    "# ========================= ParÃ¡metros (contract) =========================\n",
    "ER_COL  = \"ER_kaufman\"\n",
    "MOM_COL = \"mom_288\"\n",
    "VOL_COL = \"vol_logret_288\"\n",
    "\n",
    "REQ_FEAT_COLS = [\"time_utc\", ER_COL, MOM_COL, VOL_COL]\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"strategy_signals\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    return _ensure_utc(datetime.fromisoformat(s))\n",
    "\n",
    "def _pick_path_case_insensitive(d: dict, sym_u: str) -> str:\n",
    "    if sym_u in d:\n",
    "        return str(d[sym_u])\n",
    "    keys = {str(k).upper().strip(): k for k in d.keys()}\n",
    "    if sym_u in keys:\n",
    "        return str(d[keys[sym_u]])\n",
    "    raise KeyError(f\"[Celda 07] ERROR: no encuentro path para {sym_u}. keys_sample={list(d)[:10]}\")\n",
    "\n",
    "def _need_cols(df: pl.DataFrame, cols: list[str], sym: str, tag: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 07] ERROR: {sym} ({tag}) missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "def _cost_roundtrip_dec(cost_bps: float, reported_is_roundtrip: bool) -> float:\n",
    "    c = float(cost_bps) / 10000.0\n",
    "    return c if reported_is_roundtrip else (2.0 * c)\n",
    "\n",
    "def _apply_gate_expr(thr_er: float, thr_mom: float, thr_vol: float) -> pl.Expr:\n",
    "    thr_mom_eff = max(0.0, float(thr_mom))\n",
    "    return (\n",
    "        (pl.col(ER_COL) >= pl.lit(float(thr_er))) &\n",
    "        (pl.col(MOM_COL) >= pl.lit(float(thr_mom_eff))) &\n",
    "        (pl.col(VOL_COL) <= pl.lit(float(thr_vol)))\n",
    "    ).alias(\"signal_gate\")\n",
    "\n",
    "def _entry_exit_prices(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # entry at t+1, exit at t+2; prefer open else close\n",
    "    return df.with_columns([\n",
    "        pl.col(\"time_utc\").shift(-1).alias(\"entry_time\"),\n",
    "        pl.col(\"time_utc\").shift(-2).alias(\"exit_time\"),\n",
    "        pl.when(pl.col(\"open\").shift(-1).is_not_null())\n",
    "          .then(pl.col(\"open\").shift(-1))\n",
    "          .otherwise(pl.col(\"close\").shift(-1))\n",
    "          .alias(\"entry_price\"),\n",
    "        pl.when(pl.col(\"open\").shift(-2).is_not_null())\n",
    "          .then(pl.col(\"open\").shift(-2))\n",
    "          .otherwise(pl.col(\"close\").shift(-2))\n",
    "          .alias(\"exit_price\"),\n",
    "    ])\n",
    "\n",
    "def _segment_label_expr(is_s: datetime, is_e: datetime, o_s: datetime, o_e: datetime) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when((pl.col(\"entry_time\") >= pl.lit(is_s)) & (pl.col(\"entry_time\") <= pl.lit(is_e))).then(pl.lit(\"IS\"))\n",
    "         .when((pl.col(\"entry_time\") >= pl.lit(o_s)) & (pl.col(\"entry_time\") <= pl.lit(o_e))).then(pl.lit(\"OOS\"))\n",
    "         .otherwise(pl.lit(None))\n",
    "         .alias(\"segment\")\n",
    "    )\n",
    "\n",
    "# ========================= Main =========================\n",
    "all_trades: list[pl.DataFrame] = []\n",
    "summary_rows: list[dict] = []\n",
    "per_symbol_outputs: dict[str, str] = {}\n",
    "trades_by_symbol: dict[str, list[pl.DataFrame]] = {s: [] for s in symbols_u}\n",
    "\n",
    "print(f\"[Celda 07] symbols = {symbols_u}\")\n",
    "print(f\"[Celda 07] folds   = {[f.get('fold_id') for f in folds]}\")\n",
    "\n",
    "for sym in symbols_u:\n",
    "    sym_u = sym.upper().strip()\n",
    "\n",
    "    # --- Load OHLCV ---\n",
    "    m5_path = _pick_path_case_insensitive(m5_paths, sym_u)\n",
    "    df_px = pl.read_parquet(str(m5_path))\n",
    "    _need_cols(df_px, [\"time_utc\", \"close\"], sym_u, \"OHLCV\")\n",
    "    if \"open\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"open\"))\n",
    "\n",
    "    df_px = (\n",
    "        df_px\n",
    "        .with_columns([\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "            pl.col(\"open\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "        ])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    # --- Load Features ---\n",
    "    feat_path = _pick_path_case_insensitive(feat_paths, sym_u)\n",
    "    df_f = pl.read_parquet(str(feat_path))\n",
    "    _need_cols(df_f, REQ_FEAT_COLS, sym_u, \"FEATURES\")\n",
    "    df_f = df_f.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False)).sort(\"time_utc\")\n",
    "\n",
    "    # --- Join ---\n",
    "    df = df_px.join(df_f.select(REQ_FEAT_COLS), on=\"time_utc\", how=\"inner\")\n",
    "\n",
    "    # cost for symbol\n",
    "    cinfo = costs_by_symbol.get(sym_u) or costs_by_symbol.get(sym) or {}\n",
    "    cost_base_bps = float(cinfo.get(\"COST_BASE_BPS\", 0.0))\n",
    "    cost_stress_bps = float(cinfo.get(\"COST_STRESS_BPS\", 0.0))\n",
    "    cost_base_dec = _cost_roundtrip_dec(cost_base_bps, cost_reported_is_roundtrip)\n",
    "    cost_stress_dec = _cost_roundtrip_dec(cost_stress_bps, cost_reported_is_roundtrip)\n",
    "\n",
    "    # --- Per fold build trades ---\n",
    "    for f in folds:\n",
    "        fid = str(f[\"fold_id\"])\n",
    "        is_s = _parse_iso_utc(f[\"IS_start\"])\n",
    "        is_e = _parse_iso_utc(f[\"IS_end\"])\n",
    "        o_s  = _parse_iso_utc(f[\"OOS_start\"])\n",
    "        o_e  = _parse_iso_utc(f[\"OOS_end\"])\n",
    "        emb  = int(f.get(\"embargo_days\", 0))\n",
    "\n",
    "        g = gate_df.filter((pl.col(\"symbol\") == sym_u) & (pl.col(\"fold_id\") == fid))\n",
    "        if g.is_empty():\n",
    "            raise RuntimeError(f\"[Celda 07] ERROR: no hay thresholds en gate_df para {sym_u} {fid}.\")\n",
    "\n",
    "        thr_er = float(g.select(pl.col(\"thr_er\")).item())\n",
    "        thr_mom = float(g.select(pl.col(\"thr_mom\")).item())\n",
    "        thr_vol = float(g.select(pl.col(\"thr_vol\")).item())\n",
    "\n",
    "        dfx = df.with_columns([_apply_gate_expr(thr_er, thr_mom, thr_vol)])\n",
    "        dfx = _entry_exit_prices(dfx)\n",
    "        dfx = dfx.with_columns(_segment_label_expr(is_s, is_e, o_s, o_e))\n",
    "\n",
    "        # trade list: solo filas donde gate=True\n",
    "        trades = (\n",
    "            dfx\n",
    "            .filter(pl.col(\"segment\").is_not_null())\n",
    "            .filter(pl.col(\"entry_price\").is_not_null() & pl.col(\"exit_price\").is_not_null())\n",
    "            .filter((pl.col(\"entry_price\") > 0.0) & (pl.col(\"exit_price\") > 0.0))\n",
    "            .filter(pl.col(\"signal_gate\") == True)\n",
    "            .with_columns([\n",
    "                pl.lit(sym_u).alias(\"symbol\"),\n",
    "                pl.lit(fid).alias(\"fold_id\"),\n",
    "                pl.lit(emb).alias(\"embargo_days\"),\n",
    "                (pl.col(\"exit_price\") / pl.col(\"entry_price\") - 1.0).alias(\"gross_ret\"),\n",
    "                (pl.col(\"exit_price\") / pl.col(\"entry_price\") - 1.0 - pl.lit(cost_base_dec)).alias(\"net_ret_base\"),\n",
    "                (pl.col(\"exit_price\") / pl.col(\"entry_price\") - 1.0 - pl.lit(cost_stress_dec)).alias(\"net_ret_stress\"),\n",
    "                pl.lit(cost_base_bps).alias(\"cost_base_bps\"),\n",
    "                pl.lit(cost_stress_bps).alias(\"cost_stress_bps\"),\n",
    "            ])\n",
    "            .select([\n",
    "                \"symbol\",\"fold_id\",\"segment\",\"embargo_days\",\n",
    "                \"time_utc\",\"entry_time\",\"exit_time\",\n",
    "                \"entry_price\",\"exit_price\",\n",
    "                \"signal_gate\",\n",
    "                \"gross_ret\",\"net_ret_base\",\"net_ret_stress\",\n",
    "                \"cost_base_bps\",\"cost_stress_bps\",\n",
    "                ER_COL,MOM_COL,VOL_COL,\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        if trades.height == 0:\n",
    "            print(f\"[Celda 07] WARN: {sym_u} {fid} :: 0 trades (segment split por entry_time).\")\n",
    "            continue\n",
    "\n",
    "        # summary por segmento (diagnÃ³stico)\n",
    "        seg_sum = (\n",
    "            trades\n",
    "            .group_by([\"symbol\",\"fold_id\",\"segment\"])\n",
    "            .agg([\n",
    "                pl.len().alias(\"n_trades\"),\n",
    "                pl.col(\"gross_ret\").mean().alias(\"gross_mean\"),\n",
    "                pl.col(\"net_ret_base\").mean().alias(\"net_base_mean\"),\n",
    "                pl.col(\"net_ret_stress\").mean().alias(\"net_stress_mean\"),\n",
    "                pl.col(\"net_ret_base\").std().alias(\"net_base_std\"),\n",
    "                pl.col(\"net_ret_base\").median().alias(\"net_base_median\"),\n",
    "                pl.col(\"net_ret_base\").quantile(0.05, interpolation=\"nearest\").alias(\"net_base_p05\"),\n",
    "                pl.col(\"net_ret_base\").quantile(0.95, interpolation=\"nearest\").alias(\"net_base_p95\"),\n",
    "                (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col(\"net_base_mean\") / pl.col(\"net_base_std\")).alias(\"sharpe_like_base\"),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        summary_rows.extend(seg_sum.to_dicts())\n",
    "        all_trades.append(trades)\n",
    "        trades_by_symbol[sym_u].append(trades)\n",
    "\n",
    "        print(f\"[Celda 07] {sym_u} {fid} :: trades={trades.height} | IS/OOS split OK | cost_base_bps={cost_base_bps} cost_stress_bps={cost_stress_bps}\")\n",
    "\n",
    "    # persist per-symbol\n",
    "    if trades_by_symbol[sym_u]:\n",
    "        df_sym = pl.concat(trades_by_symbol[sym_u], how=\"vertical\")\n",
    "        out_sym = OUT_DIR / f\"trades_m5_{sym_u}.parquet\"\n",
    "        df_sym.write_parquet(str(out_sym), compression=\"zstd\")\n",
    "        per_symbol_outputs[sym_u] = str(out_sym)\n",
    "\n",
    "# ========================= Persist all + summary =========================\n",
    "if not all_trades:\n",
    "    raise RuntimeError(\"[Celda 07] GATE FAIL: no se generÃ³ ningÃºn trade en ningÃºn sÃ­mbolo.\")\n",
    "\n",
    "trades_all = pl.concat(all_trades, how=\"vertical\")\n",
    "out_all = OUT_DIR / \"trades_m5_all.parquet\"\n",
    "trades_all.write_parquet(str(out_all), compression=\"zstd\")\n",
    "\n",
    "summary_df = pl.DataFrame(summary_rows).sort([\"symbol\",\"fold_id\",\"segment\"]) if summary_rows else pl.DataFrame()\n",
    "out_sum = OUT_DIR / \"summary_by_symbol_fold.parquet\"\n",
    "summary_df.write_parquet(str(out_sum), compression=\"zstd\")\n",
    "\n",
    "# snapshots\n",
    "snap_all = SNAP_DIR / \"trades_m5_all.parquet\"\n",
    "trades_all.write_parquet(str(snap_all), compression=\"zstd\")\n",
    "\n",
    "snap_sum = SNAP_DIR / \"summary_by_symbol_fold.parquet\"\n",
    "summary_df.write_parquet(str(snap_sum), compression=\"zstd\")\n",
    "\n",
    "snap_json = SNAP_DIR / \"signals_snapshot.json\"\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": [f.get(\"fold_id\") for f in folds],\n",
    "    \"execution_convention\": (GLOBAL_STATE.get(\"execution\", {}) or {}).get(\"convention\"),\n",
    "    \"cost_reported_is_roundtrip\": cost_reported_is_roundtrip,\n",
    "    \"outputs\": {\n",
    "        \"per_symbol\": per_symbol_outputs,\n",
    "        \"all_trades\": str(out_all),\n",
    "        \"summary\": str(out_sum),\n",
    "        \"snap_all_trades\": str(snap_all),\n",
    "        \"snap_summary\": str(snap_sum),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"trade list: solo filas con signal_gate=True\",\n",
    "        \"segment split por entry_time para evitar leakage en bordes\",\n",
    "        \"entry=open(t+1) fallback close(t+1); exit=open(t+2) fallback close(t+2)\",\n",
    "    ],\n",
    "}\n",
    "snap_json.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"signals\"] = {\n",
    "    \"trades_dir\": str(OUT_DIR),\n",
    "    \"trades_all_path\": str(out_all),\n",
    "    \"summary_path\": str(out_sum),\n",
    "    \"per_symbol_paths\": per_symbol_outputs,\n",
    "    \"snapshot_json\": str(snap_json),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_all} (OK) | rows={trades_all.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_sum} (OK) | rows={summary_df.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_all} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_sum} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json} (OK)\")\n",
    "print(\">>> Celda 07 v1.0.2 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14ce8167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07B v1.0 :: QA timing trades (entry/hold gaps)\n",
      "\n",
      "[Celda 07B] QA timing por symbol/segment:\n",
      "shape: (8, 14)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† segment â”† n_trades â”† entry_med_ â”† â€¦ â”† share_entr â”† share_hold â”† share_hold â”† share_hold â”‚\n",
      "â”‚ ---    â”† ---     â”† ---      â”† s          â”†   â”† y_gt_900s  â”† _gt_900s   â”† _gt_1h     â”† _gt_1d     â”‚\n",
      "â”‚ str    â”† str     â”† u32      â”† ---        â”†   â”† ---        â”† ---        â”† ---        â”† ---        â”‚\n",
      "â”‚        â”†         â”†          â”† f64        â”†   â”† f64        â”† f64        â”† f64        â”† f64        â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BNBUSD â”† IS      â”† 5589     â”† 300.0      â”† â€¦ â”† 0.000716   â”† 0.000537   â”† 0.000537   â”† 0.0        â”‚\n",
      "â”‚ BNBUSD â”† OOS     â”† 1096     â”† 300.0      â”† â€¦ â”† 0.000912   â”† 0.000912   â”† 0.000912   â”† 0.0        â”‚\n",
      "â”‚ BTCUSD â”† IS      â”† 5458     â”† 300.0      â”† â€¦ â”† 0.000733   â”† 0.000916   â”† 0.000916   â”† 0.0        â”‚\n",
      "â”‚ BTCUSD â”† OOS     â”† 950      â”† 300.0      â”† â€¦ â”† 0.002105   â”† 0.003158   â”† 0.003158   â”† 0.0        â”‚\n",
      "â”‚ LVMH   â”† IS      â”† 1425     â”† 300.0      â”† â€¦ â”† 0.014035   â”† 0.01193    â”† 0.01193    â”† 0.002105   â”‚\n",
      "â”‚ LVMH   â”† OOS     â”† 494      â”† 300.0      â”† â€¦ â”† 0.010121   â”† 0.010121   â”† 0.010121   â”† 0.002024   â”‚\n",
      "â”‚ XAUAUD â”† IS      â”† 4062     â”† 300.0      â”† â€¦ â”† 0.004431   â”† 0.003447   â”† 0.003447   â”† 0.000739   â”‚\n",
      "â”‚ XAUAUD â”† OOS     â”† 578      â”† 300.0      â”† â€¦ â”† 0.00346    â”† 0.00346    â”† 0.00346    â”† 0.0        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\strategy_signals\\qa_trade_timing.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_trade_timing.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_trade_timing_snapshot.json (OK)\n",
      ">>> Celda 07B v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 07B v1.0 â€” QA timing trades (gap-aware diagnostics) =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 07B v1.0 :: QA timing trades (entry/hold gaps)\")\n",
    "\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 07B] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "if \"paths\" not in GLOBAL_STATE or \"signals\" not in GLOBAL_STATE:\n",
    "    raise RuntimeError(\"[Celda 07B] ERROR: faltan GLOBAL_STATE['paths'] o GLOBAL_STATE['signals'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "sig = GLOBAL_STATE[\"signals\"]\n",
    "\n",
    "trades_all_path = sig.get(\"trades_all_path\")\n",
    "if not trades_all_path:\n",
    "    raise RuntimeError(\"[Celda 07B] ERROR: no existe GLOBAL_STATE['signals']['trades_all_path']. Ejecuta Celda 07.\")\n",
    "\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"strategy_signals\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pl.read_parquet(str(trades_all_path))\n",
    "\n",
    "need = [\"symbol\",\"segment\",\"time_utc\",\"entry_time\",\"exit_time\",\"entry_price\",\"exit_price\",\"net_ret_base\",\"net_ret_stress\"]\n",
    "miss = [c for c in need if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 07B] ERROR: trades_all missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "# asegurar tipos datetime\n",
    "df = df.with_columns([\n",
    "    pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "    pl.col(\"entry_time\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "    pl.col(\"exit_time\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "])\n",
    "\n",
    "# deltas en segundos\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"entry_time\") - pl.col(\"time_utc\")).dt.total_seconds().alias(\"dt_signal_to_entry_s\"),\n",
    "    (pl.col(\"exit_time\") - pl.col(\"entry_time\")).dt.total_seconds().alias(\"dt_hold_s\"),\n",
    "    (pl.col(\"exit_time\") - pl.col(\"time_utc\")).dt.total_seconds().alias(\"dt_signal_to_exit_s\"),\n",
    "])\n",
    "\n",
    "# thresholds institucionales para diagnÃ³stico (M5 ~300s)\n",
    "TH = {\n",
    "    \"entry_gt_900s\": 900,   # > 15 min\n",
    "    \"hold_gt_900s\": 900,    # > 15 min\n",
    "    \"hold_gt_3600s\": 3600,  # > 1h\n",
    "    \"hold_gt_86400s\": 86400 # > 1d\n",
    "}\n",
    "\n",
    "qa = (\n",
    "    df.group_by([\"symbol\",\"segment\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"n_trades\"),\n",
    "          pl.col(\"dt_signal_to_entry_s\").median().alias(\"entry_med_s\"),\n",
    "          pl.col(\"dt_signal_to_entry_s\").quantile(0.90, interpolation=\"nearest\").alias(\"entry_p90_s\"),\n",
    "          pl.col(\"dt_signal_to_entry_s\").max().alias(\"entry_max_s\"),\n",
    "\n",
    "          pl.col(\"dt_hold_s\").median().alias(\"hold_med_s\"),\n",
    "          pl.col(\"dt_hold_s\").quantile(0.90, interpolation=\"nearest\").alias(\"hold_p90_s\"),\n",
    "          pl.col(\"dt_hold_s\").quantile(0.99, interpolation=\"nearest\").alias(\"hold_p99_s\"),\n",
    "          pl.col(\"dt_hold_s\").max().alias(\"hold_max_s\"),\n",
    "\n",
    "          (pl.col(\"dt_signal_to_entry_s\") > TH[\"entry_gt_900s\"]).mean().alias(\"share_entry_gt_900s\"),\n",
    "          (pl.col(\"dt_hold_s\") > TH[\"hold_gt_900s\"]).mean().alias(\"share_hold_gt_900s\"),\n",
    "          (pl.col(\"dt_hold_s\") > TH[\"hold_gt_3600s\"]).mean().alias(\"share_hold_gt_1h\"),\n",
    "          (pl.col(\"dt_hold_s\") > TH[\"hold_gt_86400s\"]).mean().alias(\"share_hold_gt_1d\"),\n",
    "      ])\n",
    "      .sort([\"symbol\",\"segment\"])\n",
    ")\n",
    "\n",
    "print(\"\\n[Celda 07B] QA timing por symbol/segment:\")\n",
    "print(qa)\n",
    "\n",
    "out_parq = OUT_DIR / \"qa_trade_timing.parquet\"\n",
    "qa.write_parquet(str(out_parq), compression=\"zstd\")\n",
    "\n",
    "snap_parq = SNAP_DIR / \"qa_trade_timing.parquet\"\n",
    "qa.write_parquet(str(snap_parq), compression=\"zstd\")\n",
    "\n",
    "snap_json = SNAP_DIR / \"qa_trade_timing_snapshot.json\"\n",
    "payload = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\"trades_all_path\": str(trades_all_path)},\n",
    "    \"thresholds_seconds\": TH,\n",
    "    \"outputs\": {\"artifacts_parquet\": str(out_parq), \"snapshot_parquet\": str(snap_parq)},\n",
    "}\n",
    "snap_json.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUT   â†’ {out_parq} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_parq} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json} (OK)\")\n",
    "print(\">>> Celda 07B v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afa586ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07C v1.0 :: Alpha Report MULTI-HORIZON (LONG/SHORT) + Costs + Monâ€“Fri [WFO-safe]\n",
      "[Celda 07C] symbols   = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 07C] folds     = ['F1']\n",
      "[Celda 07C] horizons  = [1, 3, 6, 12, 24, 48, 96, 288]\n",
      "[Celda 07C] mon_fri   = True\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\alpha_reports\\alpha_multi_horizon_report.parquet (OK) | rows=128\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\alpha_multi_horizon_report.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\alpha_multi_horizon_snapshot.json (OK)\n",
      ">>> Celda 07C v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 07C v1.0 â€” Alpha Report MULTI-HORIZON (LONG/SHORT) + Costs + Monâ€“Fri [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 07C v1.0 :: Alpha Report MULTI-HORIZON (LONG/SHORT) + Costs + Monâ€“Fri [WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"data\", \"features\", \"wfo\", \"regime_gate\", \"cost_model\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 07C] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "feat_state = GLOBAL_STATE[\"features\"]\n",
    "wfo_state  = GLOBAL_STATE[\"wfo\"]\n",
    "gate_state = GLOBAL_STATE[\"regime_gate\"]\n",
    "cost_state = GLOBAL_STATE[\"cost_model\"]\n",
    "\n",
    "# sÃ­mbolos efectivos (preferir QA final)\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "symbols_u = [str(s).upper().strip() for s in symbols]\n",
    "\n",
    "# folds\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 07C] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: folds vacÃ­o (Celda 04 no produjo folds).\")\n",
    "\n",
    "# m5 paths\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"m5_clean_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not isinstance(m5_paths, dict) or not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: no hay m5 paths (Celda 02/02C).\")\n",
    "\n",
    "# features paths\n",
    "feat_paths = feat_state.get(\"features_base_paths\") or {}\n",
    "if not feat_paths:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: no hay features_base_paths. Ejecuta Celda 05.\")\n",
    "\n",
    "# gate table\n",
    "gate_path = gate_state.get(\"gate_table_path\")\n",
    "if not gate_path:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: no hay regime_gate.gate_table_path. Ejecuta Celda 06.\")\n",
    "gate_df = pl.read_parquet(str(gate_path))\n",
    "\n",
    "# costs\n",
    "costs_by_symbol = cost_state.get(\"costs_by_symbol\") or cost_state.get(\"symbols\") or {}\n",
    "if not costs_by_symbol:\n",
    "    raise RuntimeError(\"[Celda 07C] ERROR: no hay cost_model.costs_by_symbol. Ejecuta Celda 03.\")\n",
    "cost_reported_is_roundtrip = bool(cost_state.get(\"cost_reported_is_roundtrip\", False))\n",
    "\n",
    "# ========================= ParÃ¡metros =========================\n",
    "ER_COL  = \"ER_kaufman\"\n",
    "MOM_COL = \"mom_288\"\n",
    "VOL_COL = \"vol_logret_288\"\n",
    "REQ_FEAT_COLS = [\"time_utc\", ER_COL, MOM_COL, VOL_COL]\n",
    "\n",
    "# horizontes en â€œbarras despuÃ©s de entryâ€ (entry=t+1). Ej: h=1 => exit=t+2 (tu celda 07 original)\n",
    "HORIZONS = [1, 3, 6, 12, 24, 48, 96, 288]\n",
    "\n",
    "# Institucional: operar Lunâ€“Vie (aplica tambiÃ©n a CRYPTO si quieres estandarizar)\n",
    "ENFORCE_MON_FRI = True  # si True, descarta entries con weekday>=5\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"alpha_reports\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    return _ensure_utc(datetime.fromisoformat(s))\n",
    "\n",
    "def _pick_path_case_insensitive(d: dict, sym_u: str) -> str:\n",
    "    if sym_u in d:\n",
    "        return str(d[sym_u])\n",
    "    keys = {str(k).upper().strip(): k for k in d.keys()}\n",
    "    if sym_u in keys:\n",
    "        return str(d[keys[sym_u]])\n",
    "    raise KeyError(f\"[Celda 07C] ERROR: no encuentro path para {sym_u}. keys_sample={list(d)[:10]}\")\n",
    "\n",
    "def _need_cols(df: pl.DataFrame, cols: list[str], sym: str, tag: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 07C] ERROR: {sym} ({tag}) missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "def _cost_roundtrip_dec(cost_bps: float, reported_is_roundtrip: bool) -> float:\n",
    "    c = float(cost_bps) / 10000.0\n",
    "    return c if reported_is_roundtrip else (2.0 * c)\n",
    "\n",
    "def _segment_label_expr(is_s: datetime, is_e: datetime, o_s: datetime, o_e: datetime) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when((pl.col(\"entry_time\") >= pl.lit(is_s)) & (pl.col(\"entry_time\") <= pl.lit(is_e))).then(pl.lit(\"IS\"))\n",
    "         .when((pl.col(\"entry_time\") >= pl.lit(o_s)) & (pl.col(\"entry_time\") <= pl.lit(o_e))).then(pl.lit(\"OOS\"))\n",
    "         .otherwise(pl.lit(None))\n",
    "         .alias(\"segment\")\n",
    "    )\n",
    "\n",
    "def _weekday_expr() -> pl.Expr:\n",
    "    # Polars: weekday() => Monday=0 ... Sunday=6 (en Datetime)\n",
    "    return pl.col(\"entry_time\").dt.weekday()\n",
    "\n",
    "# ========================= Main =========================\n",
    "rows = []\n",
    "print(f\"[Celda 07C] symbols   = {symbols_u}\")\n",
    "print(f\"[Celda 07C] folds     = {[f.get('fold_id') for f in folds]}\")\n",
    "print(f\"[Celda 07C] horizons  = {HORIZONS}\")\n",
    "print(f\"[Celda 07C] mon_fri   = {ENFORCE_MON_FRI}\")\n",
    "\n",
    "for sym in symbols_u:\n",
    "    sym_u = sym.upper().strip()\n",
    "\n",
    "    # --- Load OHLCV ---\n",
    "    m5_path = _pick_path_case_insensitive(m5_paths, sym_u)\n",
    "    df_px = pl.read_parquet(str(m5_path))\n",
    "    _need_cols(df_px, [\"time_utc\", \"close\"], sym_u, \"OHLCV\")\n",
    "    if \"open\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"open\"))\n",
    "    df_px = (\n",
    "        df_px\n",
    "        .with_columns([\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "            pl.col(\"open\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "        ])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    # --- Load Features ---\n",
    "    feat_path = _pick_path_case_insensitive(feat_paths, sym_u)\n",
    "    df_f = pl.read_parquet(str(feat_path))\n",
    "    _need_cols(df_f, REQ_FEAT_COLS, sym_u, \"FEATURES\")\n",
    "    df_f = df_f.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False)).sort(\"time_utc\")\n",
    "\n",
    "    # --- Join ---\n",
    "    df = df_px.join(df_f.select(REQ_FEAT_COLS), on=\"time_utc\", how=\"inner\")\n",
    "\n",
    "    # costs\n",
    "    cinfo = costs_by_symbol.get(sym_u) or costs_by_symbol.get(sym) or {}\n",
    "    cost_base_bps   = float(cinfo.get(\"COST_BASE_BPS\", 0.0))\n",
    "    cost_stress_bps = float(cinfo.get(\"COST_STRESS_BPS\", 0.0))\n",
    "    cost_base_dec   = _cost_roundtrip_dec(cost_base_bps, cost_reported_is_roundtrip)\n",
    "    cost_stress_dec = _cost_roundtrip_dec(cost_stress_bps, cost_reported_is_roundtrip)\n",
    "\n",
    "    for f in folds:\n",
    "        fid = str(f[\"fold_id\"])\n",
    "        is_s = _parse_iso_utc(f[\"IS_start\"])\n",
    "        is_e = _parse_iso_utc(f[\"IS_end\"])\n",
    "        o_s  = _parse_iso_utc(f[\"OOS_start\"])\n",
    "        o_e  = _parse_iso_utc(f[\"OOS_end\"])\n",
    "\n",
    "        g = gate_df.filter((pl.col(\"symbol\") == sym_u) & (pl.col(\"fold_id\") == fid))\n",
    "        if g.is_empty():\n",
    "            raise RuntimeError(f\"[Celda 07C] ERROR: no hay thresholds en gate_df para {sym_u} {fid}.\")\n",
    "\n",
    "        thr_er  = float(g.select(pl.col(\"thr_er\")).item())\n",
    "        thr_mom = float(g.select(pl.col(\"thr_mom\")).item())\n",
    "        thr_vol = float(g.select(pl.col(\"thr_vol\")).item())\n",
    "\n",
    "        # NOTE: gate de rÃ©gimen es direccionalidad+vol; para SHORT usamos simetrÃ­a simple con -thr_mom_eff.\n",
    "        thr_mom_eff = max(0.0, float(thr_mom))\n",
    "\n",
    "        df0 = (\n",
    "            df\n",
    "            .with_columns([\n",
    "                pl.col(\"time_utc\").shift(-1).alias(\"entry_time\"),\n",
    "                pl.when(pl.col(\"open\").shift(-1).is_not_null()).then(pl.col(\"open\").shift(-1)).otherwise(pl.col(\"close\").shift(-1)).alias(\"entry_price\"),\n",
    "                (\n",
    "                    (pl.col(ER_COL) >= pl.lit(thr_er)) &\n",
    "                    (pl.col(MOM_COL) >= pl.lit(thr_mom_eff)) &\n",
    "                    (pl.col(VOL_COL) <= pl.lit(thr_vol))\n",
    "                ).alias(\"signal_long\"),\n",
    "                (\n",
    "                    (pl.col(ER_COL) >= pl.lit(thr_er)) &\n",
    "                    (pl.col(MOM_COL) <= pl.lit(-thr_mom_eff)) &\n",
    "                    (pl.col(VOL_COL) <= pl.lit(thr_vol))\n",
    "                ).alias(\"signal_short\"),\n",
    "            ])\n",
    "            .with_columns(_segment_label_expr(is_s, is_e, o_s, o_e))\n",
    "        )\n",
    "\n",
    "        if ENFORCE_MON_FRI:\n",
    "            df0 = df0.filter(_weekday_expr() < 5)\n",
    "\n",
    "        # por cada horizonte, medir retorno desde entry (t+1) a exit (t+1+h)\n",
    "        for h in HORIZONS:\n",
    "            exit_shift = -(1 + int(h))\n",
    "            tag_h = f\"H{h}\"\n",
    "\n",
    "            dfx = (\n",
    "                df0\n",
    "                .with_columns([\n",
    "                    pl.col(\"time_utc\").shift(exit_shift).alias(\"exit_time\"),\n",
    "                    pl.when(pl.col(\"open\").shift(exit_shift).is_not_null())\n",
    "                      .then(pl.col(\"open\").shift(exit_shift))\n",
    "                      .otherwise(pl.col(\"close\").shift(exit_shift))\n",
    "                      .alias(\"exit_price\"),\n",
    "                ])\n",
    "                .filter(pl.col(\"segment\").is_not_null())\n",
    "                .filter(pl.col(\"entry_time\").is_not_null() & pl.col(\"exit_time\").is_not_null())\n",
    "                .filter(pl.col(\"entry_price\").is_not_null() & pl.col(\"exit_price\").is_not_null())\n",
    "                .filter((pl.col(\"entry_price\") > 0.0) & (pl.col(\"exit_price\") > 0.0))\n",
    "                .drop_nulls([ER_COL, MOM_COL, VOL_COL])\n",
    "            )\n",
    "\n",
    "            if dfx.height == 0:\n",
    "                continue\n",
    "\n",
    "            # retorno base\n",
    "            dfx = dfx.with_columns([\n",
    "                (pl.col(\"exit_price\") / pl.col(\"entry_price\") - 1.0).alias(\"fwd_ret\"),\n",
    "            ])\n",
    "\n",
    "            # LONG sample\n",
    "            long_tr = dfx.filter(pl.col(\"signal_long\") == True).with_columns([\n",
    "                pl.lit(\"LONG\").alias(\"side\"),\n",
    "                pl.col(\"fwd_ret\").alias(\"gross_ret\"),\n",
    "                (pl.col(\"fwd_ret\") - pl.lit(cost_base_dec)).alias(\"net_ret_base\"),\n",
    "                (pl.col(\"fwd_ret\") - pl.lit(cost_stress_dec)).alias(\"net_ret_stress\"),\n",
    "            ])\n",
    "\n",
    "            # SHORT sample (PnL simÃ©trico lineal)\n",
    "            short_tr = dfx.filter(pl.col(\"signal_short\") == True).with_columns([\n",
    "                pl.lit(\"SHORT\").alias(\"side\"),\n",
    "                (-pl.col(\"fwd_ret\")).alias(\"gross_ret\"),\n",
    "                (-pl.col(\"fwd_ret\") - pl.lit(cost_base_dec)).alias(\"net_ret_base\"),\n",
    "                (-pl.col(\"fwd_ret\") - pl.lit(cost_stress_dec)).alias(\"net_ret_stress\"),\n",
    "            ])\n",
    "\n",
    "            comb = []\n",
    "            if long_tr.height > 0:\n",
    "                comb.append(long_tr)\n",
    "            if short_tr.height > 0:\n",
    "                comb.append(short_tr)\n",
    "            if not comb:\n",
    "                continue\n",
    "\n",
    "            tr = pl.concat(comb, how=\"vertical\")\n",
    "\n",
    "            # resumen\n",
    "            agg = (\n",
    "                tr\n",
    "                .group_by([\"segment\", \"side\"])\n",
    "                .agg([\n",
    "                    pl.len().alias(\"n_trades\"),\n",
    "                    pl.col(\"gross_ret\").mean().alias(\"gross_mean\"),\n",
    "                    pl.col(\"net_ret_base\").mean().alias(\"net_base_mean\"),\n",
    "                    pl.col(\"net_ret_stress\").mean().alias(\"net_stress_mean\"),\n",
    "                    pl.col(\"net_ret_base\").std().alias(\"net_base_std\"),\n",
    "                    pl.col(\"net_ret_base\").median().alias(\"net_base_median\"),\n",
    "                    pl.col(\"net_ret_base\").quantile(0.05, interpolation=\"nearest\").alias(\"net_base_p05\"),\n",
    "                    pl.col(\"net_ret_base\").quantile(0.95, interpolation=\"nearest\").alias(\"net_base_p95\"),\n",
    "                    (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "                ])\n",
    "                .with_columns([\n",
    "                    pl.when(pl.col(\"net_base_std\") > 0)\n",
    "                      .then(pl.col(\"net_base_mean\") / pl.col(\"net_base_std\"))\n",
    "                      .otherwise(None)\n",
    "                      .alias(\"sharpe_like_base\")\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            for r in agg.to_dicts():\n",
    "                rows.append({\n",
    "                    \"symbol\": sym_u,\n",
    "                    \"fold_id\": fid,\n",
    "                    \"horizon_tag\": tag_h,\n",
    "                    \"h_bars_after_entry\": int(h),\n",
    "                    \"segment\": r[\"segment\"],\n",
    "                    \"side\": r[\"side\"],\n",
    "                    \"n_trades\": int(r[\"n_trades\"]),\n",
    "                    \"gross_mean\": float(r[\"gross_mean\"]) if r[\"gross_mean\"] is not None else None,\n",
    "                    \"net_base_mean\": float(r[\"net_base_mean\"]) if r[\"net_base_mean\"] is not None else None,\n",
    "                    \"net_stress_mean\": float(r[\"net_stress_mean\"]) if r[\"net_stress_mean\"] is not None else None,\n",
    "                    \"net_base_std\": float(r[\"net_base_std\"]) if r[\"net_base_std\"] is not None else None,\n",
    "                    \"net_base_median\": float(r[\"net_base_median\"]) if r[\"net_base_median\"] is not None else None,\n",
    "                    \"net_base_p05\": float(r[\"net_base_p05\"]) if r[\"net_base_p05\"] is not None else None,\n",
    "                    \"net_base_p95\": float(r[\"net_base_p95\"]) if r[\"net_base_p95\"] is not None else None,\n",
    "                    \"win_rate_base\": float(r[\"win_rate_base\"]) if r[\"win_rate_base\"] is not None else None,\n",
    "                    \"sharpe_like_base\": float(r[\"sharpe_like_base\"]) if r[\"sharpe_like_base\"] is not None else None,\n",
    "                    \"cost_base_bps\": float(cost_base_bps),\n",
    "                    \"cost_stress_bps\": float(cost_stress_bps),\n",
    "                    \"thr_er\": float(thr_er),\n",
    "                    \"thr_mom_eff\": float(thr_mom_eff),\n",
    "                    \"thr_vol\": float(thr_vol),\n",
    "                })\n",
    "\n",
    "alpha_df = pl.DataFrame(rows) if rows else pl.DataFrame()\n",
    "\n",
    "out_parq = OUT_DIR / \"alpha_multi_horizon_report.parquet\"\n",
    "alpha_df.write_parquet(str(out_parq), compression=\"zstd\")\n",
    "\n",
    "snap_parq = SNAP_DIR / \"alpha_multi_horizon_report.parquet\"\n",
    "alpha_df.write_parquet(str(snap_parq), compression=\"zstd\")\n",
    "\n",
    "snap_json = SNAP_DIR / \"alpha_multi_horizon_snapshot.json\"\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": [f.get(\"fold_id\") for f in folds],\n",
    "    \"params\": {\n",
    "        \"HORIZONS\": HORIZONS,\n",
    "        \"ENFORCE_MON_FRI\": ENFORCE_MON_FRI,\n",
    "        \"ER_COL\": ER_COL,\n",
    "        \"MOM_COL\": MOM_COL,\n",
    "        \"VOL_COL\": VOL_COL,\n",
    "        \"NOTE\": \"SHORT usa simetrÃ­a simple con -thr_mom_eff (mejora: calibrar umbral short sobre abs(mom) o cola negativa).\",\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"artifacts_parquet\": str(out_parq),\n",
    "        \"snapshot_parquet\": str(snap_parq),\n",
    "    }\n",
    "}\n",
    "snap_json.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"alpha_report\"] = {\n",
    "    \"alpha_report_path\": str(out_parq),\n",
    "    \"alpha_report_snapshot_path\": str(snap_parq),\n",
    "    \"snapshot_json\": str(snap_json),\n",
    "    \"params\": snapshot[\"params\"],\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_parq} (OK) | rows={alpha_df.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_parq} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json} (OK)\")\n",
    "print(\">>> Celda 07C v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ff8c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 08 v1.1.1 :: Backtest Engine (TREND, M5) [LONG/SHORT + SL/TP/Trail + Gate-Hysteresis + Cooldown + Monâ€“Fri FLATTEN + Costs + WFO-safe]\n",
      "[Celda 08] symbols = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 08] folds   = ['F1']\n",
      "[Celda 08] params  = SL_ATR=2.5 TP_ATR=5.0 TRAIL_ATR=2.0 TIME_STOP_BARS=288 ENTRY_CONFIRM_BARS=12 EXIT_GATE_OFF_BARS=12 MIN_HOLD_BARS=6 COOLDOWN_BARS=24 EMA_FILTER=True MON_FRI=True\n",
      "[Celda 08] BNBUSD F1 :: done (sim) | trades=184 | costs(bps) base=8.0 stress=16.0\n",
      "[Celda 08] BTCUSD F1 :: done (sim) | trades=182 | costs(bps) base=8.0 stress=16.0\n",
      "[Celda 08] LVMH F1 :: done (sim) | trades=68 | costs(bps) base=12.0 stress=25.0\n",
      "[Celda 08] XAUAUD F1 :: done (sim) | trades=122 | costs(bps) base=4.0 stress=8.0\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\trades_engine_v10.parquet (OK) | rows=556\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\summary_engine_v10.parquet (OK) | rows=16\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\trades_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\summary_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\backtest_engine_v10_snapshot.json (OK)\n",
      ">>> Celda 08 v1.1.1 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 08 v1.1.1 â€” Backtest Engine (TREND, M5) [LONG/SHORT + SL/TP/Trail + Gate-Hysteresis + Cooldown + Monâ€“Fri FLATTEN + Costs + WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 08 v1.1.1 :: Backtest Engine (TREND, M5) [LONG/SHORT + SL/TP/Trail + Gate-Hysteresis + Cooldown + Monâ€“Fri FLATTEN + Costs + WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"data\", \"features\", \"wfo\", \"regime_gate\", \"cost_model\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 08] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths      = GLOBAL_STATE[\"paths\"]\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "feat_state = GLOBAL_STATE[\"features\"]\n",
    "wfo_state  = GLOBAL_STATE[\"wfo\"]\n",
    "gate_state = GLOBAL_STATE[\"regime_gate\"]\n",
    "cost_state = GLOBAL_STATE[\"cost_model\"]\n",
    "\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "symbols_u = [str(s).upper().strip() for s in symbols]\n",
    "\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 08] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: folds vacÃ­o (Celda 04 no produjo folds).\")\n",
    "\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"m5_clean_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not isinstance(m5_paths, dict) or not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: no hay m5_ohlcv_paths/m5_clean_paths/ohlcv_clean_paths. Ejecuta Celda 02/02C.\")\n",
    "\n",
    "feat_paths = feat_state.get(\"features_base_paths\") or {}\n",
    "if not feat_paths:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: no hay features_base_paths. Ejecuta Celda 05.\")\n",
    "\n",
    "gate_path = gate_state.get(\"gate_table_path\")\n",
    "if not gate_path:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: no hay regime_gate.gate_table_path. Ejecuta Celda 06.\")\n",
    "gate_df = pl.read_parquet(str(gate_path))\n",
    "\n",
    "costs_by_symbol = cost_state.get(\"costs_by_symbol\") or cost_state.get(\"symbols\") or {}\n",
    "if not costs_by_symbol:\n",
    "    raise RuntimeError(\"[Celda 08] ERROR: no hay cost_model.costs_by_symbol. Ejecuta Celda 03.\")\n",
    "cost_reported_is_roundtrip = bool(cost_state.get(\"cost_reported_is_roundtrip\", False))\n",
    "\n",
    "# ========================= ParÃ¡metros (engine v1.1.1) =========================\n",
    "ENGINE_LOGIC_VERSION = \"v1.1.1\"\n",
    "\n",
    "# SeÃ±al/rÃ©gimen (feature cols)\n",
    "ER_COL   = \"ER_kaufman\"\n",
    "MOM_COL  = \"mom_288\"\n",
    "VOL_COL  = \"vol_logret_288\"\n",
    "\n",
    "# GestiÃ³n (ATR)\n",
    "ATR_COL_CANDIDATES = [\"atr\", \"ATR\", \"atr_14\", \"atr_28\"]\n",
    "ATR_PCT_FALLBACK = 0.005  # si no hay ATR, proxy 0.5% del precio\n",
    "\n",
    "# Motor anti-churn\n",
    "SL_ATR    = 2.5\n",
    "TP_ATR    = 5.0\n",
    "TRAIL_ATR = 2.0\n",
    "TIME_STOP_BARS = 288          # ~1 dÃ­a M5\n",
    "\n",
    "ENTRY_CONFIRM_BARS = 12       # gate ON sostenido (1h)\n",
    "EXIT_GATE_OFF_BARS = 12       # gate OFF sostenido (1h)\n",
    "MIN_HOLD_BARS      = 6        # 30 min antes de permitir REGIME_OFF\n",
    "COOLDOWN_BARS      = 24       # 2h cooldown tras salida\n",
    "\n",
    "# Trend filter\n",
    "EMA_FILTER = True\n",
    "EMA_FAST = 48\n",
    "EMA_SLOW = 288\n",
    "\n",
    "# Calendario\n",
    "MON_FRI = True   # Monâ€“Fri only + flatten\n",
    "\n",
    "# Sizing simple risk-based\n",
    "USE_RISK_SIZING = True\n",
    "RISK_PER_TRADE  = 0.01\n",
    "MIN_POS_SIZE    = 0.25\n",
    "MAX_POS_SIZE    = 3.00\n",
    "\n",
    "print(f\"[Celda 08] symbols = {symbols_u}\")\n",
    "print(f\"[Celda 08] folds   = {[f.get('fold_id') for f in folds]}\")\n",
    "print(f\"[Celda 08] params  = SL_ATR={SL_ATR} TP_ATR={TP_ATR} TRAIL_ATR={TRAIL_ATR} TIME_STOP_BARS={TIME_STOP_BARS} \"\n",
    "      f\"ENTRY_CONFIRM_BARS={ENTRY_CONFIRM_BARS} EXIT_GATE_OFF_BARS={EXIT_GATE_OFF_BARS} MIN_HOLD_BARS={MIN_HOLD_BARS} COOLDOWN_BARS={COOLDOWN_BARS} \"\n",
    "      f\"EMA_FILTER={EMA_FILTER} MON_FRI={MON_FRI}\")\n",
    "\n",
    "# ========================= Outputs (compat v10) =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"backtests\" / \"backtest_engine_v10\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TRADES  = OUT_DIR / \"trades_engine_v10.parquet\"\n",
    "OUT_SUMMARY = OUT_DIR / \"summary_engine_v10.parquet\"\n",
    "SNAP_TRADES  = SNAP_DIR / \"trades_engine_v10.parquet\"\n",
    "SNAP_SUMMARY = SNAP_DIR / \"summary_engine_v10.parquet\"\n",
    "SNAP_JSON    = SNAP_DIR / \"backtest_engine_v10_snapshot.json\"\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    return _ensure_utc(datetime.fromisoformat(s))\n",
    "\n",
    "def _pick_path_case_insensitive(d: dict, sym_u: str) -> str:\n",
    "    if sym_u in d:\n",
    "        return str(d[sym_u])\n",
    "    keys = {str(k).upper().strip(): k for k in d.keys()}\n",
    "    if sym_u in keys:\n",
    "        return str(d[keys[sym_u]])\n",
    "    raise KeyError(f\"[Celda 08] ERROR: no encuentro path para {sym_u}. keys_sample={list(d)[:10]}\")\n",
    "\n",
    "def _need_cols(df: pl.DataFrame, cols: list[str], sym: str, tag: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 08] ERROR: {sym} ({tag}) missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "def _cost_roundtrip_dec(cost_bps: float, reported_is_roundtrip: bool) -> float:\n",
    "    c = float(cost_bps) / 10000.0\n",
    "    return c if reported_is_roundtrip else (2.0 * c)\n",
    "\n",
    "def _get_atr_col(df: pl.DataFrame) -> str | None:\n",
    "    for c in ATR_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _is_finite(x) -> bool:\n",
    "    if x is None:\n",
    "        return False\n",
    "    try:\n",
    "        return math.isfinite(float(x))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _is_weekend_dt(dt: datetime) -> bool:\n",
    "    # Python: 0=Mon ... 6=Sun\n",
    "    return int(dt.weekday()) >= 5\n",
    "\n",
    "# ========================= SimulaciÃ³n =========================\n",
    "def _simulate_symbol_fold(\n",
    "    sym_u: str,\n",
    "    df_join: pl.DataFrame,\n",
    "    fold: dict,\n",
    "    thr_er: float,\n",
    "    thr_mom: float,\n",
    "    thr_vol: float,\n",
    "    cost_base_dec: float,\n",
    "    cost_stress_dec: float,\n",
    "    cost_base_bps: float,\n",
    "    cost_stress_bps: float,\n",
    ") -> tuple[list[dict], dict]:\n",
    "    fid = str(fold[\"fold_id\"])\n",
    "    is_s = _parse_iso_utc(fold[\"IS_start\"])\n",
    "    is_e = _parse_iso_utc(fold[\"IS_end\"])\n",
    "    o_s  = _parse_iso_utc(fold[\"OOS_start\"])\n",
    "    o_e  = _parse_iso_utc(fold[\"OOS_end\"])\n",
    "\n",
    "    # ATR\n",
    "    atr_col = _get_atr_col(df_join)\n",
    "    if atr_col is None:\n",
    "        df_join = df_join.with_columns((pl.col(\"close\") * pl.lit(ATR_PCT_FALLBACK)).alias(\"__atr_used\"))\n",
    "        atr_col = \"__atr_used\"\n",
    "\n",
    "    # EMA filter\n",
    "    if EMA_FILTER:\n",
    "        df_join = df_join.with_columns([\n",
    "            pl.col(\"close\").ewm_mean(span=EMA_FAST, adjust=False).alias(\"__ema_fast\"),\n",
    "            pl.col(\"close\").ewm_mean(span=EMA_SLOW, adjust=False).alias(\"__ema_slow\"),\n",
    "        ])\n",
    "    else:\n",
    "        df_join = df_join.with_columns([\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"__ema_fast\"),\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"__ema_slow\"),\n",
    "        ])\n",
    "\n",
    "    # Gate raw\n",
    "    thr_mom_eff = max(0.0, float(thr_mom))\n",
    "    long_raw = (\n",
    "        (pl.col(ER_COL) >= pl.lit(float(thr_er))) &\n",
    "        (pl.col(MOM_COL) >= pl.lit(float(thr_mom_eff))) &\n",
    "        (pl.col(VOL_COL) <= pl.lit(float(thr_vol)))\n",
    "    )\n",
    "    short_raw = (\n",
    "        (pl.col(ER_COL) >= pl.lit(float(thr_er))) &\n",
    "        (pl.col(MOM_COL) <= pl.lit(-float(thr_mom_eff))) &\n",
    "        (pl.col(VOL_COL) <= pl.lit(float(thr_vol)))\n",
    "    )\n",
    "\n",
    "    if EMA_FILTER:\n",
    "        long_raw  = long_raw  & (pl.col(\"__ema_fast\") > pl.col(\"__ema_slow\"))\n",
    "        short_raw = short_raw & (pl.col(\"__ema_fast\") < pl.col(\"__ema_slow\"))\n",
    "\n",
    "    df_join = df_join.with_columns([\n",
    "        long_raw.alias(\"__gate_long_raw\"),\n",
    "        short_raw.alias(\"__gate_short_raw\"),\n",
    "    ])\n",
    "\n",
    "    # ========== DOW robusto (Polars weekday puede ser 0-6 o 1-7 segÃºn versiÃ³n) ==========\n",
    "    # 1) calcular weekday raw\n",
    "    df_join = df_join.with_columns(pl.col(\"time_utc\").dt.weekday().cast(pl.Int16).alias(\"__dow_raw\"))\n",
    "    max_dow = df_join.select(pl.col(\"__dow_raw\").max()).item()\n",
    "    # Si max > 6, asumimos convenciÃ³n 1..7 y convertimos a 0..6\n",
    "    if (max_dow is not None) and int(max_dow) > 6:\n",
    "        df_join = df_join.with_columns((pl.col(\"__dow_raw\") - 1).alias(\"__dow0\"))\n",
    "    else:\n",
    "        df_join = df_join.with_columns(pl.col(\"__dow_raw\").alias(\"__dow0\"))\n",
    "\n",
    "    df_join = df_join.with_columns((pl.col(\"__dow0\") >= 5).alias(\"__is_weekend\"))\n",
    "\n",
    "    # ConfirmaciÃ³n de entrada (rolling window)\n",
    "    df_join = df_join.with_columns([\n",
    "        (pl.col(\"__gate_long_raw\").cast(pl.Int8)\n",
    "            .rolling_sum(ENTRY_CONFIRM_BARS, min_samples=ENTRY_CONFIRM_BARS)\n",
    "            .eq(pl.lit(ENTRY_CONFIRM_BARS))\n",
    "        ).fill_null(False).alias(\"__gate_long_confirm\"),\n",
    "        (pl.col(\"__gate_short_raw\").cast(pl.Int8)\n",
    "            .rolling_sum(ENTRY_CONFIRM_BARS, min_samples=ENTRY_CONFIRM_BARS)\n",
    "            .eq(pl.lit(ENTRY_CONFIRM_BARS))\n",
    "        ).fill_null(False).alias(\"__gate_short_confirm\"),\n",
    "    ])\n",
    "\n",
    "    # --- Listas ---\n",
    "    t   = df_join.get_column(\"time_utc\").to_list()\n",
    "    o   = df_join.get_column(\"open\").to_list()\n",
    "    h   = df_join.get_column(\"high\").to_list()\n",
    "    l_  = df_join.get_column(\"low\").to_list()\n",
    "    c   = df_join.get_column(\"close\").to_list()\n",
    "    atr = df_join.get_column(atr_col).to_list()\n",
    "    mom = df_join.get_column(MOM_COL).to_list()\n",
    "\n",
    "    gateL = df_join.get_column(\"__gate_long_raw\").to_list()\n",
    "    gateS = df_join.get_column(\"__gate_short_raw\").to_list()\n",
    "    confL = df_join.get_column(\"__gate_long_confirm\").to_list()\n",
    "    confS = df_join.get_column(\"__gate_short_confirm\").to_list()\n",
    "    is_wk = df_join.get_column(\"__is_weekend\").to_list()\n",
    "\n",
    "    n = len(t)\n",
    "    if n < 10:\n",
    "        return [], {\"IS\": 0, \"OOS\": 0}\n",
    "\n",
    "    # Bars por segmento (exposure)\n",
    "    seg_bar_counts = {\"IS\": 0, \"OOS\": 0}\n",
    "    for i in range(n):\n",
    "        ti = t[i]\n",
    "        if ti is None:\n",
    "            continue\n",
    "        if is_s <= ti <= is_e:\n",
    "            seg_bar_counts[\"IS\"] += 1\n",
    "        elif o_s <= ti <= o_e:\n",
    "            seg_bar_counts[\"OOS\"] += 1\n",
    "\n",
    "    trades_rows: list[dict] = []\n",
    "\n",
    "    pos = 0\n",
    "    side_str = None\n",
    "    entry_idx = None\n",
    "    entry_time = None\n",
    "    entry_price = None\n",
    "    seg = None\n",
    "    seg_end = None\n",
    "\n",
    "    stop = None\n",
    "    tp = None\n",
    "    trail_stop = None\n",
    "    best_price = None\n",
    "    sl_dist = None\n",
    "    tp_dist = None\n",
    "    trail_dist = None\n",
    "\n",
    "    pos_size = 1.0\n",
    "    gate_off_streak = 0\n",
    "    cooldown = 0\n",
    "\n",
    "    def _is_weekend_idx(ix: int) -> bool:\n",
    "        if ix < 0 or ix >= n:\n",
    "            return False\n",
    "        v = is_wk[ix]\n",
    "        return bool(v) if v is not None else False\n",
    "\n",
    "    def _segment_for_entry_time(et: datetime):\n",
    "        if is_s <= et <= is_e:\n",
    "            return \"IS\", is_e\n",
    "        if o_s <= et <= o_e:\n",
    "            return \"OOS\", o_e\n",
    "        return None, None\n",
    "\n",
    "    for idx in range(n):\n",
    "        # ========================= Exits =========================\n",
    "        if pos != 0 and entry_idx is not None and idx >= entry_idx:\n",
    "            bars_held = idx - entry_idx + 1\n",
    "\n",
    "            gate_now = bool(gateL[idx]) if pos == 1 else bool(gateS[idx])\n",
    "            if gate_now:\n",
    "                gate_off_streak = 0\n",
    "            else:\n",
    "                gate_off_streak += 1\n",
    "\n",
    "            exit_reason = None\n",
    "            exit_price = None\n",
    "\n",
    "            hi = float(h[idx]) if _is_finite(h[idx]) else float(c[idx])\n",
    "            lo = float(l_[idx]) if _is_finite(l_[idx]) else float(c[idx])\n",
    "            cl = float(c[idx]) if _is_finite(c[idx]) else float(o[idx])\n",
    "\n",
    "            if pos == 1:\n",
    "                if best_price is None:\n",
    "                    best_price = float(entry_price)\n",
    "                best_price = max(best_price, hi)\n",
    "                if trail_dist is not None:\n",
    "                    ts = best_price - float(trail_dist)\n",
    "                    trail_stop = ts if trail_stop is None else max(float(trail_stop), ts)\n",
    "\n",
    "                if stop is not None and lo <= float(stop):\n",
    "                    exit_reason, exit_price = \"SL\", float(stop)\n",
    "                elif trail_stop is not None and lo <= float(trail_stop):\n",
    "                    exit_reason, exit_price = \"TRAIL\", float(trail_stop)\n",
    "                elif tp is not None and hi >= float(tp):\n",
    "                    exit_reason, exit_price = \"TP\", float(tp)\n",
    "\n",
    "            else:\n",
    "                if best_price is None:\n",
    "                    best_price = float(entry_price)\n",
    "                best_price = min(best_price, lo)\n",
    "                if trail_dist is not None:\n",
    "                    ts = best_price + float(trail_dist)\n",
    "                    trail_stop = ts if trail_stop is None else min(float(trail_stop), ts)\n",
    "\n",
    "                if stop is not None and hi >= float(stop):\n",
    "                    exit_reason, exit_price = \"SL\", float(stop)\n",
    "                elif trail_stop is not None and hi >= float(trail_stop):\n",
    "                    exit_reason, exit_price = \"TRAIL\", float(trail_stop)\n",
    "                elif tp is not None and lo <= float(tp):\n",
    "                    exit_reason, exit_price = \"TP\", float(tp)\n",
    "\n",
    "            if exit_reason is None:\n",
    "                if bars_held >= int(TIME_STOP_BARS):\n",
    "                    exit_reason, exit_price = \"TIME\", cl\n",
    "                elif bars_held >= int(MIN_HOLD_BARS) and gate_off_streak >= int(EXIT_GATE_OFF_BARS):\n",
    "                    exit_reason, exit_price = \"REGIME_OFF\", cl\n",
    "                elif seg_end is not None and idx + 1 < n and t[idx + 1] > seg_end:\n",
    "                    exit_reason, exit_price = \"SEGMENT_END\", cl\n",
    "                elif MON_FRI and idx + 1 < n and _is_weekend_idx(idx + 1):\n",
    "                    exit_reason, exit_price = \"WEEKEND_FLATTEN\", cl\n",
    "\n",
    "            if exit_reason is not None:\n",
    "                et = t[idx]\n",
    "                ep = float(entry_price)\n",
    "                xp = float(exit_price)\n",
    "                if ep > 0.0 and xp > 0.0:\n",
    "                    raw_ret = (xp / ep - 1.0) * float(pos)\n",
    "                    gross_ret = raw_ret * float(pos_size)\n",
    "                    net_ret_base = gross_ret - float(cost_base_dec) * float(pos_size)\n",
    "                    net_ret_stress = gross_ret - float(cost_stress_dec) * float(pos_size)\n",
    "\n",
    "                    trades_rows.append({\n",
    "                        \"symbol\": sym_u,\n",
    "                        \"fold_id\": str(fid),\n",
    "                        \"segment\": str(seg),\n",
    "                        \"side\": str(side_str),\n",
    "                        \"entry_time\": _ensure_utc(entry_time).isoformat(),\n",
    "                        \"exit_time\": _ensure_utc(et).isoformat(),\n",
    "                        \"entry_price\": float(ep),\n",
    "                        \"exit_price\": float(xp),\n",
    "                        \"bars_held\": int(bars_held),\n",
    "                        \"exit_reason\": str(exit_reason),\n",
    "                        \"gross_ret\": float(gross_ret),\n",
    "                        \"net_ret_base\": float(net_ret_base),\n",
    "                        \"net_ret_stress\": float(net_ret_stress),\n",
    "                        \"cost_base_bps\": float(cost_base_bps),\n",
    "                        \"cost_stress_bps\": float(cost_stress_bps),\n",
    "                        \"pos_size\": float(pos_size),\n",
    "                        \"sl_atr\": float(SL_ATR),\n",
    "                        \"tp_atr\": float(TP_ATR),\n",
    "                        \"trail_atr\": float(TRAIL_ATR),\n",
    "                        \"time_stop_bars\": int(TIME_STOP_BARS),\n",
    "                    })\n",
    "\n",
    "                pos = 0\n",
    "                side_str = None\n",
    "                entry_idx = None\n",
    "                entry_time = None\n",
    "                entry_price = None\n",
    "                seg = None\n",
    "                seg_end = None\n",
    "\n",
    "                stop = None\n",
    "                tp = None\n",
    "                trail_stop = None\n",
    "                best_price = None\n",
    "                sl_dist = None\n",
    "                tp_dist = None\n",
    "                trail_dist = None\n",
    "\n",
    "                gate_off_streak = 0\n",
    "                cooldown = int(COOLDOWN_BARS)\n",
    "\n",
    "        # ========================= Entradas =========================\n",
    "        if pos == 0:\n",
    "            if idx >= n - 1:\n",
    "                continue\n",
    "\n",
    "            if cooldown > 0:\n",
    "                cooldown -= 1\n",
    "                continue\n",
    "\n",
    "            entry_ix = idx + 1\n",
    "            if entry_ix >= n:\n",
    "                continue\n",
    "\n",
    "            et = t[entry_ix]\n",
    "            if et is None:\n",
    "                continue\n",
    "\n",
    "            # Monâ€“Fri (doble blindaje):\n",
    "            #  1) por flag Polars robusto\n",
    "            #  2) por Python weekday() (fuente de verdad para QA)\n",
    "            if MON_FRI and (_is_weekend_idx(entry_ix) or _is_weekend_dt(et)):\n",
    "                continue\n",
    "\n",
    "            seg2, seg_end2 = _segment_for_entry_time(et)\n",
    "            if seg2 is None:\n",
    "                continue\n",
    "\n",
    "            can_long  = bool(confL[idx])\n",
    "            can_short = bool(confS[idx])\n",
    "            if not (can_long or can_short):\n",
    "                continue\n",
    "\n",
    "            chosen_pos = None\n",
    "            if can_long and (not can_short):\n",
    "                chosen_pos = 1\n",
    "            elif can_short and (not can_long):\n",
    "                chosen_pos = -1\n",
    "            else:\n",
    "                m = float(mom[idx]) if _is_finite(mom[idx]) else 0.0\n",
    "                chosen_pos = 1 if m >= 0 else -1\n",
    "\n",
    "            op = float(o[entry_ix]) if _is_finite(o[entry_ix]) else float(c[entry_ix])\n",
    "            if op <= 0.0:\n",
    "                continue\n",
    "\n",
    "            atr_e = float(atr[entry_ix]) if _is_finite(atr[entry_ix]) else (float(atr[idx]) if _is_finite(atr[idx]) else (op * ATR_PCT_FALLBACK))\n",
    "            if atr_e <= 0.0:\n",
    "                atr_e = op * ATR_PCT_FALLBACK\n",
    "\n",
    "            sl_d = float(SL_ATR) * float(atr_e)\n",
    "            tp_d = float(TP_ATR) * float(atr_e)\n",
    "            tr_d = float(TRAIL_ATR) * float(atr_e)\n",
    "            if sl_d <= 0.0 or tp_d <= 0.0 or tr_d <= 0.0:\n",
    "                continue\n",
    "\n",
    "            ps = 1.0\n",
    "            if USE_RISK_SIZING:\n",
    "                stop_pct = sl_d / op\n",
    "                if stop_pct > 0.0:\n",
    "                    ps = float(RISK_PER_TRADE) / float(stop_pct)\n",
    "                    ps = max(float(MIN_POS_SIZE), min(float(MAX_POS_SIZE), ps))\n",
    "\n",
    "            if chosen_pos == 1:\n",
    "                st = op - sl_d\n",
    "                tpv = op + tp_d\n",
    "                ts0 = op - tr_d\n",
    "                side = \"LONG\"\n",
    "            else:\n",
    "                st = op + sl_d\n",
    "                tpv = op - tp_d\n",
    "                ts0 = op + tr_d\n",
    "                side = \"SHORT\"\n",
    "\n",
    "            pos = int(chosen_pos)\n",
    "            side_str = side\n",
    "            entry_idx = int(entry_ix)\n",
    "            entry_time = et\n",
    "            entry_price = float(op)\n",
    "            seg = str(seg2)\n",
    "            seg_end = seg_end2\n",
    "\n",
    "            stop = float(st)\n",
    "            tp = float(tpv)\n",
    "            trail_stop = float(ts0)\n",
    "            best_price = float(op)\n",
    "\n",
    "            sl_dist = float(sl_d)\n",
    "            tp_dist = float(tp_d)\n",
    "            trail_dist = float(tr_d)\n",
    "\n",
    "            pos_size = float(ps)\n",
    "            gate_off_streak = 0\n",
    "\n",
    "    return trades_rows, seg_bar_counts\n",
    "\n",
    "# ========================= Main =========================\n",
    "all_trades = []\n",
    "segbars_rows = []\n",
    "\n",
    "for sym in symbols_u:\n",
    "    sym_u = sym.upper().strip()\n",
    "\n",
    "    px_path = _pick_path_case_insensitive(m5_paths, sym_u)\n",
    "    df_px = pl.read_parquet(str(px_path))\n",
    "    _need_cols(df_px, [\"time_utc\", \"close\"], sym_u, \"OHLCV\")\n",
    "\n",
    "    if \"open\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"open\"))\n",
    "    if \"high\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"high\"))\n",
    "    if \"low\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"low\"))\n",
    "\n",
    "    df_px = (\n",
    "        df_px\n",
    "        .with_columns([\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False),\n",
    "            pl.col(\"open\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"high\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"low\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "        ])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    feat_path = _pick_path_case_insensitive(feat_paths, sym_u)\n",
    "    df_f = pl.read_parquet(str(feat_path))\n",
    "    _need_cols(df_f, [\"time_utc\", ER_COL, MOM_COL, VOL_COL], sym_u, \"FEATURES\")\n",
    "    df_f = df_f.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False)).sort(\"time_utc\")\n",
    "\n",
    "    df = df_px.join(df_f, on=\"time_utc\", how=\"inner\")\n",
    "\n",
    "    cinfo = costs_by_symbol.get(sym_u) or costs_by_symbol.get(sym) or {}\n",
    "    cost_base_bps = float(cinfo.get(\"COST_BASE_BPS\", 0.0))\n",
    "    cost_stress_bps = float(cinfo.get(\"COST_STRESS_BPS\", 0.0))\n",
    "    cost_base_dec = _cost_roundtrip_dec(cost_base_bps, cost_reported_is_roundtrip)\n",
    "    cost_stress_dec = _cost_roundtrip_dec(cost_stress_bps, cost_reported_is_roundtrip)\n",
    "\n",
    "    for f in folds:\n",
    "        fid = str(f[\"fold_id\"])\n",
    "        g = gate_df.filter((pl.col(\"symbol\") == sym_u) & (pl.col(\"fold_id\") == fid))\n",
    "        if g.is_empty():\n",
    "            raise RuntimeError(f\"[Celda 08] ERROR: no hay thresholds en gate_df para {sym_u} {fid}.\")\n",
    "\n",
    "        thr_er = float(g.select(pl.col(\"thr_er\")).item())\n",
    "        thr_mom = float(g.select(pl.col(\"thr_mom\")).item())\n",
    "        thr_vol = float(g.select(pl.col(\"thr_vol\")).item())\n",
    "\n",
    "        rows, segbars = _simulate_symbol_fold(\n",
    "            sym_u=sym_u,\n",
    "            df_join=df,\n",
    "            fold=f,\n",
    "            thr_er=thr_er,\n",
    "            thr_mom=thr_mom,\n",
    "            thr_vol=thr_vol,\n",
    "            cost_base_dec=cost_base_dec,\n",
    "            cost_stress_dec=cost_stress_dec,\n",
    "            cost_base_bps=cost_base_bps,\n",
    "            cost_stress_bps=cost_stress_bps,\n",
    "        )\n",
    "\n",
    "        if rows:\n",
    "            all_trades.extend(rows)\n",
    "\n",
    "        for segk, nbars in (segbars or {}).items():\n",
    "            segbars_rows.append({\n",
    "                \"symbol\": sym_u,\n",
    "                \"fold_id\": fid,\n",
    "                \"segment\": segk,\n",
    "                \"segment_bars\": int(nbars),\n",
    "            })\n",
    "\n",
    "        print(f\"[Celda 08] {sym_u} {fid} :: done (sim) | trades={len(rows)} | costs(bps) base={cost_base_bps} stress={cost_stress_bps}\")\n",
    "\n",
    "# ========================= Persist trades + summary =========================\n",
    "if not all_trades:\n",
    "    raise RuntimeError(\"[Celda 08] GATE FAIL: no se generÃ³ ningÃºn trade (revisa thresholds / confirm / filtros).\")\n",
    "\n",
    "trades_df = pl.DataFrame(all_trades)\n",
    "\n",
    "REQ_TRADES_COLS = [\n",
    "    \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "    \"entry_time\",\"exit_time\",\n",
    "    \"entry_price\",\"exit_price\",\n",
    "    \"bars_held\",\"exit_reason\",\n",
    "    \"gross_ret\",\"net_ret_base\",\"net_ret_stress\",\n",
    "    \"cost_base_bps\",\"cost_stress_bps\",\n",
    "    \"pos_size\",\"sl_atr\",\"tp_atr\",\"trail_atr\",\"time_stop_bars\",\n",
    "]\n",
    "miss = [c for c in REQ_TRADES_COLS if c not in trades_df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 08] ERROR: trades_df missing cols={miss}. cols={trades_df.columns}\")\n",
    "\n",
    "segbars_df = pl.DataFrame(segbars_rows).group_by([\"symbol\",\"fold_id\",\"segment\"]).agg(\n",
    "    pl.col(\"segment_bars\").max().alias(\"segment_bars\")\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    trades_df\n",
    "    .group_by([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_trades\"),\n",
    "        pl.col(\"gross_ret\").mean().alias(\"gross_mean\"),\n",
    "        pl.col(\"net_ret_base\").mean().alias(\"net_base_mean\"),\n",
    "        pl.col(\"net_ret_stress\").mean().alias(\"net_stress_mean\"),\n",
    "        pl.col(\"net_ret_base\").std().alias(\"net_base_std\"),\n",
    "        (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "        (pl.col(\"net_ret_base\").fill_null(0.0).log1p().sum().exp() - 1.0).alias(\"tot_ret_base\"),\n",
    "        (pl.col(\"net_ret_stress\").fill_null(0.0).log1p().sum().exp() - 1.0).alias(\"tot_ret_stress\"),\n",
    "        pl.col(\"bars_held\").median().alias(\"bars_held_med\"),\n",
    "        pl.col(\"bars_held\").sum().alias(\"__held_bars\"),\n",
    "    ])\n",
    "    .join(segbars_df, on=[\"symbol\",\"fold_id\",\"segment\"], how=\"left\")\n",
    "    .with_columns([\n",
    "        (pl.col(\"__held_bars\") / pl.col(\"segment_bars\").cast(pl.Float64)).alias(\"exposure_bar_share\"),\n",
    "    ])\n",
    "    .drop([\"__held_bars\"])\n",
    "    .sort([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    ")\n",
    "\n",
    "trades_df.write_parquet(str(OUT_TRADES), compression=\"zstd\")\n",
    "summary_df.write_parquet(str(OUT_SUMMARY), compression=\"zstd\")\n",
    "trades_df.write_parquet(str(SNAP_TRADES), compression=\"zstd\")\n",
    "summary_df.write_parquet(str(SNAP_SUMMARY), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"engine_logic_version\": ENGINE_LOGIC_VERSION,\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": [f.get(\"fold_id\") for f in folds],\n",
    "    \"params\": {\n",
    "        \"SL_ATR\": SL_ATR,\n",
    "        \"TP_ATR\": TP_ATR,\n",
    "        \"TRAIL_ATR\": TRAIL_ATR,\n",
    "        \"TIME_STOP_BARS\": TIME_STOP_BARS,\n",
    "        \"ENTRY_CONFIRM_BARS\": ENTRY_CONFIRM_BARS,\n",
    "        \"EXIT_GATE_OFF_BARS\": EXIT_GATE_OFF_BARS,\n",
    "        \"MIN_HOLD_BARS\": MIN_HOLD_BARS,\n",
    "        \"COOLDOWN_BARS\": COOLDOWN_BARS,\n",
    "        \"EMA_FILTER\": EMA_FILTER,\n",
    "        \"EMA_FAST\": EMA_FAST,\n",
    "        \"EMA_SLOW\": EMA_SLOW,\n",
    "        \"MON_FRI\": MON_FRI,\n",
    "        \"USE_RISK_SIZING\": USE_RISK_SIZING,\n",
    "        \"RISK_PER_TRADE\": RISK_PER_TRADE,\n",
    "        \"MIN_POS_SIZE\": MIN_POS_SIZE,\n",
    "        \"MAX_POS_SIZE\": MAX_POS_SIZE,\n",
    "        \"COST_REPORTED_IS_ROUNDTRIP\": cost_reported_is_roundtrip,\n",
    "        \"DOW_ROBUST_FIX\": True,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"trades_path\": str(OUT_TRADES),\n",
    "        \"summary_path\": str(OUT_SUMMARY),\n",
    "        \"snap_trades\": str(SNAP_TRADES),\n",
    "        \"snap_summary\": str(SNAP_SUMMARY),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Engine v1.1.1: FIX DOW robusto (Polars weekday 0-6 o 1-7) + blindaje Python weekday para Monâ€“Fri.\",\n",
    "        \"Entrada con confirmaciÃ³n y salida por rÃ©gimen con histeresis + min-hold + cooldown.\",\n",
    "        \"SL/TP/Trail intrabar (high/low).\",\n",
    "        \"Monâ€“Fri FLATTEN + Segment boundary flatten.\",\n",
    "    ],\n",
    "}\n",
    "SNAP_JSON.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"engine\"] = {\n",
    "    \"engine_logic_version\": ENGINE_LOGIC_VERSION,\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"trades_path\": str(OUT_TRADES),\n",
    "    \"summary_path\": str(OUT_SUMMARY),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "    \"params\": snapshot[\"params\"],\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_TRADES} (OK) | rows={trades_df.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_SUMMARY} (OK) | rows={summary_df.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_TRADES} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_SUMMARY} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 08 v1.1.1 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0dde4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 08B v1.0.4 :: QA Institucional del Motor (Engine) [Monâ€“Fri evidence + hard gate | weekday-robusto + dtype-fix]\n",
      "[Celda 08B] trades rows = 524\n",
      "[Celda 08B] weekend_entry_share = 0.000000 weekend_exit_share=0.000000 (ENFORCE_MON_FRI_GATE=True)\n",
      "[Celda 08B] weekend_entry_share por symbol/segment (top):\n",
      "shape: (8, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† segment â”† n_trades â”† weekend_entry_share â”† weekend_exit_share â”‚\n",
      "â”‚ ---    â”† ---     â”† ---     â”† ---      â”† ---                 â”† ---                â”‚\n",
      "â”‚ str    â”† str     â”† str     â”† u32      â”† f64                 â”† f64                â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† 149      â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† 133      â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† IS      â”† 106      â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† 52       â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† OOS     â”† 39       â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† 18       â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† 16       â”† 0.0                 â”† 0.0                â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† OOS     â”† 11       â”† 0.0                 â”† 0.0                â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\qa_weekend_entries_engine_v10.parquet (OK) | rows=0\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_weekend_entries_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_engine_monfri_evidence_snapshot.json (OK)\n",
      ">>> Celda 08B v1.0.4 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 08B v1.0.4 â€” QA Institucional del Motor (Engine) [Monâ€“Fri evidence + hard gate | weekday-robusto + dtype-fix] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 08B v1.0.4 :: QA Institucional del Motor (Engine) [Monâ€“Fri evidence + hard gate | weekday-robusto + dtype-fix]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 08B] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "# tolerar nombres de estado (por compatibilidad)\n",
    "bt = None\n",
    "if \"backtest_engine\" in GLOBAL_STATE:\n",
    "    bt = GLOBAL_STATE[\"backtest_engine\"]\n",
    "elif \"engine\" in GLOBAL_STATE:\n",
    "    bt = GLOBAL_STATE[\"engine\"]\n",
    "else:\n",
    "    raise RuntimeError(\"[Celda 08B] ERROR: falta GLOBAL_STATE['backtest_engine'] o GLOBAL_STATE['engine'].\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\") or {}\n",
    "if not paths:\n",
    "    raise RuntimeError(\"[Celda 08B] ERROR: falta GLOBAL_STATE['paths'].\")\n",
    "\n",
    "trades_path = bt.get(\"trades_path\")\n",
    "summary_path = bt.get(\"summary_path\")\n",
    "if not trades_path or not Path(trades_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 08B] ERROR: trades_path invÃ¡lido/no existe: {trades_path}\")\n",
    "if not summary_path or not Path(summary_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 08B] ERROR: summary_path invÃ¡lido/no existe: {summary_path}\")\n",
    "\n",
    "# params (tal como los guarda Celda 08)\n",
    "params = bt.get(\"params\", {}) or {}\n",
    "ENFORCE_MON_FRI_GATE = bool(params.get(\"MON_FRI\", params.get(\"ENFORCE_MON_FRI\", True)))\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"backtests\" / \"backtest_engine_v10\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_weekend = OUT_DIR / \"qa_weekend_entries_engine_v10.parquet\"\n",
    "snap_weekend = SNAP_DIR / \"qa_weekend_entries_engine_v10.parquet\"\n",
    "snap_json = SNAP_DIR / \"qa_engine_monfri_evidence_snapshot.json\"\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_dt_any(s: str) -> datetime | None:\n",
    "    \"\"\"\n",
    "    Robusto a ISO con 'Z' o con offset.\n",
    "    Devuelve datetime UTC o None.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    try:\n",
    "        ss = str(s).strip()\n",
    "        if ss.endswith(\"Z\"):\n",
    "            ss = ss[:-1] + \"+00:00\"\n",
    "        return _ensure_utc(datetime.fromisoformat(ss))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _py_weekday(dtobj: datetime | None) -> int | None:\n",
    "    \"\"\"\n",
    "    Python weekday(): Mon=0 ... Sun=6 (estable).\n",
    "    \"\"\"\n",
    "    if dtobj is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(dtobj.weekday())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ========================= Load trades =========================\n",
    "df = pl.read_parquet(str(trades_path))\n",
    "\n",
    "REQ = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"entry_time\",\"exit_time\",\"bars_held\",\"exit_reason\",\"net_ret_base\",\"net_ret_stress\"]\n",
    "miss = [c for c in REQ if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 08B] ERROR: trades parquet no tiene columnas requeridas: {miss}\")\n",
    "\n",
    "# Cast a string y parse a datetime UTC (python)\n",
    "df = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_time\").cast(pl.Utf8),\n",
    "        pl.col(\"exit_time\").cast(pl.Utf8),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"entry_dt\"),\n",
    "        pl.col(\"exit_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"exit_dt\"),\n",
    "    ])\n",
    "    # weekday robusto vÃ­a Python; FIX dtype: declarar Int64 y luego castear a Int32\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_dt\").map_elements(_py_weekday, return_dtype=pl.Int64).cast(pl.Int32).alias(\"entry_dow\"),  # Mon=0..Sun=6\n",
    "        pl.col(\"exit_dt\").map_elements(_py_weekday, return_dtype=pl.Int64).cast(pl.Int32).alias(\"exit_dow\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"entry_dow\") >= 5).alias(\"entry_is_weekend\"),\n",
    "        (pl.col(\"exit_dow\") >= 5).alias(\"exit_is_weekend\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f\"[Celda 08B] trades rows = {df.height}\")\n",
    "\n",
    "weekend_entry_share = float(df.select(pl.col(\"entry_is_weekend\").mean()).item() or 0.0)\n",
    "weekend_exit_share  = float(df.select(pl.col(\"exit_is_weekend\").mean()).item() or 0.0)\n",
    "print(f\"[Celda 08B] weekend_entry_share = {weekend_entry_share:.6f} weekend_exit_share={weekend_exit_share:.6f} (ENFORCE_MON_FRI_GATE={ENFORCE_MON_FRI_GATE})\")\n",
    "\n",
    "# ========================= Evidencia por sÃ­mbolo =========================\n",
    "by_sym = (\n",
    "    df.group_by([\"symbol\",\"fold_id\",\"segment\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"n_trades\"),\n",
    "          pl.col(\"entry_is_weekend\").mean().alias(\"weekend_entry_share\"),\n",
    "          pl.col(\"exit_is_weekend\").mean().alias(\"weekend_exit_share\"),\n",
    "      ])\n",
    "      .sort([\"weekend_entry_share\",\"n_trades\"], descending=[True, True])\n",
    ")\n",
    "\n",
    "print(\"[Celda 08B] weekend_entry_share por symbol/segment (top):\")\n",
    "print(by_sym.head(20))\n",
    "\n",
    "# Persist offenders (para inspecciÃ³n)\n",
    "off = (\n",
    "    df.filter(pl.col(\"entry_is_weekend\") == True)\n",
    "      .select([\n",
    "          \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "          \"entry_time\",\"exit_time\",\"bars_held\",\"exit_reason\",\n",
    "          \"net_ret_base\",\"net_ret_stress\",\"entry_dow\",\"exit_dow\"\n",
    "      ])\n",
    "      .sort([\"symbol\",\"entry_time\"])\n",
    ")\n",
    "\n",
    "off.write_parquet(str(out_weekend), compression=\"zstd\")\n",
    "off.write_parquet(str(snap_weekend), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\"trades_path\": str(trades_path), \"summary_path\": str(summary_path)},\n",
    "    \"params\": {\"ENFORCE_MON_FRI_GATE\": ENFORCE_MON_FRI_GATE},\n",
    "    \"metrics\": {\n",
    "        \"weekend_entry_share\": float(weekend_entry_share),\n",
    "        \"weekend_exit_share\": float(weekend_exit_share),\n",
    "        \"n_weekend_entries\": int(off.height),\n",
    "    },\n",
    "    \"outputs\": {\"weekend_entries_artifacts\": str(out_weekend), \"weekend_entries_snapshot\": str(snap_weekend)},\n",
    "    \"notes\": [\n",
    "        \"weekday se calcula con python datetime.weekday(): Mon=0..Sun=6 (estable).\",\n",
    "        \"FIX: map_elements devuelve Int64; se castea a Int32 para consistencia.\",\n",
    "        \"El gate Monâ€“Fri debe aplicarse sobre entry_time (t+1), no sobre signal_time (t).\",\n",
    "    ],\n",
    "}\n",
    "snap_json.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {out_weekend} (OK) | rows={off.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_weekend} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json} (OK)\")\n",
    "\n",
    "# ========================= Gate: Monâ€“Fri compliance =========================\n",
    "if ENFORCE_MON_FRI_GATE and off.height > 0:\n",
    "    top_bad = by_sym.select([\"symbol\",\"segment\",\"weekend_entry_share\",\"n_trades\"]).head(8).to_dicts()\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 08B] GATE FAIL: Hay entradas en fin de semana con Monâ€“Fri ON. \"\n",
    "        f\"share={weekend_entry_share:.6f} | offenders={off.height} | top={top_bad}\"\n",
    "    )\n",
    "\n",
    "print(\">>> Celda 08B v1.0.4 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f4f24ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 09 v1.0.1 :: Engine Institutional Report (post-08B) [equity + MDD + exposure + turnover + exits | WFO-safe]\n",
      "[Celda 09] weekend_entry_share=0.122137 weekend_exit_share=0.124046 (MON_FRI=True)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\reports_engine_v10\\equity_curve_engine_v10.parquet (OK) | rows=524\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\reports_engine_v10\\kpis_engine_v10.parquet (OK)   | rows=16\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\reports_engine_v10\\exit_reasons_engine_v10.parquet (OK)  | rows=47\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\equity_curve_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\kpis_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\exit_reasons_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\engine_report_snapshot.json (OK)\n",
      ">>> Celda 09 v1.0.1 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 09 v1.0.1 â€” Engine Institutional Report (post-08B)\n",
    "# [Equity curve + MDD + Exposure + Turnover + Exit reasons + WFO-safe | NO map_groups]\n",
    "# FIX: reemplaza group_by().map_groups() por window functions (cum_sum/cum_max over group)\n",
    "# =====================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 09 v1.0.1 :: Engine Institutional Report (post-08B) [equity + MDD + exposure + turnover + exits | WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 09] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "if \"paths\" not in GLOBAL_STATE:\n",
    "    raise RuntimeError(\"[Celda 09] ERROR: falta GLOBAL_STATE['paths'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "ART = Path(paths[\"artifacts\"]).resolve()\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (ART / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Localizar inputs del engine =========================\n",
    "# Preferir rutas en GLOBAL_STATE si existen; si no, usar layout estÃ¡ndar\n",
    "engine_state = GLOBAL_STATE.get(\"backtest_engine\", {}) or GLOBAL_STATE.get(\"engine\", {}) or {}\n",
    "engine_dir = ART / \"backtests\" / \"backtest_engine_v10\"\n",
    "\n",
    "trades_path = (\n",
    "    engine_state.get(\"trades_path\")\n",
    "    or engine_state.get(\"trades_engine_path\")\n",
    "    or str(engine_dir / \"trades_engine_v10.parquet\")\n",
    ")\n",
    "summary_path = (\n",
    "    engine_state.get(\"summary_path\")\n",
    "    or engine_state.get(\"summary_engine_path\")\n",
    "    or str(engine_dir / \"summary_engine_v10.parquet\")\n",
    ")\n",
    "\n",
    "trades_path = str(Path(trades_path).resolve())\n",
    "summary_path = str(Path(summary_path).resolve())\n",
    "\n",
    "if not Path(trades_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 09] ERROR: trades parquet no existe: {trades_path}\")\n",
    "\n",
    "df = pl.read_parquet(trades_path)\n",
    "\n",
    "REQ = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"entry_time\",\"exit_time\",\"net_ret_base\",\"net_ret_stress\",\"exit_reason\",\"bars_held\"]\n",
    "miss = [c for c in REQ if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 09] ERROR: trades parquet no tiene columnas requeridas: {miss}. cols={df.columns}\")\n",
    "\n",
    "# ========================= Cast times robusto (sin map_elements) =========================\n",
    "# entry_time/exit_time pueden venir como str o datetime; estandarizamos a datetime UTC\n",
    "df = df.with_columns([\n",
    "    pl.col(\"entry_time\").cast(pl.Utf8, strict=False).str.to_datetime(time_zone=\"UTC\", strict=False).alias(\"entry_dt\"),\n",
    "    pl.col(\"exit_time\").cast(pl.Utf8, strict=False).str.to_datetime(time_zone=\"UTC\", strict=False).alias(\"exit_dt\"),\n",
    "])\n",
    "\n",
    "if df.select([pl.col(\"entry_dt\").is_null().mean()]).item() > 0.0:\n",
    "    # fallback adicional: si por alguna razÃ³n to_datetime no parsea, intenta cast directo\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"entry_time\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False).alias(\"entry_dt\"),\n",
    "        pl.col(\"exit_time\").cast(pl.Datetime(\"us\", \"UTC\"), strict=False).alias(\"exit_dt\"),\n",
    "    ])\n",
    "\n",
    "# sanity\n",
    "null_entry = float(df.select(pl.col(\"entry_dt\").is_null().mean()).item())\n",
    "null_exit  = float(df.select(pl.col(\"exit_dt\").is_null().mean()).item())\n",
    "if null_entry > 1e-9 or null_exit > 1e-9:\n",
    "    raise RuntimeError(f\"[Celda 09] ERROR: no se pudieron parsear fechas. null_entry={null_entry} null_exit={null_exit}\")\n",
    "\n",
    "BARS_PER_DAY = 288\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"bars_held\") * 5).cast(pl.Int64).alias(\"hold_minutes\"),\n",
    "    (pl.col(\"bars_held\") >= pl.lit(BARS_PER_DAY)).alias(\"hold_ge_1d\"),\n",
    "])\n",
    "\n",
    "# ========================= Monâ€“Fri evidence (deberÃ­a estar en 0 tras 08 v1.0.1) =========================\n",
    "df = df.with_columns([\n",
    "    pl.col(\"entry_dt\").dt.weekday().cast(pl.Int32).alias(\"entry_dow\"),\n",
    "    pl.col(\"exit_dt\").dt.weekday().cast(pl.Int32).alias(\"exit_dow\"),\n",
    "]).with_columns([\n",
    "    (pl.col(\"entry_dow\") >= 5).alias(\"entry_is_weekend\"),\n",
    "    (pl.col(\"exit_dow\") >= 5).alias(\"exit_is_weekend\"),\n",
    "])\n",
    "\n",
    "weekend_entry_share = float(df.select(pl.col(\"entry_is_weekend\").mean()).item())\n",
    "weekend_exit_share  = float(df.select(pl.col(\"exit_is_weekend\").mean()).item())\n",
    "\n",
    "monfri = bool((GLOBAL_STATE.get(\"execution\", {}) or {}).get(\"MON_FRI\", True))\n",
    "print(f\"[Celda 09] weekend_entry_share={weekend_entry_share:.6f} weekend_exit_share={weekend_exit_share:.6f} (MON_FRI={monfri})\")\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = engine_dir / \"reports_engine_v10\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_EQUITY = OUT_DIR / \"equity_curve_engine_v10.parquet\"\n",
    "OUT_KPIS   = OUT_DIR / \"kpis_engine_v10.parquet\"\n",
    "OUT_EXITS  = OUT_DIR / \"exit_reasons_engine_v10.parquet\"\n",
    "\n",
    "SNAP_EQUITY = SNAP_DIR / \"equity_curve_engine_v10.parquet\"\n",
    "SNAP_KPIS   = SNAP_DIR / \"kpis_engine_v10.parquet\"\n",
    "SNAP_EXITS  = SNAP_DIR / \"exit_reasons_engine_v10.parquet\"\n",
    "\n",
    "# ========================= Equity curve (FIX: window functions) =========================\n",
    "group_cols = [\"symbol\",\"fold_id\",\"segment\",\"side\"]\n",
    "\n",
    "# Orden estricto para que cum_sum/cum_max sea determinista\n",
    "df2 = (\n",
    "    df.select([\n",
    "        \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "        \"entry_dt\",\"exit_dt\",\n",
    "        \"net_ret_base\",\"net_ret_stress\",\n",
    "        \"exit_reason\",\"bars_held\",\"hold_minutes\"\n",
    "    ])\n",
    "    .sort(group_cols + [\"entry_dt\",\"exit_dt\"])\n",
    "    .with_columns([\n",
    "        # clip para evitar log1p invÃ¡lido si hay retornos extremos (< -100%)\n",
    "        pl.col(\"net_ret_base\").fill_null(0.0).clip(-0.999999999, 10.0).alias(\"__r_base\"),\n",
    "        pl.col(\"net_ret_stress\").fill_null(0.0).clip(-0.999999999, 10.0).alias(\"__r_stress\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # trade_seq robusto (1..n) sin depender de cum_count/rank\n",
    "        pl.lit(1).cum_sum().over(group_cols).cast(pl.Int32).alias(\"trade_seq\"),\n",
    "        pl.col(\"__r_base\").log1p().cum_sum().over(group_cols).alias(\"__cumlog_base\"),\n",
    "        pl.col(\"__r_stress\").log1p().cum_sum().over(group_cols).alias(\"__cumlog_stress\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"__cumlog_base\").exp().alias(\"equity_base\"),\n",
    "        pl.col(\"__cumlog_stress\").exp().alias(\"equity_stress\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"equity_base\").cum_max().over(group_cols).alias(\"__peak_base\"),\n",
    "        pl.col(\"equity_stress\").cum_max().over(group_cols).alias(\"__peak_stress\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"equity_base\") / pl.col(\"__peak_base\") - 1.0).alias(\"dd_base\"),\n",
    "        (pl.col(\"equity_stress\") / pl.col(\"__peak_stress\") - 1.0).alias(\"dd_stress\"),\n",
    "    ])\n",
    "    .drop([\"__r_base\",\"__r_stress\",\"__cumlog_base\",\"__cumlog_stress\",\"__peak_base\",\"__peak_stress\"])\n",
    ")\n",
    "\n",
    "equity_df = df2.select([\n",
    "    \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "    \"trade_seq\",\"entry_dt\",\"exit_dt\",\n",
    "    \"equity_base\",\"equity_stress\",\"dd_base\",\"dd_stress\",\n",
    "    \"exit_reason\",\"bars_held\",\"hold_minutes\"\n",
    "])\n",
    "\n",
    "equity_df.write_parquet(str(OUT_EQUITY), compression=\"zstd\")\n",
    "equity_df.write_parquet(str(SNAP_EQUITY), compression=\"zstd\")\n",
    "\n",
    "# ========================= KPIs por grupo =========================\n",
    "kpis = (\n",
    "    df2.group_by(group_cols)\n",
    "       .agg([\n",
    "           pl.len().alias(\"n_trades\"),\n",
    "\n",
    "           pl.col(\"net_ret_base\").mean().alias(\"mean_ret_base\"),\n",
    "           pl.col(\"net_ret_base\").std().alias(\"std_ret_base\"),\n",
    "           (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "\n",
    "           pl.col(\"net_ret_stress\").mean().alias(\"mean_ret_stress\"),\n",
    "           pl.col(\"net_ret_stress\").std().alias(\"std_ret_stress\"),\n",
    "           (pl.col(\"net_ret_stress\") > 0).mean().alias(\"win_rate_stress\"),\n",
    "\n",
    "           # total return por log-suma (mÃ¡s estable)\n",
    "           (pl.col(\"net_ret_base\").fill_null(0.0).clip(-0.999999999, 10.0).log1p().sum().exp() - 1.0).alias(\"tot_ret_base\"),\n",
    "           (pl.col(\"net_ret_stress\").fill_null(0.0).clip(-0.999999999, 10.0).log1p().sum().exp() - 1.0).alias(\"tot_ret_stress\"),\n",
    "\n",
    "           # MDD desde equity_df: como tenemos dd_base/dd_stress por trade, min es drawdown mÃ¡ximo\n",
    "           pl.col(\"dd_base\").min().alias(\"mdd_base\"),\n",
    "           pl.col(\"dd_stress\").min().alias(\"mdd_stress\"),\n",
    "\n",
    "           pl.col(\"bars_held\").mean().alias(\"avg_bars_held\"),\n",
    "           pl.col(\"hold_minutes\").sum().alias(\"sum_hold_minutes\"),\n",
    "\n",
    "           pl.col(\"entry_dt\").min().alias(\"tmin\"),\n",
    "           pl.col(\"exit_dt\").max().alias(\"tmax\"),\n",
    "           pl.col(\"entry_dt\").dt.date().n_unique().alias(\"n_active_days\"),\n",
    "       ])\n",
    "       .with_columns([\n",
    "           # sharpe-like por trade (no anualizado; diagnÃ³stico)\n",
    "           (pl.col(\"mean_ret_base\") / pl.col(\"std_ret_base\")).alias(\"sharpe_like_base\"),\n",
    "           (pl.col(\"mean_ret_stress\") / pl.col(\"std_ret_stress\")).alias(\"sharpe_like_stress\"),\n",
    "\n",
    "           # turnover: trades por dÃ­a activo\n",
    "           (pl.col(\"n_trades\") / pl.col(\"n_active_days\").clip(1, 10_000)).alias(\"trades_per_active_day\"),\n",
    "\n",
    "           # exposure aproximada: minutos en mercado / minutos entre tmin..tmax\n",
    "           (\n",
    "               pl.col(\"sum_hold_minutes\") /\n",
    "               ((pl.col(\"tmax\") - pl.col(\"tmin\")).dt.total_minutes().clip(1, 10_000_000))\n",
    "           ).alias(\"exposure_share\"),\n",
    "       ])\n",
    "       .sort(group_cols)\n",
    ")\n",
    "\n",
    "kpis.write_parquet(str(OUT_KPIS), compression=\"zstd\")\n",
    "kpis.write_parquet(str(SNAP_KPIS), compression=\"zstd\")\n",
    "\n",
    "# ========================= Exit reasons =========================\n",
    "exits = (\n",
    "    df.group_by([\"symbol\",\"fold_id\",\"segment\",\"side\",\"exit_reason\"])\n",
    "      .agg([pl.len().alias(\"n\")])\n",
    "      .with_columns([\n",
    "          (pl.col(\"n\") / pl.col(\"n\").sum().over([\"symbol\",\"fold_id\",\"segment\",\"side\"])).alias(\"share\")\n",
    "      ])\n",
    "      .sort([\"symbol\",\"fold_id\",\"segment\",\"side\",\"n\"], descending=[False,False,False,False,True])\n",
    ")\n",
    "\n",
    "exits.write_parquet(str(OUT_EXITS), compression=\"zstd\")\n",
    "exits.write_parquet(str(SNAP_EXITS), compression=\"zstd\")\n",
    "\n",
    "# ========================= Snapshot JSON =========================\n",
    "snap = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\n",
    "        \"trades_path\": trades_path,\n",
    "        \"summary_path\": summary_path if Path(summary_path).exists() else None,\n",
    "    },\n",
    "    \"mon_fri\": {\n",
    "        \"enabled\": monfri,\n",
    "        \"weekend_entry_share\": weekend_entry_share,\n",
    "        \"weekend_exit_share\": weekend_exit_share,\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"equity_curve\": str(OUT_EQUITY),\n",
    "        \"kpis\": str(OUT_KPIS),\n",
    "        \"exit_reasons\": str(OUT_EXITS),\n",
    "        \"snap_equity\": str(SNAP_EQUITY),\n",
    "        \"snap_kpis\": str(SNAP_KPIS),\n",
    "        \"snap_exit_reasons\": str(SNAP_EXITS),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Equity curve computada con window functions (sin map_groups).\",\n",
    "        \"tot_ret_* computado por suma log1p para estabilidad numÃ©rica.\",\n",
    "        \"Exposure es aproximaciÃ³n por minutos en mercado / minutos entre tmin..tmax (por grupo).\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "snap_json = OUT_DIR / \"engine_report_snapshot.json\"\n",
    "snap_json.write_text(json.dumps(snap, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "snap_json2 = SNAP_DIR / \"engine_report_snapshot.json\"\n",
    "snap_json2.write_text(json.dumps(snap, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"engine_report\"] = {\n",
    "    \"equity_curve_path\": str(OUT_EQUITY),\n",
    "    \"kpis_path\": str(OUT_KPIS),\n",
    "    \"exit_reasons_path\": str(OUT_EXITS),\n",
    "    \"snapshot_json\": str(snap_json),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_EQUITY} (OK) | rows={equity_df.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_KPIS} (OK)   | rows={kpis.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_EXITS} (OK)  | rows={exits.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_EQUITY} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_KPIS} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_EXITS} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json2} (OK)\")\n",
    "print(\">>> Celda 09 v1.0.1 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b33c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10 v1.0 :: SelecciÃ³n Institucional post-Engine [OOS-first + gates + score]\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\selection\\selection_engine_v10.parquet (OK) | rows=4\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\selection_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\selection_engine_v10_snapshot.json (OK)\n",
      ">>> Celda 10 v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 10 v1.0 â€” SelecciÃ³n Institucional post-Engine (TREND, M5)\n",
    "# [OOS-first + gates + score + pick side per symbol | WFO-safe]\n",
    "# =====================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10 v1.0 :: SelecciÃ³n Institucional post-Engine [OOS-first + gates + score]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 10] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "ART = Path(paths[\"artifacts\"]).resolve()\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (ART / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rep = GLOBAL_STATE.get(\"engine_report\", {}) or {}\n",
    "kpis_path = rep.get(\"kpis_path\")\n",
    "if not kpis_path or not Path(kpis_path).exists():\n",
    "    raise RuntimeError(\"[Celda 10] ERROR: no encuentro engine_report.kpis_path. Ejecuta Celda 09 v1.0.1.\")\n",
    "\n",
    "kpis = pl.read_parquet(str(kpis_path))\n",
    "\n",
    "REQ = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"n_trades\",\"tot_ret_base\",\"mdd_base\",\"win_rate_base\",\"exposure_share\",\"trades_per_active_day\",\"sharpe_like_base\"]\n",
    "miss = [c for c in REQ if c not in kpis.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 10] ERROR: kpis missing cols={miss}. cols={kpis.columns}\")\n",
    "\n",
    "# ========================= ParÃ¡metros institucionales (gates) =========================\n",
    "# Nota: Estos gates son â€œprimer paseâ€ (comitÃ©). Luego viene la etapa de WFO/optim por IS.\n",
    "MIN_OOS_TRADES = 80\n",
    "MAX_MDD_BASE   = -0.20     # no peor que -20% (trade-level equity, por grupo)\n",
    "MIN_TOTRET_OOS = 0.00      # OOS >= 0 (base)\n",
    "MIN_WINRATE    = 0.48      # permisivo (depende del payoff)\n",
    "MAX_EXPOSURE   = 0.65      # no estar siempre en mercado\n",
    "\n",
    "# Score institucional (simple, explÃ­cito, audit-able)\n",
    "# - premia retorno OOS\n",
    "# - penaliza drawdown\n",
    "# - penaliza exceso de exposiciÃ³n\n",
    "# - bonus leve por sharpe-like\n",
    "def _score_expr():\n",
    "    return (\n",
    "        pl.col(\"tot_ret_base\")\n",
    "        + 0.15 * pl.col(\"sharpe_like_base\").fill_null(0.0).clip(-5, 5)\n",
    "        + 0.05 * (pl.col(\"win_rate_base\").fill_null(0.0) - 0.5)\n",
    "        - 1.25 * (-pl.col(\"mdd_base\")).fill_null(0.0)   # mdd_base es negativo\n",
    "        - 0.25 * pl.col(\"exposure_share\").fill_null(0.0)\n",
    "    ).alias(\"score_oos\")\n",
    "\n",
    "# ========================= Filtrar OOS y evaluar gates =========================\n",
    "oos = kpis.filter(pl.col(\"segment\") == \"OOS\").with_columns([_score_expr()])\n",
    "\n",
    "oos = oos.with_columns([\n",
    "    (pl.col(\"n_trades\") >= pl.lit(MIN_OOS_TRADES)).alias(\"gate_trades\"),\n",
    "    (pl.col(\"mdd_base\") >= pl.lit(MAX_MDD_BASE)).alias(\"gate_mdd\"),\n",
    "    (pl.col(\"tot_ret_base\") >= pl.lit(MIN_TOTRET_OOS)).alias(\"gate_ret\"),\n",
    "    (pl.col(\"win_rate_base\") >= pl.lit(MIN_WINRATE)).alias(\"gate_wr\"),\n",
    "    (pl.col(\"exposure_share\") <= pl.lit(MAX_EXPOSURE)).alias(\"gate_exposure\"),\n",
    "]).with_columns([\n",
    "    (pl.all_horizontal([\"gate_trades\",\"gate_mdd\",\"gate_ret\",\"gate_wr\",\"gate_exposure\"])).alias(\"gate_pass\"),\n",
    "])\n",
    "\n",
    "# ========================= Elegir side por sÃ­mbolo (mejor score OOS) =========================\n",
    "# Si ningÃºn side pasa gate_pass, igual guardamos diagnÃ³stico como FAIL.\n",
    "best = (\n",
    "    oos.sort([\"symbol\",\"fold_id\",\"gate_pass\",\"score_oos\"], descending=[False,False,True,True])\n",
    "       .group_by([\"symbol\",\"fold_id\"], maintain_order=True)\n",
    "       .agg([\n",
    "           pl.first(\"side\").alias(\"picked_side\"),\n",
    "           pl.first(\"gate_pass\").alias(\"picked_gate_pass\"),\n",
    "           pl.first(\"score_oos\").alias(\"picked_score_oos\"),\n",
    "           pl.first(\"n_trades\").alias(\"picked_n_trades_oos\"),\n",
    "           pl.first(\"tot_ret_base\").alias(\"picked_tot_ret_oos\"),\n",
    "           pl.first(\"mdd_base\").alias(\"picked_mdd_oos\"),\n",
    "           pl.first(\"win_rate_base\").alias(\"picked_win_rate_oos\"),\n",
    "           pl.first(\"exposure_share\").alias(\"picked_exposure_oos\"),\n",
    "           pl.first(\"sharpe_like_base\").alias(\"picked_sharpe_like_oos\"),\n",
    "           pl.first(\"trades_per_active_day\").alias(\"picked_trades_per_day_oos\"),\n",
    "       ])\n",
    ")\n",
    "\n",
    "# diagnÃ³stico: cuÃ¡ntos pasan gates\n",
    "diag = (\n",
    "    oos.group_by([\"symbol\",\"fold_id\"])\n",
    "       .agg([\n",
    "           pl.len().alias(\"n_candidates\"),\n",
    "           pl.col(\"gate_pass\").sum().alias(\"n_gate_pass\"),\n",
    "           pl.col(\"score_oos\").max().alias(\"best_score_any\"),\n",
    "       ])\n",
    ")\n",
    "\n",
    "sel = best.join(diag, on=[\"symbol\",\"fold_id\"], how=\"left\").with_columns([\n",
    "    pl.when(pl.col(\"picked_gate_pass\") == True).then(pl.lit(\"GO\")).otherwise(pl.lit(\"NO_GO\")).alias(\"status\"),\n",
    "])\n",
    "\n",
    "# ========================= Persistencia =========================\n",
    "OUT_SEL_DIR = ART / \"selection\"\n",
    "OUT_SEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_SEL = OUT_SEL_DIR / \"selection_engine_v10.parquet\"\n",
    "SNAP_SEL = SNAP_DIR / \"selection_engine_v10.parquet\"\n",
    "\n",
    "sel.write_parquet(str(OUT_SEL), compression=\"zstd\")\n",
    "sel.write_parquet(str(SNAP_SEL), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\"kpis_path\": str(kpis_path)},\n",
    "    \"gates\": {\n",
    "        \"MIN_OOS_TRADES\": MIN_OOS_TRADES,\n",
    "        \"MAX_MDD_BASE\": MAX_MDD_BASE,\n",
    "        \"MIN_TOTRET_OOS\": MIN_TOTRET_OOS,\n",
    "        \"MIN_WINRATE\": MIN_WINRATE,\n",
    "        \"MAX_EXPOSURE\": MAX_EXPOSURE,\n",
    "    },\n",
    "    \"outputs\": {\"selection\": str(OUT_SEL), \"snapshot_selection\": str(SNAP_SEL)},\n",
    "    \"notes\": [\n",
    "        \"SelecciÃ³n basada en OOS (post-engine) por sÃ­mbolo/fold: elige side con mejor score si pasa gates.\",\n",
    "        \"Si ningÃºn side pasa, status=NO_GO (pero queda diagnÃ³stico y best_score_any).\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "snap_json = OUT_SEL_DIR / \"selection_engine_v10_snapshot.json\"\n",
    "snap_json.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "snap_json2 = SNAP_DIR / \"selection_engine_v10_snapshot.json\"\n",
    "snap_json2.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"selection\"] = {\n",
    "    \"selection_path\": str(OUT_SEL),\n",
    "    \"snapshot_parquet\": str(SNAP_SEL),\n",
    "    \"snapshot_json\": str(snap_json),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_SEL} (OK) | rows={sel.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_SEL} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {snap_json2} (OK)\")\n",
    "print(\">>> Celda 10 v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "112a851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 11 v1.0.2 :: Deploy Pack institucional (freeze) [fix score-detect + fallback TOPK | WFO-safe]\n",
      "[Celda 11] selection rows = 4\n",
      "[Celda 11] selection cols = ['symbol', 'fold_id', 'picked_side', 'picked_gate_pass', 'picked_score_oos', 'picked_n_trades_oos', 'picked_tot_ret_oos', 'picked_mdd_oos', 'picked_win_rate_oos', 'picked_exposure_oos', 'picked_sharpe_like_oos', 'picked_trades_per_day_oos', 'n_candidates', 'n_gate_pass', 'best_score_any', 'status']\n",
      "[Celda 11] status counts:\n",
      "shape: (1, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
      "â”‚ status â”† n   â”‚\n",
      "â”‚ ---    â”† --- â”‚\n",
      "â”‚ str    â”† u32 â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
      "â”‚ NO_GO  â”† 4   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 11] deploy selection mode = FALLBACK_TOPK_BY_picked_score_oos | rows=2\n",
      "[Celda 11] exported rows = 2 | mode=FALLBACK_TOPK_BY_picked_score_oos\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\deploy\\deploy_pack_v10.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\deploy\\deploy_pack_v10.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\deploy_pack_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\deploy_pack_v10.json (OK)\n",
      ">>> Celda 11 v1.0.2 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 11 v1.0.2 â€” Deploy Pack institucional (Freeze config)\n",
    "# [robusto: GO-like -> si no hay, fallback TOPK por score (incluye picked_score_oos) -> si no hay score, export ALL]\n",
    "# =====================================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 11 v1.0.2 :: Deploy Pack institucional (freeze) [fix score-detect + fallback TOPK | WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "ART = Path(paths[\"artifacts\"]).resolve()\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (ART / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Config =========================\n",
    "FILTER_GO_LIKE_FIRST = True\n",
    "GO_LIKE = {\"GO\", \"OK\", \"PASS\", \"SELECTED\", \"SELECT\"}   # ampliable\n",
    "\n",
    "# Si NO hay GO-like, intentar TOPK por score OOS (institucional research deploy)\n",
    "FALLBACK_TOPK = 2\n",
    "\n",
    "# Si existe picked_gate_pass, puedes forzar que el TOPK salga de esos (si no quedan, usa todos)\n",
    "FALLBACK_PREFER_GATE_PASS = True\n",
    "\n",
    "# Ãšltimo fallback si no hay ninguna columna score detectada\n",
    "FALLBACK_EXPORT_ALL_IF_NO_SCORE = True\n",
    "\n",
    "# ========================= Load selection =========================\n",
    "sel_state = GLOBAL_STATE.get(\"selection\", {}) or {}\n",
    "sel_path = sel_state.get(\"selection_path\") or str((ART / \"selection\" / \"selection_engine_v10.parquet\").resolve())\n",
    "sel_path = str(Path(sel_path).resolve())\n",
    "if not Path(sel_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 11] ERROR: selection parquet no existe: {sel_path}\")\n",
    "\n",
    "sel = pl.read_parquet(sel_path)\n",
    "if sel.height == 0:\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: selection vacÃ­o (0 filas).\")\n",
    "\n",
    "print(f\"[Celda 11] selection rows = {sel.height}\")\n",
    "print(f\"[Celda 11] selection cols = {sel.columns}\")\n",
    "\n",
    "REQ_BASE = [\"symbol\", \"fold_id\"]\n",
    "miss = [c for c in REQ_BASE if c not in sel.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 11] ERROR: selection missing cols={miss}. cols={sel.columns}\")\n",
    "\n",
    "# side col\n",
    "SIDE_COL_CANDS = [\"picked_side\", \"best_side\", \"side\"]\n",
    "SIDE_COL = next((c for c in SIDE_COL_CANDS if c in sel.columns), None)\n",
    "if SIDE_COL is None:\n",
    "    raise RuntimeError(f\"[Celda 11] ERROR: no encuentro columna de lado. cands={SIDE_COL_CANDS} cols={sel.columns}\")\n",
    "\n",
    "# status col\n",
    "STATUS_COL = \"status\" if \"status\" in sel.columns else None\n",
    "if STATUS_COL is None:\n",
    "    sel = sel.with_columns(pl.lit(\"UNKNOWN\").alias(\"status\"))\n",
    "    STATUS_COL = \"status\"\n",
    "\n",
    "# evidencia de status\n",
    "status_counts = sel.group_by(STATUS_COL).agg(pl.len().alias(\"n\")).sort(\"n\", descending=True)\n",
    "print(\"[Celda 11] status counts:\")\n",
    "print(status_counts)\n",
    "\n",
    "# ========================= Folds (WFO windows) =========================\n",
    "wfo = GLOBAL_STATE.get(\"wfo\", {}) or {}\n",
    "folds = wfo.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo.get(\"folds_path\")\n",
    "    if folds_path and Path(folds_path).exists():\n",
    "        folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: no hay folds en GLOBAL_STATE['wfo']. Ejecuta Celda 04.\")\n",
    "\n",
    "fold_map = {str(f[\"fold_id\"]): f for f in folds if \"fold_id\" in f}\n",
    "if not fold_map:\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: fold_map vacÃ­o.\")\n",
    "\n",
    "def _fold_get(fid: str, k: str):\n",
    "    f = fold_map.get(str(fid))\n",
    "    return None if f is None else f.get(k)\n",
    "\n",
    "# ========================= Gate thresholds (Celda 06) =========================\n",
    "gate_state = GLOBAL_STATE.get(\"regime_gate\", {}) or {}\n",
    "gate_path = gate_state.get(\"gate_table_path\")\n",
    "if not gate_path or not Path(gate_path).exists():\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: no encuentro regime_gate.gate_table_path. Ejecuta Celda 06.\")\n",
    "gate_df = pl.read_parquet(str(gate_path))\n",
    "\n",
    "REQ_GATE = [\"symbol\",\"fold_id\",\"thr_er\",\"thr_mom\",\"thr_vol\",\"scheme\"]\n",
    "miss = [c for c in REQ_GATE if c not in gate_df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 11] ERROR: gate_table missing cols={miss}. cols={gate_df.columns}\")\n",
    "\n",
    "# ========================= Costs (Celda 03) =========================\n",
    "cost_state = GLOBAL_STATE.get(\"cost_model\", {}) or {}\n",
    "costs_by_symbol = cost_state.get(\"costs_by_symbol\") or cost_state.get(\"symbols\") or {}\n",
    "if not costs_by_symbol:\n",
    "    raise RuntimeError(\"[Celda 11] ERROR: no encuentro cost_model.costs_by_symbol. Ejecuta Celda 03.\")\n",
    "cost_roundtrip = bool(cost_state.get(\"cost_reported_is_roundtrip\", False))\n",
    "\n",
    "def _cost(sym: str, key: str, default=0.0):\n",
    "    d = costs_by_symbol.get(sym) or costs_by_symbol.get(sym.upper()) or {}\n",
    "    return float(d.get(key, default))\n",
    "\n",
    "# ========================= Engine params (Celda 08) =========================\n",
    "engine_state = GLOBAL_STATE.get(\"backtest_engine\", {}) or GLOBAL_STATE.get(\"engine\", {}) or {}\n",
    "engine_params = engine_state.get(\"params\") or engine_state.get(\"engine_params\") or {}\n",
    "\n",
    "engine_dir = ART / \"backtests\" / \"backtest_engine_v10\"\n",
    "snap_json = engine_dir / \"backtest_engine_v10_snapshot.json\"\n",
    "if (not engine_params) and snap_json.exists():\n",
    "    try:\n",
    "        j = json.loads(snap_json.read_text(encoding=\"utf-8\"))\n",
    "        engine_params = j.get(\"params\") or {}\n",
    "    except Exception:\n",
    "        engine_params = {}\n",
    "\n",
    "SL_ATR = float(engine_params.get(\"SL_ATR\", 2.5))\n",
    "TP_ATR = float(engine_params.get(\"TP_ATR\", 5.0))\n",
    "TRAIL_ATR = float(engine_params.get(\"TRAIL_ATR\", 2.0))\n",
    "TIME_STOP_BARS = int(engine_params.get(\"TIME_STOP_BARS\", 288))\n",
    "EMA_FILTER = bool(engine_params.get(\"EMA_FILTER\", True))\n",
    "MON_FRI = bool(engine_params.get(\"MON_FRI\", True))\n",
    "\n",
    "exec_conv = (GLOBAL_STATE.get(\"execution\", {}) or {}).get(\"convention\") or \"signal@close(t) -> entry@t+1\"\n",
    "\n",
    "# ========================= SelecciÃ³n efectiva (GO-like o fallback TOPK) =========================\n",
    "sel_norm = sel.with_columns([\n",
    "    pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "    pl.col(\"fold_id\").cast(pl.Utf8).alias(\"fold_id\"),\n",
    "    pl.col(SIDE_COL).cast(pl.Utf8).alias(\"picked_side\"),\n",
    "    pl.col(STATUS_COL).cast(pl.Utf8).alias(\"status\"),\n",
    "])\n",
    "\n",
    "mode = None\n",
    "sel_eff = None\n",
    "\n",
    "# GO-like first\n",
    "if FILTER_GO_LIKE_FIRST:\n",
    "    sel_go = sel_norm.filter(pl.col(\"status\").is_in(list(GO_LIKE)))\n",
    "    if sel_go.height > 0:\n",
    "        sel_eff = sel_go\n",
    "        mode = \"GO_LIKE_ONLY\"\n",
    "\n",
    "# Fallback TOPK por score (incluye picked_score_oos)\n",
    "if sel_eff is None:\n",
    "    SCORE_CANDS = [\n",
    "        # preferido (tu caso)\n",
    "        \"picked_score_oos\",\n",
    "        # alternativos razonables\n",
    "        \"best_score_any\",\n",
    "        \"picked_sharpe_like_oos\",\n",
    "        \"picked_tot_ret_oos\",\n",
    "        \"picked_win_rate_oos\",\n",
    "        # genÃ©ricos\n",
    "        \"score\", \"score_oos\", \"oos_score\", \"score_final\", \"score_engine\",\n",
    "        \"score_total\", \"score_oos_final\", \"score_rank\",\n",
    "    ]\n",
    "    score_col = next((c for c in SCORE_CANDS if c in sel_norm.columns), None)\n",
    "\n",
    "    base_pool = sel_norm\n",
    "    if FALLBACK_PREFER_GATE_PASS and (\"picked_gate_pass\" in sel_norm.columns):\n",
    "        pool_gp = sel_norm.filter(pl.col(\"picked_gate_pass\") == True)\n",
    "        if pool_gp.height > 0:\n",
    "            base_pool = pool_gp\n",
    "\n",
    "    if score_col is not None:\n",
    "        sel_eff = base_pool.sort(score_col, descending=True).head(int(min(FALLBACK_TOPK, base_pool.height)))\n",
    "        mode = f\"FALLBACK_TOPK_BY_{score_col}\"\n",
    "    else:\n",
    "        if not FALLBACK_EXPORT_ALL_IF_NO_SCORE:\n",
    "            raise RuntimeError(\"[Celda 11] GATE FAIL: no hay GO-like y no hay score para fallback TOPK.\")\n",
    "        sel_eff = base_pool\n",
    "        mode = \"FALLBACK_EXPORT_ALL_NO_SCORE\"\n",
    "\n",
    "print(f\"[Celda 11] deploy selection mode = {mode} | rows={sel_eff.height}\")\n",
    "\n",
    "# ========================= Enrich + joins =========================\n",
    "sel3 = sel_eff.join(\n",
    "    gate_df.select([\"symbol\",\"fold_id\",\"thr_er\",\"thr_mom\",\"thr_vol\",\"scheme\"]),\n",
    "    on=[\"symbol\",\"fold_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "sel3 = sel3.with_columns([\n",
    "    pl.col(\"fold_id\").map_elements(lambda x: _fold_get(str(x), \"IS_start\"), return_dtype=pl.Utf8).alias(\"IS_start\"),\n",
    "    pl.col(\"fold_id\").map_elements(lambda x: _fold_get(str(x), \"IS_end\"), return_dtype=pl.Utf8).alias(\"IS_end\"),\n",
    "    pl.col(\"fold_id\").map_elements(lambda x: _fold_get(str(x), \"OOS_start\"), return_dtype=pl.Utf8).alias(\"OOS_start\"),\n",
    "    pl.col(\"fold_id\").map_elements(lambda x: _fold_get(str(x), \"OOS_end\"), return_dtype=pl.Utf8).alias(\"OOS_end\"),\n",
    "])\n",
    "\n",
    "sel3 = sel3.with_columns([\n",
    "    pl.col(\"symbol\").map_elements(lambda s: _cost(str(s), \"COST_BASE_BPS\", 0.0), return_dtype=pl.Float64).alias(\"COST_BASE_BPS\"),\n",
    "    pl.col(\"symbol\").map_elements(lambda s: _cost(str(s), \"COST_STRESS_BPS\", 0.0), return_dtype=pl.Float64).alias(\"COST_STRESS_BPS\"),\n",
    "])\n",
    "\n",
    "sel3 = sel3.with_columns([\n",
    "    pl.lit(mode).alias(\"deploy_mode\"),\n",
    "    pl.lit(SL_ATR).alias(\"SL_ATR\"),\n",
    "    pl.lit(TP_ATR).alias(\"TP_ATR\"),\n",
    "    pl.lit(TRAIL_ATR).alias(\"TRAIL_ATR\"),\n",
    "    pl.lit(TIME_STOP_BARS).alias(\"TIME_STOP_BARS\"),\n",
    "    pl.lit(EMA_FILTER).alias(\"EMA_FILTER\"),\n",
    "    pl.lit(MON_FRI).alias(\"MON_FRI\"),\n",
    "    pl.lit(cost_roundtrip).alias(\"COST_REPORTED_IS_ROUNDTRIP\"),\n",
    "    pl.lit(exec_conv).alias(\"EXEC_CONVENTION\"),\n",
    "])\n",
    "\n",
    "# columnas de evidencia si existen (tu selection trae varias picked_*)\n",
    "EVID_CANDS = [\n",
    "    \"picked_gate_pass\",\"picked_score_oos\",\"picked_n_trades_oos\",\"picked_tot_ret_oos\",\"picked_mdd_oos\",\n",
    "    \"picked_win_rate_oos\",\"picked_exposure_oos\",\"picked_sharpe_like_oos\",\"picked_trades_per_day_oos\",\n",
    "    \"n_candidates\",\"n_gate_pass\",\"best_score_any\"\n",
    "]\n",
    "EVID_PRESENT = [c for c in EVID_CANDS if c in sel3.columns]\n",
    "\n",
    "DEPLOY_COLS = [\n",
    "    \"symbol\",\"fold_id\",\"picked_side\",\"status\",\"deploy_mode\",\n",
    "    *EVID_PRESENT,\n",
    "    \"IS_start\",\"IS_end\",\"OOS_start\",\"OOS_end\",\n",
    "    \"scheme\",\"thr_er\",\"thr_mom\",\"thr_vol\",\n",
    "    \"SL_ATR\",\"TP_ATR\",\"TRAIL_ATR\",\"TIME_STOP_BARS\",\n",
    "    \"EMA_FILTER\",\"MON_FRI\",\n",
    "    \"COST_BASE_BPS\",\"COST_STRESS_BPS\",\"COST_REPORTED_IS_ROUNDTRIP\",\n",
    "    \"EXEC_CONVENTION\",\n",
    "]\n",
    "deploy_df = sel3.select([c for c in DEPLOY_COLS if c in sel3.columns]).sort([\"symbol\",\"fold_id\"])\n",
    "\n",
    "# ========================= Persistencia =========================\n",
    "OUT_DIR = ART / \"deploy\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PARQ = OUT_DIR / \"deploy_pack_v10.parquet\"\n",
    "SNAP_PARQ = SNAP_DIR / \"deploy_pack_v10.parquet\"\n",
    "\n",
    "deploy_df.write_parquet(str(OUT_PARQ), compression=\"zstd\")\n",
    "deploy_df.write_parquet(str(SNAP_PARQ), compression=\"zstd\")\n",
    "\n",
    "deploy_pack = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"source_selection\": sel_path,\n",
    "    \"deploy_mode\": mode,\n",
    "    \"engine_params_frozen\": {\n",
    "        \"SL_ATR\": SL_ATR,\n",
    "        \"TP_ATR\": TP_ATR,\n",
    "        \"TRAIL_ATR\": TRAIL_ATR,\n",
    "        \"TIME_STOP_BARS\": TIME_STOP_BARS,\n",
    "        \"EMA_FILTER\": EMA_FILTER,\n",
    "        \"MON_FRI\": MON_FRI,\n",
    "        \"execution_convention\": exec_conv,\n",
    "        \"cost_reported_is_roundtrip\": cost_roundtrip,\n",
    "    },\n",
    "    \"rows\": deploy_df.to_dicts(),\n",
    "    \"notes\": [\n",
    "        \"Si deploy_mode != GO_LIKE_ONLY, esto es RESEARCH fallback (no producciÃ³n).\",\n",
    "        \"Thresholds de regime gate por fold (IS-calibrated).\",\n",
    "        \"Incluye costos y parÃ¡metros del motor.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "OUT_JSON = OUT_DIR / \"deploy_pack_v10.json\"\n",
    "SNAP_JSON = SNAP_DIR / \"deploy_pack_v10.json\"\n",
    "OUT_JSON.write_text(json.dumps(deploy_pack, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "SNAP_JSON.write_text(json.dumps(deploy_pack, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"deploy\"] = {\n",
    "    \"deploy_pack_parquet\": str(OUT_PARQ),\n",
    "    \"deploy_pack_json\": str(OUT_JSON),\n",
    "    \"snapshot_parquet\": str(SNAP_PARQ),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "    \"deploy_mode\": mode,\n",
    "}\n",
    "\n",
    "print(f\"[Celda 11] exported rows = {deploy_df.height} | mode={mode}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_PARQ} (OK)\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_JSON} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_PARQ} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 11 v1.0.2 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f3f10b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 12 v1.0 :: Deploy Pack QA + MaterializaciÃ³n por sÃ­mbolo [schema-check + exports]\n",
      "[Celda 12] deploy_pack rows = 2\n",
      "[Celda 12] deploy_pack cols = ['symbol', 'fold_id', 'picked_side', 'status', 'deploy_mode', 'picked_gate_pass', 'picked_score_oos', 'picked_n_trades_oos', 'picked_tot_ret_oos', 'picked_mdd_oos', 'picked_win_rate_oos', 'picked_exposure_oos', 'picked_sharpe_like_oos', 'picked_trades_per_day_oos', 'n_candidates', 'n_gate_pass', 'best_score_any', 'IS_start', 'IS_end', 'OOS_start', 'OOS_end', 'scheme', 'thr_er', 'thr_mom', 'thr_vol', 'SL_ATR', 'TP_ATR', 'TRAIL_ATR', 'TIME_STOP_BARS', 'EMA_FILTER', 'MON_FRI', 'COST_BASE_BPS', 'COST_STRESS_BPS', 'COST_REPORTED_IS_ROUNDTRIP', 'EXEC_CONVENTION']\n",
      "[Celda 12] typical_present=['picked_side', 'picked_score_oos', 'picked_gate_pass', 'status', 'thr_er', 'thr_mom', 'thr_vol', 'COST_BASE_BPS', 'COST_STRESS_BPS']\n",
      "[Celda 12] typical_missing=[]\n",
      "[Celda 12] preview (top 10):\n",
      "shape: (2, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† status â”† picked_side â”† picked_score_oos â”† picked_gate_pass â”‚\n",
      "â”‚ ---    â”† ---     â”† ---    â”† ---         â”† ---              â”† ---              â”‚\n",
      "â”‚ str    â”† str     â”† str    â”† str         â”† f64              â”† bool             â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ LVMH   â”† F1      â”† NO_GO  â”† SHORT       â”† -0.123865        â”† false            â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† NO_GO  â”† LONG        â”† -0.158519        â”† false            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 12] exported per-symbol json = 2 files â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\deploy\\per_symbol_configs_v10\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\deploy_pack_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\deploy_pack_v10_materialization_snapshot.json (OK)\n",
      ">>> Celda 12 v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 12 v1.0 â€” Deploy Pack QA + MaterializaciÃ³n por sÃ­mbolo [schema-check + exports] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 12 v1.0 :: Deploy Pack QA + MaterializaciÃ³n por sÃ­mbolo [schema-check + exports]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 12] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "if \"paths\" not in GLOBAL_STATE or not isinstance(GLOBAL_STATE[\"paths\"], dict):\n",
    "    raise RuntimeError(\"[Celda 12] ERROR: falta GLOBAL_STATE['paths'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "ART_DIR = Path(paths[\"artifacts\"]).resolve()\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (ART_DIR / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= Localizar deploy_pack =========================\n",
    "DEPLOY_DIR = ART_DIR / \"deploy\"\n",
    "DEPLOY_PARQ = DEPLOY_DIR / \"deploy_pack_v10.parquet\"\n",
    "DEPLOY_JSON = DEPLOY_DIR / \"deploy_pack_v10.json\"\n",
    "\n",
    "if not DEPLOY_PARQ.exists():\n",
    "    raise RuntimeError(f\"[Celda 12] ERROR: no existe {DEPLOY_PARQ}. Ejecuta Celda 11.\")\n",
    "if not DEPLOY_JSON.exists():\n",
    "    print(f\"[Celda 12] WARN: no existe {DEPLOY_JSON}. Continuo solo con parquet.\")\n",
    "\n",
    "df = pl.read_parquet(str(DEPLOY_PARQ))\n",
    "\n",
    "print(f\"[Celda 12] deploy_pack rows = {df.height}\")\n",
    "print(f\"[Celda 12] deploy_pack cols = {df.columns}\")\n",
    "\n",
    "# ========================= Gate: esquema mÃ­nimo esperado =========================\n",
    "# (No forzamos columnas que puedan variar entre versiones; pero sÃ­ lo mÃ­nimo institucional)\n",
    "MIN_REQ = [\"symbol\", \"fold_id\"]\n",
    "miss = [c for c in MIN_REQ if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 12] GATE FAIL: faltan columnas mÃ­nimas {miss} en deploy_pack.\")\n",
    "\n",
    "# Tip: columnas tÃ­picas que deberÃ­an existir si el freeze estÃ¡ completo (no estrictas)\n",
    "TYPICAL = [\n",
    "    \"picked_side\", \"picked_score_oos\", \"picked_gate_pass\", \"status\",\n",
    "    \"thr_er\", \"thr_mom\", \"thr_vol\",\n",
    "    \"COST_BASE_BPS\", \"COST_STRESS_BPS\",\n",
    "]\n",
    "present_typical = [c for c in TYPICAL if c in df.columns]\n",
    "missing_typical = [c for c in TYPICAL if c not in df.columns]\n",
    "print(f\"[Celda 12] typical_present={present_typical}\")\n",
    "print(f\"[Celda 12] typical_missing={missing_typical}\")\n",
    "\n",
    "# ========================= NormalizaciÃ³n + vista rÃ¡pida =========================\n",
    "df2 = df.with_columns([\n",
    "    pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "    pl.col(\"fold_id\").cast(pl.Utf8).alias(\"fold_id\"),\n",
    "])\n",
    "\n",
    "# Orden: mejores primero si existe score\n",
    "if \"picked_score_oos\" in df2.columns:\n",
    "    df2 = df2.sort(\"picked_score_oos\", descending=True)\n",
    "else:\n",
    "    df2 = df2.sort([\"symbol\", \"fold_id\"])\n",
    "\n",
    "# Resumen breve\n",
    "cols_preview = [c for c in [\"symbol\",\"fold_id\",\"status\",\"picked_side\",\"picked_score_oos\",\"picked_gate_pass\"] if c in df2.columns]\n",
    "if cols_preview:\n",
    "    print(\"[Celda 12] preview (top 10):\")\n",
    "    print(df2.select(cols_preview).head(10))\n",
    "\n",
    "# ========================= Export por sÃ­mbolo (JSON) =========================\n",
    "OUT_CFG_DIR = DEPLOY_DIR / \"per_symbol_configs_v10\"\n",
    "OUT_CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows = df2.to_dicts()\n",
    "by_symbol = {}\n",
    "for r in rows:\n",
    "    sym = str(r.get(\"symbol\", \"\")).upper().strip()\n",
    "    if not sym:\n",
    "        continue\n",
    "    by_symbol.setdefault(sym, []).append(r)\n",
    "\n",
    "# Si hay mÃºltiples filas por sÃ­mbolo (ej. multi-fold), guardamos lista; si es 1, guardamos dict\n",
    "exported = []\n",
    "for sym, items in by_symbol.items():\n",
    "    payload = items[0] if len(items) == 1 else {\"symbol\": sym, \"rows\": items}\n",
    "    outp = OUT_CFG_DIR / f\"{sym}_deploy_config_v10.json\"\n",
    "    outp.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    exported.append(str(outp))\n",
    "\n",
    "print(f\"[Celda 12] exported per-symbol json = {len(exported)} files â†’ {OUT_CFG_DIR}\")\n",
    "\n",
    "# ========================= Snapshot: copia deploy_pack + meta =========================\n",
    "ts = datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n",
    "meta = {\n",
    "    \"created_utc\": ts,\n",
    "    \"deploy_pack_parquet\": str(DEPLOY_PARQ),\n",
    "    \"deploy_pack_json\": str(DEPLOY_JSON) if DEPLOY_JSON.exists() else None,\n",
    "    \"per_symbol_configs_dir\": str(OUT_CFG_DIR),\n",
    "    \"n_rows\": int(df2.height),\n",
    "    \"symbols\": sorted(list(by_symbol.keys())),\n",
    "    \"notes\": [\n",
    "        \"Este export NO implica GO. Si status=NO_GO, es para anÃ¡lisis/iteraciÃ³n.\",\n",
    "        \"Configs por sÃ­mbolo pensadas para conectar con motor/EA o notebooks posteriores.\",\n",
    "    ],\n",
    "}\n",
    "META_PATH = SNAP_DIR / \"deploy_pack_v10_materialization_snapshot.json\"\n",
    "META_PATH.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# Guardar tambiÃ©n una copia del parquet como snapshot\n",
    "SNAP_PARQ = SNAP_DIR / \"deploy_pack_v10.parquet\"\n",
    "df2.write_parquet(str(SNAP_PARQ), compression=\"zstd\")\n",
    "\n",
    "GLOBAL_STATE[\"deploy_pack\"] = {\n",
    "    \"deploy_pack_parquet\": str(DEPLOY_PARQ),\n",
    "    \"deploy_pack_json\": str(DEPLOY_JSON) if DEPLOY_JSON.exists() else None,\n",
    "    \"per_symbol_configs_dir\": str(OUT_CFG_DIR),\n",
    "    \"materialization_snapshot\": str(META_PATH),\n",
    "    \"snapshot_parquet\": str(SNAP_PARQ),\n",
    "    \"n_rows\": int(df2.height),\n",
    "    \"symbols\": sorted(list(by_symbol.keys())),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_PARQ} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {META_PATH} (OK)\")\n",
    "print(\">>> Celda 12 v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2cf9770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 13 v1.1.0 :: DiagnÃ³stico de Rentabilidad + Edge Alignment (alphaâ†”motor) [WFO-safe]\n",
      "[Celda 13] trades rows = 524\n",
      "[Celda 13] weekend_entry_share=0.000000 weekend_exit_share=0.000000 (MON_FRI=True)\n",
      "[Celda 13] KPIs (top 20):\n",
      "shape: (16, 14)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† segment â”† side  â”† â€¦ â”† mean_bars_he â”† med_bars_hel â”† p90_bars_he â”† sharpe_like â”‚\n",
      "â”‚ ---    â”† ---     â”† ---     â”† ---   â”†   â”† ld           â”† d            â”† ld          â”† _base       â”‚\n",
      "â”‚ str    â”† str     â”† str     â”† str   â”†   â”† ---          â”† ---          â”† ---         â”† ---         â”‚\n",
      "â”‚        â”†         â”†         â”†       â”†   â”† f64          â”† f64          â”† f64         â”† f64         â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 7.023256     â”† 5.0          â”† 14.0        â”† -0.523515   â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† IS      â”† LONG  â”† â€¦ â”† 9.493333     â”† 6.0          â”† 20.0        â”† -0.66334    â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† â€¦ â”† 7.444444     â”† 5.5          â”† 18.0        â”† -0.686023   â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 8.5          â”† 6.5          â”† 16.0        â”† -0.751608   â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 7.464286     â”† 6.0          â”† 15.0        â”† -0.841528   â”‚\n",
      "â”‚ â€¦      â”† â€¦       â”† â€¦       â”† â€¦     â”† â€¦ â”† â€¦            â”† â€¦            â”† â€¦           â”† â€¦           â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 7.625        â”† 6.5          â”† 14.0        â”† -0.973767   â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† LONG  â”† â€¦ â”† 7.555556     â”† 5.0          â”† 12.0        â”† -1.179703   â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† LONG  â”† â€¦ â”† 5.25         â”† 4.0          â”† 10.0        â”† -1.22526    â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 7.7          â”† 6.5          â”† 15.0        â”† -1.248397   â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 12.0         â”† 12.0         â”† 14.0        â”† -1.308695   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 13] Exit reason mix (top 30):\n",
      "shape: (30, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† segment â”† side  â”† exit_reason     â”† n   â”† share    â”‚\n",
      "â”‚ ---    â”† ---     â”† ---     â”† ---   â”† ---             â”† --- â”† ---      â”‚\n",
      "â”‚ str    â”† str     â”† str     â”† str   â”† str             â”† u32 â”† f64      â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† LONG  â”† TRAIL           â”† 81  â”† 0.9      â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† LONG  â”† SL              â”† 5   â”† 0.055556 â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† LONG  â”† REGIME_OFF      â”† 4   â”† 0.044444 â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† SHORT â”† TRAIL           â”† 36  â”† 0.837209 â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† SHORT â”† SL              â”† 5   â”† 0.116279 â”‚\n",
      "â”‚ â€¦      â”† â€¦       â”† â€¦       â”† â€¦     â”† â€¦               â”† â€¦   â”† â€¦        â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† SL              â”† 2   â”† 0.055556 â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† WEEKEND_FLATTEN â”† 1   â”† 0.027778 â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† REGIME_OFF      â”† 1   â”† 0.027778 â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† TP              â”† 1   â”† 0.027778 â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† SHORT â”† TRAIL           â”† 11  â”† 0.6875   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 13] TRAIL dominance (top 20):\n",
      "shape: (16, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† segment â”† side  â”† trail_share â”‚\n",
      "â”‚ ---    â”† ---     â”† ---     â”† ---   â”† ---         â”‚\n",
      "â”‚ str    â”† str     â”† str     â”† str   â”† f64         â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ XAUAUD â”† F1      â”† IS      â”† SHORT â”† 0.935484    â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† SHORT â”† 0.910714    â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† IS      â”† LONG  â”† 0.9         â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† LONG  â”† 0.861111    â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† LONG  â”† 0.849462    â”‚\n",
      "â”‚ â€¦      â”† â€¦       â”† â€¦       â”† â€¦     â”† â€¦           â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† LONG  â”† 0.888889    â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† OOS     â”† SHORT â”† 0.875       â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† SHORT â”† 0.8         â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† LONG  â”† 0.75        â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† SHORT â”† 0.571429    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 13] Hold stats (top 20):\n",
      "shape: (16, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† segment â”† side  â”† â€¦ â”† p90_bars â”† share_le_6 â”† share_le_12 â”† share_ge_1d â”‚\n",
      "â”‚ ---    â”† ---     â”† ---     â”† ---   â”†   â”† ---      â”† ---        â”† ---         â”† ---         â”‚\n",
      "â”‚ str    â”† str     â”† str     â”† str   â”†   â”† f64      â”† f64        â”† f64         â”† f64         â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ XAUAUD â”† F1      â”† IS      â”† LONG  â”† â€¦ â”† 20.0     â”† 0.506667   â”† 0.706667    â”† 0.0         â”‚\n",
      "â”‚ LVMH   â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 16.0     â”† 0.5        â”† 0.75        â”† 0.0         â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 16.0     â”† 0.516129   â”† 0.709677    â”† 0.0         â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† LONG  â”† â€¦ â”† 18.0     â”† 0.548387   â”† 0.817204    â”† 0.0         â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† IS      â”† SHORT â”† â€¦ â”† 15.0     â”† 0.517857   â”† 0.803571    â”† 0.0         â”‚\n",
      "â”‚ â€¦      â”† â€¦       â”† â€¦       â”† â€¦     â”† â€¦ â”† â€¦        â”† â€¦          â”† â€¦           â”† â€¦           â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 12.0     â”† 0.428571   â”† 0.857143    â”† 0.0         â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 15.0     â”† 0.5        â”† 0.8         â”† 0.0         â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† OOS     â”† SHORT â”† â€¦ â”† 14.0     â”† 0.5        â”† 0.875       â”† 0.0         â”‚\n",
      "â”‚ LVMH   â”† F1      â”† OOS     â”† LONG  â”† â€¦ â”† 12.0     â”† 0.555556   â”† 0.888889    â”† 0.0         â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† OOS     â”† LONG  â”† â€¦ â”† 10.0     â”† 0.75       â”† 0.875       â”† 0.0         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 13] Alpha best (OOS) por sÃ­mbolo:\n",
      "shape: (4, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† best_side_oos â”† best_horizon_bars_o â”† best_mean_net_stres â”† best_n_trades_oos â”‚\n",
      "â”‚ ---    â”† ---     â”† ---           â”† os                  â”† s_oos               â”† ---               â”‚\n",
      "â”‚ str    â”† str     â”† str           â”† ---                 â”† ---                 â”† i64               â”‚\n",
      "â”‚        â”†         â”†               â”† i64                 â”† f64                 â”†                   â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BTCUSD â”† F1      â”† SHORT         â”† 288                 â”† 0.012196            â”† 432               â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† SHORT         â”† 288                 â”† 0.010817            â”† 908               â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† LONG          â”† 288                 â”† 0.006614            â”† 431               â”‚\n",
      "â”‚ LVMH   â”† F1      â”† SHORT         â”† 1                   â”† -0.004995           â”† 273               â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 13] WARN: Horizon mismatch detectado (p90 < 10% del horizonte objetivo):\n",
      "shape: (3, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† best_side_oos â”† best_horizon_bars_oos â”† p90_bars â”† p90_vs_target â”‚\n",
      "â”‚ ---    â”† ---           â”† ---                   â”† ---      â”† ---           â”‚\n",
      "â”‚ str    â”† str           â”† i64                   â”† f64      â”† f64           â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BTCUSD â”† SHORT         â”† 288                   â”† 15.0     â”† 0.052083      â”‚\n",
      "â”‚ XAUAUD â”† LONG          â”† 288                   â”† 15.0     â”† 0.052083      â”‚\n",
      "â”‚ BNBUSD â”† SHORT         â”† 288                   â”† 14.0     â”† 0.048611      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\diag_kpis_engine_v10.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\diag_exit_reason_mix_engine_v10.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\diag_hold_stats_engine_v10.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\alpha_best_horizon_side_engine_v10.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\tuning_plan_engine_v10.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\diag_kpis_engine_v10.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\diag_engine_alignment_snapshot.json (OK)\n",
      ">>> Celda 13 v1.1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 13 v1.1.0 â€” DiagnÃ³stico de Rentabilidad + Edge Alignment (alphaâ†”motor) [AUTO-RECOVERY + schema canonical | WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 13 v1.1.0 :: DiagnÃ³stico de Rentabilidad + Edge Alignment (alphaâ†”motor) [WFO-safe]\")\n",
    "\n",
    "# ========================= Utilidades =========================\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n",
    "\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_dt_any(s: str) -> datetime:\n",
    "    dt0 = datetime.fromisoformat(s)\n",
    "    return _ensure_utc(dt0)\n",
    "\n",
    "def _py_weekday(dt0: datetime) -> int:\n",
    "    return int(dt0.weekday())  # Mon=0..Sun=6\n",
    "\n",
    "def _pick_latest(files: list[Path]) -> Path | None:\n",
    "    files2 = [p for p in files if p.exists()]\n",
    "    if not files2:\n",
    "        return None\n",
    "    return sorted(files2, key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "\n",
    "def _find_latest_by_glob(root: Path, pattern: str) -> Path | None:\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    hits = list(root.rglob(pattern))\n",
    "    return _pick_latest(hits)\n",
    "\n",
    "def _ensure_backtest_engine_state() -> None:\n",
    "    \"\"\"Garantiza GLOBAL_STATE['backtest_engine'] incluso despuÃ©s de restart.\"\"\"\n",
    "    global GLOBAL_STATE\n",
    "\n",
    "    if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "        GLOBAL_STATE = {}\n",
    "\n",
    "    if \"paths\" not in GLOBAL_STATE or not isinstance(GLOBAL_STATE[\"paths\"], dict):\n",
    "        cwd = Path.cwd().resolve()\n",
    "        GLOBAL_STATE[\"paths\"] = {\n",
    "            \"artifacts\": str((cwd / \"artifacts\").resolve()),\n",
    "            \"run_snapshots\": str((cwd / \"snapshots\").resolve()),\n",
    "        }\n",
    "\n",
    "    paths = GLOBAL_STATE[\"paths\"]\n",
    "    artifacts = Path(paths.get(\"artifacts\", \"\")).resolve()\n",
    "    snap_dir  = Path(paths.get(\"run_snapshots\", \"\")).resolve()\n",
    "\n",
    "    bt = GLOBAL_STATE.get(\"backtest_engine\", {}) if isinstance(GLOBAL_STATE.get(\"backtest_engine\", {}), dict) else {}\n",
    "    tp = bt.get(\"trades_path\"); sp = bt.get(\"summary_path\")\n",
    "\n",
    "    if tp and Path(tp).exists() and sp and Path(sp).exists():\n",
    "        return\n",
    "\n",
    "    # RecuperaciÃ³n desde snapshot (prioriza overlay)\n",
    "    cand_snap = []\n",
    "    if snap_dir.exists():\n",
    "        cand_snap += [\n",
    "            snap_dir / \"overlay_engine_v16_snapshot.json\",\n",
    "            snap_dir / \"backtest_engine_v10_snapshot.json\",\n",
    "        ]\n",
    "    if artifacts.exists():\n",
    "        cand_snap += [\n",
    "            artifacts / \"snapshots\" / \"overlay_engine_v16_snapshot.json\",\n",
    "            artifacts / \"snapshots\" / \"backtest_engine_v10_snapshot.json\",\n",
    "        ]\n",
    "\n",
    "    snap_file = _pick_latest(cand_snap)\n",
    "    params = {}\n",
    "\n",
    "    if snap_file and snap_file.exists():\n",
    "        try:\n",
    "            j = json.loads(snap_file.read_text(encoding=\"utf-8\"))\n",
    "            params = j.get(\"params\", {}) if isinstance(j.get(\"params\", {}), dict) else {}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Paths por glob (overlay primero)\n",
    "    tp2 = _find_latest_by_glob(artifacts, \"trades_engine_v10*_overlay_*.parquet\") \\\n",
    "          or _find_latest_by_glob(artifacts, \"trades_engine_v10*.parquet\")\n",
    "    sp2 = _find_latest_by_glob(artifacts, \"summary_engine_v10*_overlay_*.parquet\") \\\n",
    "          or _find_latest_by_glob(artifacts, \"summary_engine_v10*.parquet\")\n",
    "\n",
    "    if not tp2 or not tp2.exists():\n",
    "        raise RuntimeError(f\"[Celda 13] ERROR: no encuentro trades_engine_v10*.parquet bajo artifacts={artifacts}\")\n",
    "    if not sp2 or not sp2.exists():\n",
    "        raise RuntimeError(f\"[Celda 13] ERROR: no encuentro summary_engine_v10*.parquet bajo artifacts={artifacts}\")\n",
    "\n",
    "    GLOBAL_STATE[\"backtest_engine\"] = {\n",
    "        \"trades_path\": str(tp2),\n",
    "        \"summary_path\": str(sp2),\n",
    "        \"params\": params,\n",
    "        \"recovered_utc\": _utc_now_iso(),\n",
    "        \"recovered_from\": str(snap_file) if snap_file else None,\n",
    "    }\n",
    "\n",
    "# ========================= Estado mÃ­nimo =========================\n",
    "_ensure_backtest_engine_state()\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "bt = GLOBAL_STATE[\"backtest_engine\"]\n",
    "\n",
    "trades_path = bt.get(\"trades_path\")\n",
    "summary_path = bt.get(\"summary_path\")\n",
    "if not trades_path or not Path(trades_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 13] ERROR: trades_path invÃ¡lido/no existe: {trades_path}\")\n",
    "if not summary_path or not Path(summary_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 13] ERROR: summary_path invÃ¡lido/no existe: {summary_path}\")\n",
    "\n",
    "params = bt.get(\"params\", {}) or {}\n",
    "MON_FRI = bool(params.get(\"MON_FRI\", True))\n",
    "\n",
    "# Alpha report (07C) â€” localizar robusto\n",
    "alpha_path = (\n",
    "    (GLOBAL_STATE.get(\"alpha_reports\", {}) or {}).get(\"alpha_multi_horizon_report_path\")\n",
    "    or (GLOBAL_STATE.get(\"alpha_report\", {}) or {}).get(\"report_path\")\n",
    ")\n",
    "if not alpha_path:\n",
    "    alpha_path = str(Path(paths[\"artifacts\"]).resolve() / \"alpha_reports\" / \"alpha_multi_horizon_report.parquet\")\n",
    "alpha_exists = Path(alpha_path).exists()\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"backtests\" / \"backtest_engine_v10\" / \"diagnostics_engine_v10\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_KPIS = OUT_DIR / \"diag_kpis_engine_v10.parquet\"\n",
    "OUT_EXITS = OUT_DIR / \"diag_exit_reason_mix_engine_v10.parquet\"\n",
    "OUT_HOLDS = OUT_DIR / \"diag_hold_stats_engine_v10.parquet\"\n",
    "OUT_ALPHA_PICK = OUT_DIR / \"alpha_best_horizon_side_engine_v10.parquet\"\n",
    "OUT_TUNING_JSON = OUT_DIR / \"tuning_plan_engine_v10.json\"\n",
    "\n",
    "SNAP_KPIS = SNAP_DIR / \"diag_kpis_engine_v10.parquet\"\n",
    "SNAP_JSON = SNAP_DIR / \"diag_engine_alignment_snapshot.json\"\n",
    "\n",
    "# ========================= Load trades =========================\n",
    "df = pl.read_parquet(str(trades_path))\n",
    "\n",
    "REQ = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"entry_time\",\"exit_time\",\"bars_held\",\"exit_reason\",\"net_ret_base\",\"net_ret_stress\"]\n",
    "miss = [c for c in REQ if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 13] ERROR: trades parquet no tiene columnas requeridas: {miss}\")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_time\").cast(pl.Utf8),\n",
    "        pl.col(\"exit_time\").cast(pl.Utf8),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"entry_dt\"),\n",
    "        pl.col(\"exit_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"exit_dt\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_dt\").map_elements(_py_weekday, return_dtype=pl.Int64).cast(pl.Int32).alias(\"entry_dow\"),\n",
    "        pl.col(\"exit_dt\").map_elements(_py_weekday, return_dtype=pl.Int64).cast(pl.Int32).alias(\"exit_dow\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"entry_dow\") >= 5).alias(\"entry_is_weekend\"),\n",
    "        (pl.col(\"exit_dow\") >= 5).alias(\"exit_is_weekend\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "n_rows = df.height\n",
    "weekend_entry_share = float(df.select(pl.col(\"entry_is_weekend\").mean()).item() or 0.0)\n",
    "weekend_exit_share  = float(df.select(pl.col(\"exit_is_weekend\").mean()).item() or 0.0)\n",
    "\n",
    "print(f\"[Celda 13] trades rows = {n_rows}\")\n",
    "print(f\"[Celda 13] weekend_entry_share={weekend_entry_share:.6f} weekend_exit_share={weekend_exit_share:.6f} (MON_FRI={MON_FRI})\")\n",
    "\n",
    "# ========================= KPIs (trade-level aggregation) =========================\n",
    "agg_exprs = [\n",
    "    pl.len().alias(\"n_trades\"),\n",
    "    pl.col(\"net_ret_base\").mean().alias(\"mean_net_base\"),\n",
    "    pl.col(\"net_ret_base\").std().alias(\"std_net_base\"),\n",
    "    (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "    pl.col(\"net_ret_stress\").mean().alias(\"mean_net_stress\"),\n",
    "    (pl.col(\"net_ret_stress\") > 0).mean().alias(\"win_rate_stress\"),\n",
    "    pl.col(\"bars_held\").mean().alias(\"mean_bars_held\"),\n",
    "    pl.col(\"bars_held\").median().alias(\"med_bars_held\"),\n",
    "    pl.col(\"bars_held\").quantile(0.90, interpolation=\"nearest\").alias(\"p90_bars_held\"),\n",
    "]\n",
    "\n",
    "kpis = (\n",
    "    df.group_by([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    "      .agg(agg_exprs)\n",
    "      .with_columns([\n",
    "          (pl.col(\"mean_net_base\") / pl.col(\"std_net_base\")).alias(\"sharpe_like_base\"),\n",
    "      ])\n",
    "      .sort([\"segment\",\"sharpe_like_base\",\"n_trades\"], descending=[False, True, True])\n",
    ")\n",
    "\n",
    "print(\"[Celda 13] KPIs (top 20):\")\n",
    "print(kpis.head(20))\n",
    "\n",
    "# ========================= Exit reason mix + trailing dominance =========================\n",
    "exit_mix = (\n",
    "    df.group_by([\"symbol\",\"fold_id\",\"segment\",\"side\",\"exit_reason\"])\n",
    "      .agg([pl.len().alias(\"n\")])\n",
    "      .with_columns([\n",
    "          (pl.col(\"n\") / pl.col(\"n\").sum().over([\"symbol\",\"fold_id\",\"segment\",\"side\"])).alias(\"share\"),\n",
    "      ])\n",
    "      .sort([\"symbol\",\"segment\",\"side\",\"n\"], descending=[False,False,False,True])\n",
    ")\n",
    "\n",
    "trail_dom = (\n",
    "    exit_mix.filter(pl.col(\"exit_reason\") == \"TRAIL\")\n",
    "            .select([\"symbol\",\"fold_id\",\"segment\",\"side\",\"share\"])\n",
    "            .rename({\"share\":\"trail_share\"})\n",
    "            .sort([\"segment\",\"trail_share\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "print(\"[Celda 13] Exit reason mix (top 30):\")\n",
    "print(exit_mix.head(30))\n",
    "print(\"[Celda 13] TRAIL dominance (top 20):\")\n",
    "print(trail_dom.head(20))\n",
    "\n",
    "# ========================= Hold stats distribution =========================\n",
    "holds = (\n",
    "    df.group_by([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"n_trades\"),\n",
    "          pl.col(\"bars_held\").mean().alias(\"mean_bars\"),\n",
    "          pl.col(\"bars_held\").median().alias(\"med_bars\"),\n",
    "          pl.col(\"bars_held\").quantile(0.90, interpolation=\"nearest\").alias(\"p90_bars\"),\n",
    "          (pl.col(\"bars_held\") <= 6).mean().alias(\"share_le_6\"),\n",
    "          (pl.col(\"bars_held\") <= 12).mean().alias(\"share_le_12\"),\n",
    "          (pl.col(\"bars_held\") >= 288).mean().alias(\"share_ge_1d\"),\n",
    "      ])\n",
    "      .sort([\"segment\",\"mean_bars\"], descending=[False, True])\n",
    ")\n",
    "\n",
    "print(\"[Celda 13] Hold stats (top 20):\")\n",
    "print(holds.head(20))\n",
    "\n",
    "# ========================= Alpha report canonicalization + best horizon/side =========================\n",
    "alpha_best = pl.DataFrame()\n",
    "alpha_metric_used = None\n",
    "\n",
    "def _canonicalize_alpha(ar: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Mapeo desde tu schema real 07C â†’ schema canÃ³nico\n",
    "    rename_map = {}\n",
    "    if \"h_bars_after_entry\" in ar.columns and \"horizon_bars\" not in ar.columns:\n",
    "        rename_map[\"h_bars_after_entry\"] = \"horizon_bars\"\n",
    "    if \"net_base_mean\" in ar.columns and \"mean_net_base\" not in ar.columns:\n",
    "        rename_map[\"net_base_mean\"] = \"mean_net_base\"\n",
    "    if \"net_stress_mean\" in ar.columns and \"mean_net_stress\" not in ar.columns:\n",
    "        rename_map[\"net_stress_mean\"] = \"mean_net_stress\"\n",
    "    if \"sharpe_like_base\" not in ar.columns and \"sharpe_like_base\" in ar.columns:\n",
    "        pass\n",
    "    if rename_map:\n",
    "        ar = ar.rename(rename_map)\n",
    "    return ar\n",
    "\n",
    "if alpha_exists:\n",
    "    ar0 = pl.read_parquet(str(alpha_path))\n",
    "    ar = _canonicalize_alpha(ar0)\n",
    "\n",
    "    needed = {\"symbol\",\"fold_id\",\"segment\",\"side\",\"horizon_bars\"}\n",
    "    if needed.issubset(set(ar.columns)):\n",
    "        # MÃ©trica preferida: stress (institucional); si no existe, base.\n",
    "        if \"mean_net_stress\" in ar.columns:\n",
    "            metric = \"mean_net_stress\"\n",
    "        elif \"mean_net_base\" in ar.columns:\n",
    "            metric = \"mean_net_base\"\n",
    "        else:\n",
    "            metric = None\n",
    "\n",
    "        if metric:\n",
    "            alpha_metric_used = metric\n",
    "            alpha_best = (\n",
    "                ar.filter(pl.col(\"segment\") == \"OOS\")\n",
    "                  .sort([metric], descending=True)\n",
    "                  .group_by([\"symbol\",\"fold_id\"])\n",
    "                  .agg([\n",
    "                      pl.first(\"side\").alias(\"best_side_oos\"),\n",
    "                      pl.first(\"horizon_bars\").alias(\"best_horizon_bars_oos\"),\n",
    "                      pl.first(metric).alias(f\"best_{metric}_oos\"),\n",
    "                      pl.first(\"n_trades\").alias(\"best_n_trades_oos\") if \"n_trades\" in ar.columns else pl.lit(None).alias(\"best_n_trades_oos\"),\n",
    "                  ])\n",
    "                  .sort([f\"best_{metric}_oos\"], descending=True)\n",
    "            )\n",
    "            alpha_best.write_parquet(str(OUT_ALPHA_PICK), compression=\"zstd\")\n",
    "            print(\"[Celda 13] Alpha best (OOS) por sÃ­mbolo:\")\n",
    "            print(alpha_best)\n",
    "        else:\n",
    "            print(f\"[Celda 13] WARN: alpha report existe pero no encuentro mean_net_stress/mean_net_base. cols={ar.columns}\")\n",
    "    else:\n",
    "        print(f\"[Celda 13] WARN: alpha report existe pero faltan cols esperadas (canonical). cols={ar.columns}\")\n",
    "else:\n",
    "    print(f\"[Celda 13] WARN: no encuentro alpha report en: {alpha_path}\")\n",
    "\n",
    "# ========================= Alignment QA: horizonte objetivo vs holds reales =========================\n",
    "alignment = pl.DataFrame()\n",
    "if alpha_best.height > 0:\n",
    "    # Unimos con holds (OOS) para ver si el engine estÃ¡ dejando respirar al horizonte ganador\n",
    "    oos_holds = holds.filter(pl.col(\"segment\") == \"OOS\").select([\"symbol\",\"fold_id\",\"side\",\"p90_bars\",\"mean_bars\",\"n_trades\"])\n",
    "    alignment = (\n",
    "        alpha_best.join(oos_holds, left_on=[\"symbol\",\"fold_id\",\"best_side_oos\"], right_on=[\"symbol\",\"fold_id\",\"side\"], how=\"left\")\n",
    "                 .with_columns([\n",
    "                     (pl.col(\"p90_bars\") / pl.col(\"best_horizon_bars_oos\")).alias(\"p90_vs_target\"),\n",
    "                 ])\n",
    "                 .sort([\"p90_vs_target\"], descending=True)\n",
    "    )\n",
    "    # Flags institucionales\n",
    "    bad = alignment.filter((pl.col(\"p90_vs_target\").is_not_null()) & (pl.col(\"p90_vs_target\") < 0.10))\n",
    "    if bad.height > 0:\n",
    "        print(\"[Celda 13] WARN: Horizon mismatch detectado (p90 < 10% del horizonte objetivo):\")\n",
    "        print(bad.select([\"symbol\",\"best_side_oos\",\"best_horizon_bars_oos\",\"p90_bars\",\"p90_vs_target\"]).head(20))\n",
    "\n",
    "# ========================= Tuning plan (sugerido) =========================\n",
    "DEFAULT_GRID = {\n",
    "    \"SL_ATR\":    [2.0, 3.0, 5.0, 8.0],            # mÃ¡s tipo catastrophic para swing\n",
    "    \"TP_ATR\":    [4.0, 6.0, 8.0],\n",
    "    \"TRAIL_ATR\": [3.0, 4.0, 5.0],\n",
    "    \"TRAIL_START_ATR\": [2.0, 3.0],                # evita trailing temprano\n",
    "    \"TIME_STOP_BARS\": [96, 288],                  # enfoca en horizontes con edge\n",
    "    \"MIN_HOLD_BARS\":  [24, 72],                   # â‰ˆ 0.25*288\n",
    "    \"ENTRY_CONFIRM_BARS\": [6, 12],\n",
    "    \"COOLDOWN_BARS\": [12, 24, 48],\n",
    "    \"EMA_FILTER\": [True],\n",
    "    \"MON_FRI\": [True],\n",
    "    \"EVAL_SIDE_MODE\": [\"ALPHA_SIDE_ONLY\"],        # no mezclar lados\n",
    "}\n",
    "\n",
    "tuning = {\n",
    "    \"created_utc\": _utc_now_iso(),\n",
    "    \"inputs\": {\n",
    "        \"trades_path\": str(trades_path),\n",
    "        \"summary_path\": str(summary_path),\n",
    "        \"alpha_report_path\": str(alpha_path),\n",
    "        \"alpha_report_exists\": bool(alpha_exists),\n",
    "        \"alpha_metric_used\": alpha_metric_used,\n",
    "    },\n",
    "    \"current_engine_params\": params,\n",
    "    \"monfri_metrics\": {\n",
    "        \"MON_FRI\": MON_FRI,\n",
    "        \"weekend_entry_share\": weekend_entry_share,\n",
    "        \"weekend_exit_share\": weekend_exit_share,\n",
    "    },\n",
    "    \"red_flags\": [\n",
    "        \"Si TRAIL domina y p90_bars << horizonte ganador, el motor estÃ¡ matando el alpha.\",\n",
    "        \"Si el mejor lado OOS del alpha es SHORT y tÃº ejecutas LONG, estÃ¡s optimizando el error.\",\n",
    "    ],\n",
    "    \"suggested_param_grid\": DEFAULT_GRID,\n",
    "    \"alpha_best_oos\": alpha_best.to_dicts() if alpha_best.height > 0 else [],\n",
    "    \"alignment_preview\": alignment.to_dicts() if alignment.height > 0 else [],\n",
    "    \"next_steps\": [\n",
    "        \"1) Baseline engine que replique 07C: entrada t+1, salida fija a H (p.ej. 288), costos STRESS, mismas restricciones reales.\",\n",
    "        \"2) Ejecutar engine solo en el lado ganador por sÃ­mbolo (OOS-first) y con MIN_HOLDâ‰ˆ0.25H, TIME_STOP=H, trailing con START alto o apagado.\",\n",
    "        \"3) Exigir GO con mÃ©tricas STRESS OOS + >=3 folds + N OOS suficiente; eliminar fallback TOPK en producciÃ³n.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "OUT_TUNING_JSON.write_text(json.dumps(tuning, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# ========================= Persist =========================\n",
    "kpis.write_parquet(str(OUT_KPIS), compression=\"zstd\")\n",
    "kpis.write_parquet(str(SNAP_KPIS), compression=\"zstd\")\n",
    "exit_mix.write_parquet(str(OUT_EXITS), compression=\"zstd\")\n",
    "holds.write_parquet(str(OUT_HOLDS), compression=\"zstd\")\n",
    "\n",
    "snap = {\n",
    "    \"created_utc\": tuning[\"created_utc\"],\n",
    "    \"outputs\": {\n",
    "        \"kpis\": str(OUT_KPIS),\n",
    "        \"exit_mix\": str(OUT_EXITS),\n",
    "        \"holds\": str(OUT_HOLDS),\n",
    "        \"alpha_best\": str(OUT_ALPHA_PICK) if alpha_best.height > 0 else None,\n",
    "        \"tuning_plan\": str(OUT_TUNING_JSON),\n",
    "    },\n",
    "    \"monfri\": tuning[\"monfri_metrics\"],\n",
    "    \"alpha_report_exists\": bool(alpha_exists),\n",
    "    \"alpha_metric_used\": alpha_metric_used,\n",
    "}\n",
    "SNAP_JSON.write_text(json.dumps(snap, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"diagnostics_engine\"] = {\n",
    "    \"kpis_path\": str(OUT_KPIS),\n",
    "    \"exit_mix_path\": str(OUT_EXITS),\n",
    "    \"holds_path\": str(OUT_HOLDS),\n",
    "    \"alpha_best_path\": str(OUT_ALPHA_PICK) if alpha_best.height > 0 else None,\n",
    "    \"tuning_plan_path\": str(OUT_TUNING_JSON),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_KPIS} (OK)\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_EXITS} (OK)\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_HOLDS} (OK)\")\n",
    "if alpha_best.height > 0:\n",
    "    print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_ALPHA_PICK} (OK)\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_TUNING_JSON} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_KPIS} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 13 v1.1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c68cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 14 v1.0 :: Engine Tuning (IS-only) [alphaâ†”motor alignment + TRAIL_START + longer holds | WFO-safe]\n",
      "[Celda 14] symbols = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 14] folds   = ['F1']\n",
      "[Celda 14] OUT_DIR = C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\tuning_engine_v14\n",
      "[Celda 14] BEST BNBUSD F1 :: score_IS=-1000000.066 | IS(n=102, tot=-0.0662798607020612) | OOS(n=28, tot=-0.030626756246091243) | SL=2.0 TP=6.0 TRAIL=4.0 START=0.0 | MINH=6 EC=12 TS=288\n",
      "[Celda 14] BEST BTCUSD F1 :: score_IS=-1000000.165 | IS(n=112, tot=-0.1645285823913961) | OOS(n=16, tot=-0.03948348129233368) | SL=2.0 TP=4.0 TRAIL=3.0 START=2.0 | MINH=6 EC=12 TS=288\n",
      "[Celda 14] BEST LVMH F1 :: score_IS=-1000000.055 | IS(n=47, tot=-0.05502471212782521) | OOS(n=14, tot=-0.021841151967083493) | SL=2.0 TP=6.0 TRAIL=3.0 START=2.0 | MINH=24 EC=12 TS=288\n",
      "[Celda 14] BEST XAUAUD F1 :: score_IS=-1000000.068 | IS(n=100, tot=-0.06755642775164171) | OOS(n=9, tot=0.004039572464097953) | SL=2.0 TP=4.0 TRAIL=5.0 START=0.0 | MINH=24 EC=12 TS=288\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\tuning_engine_v14\\tuning_results_engine_v14.parquet (OK) | rows=880\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\tuning_engine_v14\\best_params_engine_v14.parquet (OK) | rows=4\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\tuning_results_engine_v14.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\best_params_engine_v14.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\tuning_engine_v14_snapshot.json (OK)\n",
      ">>> Celda 14 v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 14 v1.0 â€” Engine Tuning (IS-only) [alphaâ†”motor alignment + TRAIL_START + longer holds | WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 14 v1.0 :: Engine Tuning (IS-only) [alphaâ†”motor alignment + TRAIL_START + longer holds | WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"data\", \"features\", \"wfo\", \"regime_gate\", \"cost_model\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 14] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "data_state = GLOBAL_STATE[\"data\"]\n",
    "feat_state = GLOBAL_STATE[\"features\"]\n",
    "wfo_state  = GLOBAL_STATE[\"wfo\"]\n",
    "gate_state = GLOBAL_STATE[\"regime_gate\"]\n",
    "cost_state = GLOBAL_STATE[\"cost_model\"]\n",
    "\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "symbols_u = [str(s).upper().strip() for s in symbols]\n",
    "\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 14] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: folds vacÃ­o.\")\n",
    "\n",
    "m5_paths = (\n",
    "    data_state.get(\"m5_ohlcv_paths\")\n",
    "    or data_state.get(\"m5_clean_paths\")\n",
    "    or data_state.get(\"ohlcv_clean_paths\")\n",
    "    or {}\n",
    ")\n",
    "if not isinstance(m5_paths, dict) or not m5_paths:\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: no hay m5_paths. Ejecuta Celda 02/02C.\")\n",
    "\n",
    "feat_paths = feat_state.get(\"features_base_paths\") or {}\n",
    "if not feat_paths:\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: no hay features_base_paths. Ejecuta Celda 05.\")\n",
    "\n",
    "gate_path = gate_state.get(\"gate_table_path\")\n",
    "if not gate_path or not Path(gate_path).exists():\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: no hay regime_gate.gate_table_path. Ejecuta Celda 06.\")\n",
    "gate_df = pl.read_parquet(str(gate_path))\n",
    "\n",
    "costs_by_symbol = cost_state.get(\"costs_by_symbol\") or cost_state.get(\"symbols\") or {}\n",
    "if not costs_by_symbol:\n",
    "    raise RuntimeError(\"[Celda 14] ERROR: no hay cost_model.costs_by_symbol. Ejecuta Celda 03.\")\n",
    "cost_reported_is_roundtrip = bool(cost_state.get(\"cost_reported_is_roundtrip\", False))\n",
    "\n",
    "# ========================= Columnas contract =========================\n",
    "ER_COL  = \"ER_kaufman\"\n",
    "MOM_COL = \"mom_288\"\n",
    "VOL_COL = \"vol_logret_288\"\n",
    "REQ_FEAT_COLS = [\"time_utc\", ER_COL, MOM_COL, VOL_COL]\n",
    "\n",
    "# ATR: preferir feature \"atr\" (Celda 05) o \"atr_72\"; si no existe, aproximar con TR/ewm\n",
    "ATR_PREF_COLS = [\"atr\", \"atr_72\"]\n",
    "\n",
    "# OHLC mÃ­nimos\n",
    "REQ_PX_COLS = [\"time_utc\", \"open\", \"high\", \"low\", \"close\"]\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"backtests\" / \"backtest_engine_v10\" / \"tuning_engine_v14\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_RES  = OUT_DIR / \"tuning_results_engine_v14.parquet\"\n",
    "OUT_BEST = OUT_DIR / \"best_params_engine_v14.parquet\"\n",
    "SNAP_RES  = SNAP_DIR / \"tuning_results_engine_v14.parquet\"\n",
    "SNAP_BEST = SNAP_DIR / \"best_params_engine_v14.parquet\"\n",
    "SNAP_JSON = SNAP_DIR / \"tuning_engine_v14_snapshot.json\"\n",
    "\n",
    "print(f\"[Celda 14] symbols = {symbols_u}\")\n",
    "print(f\"[Celda 14] folds   = {[f.get('fold_id') for f in folds]}\")\n",
    "print(f\"[Celda 14] OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_iso_utc(s: str) -> datetime:\n",
    "    return _ensure_utc(datetime.fromisoformat(s))\n",
    "\n",
    "def _pick_path_case_insensitive(d: dict, sym_u: str) -> str:\n",
    "    if sym_u in d:\n",
    "        return str(d[sym_u])\n",
    "    keys = {str(k).upper().strip(): k for k in d.keys()}\n",
    "    if sym_u in keys:\n",
    "        return str(d[keys[sym_u]])\n",
    "    raise KeyError(f\"[Celda 14] ERROR: no encuentro path para {sym_u}. keys_sample={list(d)[:10]}\")\n",
    "\n",
    "def _need_cols(df: pl.DataFrame, cols: list[str], sym: str, tag: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 14] ERROR: {sym} ({tag}) missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "def _cost_roundtrip_dec(cost_bps: float, reported_is_roundtrip: bool) -> float:\n",
    "    c = float(cost_bps) / 10000.0\n",
    "    return c if reported_is_roundtrip else (2.0 * c)\n",
    "\n",
    "def _ema_np(x: np.ndarray, span: int) -> np.ndarray:\n",
    "    # EMA estÃ¡ndar (alpha = 2/(span+1))\n",
    "    span = int(span)\n",
    "    if span <= 1:\n",
    "        return x.copy()\n",
    "    a = 2.0 / (span + 1.0)\n",
    "    out = np.empty_like(x, dtype=np.float64)\n",
    "    out[:] = np.nan\n",
    "    # init\n",
    "    first = np.where(np.isfinite(x))[0]\n",
    "    if first.size == 0:\n",
    "        return out\n",
    "    i0 = int(first[0])\n",
    "    out[i0] = float(x[i0])\n",
    "    for i in range(i0 + 1, x.size):\n",
    "        xi = x[i]\n",
    "        out[i] = (a * xi + (1.0 - a) * out[i - 1]) if math.isfinite(float(xi)) else out[i - 1]\n",
    "    return out\n",
    "\n",
    "def _compute_atr_wilder_from_ohlc(df: pl.DataFrame, n: int = 72) -> pl.Series:\n",
    "    # True Range + ewm_mean(alpha=1/n, adjust=False)\n",
    "    prev_close = pl.col(\"close\").shift(1)\n",
    "    tr = pl.max_horizontal([\n",
    "        (pl.col(\"high\") - pl.col(\"low\")).abs(),\n",
    "        (pl.col(\"high\") - prev_close).abs(),\n",
    "        (pl.col(\"low\") - prev_close).abs(),\n",
    "    ]).alias(\"__tr__\")\n",
    "    atr = tr.ewm_mean(alpha=(1.0 / float(n)), adjust=False).alias(\"atr\")\n",
    "    return df.select([atr]).get_column(\"atr\")\n",
    "\n",
    "def _segment_of_entry(entry_dt: datetime, is_s: datetime, is_e: datetime, o_s: datetime, o_e: datetime) -> str | None:\n",
    "    if is_s <= entry_dt <= is_e:\n",
    "        return \"IS\"\n",
    "    if o_s <= entry_dt <= o_e:\n",
    "        return \"OOS\"\n",
    "    return None\n",
    "\n",
    "def _kpis_from_rets(rets: np.ndarray) -> dict:\n",
    "    # rets = net returns por trade (decimal)\n",
    "    rets = rets.astype(np.float64)\n",
    "    rets = rets[np.isfinite(rets)]\n",
    "    n = int(rets.size)\n",
    "    if n == 0:\n",
    "        return {\"n\": 0, \"mean\": None, \"std\": None, \"tot\": None, \"mdd\": None, \"win\": None, \"sharpe_like\": None}\n",
    "    mean = float(np.mean(rets))\n",
    "    std = float(np.std(rets, ddof=1)) if n >= 2 else 0.0\n",
    "    # total return compuesta\n",
    "    tot = float(np.expm1(np.sum(np.log1p(rets))))\n",
    "    win = float(np.mean(rets > 0.0))\n",
    "    sharpe_like = float(mean / std) if std > 1e-12 else None\n",
    "\n",
    "    # MDD sobre equity compuesta\n",
    "    eq = np.exp(np.cumsum(np.log1p(rets)))\n",
    "    peak = np.maximum.accumulate(eq)\n",
    "    dd = (eq / peak) - 1.0\n",
    "    mdd = float(np.min(dd)) if dd.size else 0.0\n",
    "\n",
    "    return {\"n\": n, \"mean\": mean, \"std\": std, \"tot\": tot, \"mdd\": mdd, \"win\": win, \"sharpe_like\": sharpe_like}\n",
    "\n",
    "def _score_is(k: dict, min_trades: int, max_mdd_abs: float) -> float:\n",
    "    # score para elegir params SIN tocar OOS\n",
    "    if k[\"n\"] < min_trades:\n",
    "        return -1e9\n",
    "    if k[\"tot\"] is None or k[\"mean\"] is None:\n",
    "        return -1e9\n",
    "    if k[\"tot\"] <= 0.0:\n",
    "        return -1e6 + float(k[\"tot\"])\n",
    "    if k[\"mdd\"] is not None and abs(float(k[\"mdd\"])) > max_mdd_abs:\n",
    "        return -1e6 - abs(float(k[\"mdd\"]))\n",
    "    # t-stat like (mean/std * sqrt(n)) penaliza drawdown\n",
    "    if k[\"std\"] is None or float(k[\"std\"]) <= 1e-12:\n",
    "        core = 0.0\n",
    "    else:\n",
    "        core = (float(k[\"mean\"]) / float(k[\"std\"])) * math.sqrt(float(k[\"n\"]))\n",
    "    pen = 2.0 * abs(float(k[\"mdd\"] or 0.0))\n",
    "    return float(core - pen)\n",
    "\n",
    "# ========================= Motor sim (stateful, numpy) =========================\n",
    "def _simulate_engine(\n",
    "    df: pl.DataFrame,\n",
    "    sym: str,\n",
    "    fid: str,\n",
    "    is_s: datetime, is_e: datetime, o_s: datetime, o_e: datetime,\n",
    "    thr_er: float, thr_mom: float, thr_vol: float,\n",
    "    cost_base_dec: float, cost_stress_dec: float,\n",
    "    params: dict,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Simula trades usando convenciÃ³n:\n",
    "      - seÃ±al al close(t)\n",
    "      - entrada en open(t+1)\n",
    "      - stops/tp/trail evaluados intrabar (OHLC del bar de holding)\n",
    "    \"\"\"\n",
    "    # arrays\n",
    "    t = df.get_column(\"time_utc\").to_list()  # list[datetime]\n",
    "    n = len(t)\n",
    "    if n < 10:\n",
    "        return []\n",
    "\n",
    "    op = df.get_column(\"open\").to_numpy()\n",
    "    hi = df.get_column(\"high\").to_numpy()\n",
    "    lo = df.get_column(\"low\").to_numpy()\n",
    "    cl = df.get_column(\"close\").to_numpy()\n",
    "\n",
    "    er  = df.get_column(ER_COL).to_numpy()\n",
    "    mom = df.get_column(MOM_COL).to_numpy()\n",
    "    vol = df.get_column(VOL_COL).to_numpy()\n",
    "\n",
    "    # weekday robusto desde time_utc (ya UTC)\n",
    "    dow = df.get_column(\"__dow__\").to_numpy()  # Mon=0..Sun=6\n",
    "\n",
    "    # ATR\n",
    "    atr = df.get_column(\"__atr__\").to_numpy()\n",
    "\n",
    "    # EMA filter (opcional)\n",
    "    ema_filter = bool(params.get(\"EMA_FILTER\", True))\n",
    "    if ema_filter:\n",
    "        ema_fast = _ema_np(cl.astype(np.float64), span=48)\n",
    "        ema_slow = _ema_np(cl.astype(np.float64), span=288)\n",
    "    else:\n",
    "        ema_fast = None\n",
    "        ema_slow = None\n",
    "\n",
    "    thr_mom_eff = max(0.0, float(thr_mom))\n",
    "    sig_long = (er >= thr_er) & (mom >= thr_mom_eff) & (vol <= thr_vol)\n",
    "    sig_short = (er >= thr_er) & (mom <= -thr_mom_eff) & (vol <= thr_vol)\n",
    "\n",
    "    if ema_filter:\n",
    "        sig_long = sig_long & (ema_fast > ema_slow)\n",
    "        sig_short = sig_short & (ema_fast < ema_slow)\n",
    "\n",
    "    # params\n",
    "    SL_ATR = float(params[\"SL_ATR\"])\n",
    "    TP_ATR = float(params[\"TP_ATR\"])\n",
    "    TRAIL_ATR = float(params[\"TRAIL_ATR\"])\n",
    "    TRAIL_START_ATR = float(params[\"TRAIL_START_ATR\"])\n",
    "    TIME_STOP_BARS = int(params[\"TIME_STOP_BARS\"])\n",
    "    ENTRY_CONFIRM_BARS = int(params[\"ENTRY_CONFIRM_BARS\"])\n",
    "    EXIT_GATE_OFF_BARS = int(params[\"EXIT_GATE_OFF_BARS\"])\n",
    "    MIN_HOLD_BARS = int(params[\"MIN_HOLD_BARS\"])\n",
    "    COOLDOWN_BARS = int(params[\"COOLDOWN_BARS\"])\n",
    "    MON_FRI = bool(params.get(\"MON_FRI\", True))\n",
    "\n",
    "    trades = []\n",
    "\n",
    "    pos = 0  # 0 flat, +1 long, -1 short\n",
    "    entry_idx = -1\n",
    "    entry_px = None\n",
    "    entry_atr = None\n",
    "\n",
    "    stop_px = None\n",
    "    tp_px = None\n",
    "    trail_px = None\n",
    "    peak = None\n",
    "    trough = None\n",
    "    trail_armed = False\n",
    "    gate_off_count = 0\n",
    "    cooldown = 0\n",
    "\n",
    "    long_consec = 0\n",
    "    short_consec = 0\n",
    "\n",
    "    # loop: i es Ã­ndice del bar ACTUAL; la entrada ocurre en i usando seÃ±al de i-1\n",
    "    for i in range(1, n):\n",
    "        # cooldown update\n",
    "        if cooldown > 0 and pos == 0:\n",
    "            cooldown -= 1\n",
    "\n",
    "        # ====== gestionar posiciÃ³n abierta ======\n",
    "        if pos != 0 and entry_idx >= 0:\n",
    "            bars_held = i - entry_idx\n",
    "\n",
    "            # weekend flatten: si MON_FRI, forzar salida antes del fin de semana\n",
    "            if MON_FRI:\n",
    "                # si el bar actual ya es fin de semana, salimos al close (deberÃ­a no ocurrir si entry filtrado bien)\n",
    "                if int(dow[i]) >= 5:\n",
    "                    exit_px = float(cl[i])\n",
    "                    exit_dt = t[i]\n",
    "                    reason = \"WEEKEND_FLATTEN\"\n",
    "                else:\n",
    "                    # si hoy es viernes y el siguiente bar cae en fin de semana, flatten ahora al close\n",
    "                    if i + 1 < n and int(dow[i]) == 4 and int(dow[i + 1]) >= 5:\n",
    "                        exit_px = float(cl[i])\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"WEEKEND_FLATTEN\"\n",
    "                    else:\n",
    "                        exit_px = None\n",
    "                        exit_dt = None\n",
    "                        reason = None\n",
    "            else:\n",
    "                exit_px = None\n",
    "                exit_dt = None\n",
    "                reason = None\n",
    "\n",
    "            # stops/tp/trail intrabar (solo si no salimos ya por weekend)\n",
    "            if reason is None:\n",
    "                # protecciÃ³n: ATR debe ser finito\n",
    "                a_i = float(atr[i]) if math.isfinite(float(atr[i])) else (float(entry_atr) if entry_atr else 0.0)\n",
    "                if a_i <= 0.0 or not math.isfinite(a_i):\n",
    "                    a_i = float(entry_atr) if entry_atr and entry_atr > 0 else 0.0\n",
    "\n",
    "                if pos == 1:\n",
    "                    # actualizar peak/trailing\n",
    "                    peak = float(max(peak, hi[i])) if peak is not None else float(hi[i])\n",
    "                    if (not trail_armed) and entry_px is not None and entry_atr is not None:\n",
    "                        if peak >= float(entry_px) + (TRAIL_START_ATR * float(entry_atr)):\n",
    "                            trail_armed = True\n",
    "                            # inicializar trail desde peak\n",
    "                            trail_px = float(peak - (TRAIL_ATR * float(entry_atr)))\n",
    "\n",
    "                    if trail_armed and entry_atr is not None:\n",
    "                        cand = float(peak - (TRAIL_ATR * float(entry_atr)))\n",
    "                        trail_px = float(max(trail_px, cand)) if trail_px is not None else cand\n",
    "\n",
    "                    # prioridades conservadoras: SL -> TP -> TRAIL\n",
    "                    sl_hit = (stop_px is not None) and (float(lo[i]) <= float(stop_px))\n",
    "                    tp_hit = (tp_px is not None) and (float(hi[i]) >= float(tp_px))\n",
    "                    tr_hit = (trail_armed and trail_px is not None) and (float(lo[i]) <= float(trail_px))\n",
    "\n",
    "                    if sl_hit:\n",
    "                        exit_px = float(stop_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"SL\"\n",
    "                    elif tp_hit:\n",
    "                        exit_px = float(tp_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"TP\"\n",
    "                    elif tr_hit:\n",
    "                        exit_px = float(trail_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"TRAIL\"\n",
    "\n",
    "                else:  # pos == -1 short\n",
    "                    trough = float(min(trough, lo[i])) if trough is not None else float(lo[i])\n",
    "                    if (not trail_armed) and entry_px is not None and entry_atr is not None:\n",
    "                        if trough <= float(entry_px) - (TRAIL_START_ATR * float(entry_atr)):\n",
    "                            trail_armed = True\n",
    "                            trail_px = float(trough + (TRAIL_ATR * float(entry_atr)))\n",
    "\n",
    "                    if trail_armed and entry_atr is not None:\n",
    "                        cand = float(trough + (TRAIL_ATR * float(entry_atr)))\n",
    "                        trail_px = float(min(trail_px, cand)) if trail_px is not None else cand\n",
    "\n",
    "                    # prioridades conservadoras: SL -> TP -> TRAIL\n",
    "                    sl_hit = (stop_px is not None) and (float(hi[i]) >= float(stop_px))\n",
    "                    tp_hit = (tp_px is not None) and (float(lo[i]) <= float(tp_px))\n",
    "                    tr_hit = (trail_armed and trail_px is not None) and (float(hi[i]) >= float(trail_px))\n",
    "\n",
    "                    if sl_hit:\n",
    "                        exit_px = float(stop_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"SL\"\n",
    "                    elif tp_hit:\n",
    "                        exit_px = float(tp_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"TP\"\n",
    "                    elif tr_hit:\n",
    "                        exit_px = float(trail_px)\n",
    "                        exit_dt = t[i]\n",
    "                        reason = \"TRAIL\"\n",
    "\n",
    "            # gate-off / time-stop (al close), respetando MIN_HOLD_BARS\n",
    "            if reason is None and bars_held >= MIN_HOLD_BARS:\n",
    "                # gate-off hysteresis (miramos la seÃ±al del bar i-1)\n",
    "                if pos == 1:\n",
    "                    gate_on = bool(sig_long[i - 1])\n",
    "                else:\n",
    "                    gate_on = bool(sig_short[i - 1])\n",
    "\n",
    "                gate_off_count = 0 if gate_on else (gate_off_count + 1)\n",
    "\n",
    "                if gate_off_count >= EXIT_GATE_OFF_BARS:\n",
    "                    exit_px = float(cl[i])\n",
    "                    exit_dt = t[i]\n",
    "                    reason = \"REGIME_OFF\"\n",
    "                elif bars_held >= TIME_STOP_BARS:\n",
    "                    exit_px = float(cl[i])\n",
    "                    exit_dt = t[i]\n",
    "                    reason = \"TIME_STOP\"\n",
    "\n",
    "            # ejecutar salida si aplica\n",
    "            if reason is not None and exit_px is not None and entry_px is not None:\n",
    "                entry_dt = t[entry_idx]\n",
    "                seg = _segment_of_entry(entry_dt, is_s, is_e, o_s, o_e)\n",
    "                if seg is not None:\n",
    "                    if pos == 1:\n",
    "                        gross = (float(exit_px) / float(entry_px)) - 1.0\n",
    "                    else:\n",
    "                        gross = (float(entry_px) / float(exit_px)) - 1.0\n",
    "\n",
    "                    trades.append({\n",
    "                        \"symbol\": sym,\n",
    "                        \"fold_id\": fid,\n",
    "                        \"segment\": seg,\n",
    "                        \"side\": \"LONG\" if pos == 1 else \"SHORT\",\n",
    "                        \"entry_time\": entry_dt.isoformat(),\n",
    "                        \"exit_time\": exit_dt.isoformat(),\n",
    "                        \"entry_price\": float(entry_px),\n",
    "                        \"exit_price\": float(exit_px),\n",
    "                        \"bars_held\": int(exit_dt == exit_dt and (i - entry_idx)),\n",
    "                        \"exit_reason\": str(reason),\n",
    "                        \"gross_ret\": float(gross),\n",
    "                        \"net_ret_base\": float(gross - cost_base_dec),\n",
    "                        \"net_ret_stress\": float(gross - cost_stress_dec),\n",
    "                    })\n",
    "\n",
    "                # reset state\n",
    "                pos = 0\n",
    "                entry_idx = -1\n",
    "                entry_px = None\n",
    "                entry_atr = None\n",
    "                stop_px = None\n",
    "                tp_px = None\n",
    "                trail_px = None\n",
    "                peak = None\n",
    "                trough = None\n",
    "                trail_armed = False\n",
    "                gate_off_count = 0\n",
    "                cooldown = COOLDOWN_BARS\n",
    "\n",
    "        # ====== si estamos flat, evaluar entrada (en open[i]) usando seÃ±al de i-1 ======\n",
    "        if pos == 0 and cooldown == 0:\n",
    "            # confirmar consecutivos con seÃ±al del bar i-1\n",
    "            long_consec = (long_consec + 1) if bool(sig_long[i - 1]) else 0\n",
    "            short_consec = (short_consec + 1) if bool(sig_short[i - 1]) else 0\n",
    "\n",
    "            # entrada ocurre en open[i]; por lo tanto el filtro Monâ€“Fri debe aplicarse sobre el bar i (entry_time)\n",
    "            if MON_FRI and int(dow[i]) >= 5:\n",
    "                continue\n",
    "\n",
    "            # ATR en entry bar\n",
    "            a_entry = float(atr[i]) if math.isfinite(float(atr[i])) else 0.0\n",
    "            if (not math.isfinite(a_entry)) or a_entry <= 0.0:\n",
    "                continue\n",
    "\n",
    "            # preferencia: si ambos confirman (posible si thr_mom_eff=0), resolvemos por signo mom\n",
    "            enter_long = long_consec >= ENTRY_CONFIRM_BARS\n",
    "            enter_short = short_consec >= ENTRY_CONFIRM_BARS\n",
    "\n",
    "            if enter_long and enter_short:\n",
    "                enter_long = bool(mom[i - 1] >= 0.0)\n",
    "                enter_short = not enter_long\n",
    "\n",
    "            if enter_long:\n",
    "                px = float(op[i]) if float(op[i]) > 0 else float(cl[i])\n",
    "                if px <= 0:\n",
    "                    continue\n",
    "                pos = 1\n",
    "                entry_idx = i\n",
    "                entry_px = px\n",
    "                entry_atr = a_entry\n",
    "                stop_px = float(entry_px - SL_ATR * entry_atr)\n",
    "                tp_px   = float(entry_px + TP_ATR * entry_atr)\n",
    "                peak = float(hi[i])\n",
    "                trail_armed = (TRAIL_START_ATR <= 0.0)\n",
    "                trail_px = float(peak - TRAIL_ATR * entry_atr) if trail_armed else None\n",
    "                gate_off_count = 0\n",
    "\n",
    "                # reset counters para evitar re-entradas inmediatas\n",
    "                long_consec = 0\n",
    "                short_consec = 0\n",
    "\n",
    "            elif enter_short:\n",
    "                px = float(op[i]) if float(op[i]) > 0 else float(cl[i])\n",
    "                if px <= 0:\n",
    "                    continue\n",
    "                pos = -1\n",
    "                entry_idx = i\n",
    "                entry_px = px\n",
    "                entry_atr = a_entry\n",
    "                stop_px = float(entry_px + SL_ATR * entry_atr)\n",
    "                tp_px   = float(entry_px - TP_ATR * entry_atr)\n",
    "                trough = float(lo[i])\n",
    "                trail_armed = (TRAIL_START_ATR <= 0.0)\n",
    "                trail_px = float(trough + TRAIL_ATR * entry_atr) if trail_armed else None\n",
    "                gate_off_count = 0\n",
    "\n",
    "                long_consec = 0\n",
    "                short_consec = 0\n",
    "\n",
    "    return trades\n",
    "\n",
    "# ========================= ParÃ¡metros tuning (acotados) =========================\n",
    "# Institucional: tuning en IS, reporte OOS. Grilla intencional para corregir:\n",
    "#   - holding demasiado corto\n",
    "#   - trailing dominante demasiado temprano\n",
    "GRID = {\n",
    "    \"SL_ATR\":            [2.0, 2.5, 3.0],\n",
    "    \"TP_ATR\":            [4.0, 6.0, 8.0],\n",
    "    \"TRAIL_ATR\":         [3.0, 4.0, 5.0],\n",
    "    \"TRAIL_START_ATR\":   [0.0, 1.0, 2.0],     # clave: no activar trailing hasta que avance\n",
    "    \"MIN_HOLD_BARS\":     [6, 24],            # fuerza capturar â€œlegâ€\n",
    "    \"ENTRY_CONFIRM_BARS\":[6, 12],\n",
    "    \"EXIT_GATE_OFF_BARS\":[12],               # mantener estable para no mezclar efectos\n",
    "    \"COOLDOWN_BARS\":     [12, 24],\n",
    "    \"TIME_STOP_BARS\":    [288, 576],         # 1d / 2d (M5)\n",
    "    \"EMA_FILTER\":        [True],\n",
    "    \"MON_FRI\":           [True],\n",
    "}\n",
    "\n",
    "MAX_COMBOS_PER_SYMBOL = 220  # seguridad\n",
    "MIN_TRADES_IS = 25\n",
    "MAX_MDD_ABS_IS = 0.35\n",
    "\n",
    "# ========================= Main: per sÃ­mbolo/fold tuning =========================\n",
    "rows = []\n",
    "best_rows = []\n",
    "\n",
    "for sym in symbols_u:\n",
    "    # ---- Load OHLCV ----\n",
    "    px_path = _pick_path_case_insensitive(m5_paths, sym)\n",
    "    df_px = pl.read_parquet(str(px_path))\n",
    "    # normalizar columnas faltantes\n",
    "    if \"open\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.col(\"close\").alias(\"open\"))\n",
    "    if \"high\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.max_horizontal([pl.col(\"open\"), pl.col(\"close\")]).alias(\"high\"))\n",
    "    if \"low\" not in df_px.columns:\n",
    "        df_px = df_px.with_columns(pl.min_horizontal([pl.col(\"open\"), pl.col(\"close\")]).alias(\"low\"))\n",
    "\n",
    "    _need_cols(df_px, [\"time_utc\", \"open\", \"high\", \"low\", \"close\"], sym, \"OHLCV\")\n",
    "    df_px = (\n",
    "        df_px\n",
    "        .with_columns([\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\",\"UTC\"), strict=False),\n",
    "            pl.col(\"open\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"high\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"low\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "        ])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    # ---- Load Features ----\n",
    "    f_path = _pick_path_case_insensitive(feat_paths, sym)\n",
    "    df_f = pl.read_parquet(str(f_path))\n",
    "    _need_cols(df_f, REQ_FEAT_COLS, sym, \"FEATURES\")\n",
    "    df_f = df_f.with_columns(pl.col(\"time_utc\").cast(pl.Datetime(\"us\",\"UTC\"), strict=False)).sort(\"time_utc\")\n",
    "\n",
    "    # ---- Join ----\n",
    "    df = df_px.join(df_f, on=\"time_utc\", how=\"inner\")\n",
    "    if df.height < 2000:\n",
    "        print(f\"[Celda 14] WARN: {sym} muy pocas filas post-join: {df.height}\")\n",
    "        continue\n",
    "\n",
    "    # weekday robusto desde time_utc (no strings)\n",
    "    df = df.with_columns(pl.col(\"time_utc\").dt.weekday().cast(pl.Int16).alias(\"__dow__\"))\n",
    "\n",
    "    # ATR: usar col existente si hay; si no, calcular de OHLC\n",
    "    atr_col = None\n",
    "    for c in ATR_PREF_COLS:\n",
    "        if c in df.columns:\n",
    "            atr_col = c\n",
    "            break\n",
    "    if atr_col is None:\n",
    "        atr_s = _compute_atr_wilder_from_ohlc(df.select([\"high\",\"low\",\"close\"]), n=72)\n",
    "        df = df.with_columns(pl.Series(name=\"__atr__\", values=atr_s))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col(atr_col).cast(pl.Float64, strict=False).alias(\"__atr__\"))\n",
    "\n",
    "    # costs\n",
    "    cinfo = costs_by_symbol.get(sym) or {}\n",
    "    cost_base_bps = float(cinfo.get(\"COST_BASE_BPS\", 0.0))\n",
    "    cost_stress_bps = float(cinfo.get(\"COST_STRESS_BPS\", 0.0))\n",
    "    cost_base_dec = _cost_roundtrip_dec(cost_base_bps, cost_reported_is_roundtrip)\n",
    "    cost_stress_dec = _cost_roundtrip_dec(cost_stress_bps, cost_reported_is_roundtrip)\n",
    "\n",
    "    # grid combos (determinÃ­stico)\n",
    "    keys = list(GRID.keys())\n",
    "    combos = list(itertools.product(*[GRID[k] for k in keys]))\n",
    "    if len(combos) > MAX_COMBOS_PER_SYMBOL:\n",
    "        combos = combos[:MAX_COMBOS_PER_SYMBOL]\n",
    "\n",
    "    for f in folds:\n",
    "        fid = str(f[\"fold_id\"])\n",
    "        is_s = _parse_iso_utc(f[\"IS_start\"])\n",
    "        is_e = _parse_iso_utc(f[\"IS_end\"])\n",
    "        o_s  = _parse_iso_utc(f[\"OOS_start\"])\n",
    "        o_e  = _parse_iso_utc(f[\"OOS_end\"])\n",
    "\n",
    "        g = gate_df.filter((pl.col(\"symbol\") == sym) & (pl.col(\"fold_id\") == fid))\n",
    "        if g.is_empty():\n",
    "            raise RuntimeError(f\"[Celda 14] ERROR: no hay thresholds en gate_df para {sym} {fid}.\")\n",
    "\n",
    "        thr_er = float(g.select(pl.col(\"thr_er\")).item())\n",
    "        thr_mom = float(g.select(pl.col(\"thr_mom\")).item())\n",
    "        thr_vol = float(g.select(pl.col(\"thr_vol\")).item())\n",
    "\n",
    "        best = None\n",
    "\n",
    "        for tpl in combos:\n",
    "            params = {k: tpl[i] for i, k in enumerate(keys)}\n",
    "            params[\"COST_BASE_BPS\"] = cost_base_bps\n",
    "            params[\"COST_STRESS_BPS\"] = cost_stress_bps\n",
    "\n",
    "            trades = _simulate_engine(\n",
    "                df=df,\n",
    "                sym=sym, fid=fid,\n",
    "                is_s=is_s, is_e=is_e, o_s=o_s, o_e=o_e,\n",
    "                thr_er=thr_er, thr_mom=thr_mom, thr_vol=thr_vol,\n",
    "                cost_base_dec=cost_base_dec, cost_stress_dec=cost_stress_dec,\n",
    "                params=params,\n",
    "            )\n",
    "\n",
    "            if not trades:\n",
    "                k_is = {\"n\": 0, \"mean\": None, \"std\": None, \"tot\": None, \"mdd\": None, \"win\": None, \"sharpe_like\": None}\n",
    "                k_oos = k_is.copy()\n",
    "            else:\n",
    "                # separar IS/OOS por segment (ya asignado con entry_time)\n",
    "                is_rets = np.array([t[\"net_ret_base\"] for t in trades if t[\"segment\"] == \"IS\"], dtype=np.float64)\n",
    "                oos_rets = np.array([t[\"net_ret_base\"] for t in trades if t[\"segment\"] == \"OOS\"], dtype=np.float64)\n",
    "                k_is = _kpis_from_rets(is_rets)\n",
    "                k_oos = _kpis_from_rets(oos_rets)\n",
    "\n",
    "            score_is = _score_is(k_is, min_trades=MIN_TRADES_IS, max_mdd_abs=MAX_MDD_ABS_IS)\n",
    "\n",
    "            row = {\n",
    "                \"symbol\": sym,\n",
    "                \"fold_id\": fid,\n",
    "                \"score_is\": float(score_is),\n",
    "                \"n_is\": int(k_is[\"n\"]),\n",
    "                \"tot_is\": k_is[\"tot\"],\n",
    "                \"mdd_is\": k_is[\"mdd\"],\n",
    "                \"win_is\": k_is[\"win\"],\n",
    "                \"sharpe_like_is\": k_is[\"sharpe_like\"],\n",
    "                \"n_oos\": int(k_oos[\"n\"]),\n",
    "                \"tot_oos\": k_oos[\"tot\"],\n",
    "                \"mdd_oos\": k_oos[\"mdd\"],\n",
    "                \"win_oos\": k_oos[\"win\"],\n",
    "                \"sharpe_like_oos\": k_oos[\"sharpe_like\"],\n",
    "                \"thr_er\": thr_er,\n",
    "                \"thr_mom\": thr_mom,\n",
    "                \"thr_vol\": thr_vol,\n",
    "                \"cost_base_bps\": cost_base_bps,\n",
    "                \"cost_stress_bps\": cost_stress_bps,\n",
    "                **{k: params[k] for k in keys},\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "            if best is None or score_is > best[\"score_is\"]:\n",
    "                best = row\n",
    "\n",
    "        if best is not None:\n",
    "            best_rows.append(best)\n",
    "            print(\n",
    "                f\"[Celda 14] BEST {sym} {fid} :: score_IS={best['score_is']:.3f} \"\n",
    "                f\"| IS(n={best['n_is']}, tot={best['tot_is']}) \"\n",
    "                f\"| OOS(n={best['n_oos']}, tot={best['tot_oos']}) \"\n",
    "                f\"| SL={best['SL_ATR']} TP={best['TP_ATR']} TRAIL={best['TRAIL_ATR']} START={best['TRAIL_START_ATR']} \"\n",
    "                f\"| MINH={best['MIN_HOLD_BARS']} EC={best['ENTRY_CONFIRM_BARS']} TS={best['TIME_STOP_BARS']}\"\n",
    "            )\n",
    "\n",
    "# ========================= Persist =========================\n",
    "res_df = pl.DataFrame(rows).sort([\"symbol\",\"fold_id\",\"score_is\"], descending=[False, False, True]) if rows else pl.DataFrame()\n",
    "best_df = pl.DataFrame(best_rows).sort([\"symbol\",\"fold_id\"]) if best_rows else pl.DataFrame()\n",
    "\n",
    "if res_df.height == 0 or best_df.height == 0:\n",
    "    raise RuntimeError(\"[Celda 14] GATE FAIL: tuning vacÃ­o (sin resultados).\")\n",
    "\n",
    "res_df.write_parquet(str(OUT_RES), compression=\"zstd\")\n",
    "best_df.write_parquet(str(OUT_BEST), compression=\"zstd\")\n",
    "res_df.write_parquet(str(SNAP_RES), compression=\"zstd\")\n",
    "best_df.write_parquet(str(SNAP_BEST), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": [f.get(\"fold_id\") for f in folds],\n",
    "    \"grid\": GRID,\n",
    "    \"limits\": {\"MAX_COMBOS_PER_SYMBOL\": MAX_COMBOS_PER_SYMBOL, \"MIN_TRADES_IS\": MIN_TRADES_IS, \"MAX_MDD_ABS_IS\": MAX_MDD_ABS_IS},\n",
    "    \"inputs\": {\"gate_table_path\": str(gate_path)},\n",
    "    \"outputs\": {\"tuning_results\": str(OUT_RES), \"best_params\": str(OUT_BEST), \"snap_results\": str(SNAP_RES), \"snap_best\": str(SNAP_BEST)},\n",
    "    \"notes\": [\n",
    "        \"SelecciÃ³n de parÃ¡metros: SOLO por IS (score_is).\",\n",
    "        \"OOS se reporta para validaciÃ³n, no se optimiza sobre OOS.\",\n",
    "        \"Monâ€“Fri se aplica sobre entry_time (bar de entrada).\",\n",
    "        \"TRAIL_START_ATR evita que el trailing mate trades antes de que maduren.\"\n",
    "    ],\n",
    "}\n",
    "SNAP_JSON.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "GLOBAL_STATE[\"tuning_engine_v14\"] = {\n",
    "    \"tuning_results_path\": str(OUT_RES),\n",
    "    \"best_params_path\": str(OUT_BEST),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "    \"grid\": GRID,\n",
    "    \"limits\": snapshot[\"limits\"],\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_RES} (OK) | rows={res_df.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_BEST} (OK) | rows={best_df.height}\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_RES} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_BEST} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 14 v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5cf30cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 15 v1.0.1 :: Alpha Design (IS-only) [side + horizon selection â†’ motor targets] [WFO-safe]\n",
      "[Celda 15] symbols = ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "[Celda 15] folds   = ['F1']\n",
      "[Celda 15] Alpha design preview (top 20):\n",
      "shape: (3, 17)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† picked_sid â”† picked_h_b â”† â€¦ â”† TIME_STOP_ â”† MIN_HOLD_B â”† ENTRY_CON â”† COOLDOWN_ â”‚\n",
      "â”‚ ---    â”† ---     â”† e_alpha    â”† ars        â”†   â”† BARS_targe â”† ARS_target â”† FIRM_BARS â”† BARS_targ â”‚\n",
      "â”‚ str    â”† str     â”† ---        â”† ---        â”†   â”† t          â”† ---        â”† _target   â”† et        â”‚\n",
      "â”‚        â”†         â”† str        â”† i64        â”†   â”† ---        â”† i64        â”† ---       â”† ---       â”‚\n",
      "â”‚        â”†         â”†            â”†            â”†   â”† i64        â”†            â”† i64       â”† i64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BNBUSD â”† F1      â”† LONG       â”† 288        â”† â€¦ â”† 288        â”† 72         â”† 28        â”† 28        â”‚\n",
      "â”‚ LVMH   â”† F1      â”† LONG       â”† 288        â”† â€¦ â”† 288        â”† 72         â”† 28        â”† 28        â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† LONG       â”† 288        â”† â€¦ â”† 288        â”† 72         â”† 28        â”† 28        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\alpha_design\\alpha_design_v15.parquet (OK) | rows=3\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\alpha_design\\alpha_design_v15.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\alpha_design_v15.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\alpha_design_v15_snapshot.json (OK)\n",
      ">>> Celda 15 v1.0.1 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 15 v1.0.1 â€” Alpha Design (IS-only) [side + horizon selection â†’ motor targets] [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 15 v1.0.1 :: Alpha Design (IS-only) [side + horizon selection â†’ motor targets] [WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 15] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"wfo\", \"universe\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 15] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "wfo_state = GLOBAL_STATE[\"wfo\"]\n",
    "\n",
    "# sÃ­mbolos efectivos (preferir QA final)\n",
    "dq = GLOBAL_STATE.get(\"data_quality\", {}) or {}\n",
    "symbols = dq.get(\"final_symbols\") or (GLOBAL_STATE[\"universe\"].get(\"selected_symbols_TREND\") or [])\n",
    "if not symbols:\n",
    "    raise RuntimeError(\"[Celda 15] ERROR: no hay sÃ­mbolos (data_quality.final_symbols / selected_symbols_TREND).\")\n",
    "symbols_u = [str(s).upper().strip() for s in symbols]\n",
    "\n",
    "# folds\n",
    "folds = wfo_state.get(\"folds\") or []\n",
    "if not folds:\n",
    "    folds_path = wfo_state.get(\"folds_path\")\n",
    "    if not folds_path:\n",
    "        raise RuntimeError(\"[Celda 15] ERROR: no hay wfo.folds ni wfo.folds_path. Ejecuta Celda 04.\")\n",
    "    folds = json.loads(Path(folds_path).read_text(encoding=\"utf-8\")).get(\"folds\", [])\n",
    "if not folds:\n",
    "    raise RuntimeError(\"[Celda 15] ERROR: folds vacÃ­o (Celda 04 no produjo folds).\")\n",
    "\n",
    "fold_ids = [str(f.get(\"fold_id\")) for f in folds if f.get(\"fold_id\") is not None]\n",
    "if not fold_ids:\n",
    "    raise RuntimeError(\"[Celda 15] ERROR: fold_ids vacÃ­o.\")\n",
    "\n",
    "print(f\"[Celda 15] symbols = {symbols_u}\")\n",
    "print(f\"[Celda 15] folds   = {fold_ids}\")\n",
    "\n",
    "# ========================= Inputs =========================\n",
    "alpha_report_path = (\n",
    "    (GLOBAL_STATE.get(\"alpha_reports\", {}) or {}).get(\"alpha_multi_horizon_report_path\")\n",
    "    or str(Path(paths[\"artifacts\"]).resolve() / \"alpha_reports\" / \"alpha_multi_horizon_report.parquet\")\n",
    ")\n",
    "alpha_report_path = str(Path(alpha_report_path).resolve())\n",
    "\n",
    "if not Path(alpha_report_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 15] ERROR: alpha report no existe: {alpha_report_path}. Ejecuta Celda 07C.\")\n",
    "\n",
    "alpha = pl.read_parquet(alpha_report_path)\n",
    "\n",
    "# ========================= Contract / Required cols =========================\n",
    "REQ = [\n",
    "    \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "    \"h_bars_after_entry\",\"horizon_tag\",\n",
    "    \"n_trades\",\n",
    "    \"net_base_mean\",\"net_stress_mean\",\n",
    "    \"net_base_std\",\n",
    "    \"win_rate_base\",\n",
    "    \"sharpe_like_base\",\n",
    "    \"cost_base_bps\",\"cost_stress_bps\",\n",
    "]\n",
    "miss = [c for c in REQ if c not in alpha.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 15] ERROR: alpha report missing cols={miss}. cols={alpha.columns}\")\n",
    "\n",
    "alpha = (\n",
    "    alpha\n",
    "    .with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"fold_id\").cast(pl.Utf8),\n",
    "        pl.col(\"segment\").cast(pl.Utf8),\n",
    "        pl.col(\"side\").cast(pl.Utf8),\n",
    "        pl.col(\"h_bars_after_entry\").cast(pl.Int64),\n",
    "        pl.col(\"n_trades\").cast(pl.Int64),\n",
    "    ])\n",
    "    .filter(pl.col(\"symbol\").is_in(symbols_u))\n",
    "    .filter(pl.col(\"fold_id\").is_in(fold_ids))\n",
    ")\n",
    "\n",
    "# ========================= ParÃ¡metros de selecciÃ³n (institucionales) =========================\n",
    "MIN_TRADES_IS  = int((GLOBAL_STATE.get(\"alpha_design\", {}) or {}).get(\"MIN_TRADES_IS\", 80))\n",
    "MIN_TRADES_OOS = int((GLOBAL_STATE.get(\"alpha_design\", {}) or {}).get(\"MIN_TRADES_OOS\", 20))\n",
    "MIN_NET_MEAN_IS = float((GLOBAL_STATE.get(\"alpha_design\", {}) or {}).get(\"MIN_NET_MEAN_IS\", 0.0))\n",
    "\n",
    "def _score_expr() -> pl.Expr:\n",
    "    # FIX: Polars antiguo puede no tener pl.sqrt(). Usamos pow(0.5) sobre Expr.\n",
    "    n_sqrt = pl.when(pl.col(\"n_trades\") > 0).then(pl.col(\"n_trades\").cast(pl.Float64).pow(0.5)).otherwise(pl.lit(0.0))\n",
    "    return (pl.col(\"sharpe_like_base\") * n_sqrt).alias(\"score_is\")\n",
    "\n",
    "# ========================= SelecciÃ³n IS-only (lado + horizonte) =========================\n",
    "is_tbl = (\n",
    "    alpha\n",
    "    .filter(pl.col(\"segment\") == \"IS\")\n",
    "    .filter(pl.col(\"n_trades\") >= pl.lit(MIN_TRADES_IS))\n",
    "    .with_columns([\n",
    "        _score_expr(),\n",
    "        (pl.col(\"net_base_mean\") > 0).alias(\"is_pos_base\"),\n",
    "        (pl.col(\"net_stress_mean\") > 0).alias(\"is_pos_stress\"),\n",
    "    ])\n",
    "    .filter(pl.col(\"net_base_mean\") >= pl.lit(MIN_NET_MEAN_IS))\n",
    ")\n",
    "\n",
    "if is_tbl.height == 0:\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 15] GATE FAIL: no hay candidatos IS con n>={MIN_TRADES_IS} y net_base_mean>={MIN_NET_MEAN_IS}. \"\n",
    "        f\"Esto sugiere que NO hay alpha neto bajo el gate actual o que el universo es demasiado chico.\"\n",
    "    )\n",
    "\n",
    "best_is = (\n",
    "    is_tbl\n",
    "    .sort([\"symbol\",\"fold_id\",\"score_is\"], descending=[False, False, True])\n",
    "    .group_by([\"symbol\",\"fold_id\"])\n",
    "    .agg([\n",
    "        pl.first(\"side\").alias(\"picked_side_alpha\"),\n",
    "        pl.first(\"h_bars_after_entry\").alias(\"picked_h_bars\"),\n",
    "        pl.first(\"horizon_tag\").alias(\"picked_h_tag\"),\n",
    "\n",
    "        pl.first(\"n_trades\").alias(\"is_n\"),\n",
    "        pl.first(\"net_base_mean\").alias(\"is_net_base_mean\"),\n",
    "        pl.first(\"net_stress_mean\").alias(\"is_net_stress_mean\"),\n",
    "        pl.first(\"win_rate_base\").alias(\"is_win_rate_base\"),\n",
    "        pl.first(\"sharpe_like_base\").alias(\"is_sharpe_like_base\"),\n",
    "        pl.first(\"score_is\").alias(\"is_score\"),\n",
    "        pl.first(\"cost_base_bps\").alias(\"cost_base_bps\"),\n",
    "        pl.first(\"cost_stress_bps\").alias(\"cost_stress_bps\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "oos_tbl = (\n",
    "    alpha\n",
    "    .filter(pl.col(\"segment\") == \"OOS\")\n",
    "    .select([\n",
    "        \"symbol\",\"fold_id\",\"side\",\"h_bars_after_entry\",\n",
    "        \"n_trades\",\"net_base_mean\",\"net_stress_mean\",\"win_rate_base\",\"sharpe_like_base\"\n",
    "    ])\n",
    "    .rename({\n",
    "        \"side\": \"oos_side\",\n",
    "        \"h_bars_after_entry\": \"oos_h_bars\",\n",
    "        \"n_trades\": \"oos_n\",\n",
    "        \"net_base_mean\": \"oos_net_base_mean\",\n",
    "        \"net_stress_mean\": \"oos_net_stress_mean\",\n",
    "        \"win_rate_base\": \"oos_win_rate_base\",\n",
    "        \"sharpe_like_base\": \"oos_sharpe_like_base\",\n",
    "    })\n",
    ")\n",
    "\n",
    "def _cap_min(expr: pl.Expr, minv: int) -> pl.Expr:\n",
    "    return pl.when(expr < pl.lit(minv)).then(pl.lit(minv)).otherwise(expr)\n",
    "\n",
    "design = (\n",
    "    best_is\n",
    "    .join(\n",
    "        oos_tbl,\n",
    "        left_on=[\"symbol\",\"fold_id\",\"picked_side_alpha\",\"picked_h_bars\"],\n",
    "        right_on=[\"symbol\",\"fold_id\",\"oos_side\",\"oos_h_bars\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"oos_n\").is_null()).then(pl.lit(\"NO_OOS_DATA\"))\n",
    "         .when(pl.col(\"oos_n\") < pl.lit(MIN_TRADES_OOS)).then(pl.lit(\"LOW_OOS_N\"))\n",
    "         .when((pl.col(\"oos_net_base_mean\") > 0) & (pl.col(\"oos_net_stress_mean\") > 0)).then(pl.lit(\"OOS_OK\"))\n",
    "         .when(pl.col(\"oos_net_base_mean\") > 0).then(pl.lit(\"OOS_WEAK_STRESS\"))\n",
    "         .otherwise(pl.lit(\"OOS_NEG\"))\n",
    "         .alias(\"oos_status_evidence\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Targets de motor alineados al horizonte (swing permitido: NO fuerza cierre diario)\n",
    "        pl.col(\"picked_h_bars\").cast(pl.Int64).alias(\"TIME_STOP_BARS_target\"),\n",
    "        _cap_min((pl.col(\"picked_h_bars\").cast(pl.Float64) * 0.25).cast(pl.Int64), 6).alias(\"MIN_HOLD_BARS_target\"),\n",
    "        _cap_min((pl.col(\"picked_h_bars\").cast(pl.Float64) * 0.10).cast(pl.Int64), 3).alias(\"ENTRY_CONFIRM_BARS_target\"),\n",
    "        _cap_min((pl.col(\"picked_h_bars\").cast(pl.Float64) * 0.10).cast(pl.Int64), 6).alias(\"EXIT_GATE_OFF_BARS_target\"),\n",
    "        _cap_min((pl.col(\"picked_h_bars\").cast(pl.Float64) * 0.10).cast(pl.Int64), 12).alias(\"COOLDOWN_BARS_target\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"alpha_design\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PARQ = OUT_DIR / \"alpha_design_v15.parquet\"\n",
    "OUT_JSON = OUT_DIR / \"alpha_design_v15.json\"\n",
    "SNAP_PARQ = SNAP_DIR / \"alpha_design_v15.parquet\"\n",
    "SNAP_JSON = SNAP_DIR / \"alpha_design_v15_snapshot.json\"\n",
    "\n",
    "design = design.sort([\"symbol\",\"fold_id\"])\n",
    "\n",
    "design.write_parquet(str(OUT_PARQ), compression=\"zstd\")\n",
    "design.write_parquet(str(SNAP_PARQ), compression=\"zstd\")\n",
    "\n",
    "snapshot = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\"alpha_report_path\": alpha_report_path},\n",
    "    \"symbols\": symbols_u,\n",
    "    \"fold_ids\": fold_ids,\n",
    "    \"params\": {\n",
    "        \"MIN_TRADES_IS\": MIN_TRADES_IS,\n",
    "        \"MIN_TRADES_OOS\": MIN_TRADES_OOS,\n",
    "        \"MIN_NET_MEAN_IS\": MIN_NET_MEAN_IS,\n",
    "        \"selection_rule\": \"Pick max(score_is) using IS-only; attach OOS as evidence only.\",\n",
    "        \"score_is\": \"sharpe_like_base * sqrt(n_trades) [implemented as pow(0.5)]\",\n",
    "        \"motor_targets\": [\"TIME_STOP_BARS_target\",\"MIN_HOLD_BARS_target\",\"ENTRY_CONFIRM_BARS_target\",\"EXIT_GATE_OFF_BARS_target\",\"COOLDOWN_BARS_target\"],\n",
    "    },\n",
    "    \"outputs\": {\"alpha_design_path\": str(OUT_PARQ), \"alpha_design_json\": str(OUT_JSON), \"snapshot_parquet\": str(SNAP_PARQ)},\n",
    "}\n",
    "SNAP_JSON.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "OUT_JSON.write_text(json.dumps(design.to_dicts(), indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"[Celda 15] Alpha design preview (top 20):\")\n",
    "print(design.select([\n",
    "    \"symbol\",\"fold_id\",\"picked_side_alpha\",\"picked_h_bars\",\n",
    "    \"is_n\",\"is_net_base_mean\",\"is_net_stress_mean\",\"is_sharpe_like_base\",\"is_score\",\n",
    "    \"oos_n\",\"oos_net_base_mean\",\"oos_net_stress_mean\",\"oos_status_evidence\",\n",
    "    \"TIME_STOP_BARS_target\",\"MIN_HOLD_BARS_target\",\"ENTRY_CONFIRM_BARS_target\",\"COOLDOWN_BARS_target\"\n",
    "]).head(20))\n",
    "\n",
    "GLOBAL_STATE[\"alpha_design\"] = {\n",
    "    \"alpha_report_path\": alpha_report_path,\n",
    "    \"alpha_design_path\": str(OUT_PARQ),\n",
    "    \"alpha_design_json\": str(OUT_JSON),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "    \"MIN_TRADES_IS\": MIN_TRADES_IS,\n",
    "    \"MIN_TRADES_OOS\": MIN_TRADES_OOS,\n",
    "    \"MIN_NET_MEAN_IS\": MIN_NET_MEAN_IS,\n",
    "}\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_PARQ} (OK) | rows={design.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_JSON} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_PARQ} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 15 v1.0.1 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02f9f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 16 v1.0 :: Execution & Risk Overlay institucional (post-engine) [hours + daily stops + profit lock + max trades/day] [WFO-safe]\n",
      "[Celda 16] policy :: ENTRY_WEEKDAYS_ONLY=True | DAILY_MAX_LOSS_BASE=0.02 | DAILY_MAX_PROFIT_BASE=0.03 | MAX_TRADES_PER_DAY=3 | HARD_DAILY_CUTOFF=True\n",
      "[Celda 16] policy :: TRADING_HOURS_UTC_BY_SYMBOL keys=[]\n",
      "[Celda 16] loaded trades rows = 524\n",
      "[Celda 16] kept trades = 524 | dropped trades = 0\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\trades_engine_v10_overlay_v16.parquet (OK) | rows=524\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\summary_engine_v10_overlay_v16.parquet (OK) | rows=16\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\qa_overlay_drop_reasons_v16.parquet (OK) | rows=0\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\overlay_policy_v16.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\trades_engine_v10_overlay_v16.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\summary_engine_v10_overlay_v16.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_overlay_drop_reasons_v16.parquet (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\overlay_engine_v16_snapshot.json (OK)\n",
      "[Celda 16] INFO: backtest_engine actualizado a OVERLAY outputs (para re-ejecutar 09â†’12 sin cambiar celdas).\n",
      ">>> Celda 16 v1.0 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 16 v1.0 â€” Execution & Risk Overlay institucional (post-engine) [hours + daily stops + profit lock + max trades/day] [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 16 v1.0 :: Execution & Risk Overlay institucional (post-engine) [hours + daily stops + profit lock + max trades/day] [WFO-safe]\")\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 16] ERROR: GLOBAL_STATE no existe o no es dict.\")\n",
    "\n",
    "for k in (\"paths\", \"backtest_engine\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 16] ERROR: falta GLOBAL_STATE['{k}'].\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "bt = GLOBAL_STATE[\"backtest_engine\"]\n",
    "\n",
    "trades_path = bt.get(\"trades_path\")\n",
    "summary_path = bt.get(\"summary_path\")\n",
    "if not trades_path or not Path(trades_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 16] ERROR: trades_path invÃ¡lido/no existe: {trades_path}\")\n",
    "if not summary_path or not Path(summary_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 16] ERROR: summary_path invÃ¡lido/no existe: {summary_path}\")\n",
    "\n",
    "# ========================= Config institucional (defaults razonables) =========================\n",
    "# NOTA: todo se puede sobreescribir desde GLOBAL_STATE[\"execution_policy\"] si ya existe.\n",
    "pol = GLOBAL_STATE.get(\"execution_policy\", {}) or {}\n",
    "\n",
    "# 1) Entradas por dÃ­a/horas (NO fuerza flatten)\n",
    "ENTRY_WEEKDAYS_ONLY = bool(pol.get(\"ENTRY_WEEKDAYS_ONLY\", True))  # si True: entradas solo Lun-Vie\n",
    "# Ventanas por sÃ­mbolo en UTC: {\"LVMH\": {\"start_hour\": 7, \"end_hour\": 16}, ...}\n",
    "# end_hour es EXCLUSIVO (start<=hour<end).\n",
    "TRADING_HOURS_UTC_BY_SYMBOL = pol.get(\"TRADING_HOURS_UTC_BY_SYMBOL\", {}) or {}\n",
    "\n",
    "# 2) Overlay diario (por sÃ­mbolo, por segmento/fold): stops sobre retornos netos (unidad notional = 1)\n",
    "DAILY_MAX_LOSS_BASE   = float(pol.get(\"DAILY_MAX_LOSS_BASE\", 0.02))   # -2% (corta nuevas entradas)\n",
    "DAILY_MAX_PROFIT_BASE = float(pol.get(\"DAILY_MAX_PROFIT_BASE\", 0.03)) # +3% (bloquea nuevas entradas)\n",
    "MAX_TRADES_PER_DAY    = int(pol.get(\"MAX_TRADES_PER_DAY\", 3))\n",
    "\n",
    "# Si True, al llegar a daily stop/profit, se bloquean entradas restantes del dÃ­a (comportamiento institucional)\n",
    "HARD_DAILY_CUTOFF = bool(pol.get(\"HARD_DAILY_CUTOFF\", True))\n",
    "\n",
    "# Si True, el overlay se vuelve la salida oficial del engine (para re-ejecutar 09â†’12 sin tocar celdas)\n",
    "USE_OVERLAY_AS_ENGINE_OUTPUT = bool(pol.get(\"USE_OVERLAY_AS_ENGINE_OUTPUT\", True))\n",
    "\n",
    "print(f\"[Celda 16] policy :: ENTRY_WEEKDAYS_ONLY={ENTRY_WEEKDAYS_ONLY} | DAILY_MAX_LOSS_BASE={DAILY_MAX_LOSS_BASE} | DAILY_MAX_PROFIT_BASE={DAILY_MAX_PROFIT_BASE} | MAX_TRADES_PER_DAY={MAX_TRADES_PER_DAY} | HARD_DAILY_CUTOFF={HARD_DAILY_CUTOFF}\")\n",
    "print(f\"[Celda 16] policy :: TRADING_HOURS_UTC_BY_SYMBOL keys={list(TRADING_HOURS_UTC_BY_SYMBOL.keys())[:10]}\")\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = Path(paths[\"artifacts\"]).resolve() / \"backtests\" / \"backtest_engine_v10\" / \"overlay_engine_v16\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SNAP_DIR = Path(paths.get(\"run_snapshots\") or (Path(paths[\"artifacts\"]).resolve() / \"snapshots\")).resolve()\n",
    "SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_TRADES = OUT_DIR / \"trades_engine_v10_overlay_v16.parquet\"\n",
    "OUT_SUM    = OUT_DIR / \"summary_engine_v10_overlay_v16.parquet\"\n",
    "OUT_QA     = OUT_DIR / \"qa_overlay_drop_reasons_v16.parquet\"\n",
    "OUT_JSON   = OUT_DIR / \"overlay_policy_v16.json\"\n",
    "\n",
    "SNAP_TRADES = SNAP_DIR / \"trades_engine_v10_overlay_v16.parquet\"\n",
    "SNAP_SUM    = SNAP_DIR / \"summary_engine_v10_overlay_v16.parquet\"\n",
    "SNAP_QA     = SNAP_DIR / \"qa_overlay_drop_reasons_v16.parquet\"\n",
    "SNAP_JSON   = SNAP_DIR / \"overlay_engine_v16_snapshot.json\"\n",
    "\n",
    "# ========================= Helpers (robustos) =========================\n",
    "def _ensure_utc(dt: datetime) -> datetime:\n",
    "    if dt.tzinfo is None:\n",
    "        return dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc)\n",
    "\n",
    "def _parse_dt_any(s: str) -> datetime:\n",
    "    # soporta iso con/sin tz; lo normaliza a UTC\n",
    "    dt = datetime.fromisoformat(s)\n",
    "    return _ensure_utc(dt)\n",
    "\n",
    "def _py_weekday_int(dt: datetime) -> int:\n",
    "    # Mon=0 ... Sun=6\n",
    "    return int(dt.weekday())\n",
    "\n",
    "def _py_hour_int(dt: datetime) -> int:\n",
    "    return int(dt.hour)\n",
    "\n",
    "# ========================= Load trades =========================\n",
    "df = pl.read_parquet(str(trades_path))\n",
    "\n",
    "REQ = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"entry_time\",\"exit_time\",\"net_ret_base\",\"net_ret_stress\"]\n",
    "miss = [c for c in REQ if c not in df.columns]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"[Celda 16] ERROR: trades parquet missing cols={miss}. cols={df.columns}\")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"fold_id\").cast(pl.Utf8),\n",
    "        pl.col(\"segment\").cast(pl.Utf8),\n",
    "        pl.col(\"side\").cast(pl.Utf8),\n",
    "        pl.col(\"entry_time\").cast(pl.Utf8),\n",
    "        pl.col(\"exit_time\").cast(pl.Utf8),\n",
    "        pl.col(\"net_ret_base\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"net_ret_stress\").cast(pl.Float64, strict=False),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"entry_dt\"),\n",
    "        pl.col(\"exit_time\").map_elements(_parse_dt_any, return_dtype=pl.Datetime(\"us\",\"UTC\")).alias(\"exit_dt\"),\n",
    "    ])\n",
    "    # weekday/hour via Python para evitar inconsistencias backend\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_dt\").map_elements(lambda x: int(_py_weekday_int(x)), return_dtype=pl.Int64).alias(\"entry_dow\"),\n",
    "        pl.col(\"exit_dt\").map_elements(lambda x: int(_py_weekday_int(x)), return_dtype=pl.Int64).alias(\"exit_dow\"),\n",
    "        pl.col(\"entry_dt\").map_elements(lambda x: int(_py_hour_int(x)), return_dtype=pl.Int64).alias(\"entry_hour\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"entry_dt\").dt.date().alias(\"entry_date\"),\n",
    "    ])\n",
    "    .sort([\"symbol\",\"fold_id\",\"segment\",\"entry_dt\"])\n",
    ")\n",
    "\n",
    "print(f\"[Celda 16] loaded trades rows = {df.height}\")\n",
    "\n",
    "# ========================= Pre-filtros (horas + weekdays) =========================\n",
    "drop_rows = []\n",
    "\n",
    "def _in_hours(sym: str, hour: int) -> bool:\n",
    "    w = TRADING_HOURS_UTC_BY_SYMBOL.get(sym)\n",
    "    if not w:\n",
    "        return True\n",
    "    sh = int(w.get(\"start_hour\", 0))\n",
    "    eh = int(w.get(\"end_hour\", 24))\n",
    "    # end_hour exclusivo\n",
    "    return (hour >= sh) and (hour < eh)\n",
    "\n",
    "def _weekday_ok(dow: int) -> bool:\n",
    "    if not ENTRY_WEEKDAYS_ONLY:\n",
    "        return True\n",
    "    return int(dow) < 5  # Mon-Fri\n",
    "\n",
    "# Convertimos por grupos (pocos trades, gating secuencial exacto)\n",
    "kept_dicts = []\n",
    "\n",
    "for (sym, fid, seg), gdf in df.group_by([\"symbol\",\"fold_id\",\"segment\"], maintain_order=True):\n",
    "    rows = gdf.to_dicts()\n",
    "\n",
    "    # Estado diario\n",
    "    cur_day = None\n",
    "    day_pnl = 0.0\n",
    "    day_trades = 0\n",
    "    day_blocked = False\n",
    "\n",
    "    for r in rows:\n",
    "        reason = None\n",
    "\n",
    "        # horarios + weekday (solo para ENTRADAS)\n",
    "        if not _weekday_ok(r[\"entry_dow\"]):\n",
    "            reason = \"DROP_ENTRY_WEEKEND\"\n",
    "        elif not _in_hours(sym, int(r[\"entry_hour\"])):\n",
    "            reason = \"DROP_OUTSIDE_HOURS\"\n",
    "\n",
    "        # Reset dÃ­a\n",
    "        d = r[\"entry_date\"]\n",
    "        if cur_day is None or d != cur_day:\n",
    "            cur_day = d\n",
    "            day_pnl = 0.0\n",
    "            day_trades = 0\n",
    "            day_blocked = False\n",
    "\n",
    "        # Overlay diario (aplica tras filtros de calendario)\n",
    "        if reason is None:\n",
    "            if HARD_DAILY_CUTOFF and day_blocked:\n",
    "                reason = \"DROP_DAILY_CUTOFF_ACTIVE\"\n",
    "            elif day_trades >= MAX_TRADES_PER_DAY:\n",
    "                reason = \"DROP_MAX_TRADES_PER_DAY\"\n",
    "\n",
    "        # Si pasa, se ejecuta y actualiza pnl del dÃ­a\n",
    "        if reason is None:\n",
    "            kept_dicts.append(r)\n",
    "            day_trades += 1\n",
    "            nr = float(r.get(\"net_ret_base\") or 0.0)\n",
    "            day_pnl += nr\n",
    "\n",
    "            # gatillos de cutoff\n",
    "            if HARD_DAILY_CUTOFF:\n",
    "                if day_pnl <= -abs(DAILY_MAX_LOSS_BASE):\n",
    "                    day_blocked = True\n",
    "                if day_pnl >= abs(DAILY_MAX_PROFIT_BASE):\n",
    "                    day_blocked = True\n",
    "        else:\n",
    "            drop_rows.append({\n",
    "                \"symbol\": sym,\n",
    "                \"fold_id\": fid,\n",
    "                \"segment\": seg,\n",
    "                \"entry_dt\": r[\"entry_dt\"],\n",
    "                \"entry_date\": r[\"entry_date\"],\n",
    "                \"side\": r[\"side\"],\n",
    "                \"net_ret_base\": float(r.get(\"net_ret_base\") or 0.0),\n",
    "                \"drop_reason\": reason,\n",
    "            })\n",
    "\n",
    "df_keep = pl.DataFrame(kept_dicts) if kept_dicts else pl.DataFrame(schema=df.schema)\n",
    "df_drop = pl.DataFrame(drop_rows) if drop_rows else pl.DataFrame(schema={\n",
    "    \"symbol\": pl.Utf8, \"fold_id\": pl.Utf8, \"segment\": pl.Utf8,\n",
    "    \"entry_dt\": pl.Datetime(\"us\",\"UTC\"), \"entry_date\": pl.Date,\n",
    "    \"side\": pl.Utf8, \"net_ret_base\": pl.Float64, \"drop_reason\": pl.Utf8\n",
    "})\n",
    "\n",
    "print(f\"[Celda 16] kept trades = {df_keep.height} | dropped trades = {df_drop.height}\")\n",
    "\n",
    "# ========================= Summary overlay =========================\n",
    "if df_keep.height == 0:\n",
    "    raise RuntimeError(\"[Celda 16] GATE FAIL: overlay eliminÃ³ todos los trades. Relaja horarios/stops/max_trades o revisa datos.\")\n",
    "\n",
    "summary = (\n",
    "    df_keep\n",
    "    .group_by([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_trades\"),\n",
    "        pl.col(\"net_ret_base\").sum().alias(\"tot_ret_base\"),\n",
    "        pl.col(\"net_ret_stress\").sum().alias(\"tot_ret_stress\"),\n",
    "        pl.col(\"net_ret_base\").mean().alias(\"mean_ret_base\"),\n",
    "        pl.col(\"net_ret_base\").std().alias(\"std_ret_base\"),\n",
    "        (pl.col(\"net_ret_base\") > 0).mean().alias(\"win_rate_base\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col(\"mean_ret_base\") / pl.col(\"std_ret_base\")).alias(\"sharpe_like_base\"),\n",
    "    ])\n",
    "    .sort([\"symbol\",\"fold_id\",\"segment\",\"side\"])\n",
    ")\n",
    "\n",
    "drop_qa = (\n",
    "    df_drop\n",
    "    .group_by([\"symbol\",\"fold_id\",\"segment\",\"drop_reason\"])\n",
    "    .agg([pl.len().alias(\"n_drop\")])\n",
    "    .sort([\"n_drop\"], descending=True)\n",
    ")\n",
    "\n",
    "# ========================= Persist =========================\n",
    "df_keep.write_parquet(str(OUT_TRADES), compression=\"zstd\")\n",
    "summary.write_parquet(str(OUT_SUM), compression=\"zstd\")\n",
    "drop_qa.write_parquet(str(OUT_QA), compression=\"zstd\")\n",
    "\n",
    "df_keep.write_parquet(str(SNAP_TRADES), compression=\"zstd\")\n",
    "summary.write_parquet(str(SNAP_SUM), compression=\"zstd\")\n",
    "drop_qa.write_parquet(str(SNAP_QA), compression=\"zstd\")\n",
    "\n",
    "policy_payload = {\n",
    "    \"ENTRY_WEEKDAYS_ONLY\": ENTRY_WEEKDAYS_ONLY,\n",
    "    \"TRADING_HOURS_UTC_BY_SYMBOL\": TRADING_HOURS_UTC_BY_SYMBOL,\n",
    "    \"DAILY_MAX_LOSS_BASE\": DAILY_MAX_LOSS_BASE,\n",
    "    \"DAILY_MAX_PROFIT_BASE\": DAILY_MAX_PROFIT_BASE,\n",
    "    \"MAX_TRADES_PER_DAY\": MAX_TRADES_PER_DAY,\n",
    "    \"HARD_DAILY_CUTOFF\": HARD_DAILY_CUTOFF,\n",
    "    \"USE_OVERLAY_AS_ENGINE_OUTPUT\": USE_OVERLAY_AS_ENGINE_OUTPUT,\n",
    "}\n",
    "OUT_JSON.write_text(json.dumps(policy_payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "snap = {\n",
    "    \"created_utc\": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "    \"inputs\": {\"trades_path_raw\": str(trades_path), \"summary_path_raw\": str(summary_path)},\n",
    "    \"policy\": policy_payload,\n",
    "    \"stats\": {\n",
    "        \"raw_trades\": int(df.height),\n",
    "        \"kept_trades\": int(df_keep.height),\n",
    "        \"dropped_trades\": int(df_drop.height),\n",
    "        \"drop_share\": float(df_drop.height / max(1, df.height)),\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"trades_overlay\": str(OUT_TRADES),\n",
    "        \"summary_overlay\": str(OUT_SUM),\n",
    "        \"drop_qa\": str(OUT_QA),\n",
    "        \"policy_json\": str(OUT_JSON),\n",
    "        \"snap_trades\": str(SNAP_TRADES),\n",
    "        \"snap_summary\": str(SNAP_SUM),\n",
    "        \"snap_drop_qa\": str(SNAP_QA),\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"Overlay aplicado sobre trades ya generados por el motor: filtra entradas por calendario/horas y corta nuevas entradas por reglas diarias.\",\n",
    "        \"Swing permitido: NO fuerza cierre intradÃ­a. Solo controla cuÃ¡ndo entrar y cuÃ¡ndo dejar de abrir mÃ¡s ese dÃ­a.\",\n",
    "    ],\n",
    "}\n",
    "SNAP_JSON.write_text(json.dumps(snap, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_TRADES} (OK) | rows={df_keep.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_SUM} (OK) | rows={summary.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_QA} (OK) | rows={drop_qa.height}\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_JSON} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_TRADES} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_SUM} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_QA} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "\n",
    "# ========================= Wire-up (para reusar Celdas 09â†’12 sin tocar cÃ³digo) =========================\n",
    "GLOBAL_STATE[\"execution_policy\"] = policy_payload\n",
    "GLOBAL_STATE[\"backtest_engine_overlay\"] = {\n",
    "    \"trades_path_raw\": str(trades_path),\n",
    "    \"summary_path_raw\": str(summary_path),\n",
    "    \"trades_path_overlay\": str(OUT_TRADES),\n",
    "    \"summary_path_overlay\": str(OUT_SUM),\n",
    "    \"drop_qa_path\": str(OUT_QA),\n",
    "    \"policy_json\": str(OUT_JSON),\n",
    "    \"snapshot_json\": str(SNAP_JSON),\n",
    "}\n",
    "\n",
    "if USE_OVERLAY_AS_ENGINE_OUTPUT:\n",
    "    # respaldar\n",
    "    bt[\"trades_path_raw\"] = str(trades_path)\n",
    "    bt[\"summary_path_raw\"] = str(summary_path)\n",
    "    # reemplazar para que Celda 09/10/11/12 lean el overlay\n",
    "    bt[\"trades_path\"] = str(OUT_TRADES)\n",
    "    bt[\"summary_path\"] = str(OUT_SUM)\n",
    "    GLOBAL_STATE[\"backtest_engine\"] = bt\n",
    "    print(\"[Celda 16] INFO: backtest_engine actualizado a OVERLAY outputs (para re-ejecutar 09â†’12 sin cambiar celdas).\")\n",
    "\n",
    "print(\">>> Celda 16 v1.0 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3fd9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 17 v1.0.2 :: QA AlineaciÃ³n Alphaâ†”Motor (OOS-first + mismatch report) [WFO-safe]\n",
      "[Celda 17] trades rows=524 | alpha rows=128\n",
      "[Celda 17] trades_path=C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\overlay_engine_v16\\trades_engine_v10_overlay_v16.parquet\n",
      "[Celda 17] alpha_path =C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\alpha_reports\\alpha_multi_horizon_report.parquet\n",
      "[Celda 17] Alignment table:\n",
      "shape: (4, 19)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† alpha_best â”† alpha_best â”† â€¦ â”† hold_p90_o â”† alpha_edge â”† mismatch_ â”† mismatch_ â”‚\n",
      "â”‚ ---    â”† ---     â”† _side_oos  â”† _horizon_b â”†   â”† ver_alphaH â”† _nonpos_oo â”† hold_lt_2 â”† trail_dom â”‚\n",
      "â”‚ str    â”† str     â”† ---        â”† ars_oos    â”†   â”† ---        â”† s          â”† 5pctH     â”† inates_sh â”‚\n",
      "â”‚        â”†         â”† str        â”† ---        â”†   â”† f64        â”† ---        â”† ---       â”† ortâ€¦      â”‚\n",
      "â”‚        â”†         â”†            â”† i64        â”†   â”†            â”† bool       â”† bool      â”† ---       â”‚\n",
      "â”‚        â”†         â”†            â”†            â”†   â”†            â”†            â”†           â”† bool      â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BTCUSD â”† F1      â”† SHORT      â”† 288        â”† â€¦ â”† 0.052083   â”† false      â”† true      â”† true      â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† SHORT      â”† 288        â”† â€¦ â”† 0.048611   â”† false      â”† true      â”† true      â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† LONG       â”† 288        â”† â€¦ â”† 0.052083   â”† false      â”† true      â”† true      â”‚\n",
      "â”‚ LVMH   â”† F1      â”† SHORT      â”† 1          â”† â€¦ â”† 12.0       â”† true       â”† false     â”† false     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "[Celda 17] Engine OOS by side (with alpha-best marker):\n",
      "shape: (8, 13)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ symbol â”† fold_id â”† side  â”† engine_n_tr â”† â€¦ â”† engine_time â”† alpha_best_ â”† alpha_best â”† is_alpha_b â”‚\n",
      "â”‚ ---    â”† ---     â”† ---   â”† ades_oos    â”†   â”† _share_oos  â”† side_oos    â”† _horizon_b â”† est_side   â”‚\n",
      "â”‚ str    â”† str     â”† str   â”† ---         â”†   â”† ---         â”† ---         â”† ars_oos    â”† ---        â”‚\n",
      "â”‚        â”†         â”†       â”† u32         â”†   â”† f64         â”† str         â”† ---        â”† bool       â”‚\n",
      "â”‚        â”†         â”†       â”†             â”†   â”†             â”†             â”† i64        â”†            â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ BNBUSD â”† F1      â”† SHORT â”† 24          â”† â€¦ â”† 0.0         â”† SHORT       â”† 288        â”† true       â”‚\n",
      "â”‚ BNBUSD â”† F1      â”† LONG  â”† 15          â”† â€¦ â”† 0.0         â”† SHORT       â”† 288        â”† false      â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† SHORT â”† 10          â”† â€¦ â”† 0.0         â”† SHORT       â”† 288        â”† true       â”‚\n",
      "â”‚ BTCUSD â”† F1      â”† LONG  â”† 8           â”† â€¦ â”† 0.0         â”† SHORT       â”† 288        â”† false      â”‚\n",
      "â”‚ LVMH   â”† F1      â”† SHORT â”† 7           â”† â€¦ â”† 0.0         â”† SHORT       â”† 1          â”† true       â”‚\n",
      "â”‚ LVMH   â”† F1      â”† LONG  â”† 9           â”† â€¦ â”† 0.0         â”† SHORT       â”† 1          â”† false      â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† LONG  â”† 9           â”† â€¦ â”† 0.0         â”† LONG        â”† 288        â”† true       â”‚\n",
      "â”‚ XAUAUD â”† F1      â”† SHORT â”† 2           â”† â€¦ â”† 0.0         â”† LONG        â”† 288        â”† false      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\qa_alpha_engine_alignment_v17.parquet (OK)\n",
      "ðŸ’¾ OUTPUT   â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\\backtests\\backtest_engine_v10\\diagnostics_engine_v10\\qa_alpha_engine_alignment_v17.json (OK)\n",
      "ðŸ’¾ SNAPSHOT â†’ C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\research_logs\\runs\\20251222_103043\\snapshots\\qa_alpha_engine_alignment_v17_snapshot.json (OK)\n",
      ">>> Celda 17 v1.0.2 :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 17 v1.0.2 â€” QA AlineaciÃ³n Alphaâ†”Motor (OOS-first + mismatch report) [WFO-safe] =====================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 17 v1.0.2 :: QA AlineaciÃ³n Alphaâ†”Motor (OOS-first + mismatch report) [WFO-safe]\")\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _utc_now_iso() -> str:\n",
    "    return datetime.utcnow().replace(tzinfo=timezone.utc).isoformat()\n",
    "\n",
    "def _safe_exists(p: str | Path | None) -> bool:\n",
    "    try:\n",
    "        return p is not None and Path(p).exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _first_existing(paths: list[str | Path | None]) -> str | None:\n",
    "    for p in paths:\n",
    "        if _safe_exists(p):\n",
    "            return str(Path(p))\n",
    "    return None\n",
    "\n",
    "def _canonicalize_alpha(ar: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza schemas tÃ­picos de alpha_multi_horizon_report.\n",
    "    Canonical:\n",
    "      - horizon_bars (int)\n",
    "      - mean_net_base (float)\n",
    "      - mean_net_stress (float)\n",
    "      - n_trades (int)\n",
    "      - symbol, fold_id, segment, side\n",
    "    \"\"\"\n",
    "    cols = set(ar.columns)\n",
    "\n",
    "    # horizon\n",
    "    if \"horizon_bars\" not in cols:\n",
    "        if \"h_bars_after_entry\" in cols:\n",
    "            ar = ar.with_columns(pl.col(\"h_bars_after_entry\").cast(pl.Int64).alias(\"horizon_bars\"))\n",
    "        elif \"horizon\" in cols:\n",
    "            ar = ar.with_columns(pl.col(\"horizon\").cast(pl.Int64).alias(\"horizon_bars\"))\n",
    "\n",
    "    # means (map desde nombres reales del 07C tÃ­pico)\n",
    "    if \"mean_net_base\" not in cols and \"net_base_mean\" in cols:\n",
    "        ar = ar.with_columns(pl.col(\"net_base_mean\").cast(pl.Float64).alias(\"mean_net_base\"))\n",
    "    if \"mean_net_stress\" not in cols and \"net_stress_mean\" in cols:\n",
    "        ar = ar.with_columns(pl.col(\"net_stress_mean\").cast(pl.Float64).alias(\"mean_net_stress\"))\n",
    "\n",
    "    # n_trades\n",
    "    if \"n_trades\" in ar.columns:\n",
    "        ar = ar.with_columns(pl.col(\"n_trades\").cast(pl.Int64))\n",
    "    elif \"n\" in ar.columns:\n",
    "        ar = ar.with_columns(pl.col(\"n\").cast(pl.Int64).alias(\"n_trades\"))\n",
    "\n",
    "    # required minimal\n",
    "    req = [\"symbol\", \"fold_id\", \"segment\", \"side\", \"horizon_bars\"]\n",
    "    miss = [c for c in req if c not in ar.columns]\n",
    "    if miss:\n",
    "        raise RuntimeError(f\"[Celda 17] ERROR: alpha report no tiene cols requeridas (post-canonical): {miss}. cols={ar.columns}\")\n",
    "\n",
    "    # normalizar tipos y side uppercase\n",
    "    ar = ar.with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8),\n",
    "        pl.col(\"fold_id\").cast(pl.Utf8),\n",
    "        pl.col(\"segment\").cast(pl.Utf8),\n",
    "        pl.col(\"side\").cast(pl.Utf8).str.to_uppercase(),\n",
    "        pl.col(\"horizon_bars\").cast(pl.Int64),\n",
    "    ])\n",
    "\n",
    "    for c in [\"mean_net_base\", \"mean_net_stress\"]:\n",
    "        if c in ar.columns:\n",
    "            ar = ar.with_columns(pl.col(c).cast(pl.Float64))\n",
    "\n",
    "    return ar\n",
    "\n",
    "# ========================= Validaciones GLOBAL_STATE (con fallback) =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    GLOBAL_STATE = {}\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "art_root = Path(paths.get(\"artifacts\", r\"C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\artifacts\")).resolve()\n",
    "\n",
    "bt = GLOBAL_STATE.get(\"backtest_engine\", {}) or {}\n",
    "trades_path = bt.get(\"trades_path\")\n",
    "summary_path = bt.get(\"summary_path\")\n",
    "\n",
    "trades_path = _first_existing([\n",
    "    trades_path,\n",
    "    art_root / \"backtests\" / \"backtest_engine_v10\" / \"overlay_engine_v16\" / \"trades_engine_v10_overlay_v16.parquet\",\n",
    "    art_root / \"backtests\" / \"backtest_engine_v10\" / \"trades_engine_v10.parquet\",\n",
    "])\n",
    "summary_path = _first_existing([\n",
    "    summary_path,\n",
    "    art_root / \"backtests\" / \"backtest_engine_v10\" / \"overlay_engine_v16\" / \"summary_engine_v10_overlay_v16.parquet\",\n",
    "    art_root / \"backtests\" / \"backtest_engine_v10\" / \"summary_engine_v10.parquet\",\n",
    "])\n",
    "\n",
    "if not trades_path or not Path(trades_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: no encuentro trades parquet. trades_path={trades_path}\")\n",
    "if not summary_path or not Path(summary_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: no encuentro summary parquet. summary_path={summary_path}\")\n",
    "\n",
    "alpha_path = (\n",
    "    (GLOBAL_STATE.get(\"alpha_reports\", {}) or {}).get(\"alpha_multi_horizon_report_path\")\n",
    "    or (GLOBAL_STATE.get(\"alpha_report\", {}) or {}).get(\"report_path\")\n",
    ")\n",
    "alpha_path = _first_existing([\n",
    "    alpha_path,\n",
    "    art_root / \"alpha_reports\" / \"alpha_multi_horizon_report.parquet\",\n",
    "])\n",
    "if not alpha_path or not Path(alpha_path).exists():\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: no encuentro alpha_multi_horizon_report.parquet. alpha_path={alpha_path}\")\n",
    "\n",
    "# ========================= Outputs =========================\n",
    "OUT_DIR = art_root / \"backtests\" / \"backtest_engine_v10\" / \"diagnostics_engine_v10\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_snaps = Path(paths.get(\"run_snapshots\", art_root / \"snapshots\")).resolve()\n",
    "run_snaps.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_ALIGN = OUT_DIR / \"qa_alpha_engine_alignment_v17.parquet\"\n",
    "OUT_JSON  = OUT_DIR / \"qa_alpha_engine_alignment_v17.json\"\n",
    "SNAP_JSON = run_snaps / \"qa_alpha_engine_alignment_v17_snapshot.json\"\n",
    "\n",
    "# ========================= Load data =========================\n",
    "df = pl.read_parquet(trades_path)\n",
    "ar = pl.read_parquet(alpha_path)\n",
    "ar = _canonicalize_alpha(ar)\n",
    "\n",
    "# normalizar side en trades tambiÃ©n\n",
    "if \"side\" not in df.columns:\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: trades no tiene columna 'side'. cols={df.columns}\")\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"symbol\").cast(pl.Utf8),\n",
    "    pl.col(\"fold_id\").cast(pl.Utf8),\n",
    "    pl.col(\"segment\").cast(pl.Utf8),\n",
    "    pl.col(\"side\").cast(pl.Utf8).str.to_uppercase(),\n",
    "])\n",
    "\n",
    "print(f\"[Celda 17] trades rows={df.height} | alpha rows={ar.height}\")\n",
    "print(f\"[Celda 17] trades_path={trades_path}\")\n",
    "print(f\"[Celda 17] alpha_path ={alpha_path}\")\n",
    "\n",
    "# ========================= Alpha best OOS (stress-first si existe) =========================\n",
    "metric = \"mean_net_stress\" if \"mean_net_stress\" in ar.columns else \"mean_net_base\"\n",
    "if metric not in ar.columns:\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: alpha report no tiene mean_net_stress ni mean_net_base post-canonical. cols={ar.columns}\")\n",
    "\n",
    "alpha_best_oos = (\n",
    "    ar.filter(pl.col(\"segment\") == \"OOS\")\n",
    "      .sort([metric], descending=True)\n",
    "      .group_by([\"symbol\",\"fold_id\"])\n",
    "      .agg([\n",
    "          pl.first(\"side\").alias(\"alpha_best_side_oos\"),\n",
    "          pl.first(\"horizon_bars\").alias(\"alpha_best_horizon_bars_oos\"),\n",
    "          pl.first(metric).alias(f\"alpha_best_{metric}_oos\"),\n",
    "          pl.first(\"n_trades\").alias(\"alpha_best_n_trades_oos\") if \"n_trades\" in ar.columns else pl.lit(None).alias(\"alpha_best_n_trades_oos\"),\n",
    "      ])\n",
    "      # llave canÃ³nica para join con engine\n",
    "      .with_columns(pl.col(\"alpha_best_side_oos\").cast(pl.Utf8).str.to_uppercase().alias(\"side\"))\n",
    ")\n",
    "\n",
    "# ========================= Engine behavior OOS (holds + trailing share) =========================\n",
    "req_tr = [\"symbol\",\"fold_id\",\"segment\",\"side\",\"bars_held\",\"exit_reason\"]\n",
    "miss_tr = [c for c in req_tr if c not in df.columns]\n",
    "if miss_tr:\n",
    "    raise RuntimeError(f\"[Celda 17] ERROR: trades no tiene columnas requeridas: {miss_tr}\")\n",
    "\n",
    "engine_oos = (\n",
    "    df.filter(pl.col(\"segment\") == \"OOS\")\n",
    "      .group_by([\"symbol\",\"fold_id\",\"side\"])\n",
    "      .agg([\n",
    "          pl.len().alias(\"engine_n_trades_oos\"),\n",
    "          pl.col(\"bars_held\").quantile(0.90, interpolation=\"nearest\").alias(\"engine_hold_p90_bars_oos\"),\n",
    "          pl.col(\"bars_held\").median().alias(\"engine_hold_med_bars_oos\"),\n",
    "          (pl.col(\"exit_reason\") == \"TRAIL\").mean().alias(\"engine_trail_share_oos\"),\n",
    "          (pl.col(\"exit_reason\") == \"TP\").mean().alias(\"engine_tp_share_oos\"),\n",
    "          (pl.col(\"exit_reason\") == \"SL\").mean().alias(\"engine_sl_share_oos\"),\n",
    "          (pl.col(\"exit_reason\") == \"TIME_STOP\").mean().alias(\"engine_time_share_oos\"),\n",
    "      ])\n",
    ")\n",
    "\n",
    "# ========================= Join + mismatch metrics (2-step with_columns; FIX polars) =========================\n",
    "align0 = alpha_best_oos.join(engine_oos, on=[\"symbol\",\"fold_id\",\"side\"], how=\"left\")\n",
    "\n",
    "align1 = (\n",
    "    align0\n",
    "    .with_columns([\n",
    "        pl.col(\"engine_n_trades_oos\").is_null().alias(\"engine_missing_for_best_side\"),\n",
    "        pl.when(pl.col(\"engine_hold_p90_bars_oos\").is_not_null() & (pl.col(\"alpha_best_horizon_bars_oos\") > 0))\n",
    "          .then(pl.col(\"engine_hold_p90_bars_oos\") / pl.col(\"alpha_best_horizon_bars_oos\"))\n",
    "          .otherwise(None)\n",
    "          .alias(\"hold_p90_over_alphaH\"),\n",
    "        (pl.col(f\"alpha_best_{metric}_oos\") <= 0).alias(\"alpha_edge_nonpos_oos\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "align = (\n",
    "    align1\n",
    "    .with_columns([\n",
    "        # ahora sÃ­ podemos referenciar hold_p90_over_alphaH\n",
    "        pl.when(pl.col(\"hold_p90_over_alphaH\").is_not_null())\n",
    "          .then(pl.col(\"hold_p90_over_alphaH\") < 0.25)\n",
    "          .otherwise(None)\n",
    "          .alias(\"mismatch_hold_lt_25pctH\"),\n",
    "        pl.when(pl.col(\"hold_p90_over_alphaH\").is_not_null() & pl.col(\"engine_trail_share_oos\").is_not_null())\n",
    "          .then((pl.col(\"engine_trail_share_oos\") > 0.60) & (pl.col(\"hold_p90_over_alphaH\") < 0.25))\n",
    "          .otherwise(None)\n",
    "          .alias(\"mismatch_trail_dominates_short_hold\"),\n",
    "    ])\n",
    "    .sort([f\"alpha_best_{metric}_oos\"], descending=True)\n",
    ")\n",
    "\n",
    "print(\"[Celda 17] Alignment table:\")\n",
    "print(align)\n",
    "\n",
    "# Vista auxiliar: engine OOS por ambos lados (para ver si el motor estÃ¡ â€œvivoâ€)\n",
    "engine_side_view = (\n",
    "    engine_oos\n",
    "    .join(alpha_best_oos.select([\"symbol\",\"fold_id\",\"alpha_best_side_oos\",\"alpha_best_horizon_bars_oos\"]), on=[\"symbol\",\"fold_id\"], how=\"left\")\n",
    "    .with_columns([\n",
    "        (pl.col(\"side\") == pl.col(\"alpha_best_side_oos\")).alias(\"is_alpha_best_side\"),\n",
    "    ])\n",
    "    .sort([\"symbol\",\"fold_id\",\"is_alpha_best_side\"], descending=[False,False,True])\n",
    ")\n",
    "\n",
    "print(\"[Celda 17] Engine OOS by side (with alpha-best marker):\")\n",
    "print(engine_side_view)\n",
    "\n",
    "# ========================= Persist =========================\n",
    "align.write_parquet(str(OUT_ALIGN), compression=\"zstd\")\n",
    "\n",
    "payload = {\n",
    "    \"created_utc\": _utc_now_iso(),\n",
    "    \"inputs\": {\"trades_path\": str(trades_path), \"summary_path\": str(summary_path), \"alpha_path\": str(alpha_path)},\n",
    "    \"metric_used\": metric,\n",
    "    \"rules\": {\n",
    "        \"hold_mismatch_flag\": \"hold_p90_over_alphaH < 0.25\",\n",
    "        \"trail_short_hold_flag\": \"engine_trail_share_oos > 0.60 AND hold_p90_over_alphaH < 0.25\",\n",
    "        \"alpha_nonpos_flag\": f\"alpha_best_{metric}_oos <= 0\",\n",
    "        \"engine_missing_for_best_side\": \"No hay trades OOS del motor en el lado que alpha marca como best\",\n",
    "    },\n",
    "    \"output\": {\"align_parquet\": str(OUT_ALIGN), \"align_json\": str(OUT_JSON)},\n",
    "}\n",
    "\n",
    "OUT_JSON.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "SNAP_JSON.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_ALIGN} (OK)\")\n",
    "print(f\"ðŸ’¾ OUTPUT   â†’ {OUT_JSON} (OK)\")\n",
    "print(f\"ðŸ’¾ SNAPSHOT â†’ {SNAP_JSON} (OK)\")\n",
    "print(\">>> Celda 17 v1.0.2 :: OK\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
