{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0cb19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Celda 00 v2.0.3] Manifest CREADO (nuevo run): C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\run_manifest_v2.json\n",
      "\n",
      "--- Celda 00 v2.0.3 | Estado final ---\n",
      "RUN_MODE             : NEW_RUN_DEFAULT\n",
      "PROJECT_ROOT         : C:\\Quant\\MT5_Data_Extraction\n",
      "WORKDIR              : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\n",
      "OUTPUTS_ROOT         : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\n",
      "RUN_ID               : 20251223_164647_172c5b29\n",
      "RUN_DIR              : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\n",
      "RUN_MANIFEST_PATH    : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\run_manifest_v2.json\n",
      "RUN_MANIFEST_LATEST  : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_manifest_v2_latest.json\n",
      "LATEST_RUN_MARKER    : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\_latest_run.txt\n",
      "SCHEMA_VERSION       : v2.0.3\n",
      "ENGINE_VERSION       : v2.0.3\n",
      "\n",
      "--- ARTIFACTS keys ---\n",
      "N_KEYS: 19\n",
      "['alpha_multi_horizon_report', 'alpha_multi_horizon_snapshot', 'cost_model_snapshot', 'data_qa_report', 'engine_qa_report', 'engine_report_snapshot', 'equity_engine', 'features_m5', 'instrument_specs', 'instrument_specs_snapshot', 'ohlcv_clean', 'regime_params_by_fold', 'regime_params_snapshot', 'summary_baseline', 'summary_engine', 'trades_baseline', 'trades_engine', 'wfo_folds', 'wfo_folds_snapshot']\n",
      "\n",
      "--- Dependencias ---\n",
      "polars: 1.35.1\n",
      "pandas: 2.3.3\n",
      "\n",
      "[Celda 00 v2.0.3] OK — NEW_RUN por defecto + RUN listo.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 00 v2.0.3 — Run Manifest + Paths + Canonical Schema\n",
    "# Política institucional (FINAL):\n",
    "#   - Por defecto SIEMPRE crea un run nuevo (NEW_RUN_DEFAULT).\n",
    "#   - Solo reutiliza run si:\n",
    "#       A) TREND_M5_RUN_ID está seteado (FORCED_RUN_ID)\n",
    "#       B) TREND_M5_RESUME_LATEST=1 y existe _latest_run.txt (RESUME_LATEST)\n",
    "#\n",
    "# Nota:\n",
    "#   - \"outputs\" NO es \"cargar corridas anteriores\": es el directorio de salida del run.\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import platform\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _safe_mkdir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _write_json(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    _safe_mkdir(path.parent)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "def _read_json(path: Path) -> Dict[str, Any]:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def _write_text(path: Path, text: str) -> None:\n",
    "    _safe_mkdir(path.parent)\n",
    "    path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def _read_text(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "def _sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _env(name: str, default: Optional[str] = None) -> Optional[str]:\n",
    "    v = os.getenv(name)\n",
    "    return v if v not in (None, \"\") else default\n",
    "\n",
    "def _env_bool(name: str, default: bool = False) -> bool:\n",
    "    v = os.getenv(name, \"\")\n",
    "    if v is None or v.strip() == \"\":\n",
    "        return default\n",
    "    return v.strip().lower() in (\"1\", \"true\", \"yes\", \"y\")\n",
    "\n",
    "# ---------------------------\n",
    "# Detectar PROJECT_ROOT (repo raíz) — determinístico\n",
    "# ---------------------------\n",
    "def _detect_project_root() -> Path:\n",
    "    forced = _env(\"TREND_M5_ROOT\") or _env(\"MT5_PROJECT_ROOT\") or _env(\"MT5_DE_PROJECT_ROOT\")\n",
    "    if forced:\n",
    "        return Path(forced).resolve()\n",
    "\n",
    "    start = Path.cwd().resolve()\n",
    "    target = \"mt5_data_extraction\"\n",
    "    for p in [start] + list(start.parents):\n",
    "        if p.name.lower() == target:\n",
    "            return p\n",
    "    # fallback: cwd (controlado)\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = _detect_project_root()\n",
    "\n",
    "# ---------------------------\n",
    "# OUTPUTS_ROOT (salida del strategy notebook)\n",
    "# ---------------------------\n",
    "WORKDIR = Path.cwd().resolve()  # normalmente ...\\ER_STRATEGY_LAB\\notebooks\n",
    "OUTPUTS_ROOT = Path(_env(\"TREND_M5_OUTPUTS_ROOT\", str(WORKDIR / \"outputs\" / \"trend_m5_strategy\" / \"v2\"))).resolve()\n",
    "LATEST_RUN_MARKER = OUTPUTS_ROOT / \"_latest_run.txt\"\n",
    "\n",
    "# ---------------------------\n",
    "# RUN_ID policy (FINAL)\n",
    "# ---------------------------\n",
    "FORCED_RUN_ID = (_env(\"TREND_M5_RUN_ID\") or \"\").strip() or None\n",
    "RESUME_LATEST = _env_bool(\"TREND_M5_RESUME_LATEST\", default=False)\n",
    "\n",
    "def _new_run_id() -> str:\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    salt = _sha1(f\"{ts}|{platform.node()}|{os.getpid()}\")[:8]\n",
    "    return f\"{ts}_{salt}\"\n",
    "\n",
    "if FORCED_RUN_ID:\n",
    "    RUN_MODE = \"FORCED_RUN_ID\"\n",
    "    RUN_ID = FORCED_RUN_ID\n",
    "elif RESUME_LATEST and LATEST_RUN_MARKER.exists():\n",
    "    RUN_MODE = \"RESUME_LATEST\"\n",
    "    RUN_ID = _read_text(LATEST_RUN_MARKER) or _new_run_id()\n",
    "else:\n",
    "    RUN_MODE = \"NEW_RUN_DEFAULT\"\n",
    "    RUN_ID = _new_run_id()\n",
    "\n",
    "RUN_DIR = OUTPUTS_ROOT / f\"run_{RUN_ID}\"\n",
    "RUN_MANIFEST_PATH = RUN_DIR / \"run_manifest_v2.json\"\n",
    "RUN_MANIFEST_LATEST_PATH = OUTPUTS_ROOT / \"run_manifest_v2_latest.json\"\n",
    "\n",
    "# ---------------------------\n",
    "# Versionado\n",
    "# ---------------------------\n",
    "SCHEMA_VERSION = \"v2.0.3\"\n",
    "ENGINE_VERSION = \"v2.0.3\"\n",
    "COST_MODEL_VERSION = \"v2.0.3\"\n",
    "WFO_VERSION = \"v2.0.3\"\n",
    "\n",
    "# ---------------------------\n",
    "# Canonical Schema (contrato)\n",
    "# ---------------------------\n",
    "CANONICAL_SCHEMA = {\n",
    "    \"ohlcv_m5\": {\n",
    "        \"required_columns\": [\"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"spread\", \"symbol\"],\n",
    "        \"notes\": \"UTC. time_utc monotónico por símbolo. M5 = 300s.\"\n",
    "    },\n",
    "    \"engine_trades\": {\n",
    "        \"required_columns\": [\n",
    "            \"symbol\",\"fold_id\",\"segment\",\"side\",\n",
    "            \"signal_time_utc\",\"entry_time_utc\",\"exit_time_utc\",\n",
    "            \"entry_price\",\"exit_price\",\n",
    "            \"gross_pnl\",\"net_pnl_base\",\"net_pnl_stress\",\n",
    "            \"hold_bars\",\"exit_reason\"\n",
    "        ],\n",
    "        \"notes\": \"Mon–Fri se aplica sobre entry_time_utc (t+1).\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Artifacts (salidas del run)\n",
    "# ---------------------------\n",
    "def _build_artifacts(run_dir: Path) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"instrument_specs\": str(run_dir / \"instrument_specs_v2.parquet\"),\n",
    "        \"instrument_specs_snapshot\": str(run_dir / \"instrument_specs_snapshot_v2.json\"),\n",
    "\n",
    "        \"ohlcv_clean\": str(run_dir / \"ohlcv_clean_m5.parquet\"),\n",
    "        \"data_qa_report\": str(run_dir / \"data_qa_report_v2.json\"),\n",
    "\n",
    "        \"cost_model_snapshot\": str(run_dir / \"cost_model_snapshot_v2.json\"),\n",
    "        \"wfo_folds\": str(run_dir / \"wfo_folds_v2.parquet\"),\n",
    "        \"wfo_folds_snapshot\": str(run_dir / \"wfo_folds_snapshot_v2.json\"),\n",
    "\n",
    "        \"features_m5\": str(run_dir / \"features_m5_v2.parquet\"),\n",
    "        \"regime_params_by_fold\": str(run_dir / \"regime_params_by_fold_v2.parquet\"),\n",
    "        \"regime_params_snapshot\": str(run_dir / \"regime_params_snapshot_v2.json\"),\n",
    "\n",
    "        \"trades_baseline\": str(run_dir / \"trades_baseline_v2.parquet\"),\n",
    "        \"summary_baseline\": str(run_dir / \"summary_baseline_v2.parquet\"),\n",
    "\n",
    "        \"alpha_multi_horizon_report\": str(run_dir / \"alpha_multi_horizon_report_v2.parquet\"),\n",
    "        \"alpha_multi_horizon_snapshot\": str(run_dir / \"alpha_multi_horizon_snapshot_v2.json\"),\n",
    "\n",
    "        \"trades_engine\": str(run_dir / \"trades_engine_v2.parquet\"),\n",
    "        \"summary_engine\": str(run_dir / \"summary_engine_v2.parquet\"),\n",
    "        \"equity_engine\": str(run_dir / \"equity_curve_engine_v2.parquet\"),\n",
    "        \"engine_qa_report\": str(run_dir / \"engine_qa_report_v2.json\"),\n",
    "        \"engine_report_snapshot\": str(run_dir / \"engine_report_snapshot_v2.json\"),\n",
    "    }\n",
    "\n",
    "def _build_manifest() -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"schema_version\": SCHEMA_VERSION,\n",
    "        \"engine_version\": ENGINE_VERSION,\n",
    "        \"cost_model_version\": COST_MODEL_VERSION,\n",
    "        \"wfo_version\": WFO_VERSION,\n",
    "        \"run_mode\": RUN_MODE,\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"created_utc\": _now_utc_iso(),\n",
    "        \"project_root\": str(PROJECT_ROOT),\n",
    "        \"workdir\": str(WORKDIR),\n",
    "        \"outputs_root\": str(OUTPUTS_ROOT),\n",
    "        \"run_dir\": str(RUN_DIR),\n",
    "        \"artifacts\": _build_artifacts(RUN_DIR),\n",
    "        \"canonical_schema\": CANONICAL_SCHEMA,\n",
    "        \"runtime\": {\n",
    "            \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"node\": platform.node(),\n",
    "            \"pid\": os.getpid(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Guardado (solo carga manifest si el modo es RESUME/forced y existe)\n",
    "# ---------------------------\n",
    "_safe_mkdir(RUN_DIR)\n",
    "_safe_mkdir(OUTPUTS_ROOT)\n",
    "\n",
    "manifest: Dict[str, Any]\n",
    "if RUN_MANIFEST_PATH.exists() and RUN_MODE in (\"RESUME_LATEST\", \"FORCED_RUN_ID\"):\n",
    "    manifest = _read_json(RUN_MANIFEST_PATH)\n",
    "    # normaliza/bump versiones\n",
    "    manifest[\"schema_version\"] = SCHEMA_VERSION\n",
    "    manifest[\"engine_version\"] = ENGINE_VERSION\n",
    "    manifest[\"cost_model_version\"] = COST_MODEL_VERSION\n",
    "    manifest[\"wfo_version\"] = WFO_VERSION\n",
    "    manifest[\"run_mode\"] = RUN_MODE\n",
    "    manifest[\"project_root\"] = str(PROJECT_ROOT)\n",
    "    manifest[\"workdir\"] = str(WORKDIR)\n",
    "    manifest[\"outputs_root\"] = str(OUTPUTS_ROOT)\n",
    "    manifest[\"canonical_schema\"] = CANONICAL_SCHEMA\n",
    "    manifest[\"artifacts\"] = _build_artifacts(Path(manifest.get(\"run_dir\", str(RUN_DIR))))\n",
    "    _write_json(RUN_MANIFEST_PATH, manifest)\n",
    "    print(f\"[Celda 00 v2.0.3] Manifest CARGADO (resume/forced) y normalizado: {RUN_MANIFEST_PATH}\")\n",
    "else:\n",
    "    manifest = _build_manifest()\n",
    "    _write_json(RUN_MANIFEST_PATH, manifest)\n",
    "    print(f\"[Celda 00 v2.0.3] Manifest CREADO (nuevo run): {RUN_MANIFEST_PATH}\")\n",
    "\n",
    "# latest pointers\n",
    "_write_text(LATEST_RUN_MARKER, RUN_ID)\n",
    "_write_json(RUN_MANIFEST_LATEST_PATH, manifest)\n",
    "\n",
    "# RUN object (downstream)\n",
    "RUN: Dict[str, Any] = {\n",
    "    \"RUN_ID\": manifest[\"run_id\"],\n",
    "    \"RUN_MODE\": manifest[\"run_mode\"],\n",
    "    \"RUN_DIR\": Path(manifest[\"run_dir\"]),\n",
    "    \"PROJECT_ROOT\": Path(manifest[\"project_root\"]),\n",
    "    \"WORKDIR\": Path(manifest[\"workdir\"]),\n",
    "    \"OUTPUTS_ROOT\": Path(manifest[\"outputs_root\"]),\n",
    "    \"ARTIFACTS\": {k: Path(v) for k, v in manifest[\"artifacts\"].items()},\n",
    "    \"SCHEMA_VERSION\": manifest[\"schema_version\"],\n",
    "    \"ENGINE_VERSION\": manifest[\"engine_version\"],\n",
    "    \"CANONICAL_SCHEMA\": manifest[\"canonical_schema\"],\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# PRINTS exhaustivos\n",
    "# ---------------------------\n",
    "print(\"\\n--- Celda 00 v2.0.3 | Estado final ---\")\n",
    "print(\"RUN_MODE             :\", RUN[\"RUN_MODE\"])\n",
    "print(\"PROJECT_ROOT         :\", RUN[\"PROJECT_ROOT\"])\n",
    "print(\"WORKDIR              :\", RUN[\"WORKDIR\"])\n",
    "print(\"OUTPUTS_ROOT         :\", RUN[\"OUTPUTS_ROOT\"])\n",
    "print(\"RUN_ID               :\", RUN[\"RUN_ID\"])\n",
    "print(\"RUN_DIR              :\", RUN[\"RUN_DIR\"])\n",
    "print(\"RUN_MANIFEST_PATH    :\", RUN_MANIFEST_PATH)\n",
    "print(\"RUN_MANIFEST_LATEST  :\", RUN_MANIFEST_LATEST_PATH)\n",
    "print(\"LATEST_RUN_MARKER    :\", LATEST_RUN_MARKER)\n",
    "print(\"SCHEMA_VERSION       :\", RUN[\"SCHEMA_VERSION\"])\n",
    "print(\"ENGINE_VERSION       :\", RUN[\"ENGINE_VERSION\"])\n",
    "\n",
    "keys = sorted(RUN[\"ARTIFACTS\"].keys())\n",
    "print(\"\\n--- ARTIFACTS keys ---\")\n",
    "print(\"N_KEYS:\", len(keys))\n",
    "print(keys)\n",
    "\n",
    "critical = [\"instrument_specs\",\"instrument_specs_snapshot\",\"ohlcv_clean\",\"data_qa_report\"]\n",
    "missing_critical = [k for k in critical if k not in RUN[\"ARTIFACTS\"]]\n",
    "if missing_critical:\n",
    "    raise RuntimeError(f\"[Celda 00 v2.0.3] ERROR: faltan artifacts críticos: {missing_critical}\")\n",
    "\n",
    "print(\"\\n--- Dependencias ---\")\n",
    "import polars as pl\n",
    "print(\"polars:\", pl.__version__)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"pandas:\", pd.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pandas: no disponible:\", e)\n",
    "\n",
    "print(\"\\n[Celda 00 v2.0.3] OK — NEW_RUN por defecto + RUN listo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c4695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 01 v2.0.3 | Preflight ---\n",
      "RUN_ID   : 20251223_164647_172c5b29\n",
      "RUN_MODE : NEW_RUN_DEFAULT\n",
      "RUN_DIR  : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\n",
      "ARTIFACTS keys: ['alpha_multi_horizon_report', 'alpha_multi_horizon_snapshot', 'cost_model_snapshot', 'data_qa_report', 'engine_qa_report', 'engine_report_snapshot', 'equity_engine', 'features_m5', 'instrument_specs', 'instrument_specs_snapshot', 'ohlcv_clean', 'regime_params_by_fold', 'regime_params_snapshot', 'summary_baseline', 'summary_engine', 'trades_baseline', 'trades_engine', 'wfo_folds', 'wfo_folds_snapshot']\n",
      "OUT_SPECS_PARQUET : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\instrument_specs_v2.parquet\n",
      "OUT_SPECS_SNAPSHOT: C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\instrument_specs_snapshot_v2.json\n",
      "[Celda 01 v2.0.3] building row for BNBUSD ...\n",
      "[Celda 01 v2.0.3] building row for BTCUSD ...\n",
      "[Celda 01 v2.0.3] building row for LVMH ...\n",
      "[Celda 01 v2.0.3] building row for XAUAUD ...\n",
      "\n",
      "--- Celda 01 v2.0.3 | Specs DF construido ---\n",
      "shape: (4, 12)\n",
      "shape: (4, 12)\n",
      "┌────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ symbol ┆ asset_clas ┆ base_cost_ ┆ stress_co ┆ … ┆ research_ ┆ research_ ┆ tick_size ┆ contract_ │\n",
      "│ ---    ┆ s          ┆ bps        ┆ st_bps    ┆   ┆ only      ┆ reason    ┆ _hint     ┆ hint      │\n",
      "│ str    ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│        ┆ str        ┆ f64        ┆ f64       ┆   ┆ bool      ┆ null      ┆ null      ┆ null      │\n",
      "╞════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ BNBUSD ┆ crypto     ┆ 8.0        ┆ 16.0      ┆ … ┆ false     ┆ null      ┆ null      ┆ null      │\n",
      "│ BTCUSD ┆ crypto     ┆ 8.0        ┆ 16.0      ┆ … ┆ false     ┆ null      ┆ null      ┆ null      │\n",
      "│ LVMH   ┆ equity     ┆ 12.0       ┆ 25.0      ┆ … ┆ false     ┆ null      ┆ null      ┆ null      │\n",
      "│ XAUAUD ┆ fx_metal   ┆ 4.0        ┆ 8.0       ┆ … ┆ false     ┆ null      ┆ null      ┆ null      │\n",
      "└────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "\n",
      "[Celda 01 v2.0.3] Gate costos inválidos rows: 0\n",
      "[Celda 01 v2.0.3] Gate unique symbols: 4 vs rows 4\n",
      "\n",
      "[Celda 01 v2.0.3] OK — instrument_specs guardado:\n",
      "  parquet : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\instrument_specs_v2.parquet | exists: True\n",
      "  snapshot: C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\instrument_specs_snapshot_v2.json | exists: True\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 01 v2.0.3 — Universe & Instrument Specs (por símbolo)\n",
    "# Propósito:\n",
    "#   - Crear instrument_specs en la ruta EXACTA del RUN actual.\n",
    "#   - Prints completos: paths, keys, preview, tamaños.\n",
    "#\n",
    "# Inputs:\n",
    "#   - RUN (Celda 00 v2.0.3)\n",
    "#\n",
    "# Outputs:\n",
    "#   - RUN[\"ARTIFACTS\"][\"instrument_specs\"]\n",
    "#   - RUN[\"ARTIFACTS\"][\"instrument_specs_snapshot\"]\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "if \"RUN\" not in globals():\n",
    "    raise RuntimeError(\"[Celda 01 v2.0.3] ERROR: No existe RUN. Ejecuta Celda 00 v2.0.3 primero.\")\n",
    "\n",
    "ARTIFACTS: Dict[str, Path] = RUN[\"ARTIFACTS\"]\n",
    "\n",
    "print(\"\\n--- Celda 01 v2.0.3 | Preflight ---\")\n",
    "print(\"RUN_ID   :\", RUN[\"RUN_ID\"])\n",
    "print(\"RUN_MODE :\", RUN[\"RUN_MODE\"])\n",
    "print(\"RUN_DIR  :\", RUN[\"RUN_DIR\"])\n",
    "print(\"ARTIFACTS keys:\", sorted(list(ARTIFACTS.keys())))\n",
    "\n",
    "for k in [\"instrument_specs\", \"instrument_specs_snapshot\"]:\n",
    "    if k not in ARTIFACTS:\n",
    "        raise RuntimeError(f\"[Celda 01 v2.0.3] ERROR: falta clave '{k}' en RUN['ARTIFACTS'].\")\n",
    "\n",
    "OUT_SPECS_PARQUET = ARTIFACTS[\"instrument_specs\"]\n",
    "OUT_SPECS_SNAPSHOT = ARTIFACTS[\"instrument_specs_snapshot\"]\n",
    "\n",
    "print(\"OUT_SPECS_PARQUET :\", OUT_SPECS_PARQUET)\n",
    "print(\"OUT_SPECS_SNAPSHOT:\", OUT_SPECS_SNAPSHOT)\n",
    "\n",
    "def _t(h: int, m: int = 0) -> str:\n",
    "    return f\"{h:02d}:{m:02d}\"\n",
    "\n",
    "def _session(weekdays_only: bool, windows_utc: Optional[List[Dict[str, str]]]) -> Dict[str, Any]:\n",
    "    return {\"weekdays_only\": bool(weekdays_only), \"windows_utc\": windows_utc or []}\n",
    "\n",
    "# Universo base (v2)\n",
    "UNIVERSE = [\"BNBUSD\", \"BTCUSD\", \"LVMH\", \"XAUAUD\"]\n",
    "\n",
    "# Costos (bps)\n",
    "COSTS_BPS = {\n",
    "    \"BNBUSD\": {\"base_bps\": 8.0, \"stress_bps\": 16.0},\n",
    "    \"BTCUSD\": {\"base_bps\": 8.0, \"stress_bps\": 16.0},\n",
    "    \"LVMH\":   {\"base_bps\": 12.0, \"stress_bps\": 25.0},\n",
    "    \"XAUAUD\": {\"base_bps\": 4.0, \"stress_bps\": 8.0},\n",
    "}\n",
    "\n",
    "ASSET_CLASS = {\n",
    "    \"BNBUSD\": \"crypto\",\n",
    "    \"BTCUSD\": \"crypto\",\n",
    "    \"LVMH\": \"equity\",\n",
    "    \"XAUAUD\": \"fx_metal\",\n",
    "}\n",
    "\n",
    "WEEKEND_POLICY = {\n",
    "    \"BNBUSD\": {\"entry_weekdays_only\": True, \"flatten_before_weekend\": False},\n",
    "    \"BTCUSD\": {\"entry_weekdays_only\": True, \"flatten_before_weekend\": False},\n",
    "    \"LVMH\":   {\"entry_weekdays_only\": True, \"flatten_before_weekend\": True},\n",
    "    \"XAUAUD\": {\"entry_weekdays_only\": True, \"flatten_before_weekend\": True},\n",
    "}\n",
    "\n",
    "SESSION_BY_SYMBOL = {\n",
    "    \"BNBUSD\": _session(weekdays_only=WEEKEND_POLICY[\"BNBUSD\"][\"entry_weekdays_only\"], windows_utc=[]),\n",
    "    \"BTCUSD\": _session(weekdays_only=WEEKEND_POLICY[\"BTCUSD\"][\"entry_weekdays_only\"], windows_utc=[]),\n",
    "    \"XAUAUD\": _session(weekdays_only=WEEKEND_POLICY[\"XAUAUD\"][\"entry_weekdays_only\"], windows_utc=[]),\n",
    "    # Equity: ventana explícita (UTC) para no romper operatividad\n",
    "    \"LVMH\": _session(weekdays_only=True, windows_utc=[{\"start\": _t(8, 0), \"end\": _t(16, 30)}]),\n",
    "}\n",
    "\n",
    "MICROSTRUCTURE = {s: {\"tick_size_hint\": None, \"contract_hint\": None} for s in UNIVERSE}\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "for sym in UNIVERSE:\n",
    "    print(f\"[Celda 01 v2.0.3] building row for {sym} ...\")\n",
    "\n",
    "    for req in (COSTS_BPS, ASSET_CLASS, WEEKEND_POLICY, SESSION_BY_SYMBOL):\n",
    "        if sym not in req:\n",
    "            raise RuntimeError(f\"[Celda 01 v2.0.3] ERROR: specs incompletos para {sym}\")\n",
    "\n",
    "    wp = WEEKEND_POLICY[sym]\n",
    "    sess = SESSION_BY_SYMBOL[sym]\n",
    "    c = COSTS_BPS[sym]\n",
    "    ms = MICROSTRUCTURE[sym]\n",
    "\n",
    "    research_only = False\n",
    "    research_reason = None\n",
    "    if ASSET_CLASS[sym] == \"equity\" and not sess.get(\"windows_utc\"):\n",
    "        research_only = True\n",
    "        research_reason = \"Equity sin sesión definida (windows_utc vacío).\"\n",
    "\n",
    "    rows.append({\n",
    "        \"symbol\": sym,\n",
    "        \"asset_class\": ASSET_CLASS[sym],\n",
    "        \"base_cost_bps\": float(c[\"base_bps\"]),\n",
    "        \"stress_cost_bps\": float(c[\"stress_bps\"]),\n",
    "        \"entry_weekdays_only\": bool(wp[\"entry_weekdays_only\"]),\n",
    "        \"flatten_before_weekend\": bool(wp[\"flatten_before_weekend\"]),\n",
    "        \"session_weekdays_only\": bool(sess[\"weekdays_only\"]),\n",
    "        \"session_windows_utc_json\": json.dumps(sess[\"windows_utc\"], ensure_ascii=False),\n",
    "        \"research_only\": bool(research_only),\n",
    "        \"research_reason\": research_reason,\n",
    "        \"tick_size_hint\": ms.get(\"tick_size_hint\"),\n",
    "        \"contract_hint\": ms.get(\"contract_hint\"),\n",
    "    })\n",
    "\n",
    "specs = pl.DataFrame(rows)\n",
    "\n",
    "print(\"\\n--- Celda 01 v2.0.3 | Specs DF construido ---\")\n",
    "print(\"shape:\", specs.shape)\n",
    "print(specs)\n",
    "\n",
    "# Gates duros\n",
    "bad_costs = specs.filter(\n",
    "    (pl.col(\"base_cost_bps\") <= 0) |\n",
    "    (pl.col(\"stress_cost_bps\") <= 0) |\n",
    "    (pl.col(\"stress_cost_bps\") < pl.col(\"base_cost_bps\"))\n",
    ")\n",
    "print(\"\\n[Celda 01 v2.0.3] Gate costos inválidos rows:\", bad_costs.height)\n",
    "if bad_costs.height > 0:\n",
    "    raise RuntimeError(f\"[Celda 01 v2.0.3] ERROR: costos inválidos:\\n{bad_costs}\")\n",
    "\n",
    "n_unique = specs.select(pl.col(\"symbol\").n_unique()).item()\n",
    "print(\"[Celda 01 v2.0.3] Gate unique symbols:\", n_unique, \"vs rows\", specs.height)\n",
    "if n_unique != specs.height:\n",
    "    raise RuntimeError(\"[Celda 01 v2.0.3] ERROR: símbolos duplicados en instrument_specs.\")\n",
    "\n",
    "# Persistencia\n",
    "OUT_SPECS_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "specs.write_parquet(OUT_SPECS_PARQUET)\n",
    "\n",
    "snapshot = {\n",
    "    \"cell\": \"01 v2.0.3\",\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"run_id\": RUN[\"RUN_ID\"],\n",
    "    \"run_mode\": RUN[\"RUN_MODE\"],\n",
    "    \"run_dir\": str(RUN[\"RUN_DIR\"]),\n",
    "    \"rows\": specs.to_dicts(),\n",
    "}\n",
    "OUT_SPECS_SNAPSHOT.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[Celda 01 v2.0.3] OK — instrument_specs guardado:\")\n",
    "print(\"  parquet :\", OUT_SPECS_PARQUET, \"| exists:\", OUT_SPECS_PARQUET.exists())\n",
    "print(\"  snapshot:\", OUT_SPECS_SNAPSHOT, \"| exists:\", OUT_SPECS_SNAPSHOT.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b2fd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Celda 02 v2.0.4 | Preflight ---\n",
      "RUN_ID     : 20251223_164647_172c5b29\n",
      "RUN_MODE   : NEW_RUN_DEFAULT\n",
      "PROJECT_ROOT: C:\\Quant\\MT5_Data_Extraction\n",
      "RUN_DIR    : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\n",
      "SPECS_PATH : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\instrument_specs_v2.parquet\n",
      "OUT_OHLCV  : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\ohlcv_clean_m5.parquet\n",
      "OUT_QA     : C:\\Quant\\MT5_Data_Extraction\\ER_STRATEGY_LAB\\notebooks\\outputs\\trend_m5_strategy\\v2\\run_20251223_164647_172c5b29\\data_qa_report_v2.json\n",
      "\n",
      "[Celda 02 v2.0.4] Candidatos M5 probados (exist/parquets):\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean | exists=True | parquet_count~=2000\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\data\\rates_5m | exists=False | parquet_count~=0\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\rates_5m | exists=False | parquet_count~=0\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\\m5_raw | exists=True | parquet_count~=2000\n",
      "\n",
      "[Celda 02 v2.0.4] M5_DIR seleccionado: C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 02 v2.0.4] M5_DIR_MODE       : AUTO_CANDIDATE\n",
      "\n",
      "[Celda 02 v2.0.4] Universe: ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "\n",
      "[Celda 02 v2.0.4] Layout detectado:\n",
      "  IS_HIVE(symbol=...): True\n",
      "  sample partitions: ['symbol=AAPL', 'symbol=AAVUSD', 'symbol=ADAUSD', 'symbol=AIRF', 'symbol=ALGUSD']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Celda 02 v2.0.4] Loading symbol=BNBUSD\n",
      "[Celda 02 v2.0.4] Columns(sample): ['timestamp_utc', 'timestamp_gye', 'symbol', 'open', 'high', 'low', 'close', 'tick_volume', 'real_volume', 'spread_points', 'broker', 'server_tz'] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[Celda 02 v2.0.4] ERROR: columnas OHLCV faltantes en BNBUSD. time=None, open=open, high=high, low=low, close=close. cols=['timestamp_utc', 'timestamp_gye', 'symbol', 'open', 'high', 'low', 'close', 'tick_volume', 'real_volume', 'spread_points', 'broker', 'server_tz']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 237\u001b[39m\n\u001b[32m    234\u001b[39m vcol = _pick_col(cols, V_CANDS)\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tcol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m hcol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m lcol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ccol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    238\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Celda 02 v2.0.4] ERROR: columnas OHLCV faltantes en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msym\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtime=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, open=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mocol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, high=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, low=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, close=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mccol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. cols=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m     )\n\u001b[32m    242\u001b[39m spread_col = \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols \u001b[38;5;28;01mif\u001b[39;00m c.lower() == \u001b[33m\"\u001b[39m\u001b[33mspread\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    243\u001b[39m sym_col = \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols \u001b[38;5;28;01mif\u001b[39;00m c.lower() == \u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: [Celda 02 v2.0.4] ERROR: columnas OHLCV faltantes en BNBUSD. time=None, open=open, high=high, low=low, close=close. cols=['timestamp_utc', 'timestamp_gye', 'symbol', 'open', 'high', 'low', 'close', 'tick_volume', 'real_volume', 'spread_points', 'broker', 'server_tz']"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 02 v2.0.4 — Load M5 (m5_clean) + Canonicalize + QA (AUTO-RUTAS estilo v1)\n",
    "# Propósito:\n",
    "#   - Construir ohlcv_clean_m5.parquet (schema canónico) desde tu M5 limpio REAL (v1).\n",
    "#   - QA mínimo institucional: dedup, monotonic, gaps total + intraday, share_300s.\n",
    "#\n",
    "# Inputs:\n",
    "#   - RUN (Celda 00 v2.0.3)\n",
    "#   - instrument_specs (Celda 01)\n",
    "#\n",
    "# Política de rutas (FINAL):\n",
    "#   - Si defines TREND_M5_M5_CLEAN_DIR -> usa esa (prioridad absoluta).\n",
    "#   - Si no, autodetecta en candidatos reales (como v1):\n",
    "#       * <PROJECT_ROOT>/data/historical_data/m5_clean\n",
    "#       * <PROJECT_ROOT>/data/rates_5m\n",
    "#       * <PROJECT_ROOT>/data/historical_data/rates_5m\n",
    "#       * <PROJECT_ROOT>/data/bulk_data/m5_raw   (último fallback)\n",
    "#\n",
    "# Outputs:\n",
    "#   - RUN[\"ARTIFACTS\"][\"ohlcv_clean\"]\n",
    "#   - RUN[\"ARTIFACTS\"][\"data_qa_report\"]\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import itertools\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "if \"RUN\" not in globals():\n",
    "    raise RuntimeError(\"[Celda 02 v2.0.4] ERROR: No existe RUN. Ejecuta Celda 00 v2.0.3 primero.\")\n",
    "\n",
    "ARTIFACTS: Dict[str, Path] = RUN[\"ARTIFACTS\"]\n",
    "PROJECT_ROOT: Path = RUN[\"PROJECT_ROOT\"]\n",
    "RUN_DIR: Path = RUN[\"RUN_DIR\"]\n",
    "\n",
    "SPECS_PATH = ARTIFACTS[\"instrument_specs\"]\n",
    "OUT_OHLCV = ARTIFACTS[\"ohlcv_clean\"]\n",
    "OUT_QA = ARTIFACTS[\"data_qa_report\"]\n",
    "\n",
    "print(\"\\n--- Celda 02 v2.0.4 | Preflight ---\")\n",
    "print(\"RUN_ID     :\", RUN[\"RUN_ID\"])\n",
    "print(\"RUN_MODE   :\", RUN.get(\"RUN_MODE\"))\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"RUN_DIR    :\", RUN_DIR)\n",
    "print(\"SPECS_PATH :\", SPECS_PATH)\n",
    "print(\"OUT_OHLCV  :\", OUT_OHLCV)\n",
    "print(\"OUT_QA     :\", OUT_QA)\n",
    "\n",
    "if not SPECS_PATH.exists():\n",
    "    raise RuntimeError(f\"[Celda 02 v2.0.4] ERROR: Falta instrument_specs: {SPECS_PATH}. Ejecuta Celda 01 primero.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Config / constants\n",
    "# -----------------------------\n",
    "EXPECTED_BAR_SECONDS = 300  # M5\n",
    "FORCED_M5_DIR = os.getenv(\"TREND_M5_M5_CLEAN_DIR\", \"\").strip()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "TIME_CANDS = [\"time_utc\", \"datetime\", \"timestamp\", \"time\", \"date\"]\n",
    "O_CANDS = [\"open\", \"o\"]\n",
    "H_CANDS = [\"high\", \"h\"]\n",
    "L_CANDS = [\"low\", \"l\"]\n",
    "C_CANDS = [\"close\", \"c\"]\n",
    "V_CANDS = [\"volume\", \"vol\", \"tick_volume\"]\n",
    "\n",
    "def _pick_col(cols: List[str], cands: List[str]) -> Optional[str]:\n",
    "    m = {c.lower(): c for c in cols}\n",
    "    for x in cands:\n",
    "        if x.lower() in m:\n",
    "            return m[x.lower()]\n",
    "    return None\n",
    "\n",
    "def _count_parquets_quick(p: Path, limit: int = 2000) -> int:\n",
    "    if not p.exists():\n",
    "        return 0\n",
    "    it = p.rglob(\"*.parquet\")\n",
    "    return sum(1 for _ in itertools.islice(it, limit))\n",
    "\n",
    "def _detect_m5_clean_dir() -> Tuple[Path, str]:\n",
    "    \"\"\"\n",
    "    Retorna: (dir, mode)\n",
    "      mode:\n",
    "        - FORCED_ENV\n",
    "        - AUTO_CANDIDATE\n",
    "    \"\"\"\n",
    "    if FORCED_M5_DIR:\n",
    "        d = Path(FORCED_M5_DIR).resolve()\n",
    "        return d, \"FORCED_ENV\"\n",
    "\n",
    "    # candidatos estilo v1\n",
    "    candidates = [\n",
    "        PROJECT_ROOT / \"data\" / \"historical_data\" / \"m5_clean\",\n",
    "        PROJECT_ROOT / \"data\" / \"rates_5m\",\n",
    "        PROJECT_ROOT / \"data\" / \"historical_data\" / \"rates_5m\",\n",
    "        PROJECT_ROOT / \"data\" / \"bulk_data\" / \"m5_raw\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n[Celda 02 v2.0.4] Candidatos M5 probados (exist/parquets):\")\n",
    "    best = None\n",
    "    best_n = -1\n",
    "    for c in candidates:\n",
    "        n = _count_parquets_quick(c)\n",
    "        print(f\"  - {c} | exists={c.exists()} | parquet_count~={n}\")\n",
    "        if c.exists() and n > best_n:\n",
    "            best = c\n",
    "            best_n = n\n",
    "\n",
    "    if best is None or not best.exists() or best_n <= 0:\n",
    "        raise RuntimeError(\n",
    "            \"[Celda 02 v2.0.4] ERROR: No pude detectar el directorio real de datos M5.\\n\"\n",
    "            \"Solución: define TREND_M5_M5_CLEAN_DIR apuntando a tu carpeta m5_clean.\\n\"\n",
    "            \"Ejemplo (PowerShell):\\n\"\n",
    "            \"  $env:TREND_M5_M5_CLEAN_DIR = 'C:\\\\Quant\\\\MT5_Data_Extraction\\\\data\\\\historical_data\\\\m5_clean'\\n\"\n",
    "        )\n",
    "    return best, \"AUTO_CANDIDATE\"\n",
    "\n",
    "def _coerce_time_expr(col: str, dtype: pl.DataType) -> pl.Expr:\n",
    "    if dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32):\n",
    "        # epoch detect (best-effort)\n",
    "        return pl.when(pl.col(col) > 10**17).then(pl.from_epoch(pl.col(col), time_unit=\"ns\")) \\\n",
    "                 .when(pl.col(col) > 10**14).then(pl.from_epoch(pl.col(col), time_unit=\"us\")) \\\n",
    "                 .when(pl.col(col) > 10**11).then(pl.from_epoch(pl.col(col), time_unit=\"ms\")) \\\n",
    "                 .otherwise(pl.from_epoch(pl.col(col), time_unit=\"s\")) \\\n",
    "                 .alias(\"time_utc\")\n",
    "    return pl.col(col).cast(pl.Datetime, strict=False).alias(\"time_utc\")\n",
    "\n",
    "def _gap_stats_total(times: pl.Series) -> Dict[str, Any]:\n",
    "    if times.len() < 2:\n",
    "        return {\"gap_count\": 0, \"gap_rate\": 0.0, \"missing_bars_est\": 0, \"max_gap_seconds\": 0}\n",
    "    dt = times.diff().dt.total_seconds().drop_nulls()\n",
    "    if dt.len() == 0:\n",
    "        return {\"gap_count\": 0, \"gap_rate\": 0.0, \"missing_bars_est\": 0, \"max_gap_seconds\": 0}\n",
    "    gap_count = int((dt > EXPECTED_BAR_SECONDS).sum())\n",
    "    max_gap = int(dt.max() or 0)\n",
    "    missing = ((dt // EXPECTED_BAR_SECONDS) - 1).clip_min(0)\n",
    "    missing_est = int(missing.sum() or 0)\n",
    "    gap_rate = float(gap_count / max(dt.len(), 1))\n",
    "    return {\"gap_count\": gap_count, \"gap_rate\": gap_rate, \"missing_bars_est\": missing_est, \"max_gap_seconds\": max_gap}\n",
    "\n",
    "def _gap_stats_intraday(df_times: pl.DataFrame) -> Dict[str, Any]:\n",
    "    if df_times.height < 2:\n",
    "        return {\"gap_count\": 0, \"gap_rate\": 0.0, \"missing_bars_est\": 0, \"max_gap_seconds\": 0, \"share_300s\": 1.0}\n",
    "    tmp = (\n",
    "        df_times\n",
    "        .with_columns(pl.col(\"time_utc\").dt.truncate(\"1d\").alias(\"_day\"))\n",
    "        .select(((pl.col(\"time_utc\").diff().dt.total_seconds()).over(\"_day\")).alias(\"dt_sec\"))\n",
    "        .drop_nulls()\n",
    "    )\n",
    "    if tmp.height == 0:\n",
    "        return {\"gap_count\": 0, \"gap_rate\": 0.0, \"missing_bars_est\": 0, \"max_gap_seconds\": 0, \"share_300s\": 1.0}\n",
    "    dt = tmp[\"dt_sec\"]\n",
    "    gap_count = int((dt > EXPECTED_BAR_SECONDS).sum())\n",
    "    max_gap = int(dt.max() or 0)\n",
    "    missing = ((dt // EXPECTED_BAR_SECONDS) - 1).clip_min(0)\n",
    "    missing_est = int(missing.sum() or 0)\n",
    "    gap_rate = float(gap_count / max(dt.len(), 1))\n",
    "    share_300s = float((dt == EXPECTED_BAR_SECONDS).sum() / max(dt.len(), 1))\n",
    "    return {\"gap_count\": gap_count, \"gap_rate\": gap_rate, \"missing_bars_est\": missing_est, \"max_gap_seconds\": max_gap, \"share_300s\": share_300s}\n",
    "\n",
    "def _expected_bars_data_driven(df_times: pl.DataFrame) -> int:\n",
    "    if df_times.height == 0:\n",
    "        return 0\n",
    "    per_day = (\n",
    "        df_times\n",
    "        .with_columns(pl.col(\"time_utc\").dt.truncate(\"1d\").alias(\"_day\"))\n",
    "        .group_by(\"_day\")\n",
    "        .agg([pl.min(\"time_utc\").alias(\"t0\"), pl.max(\"time_utc\").alias(\"t1\")])\n",
    "        .with_columns(((pl.col(\"t1\") - pl.col(\"t0\")).dt.total_seconds() // EXPECTED_BAR_SECONDS + 1).cast(pl.Int64).alias(\"exp\"))\n",
    "    )\n",
    "    return int(per_day.select(pl.col(\"exp\").sum()).item() or 0)\n",
    "\n",
    "# -----------------------------\n",
    "# Detect M5 dir\n",
    "# -----------------------------\n",
    "M5_DIR, M5_DIR_MODE = _detect_m5_clean_dir()\n",
    "print(\"\\n[Celda 02 v2.0.4] M5_DIR seleccionado:\", M5_DIR)\n",
    "print(\"[Celda 02 v2.0.4] M5_DIR_MODE       :\", M5_DIR_MODE)\n",
    "\n",
    "# -----------------------------\n",
    "# Universe\n",
    "# -----------------------------\n",
    "specs = pl.read_parquet(SPECS_PATH)\n",
    "universe = specs.select(\"symbol\").to_series().to_list()\n",
    "print(\"\\n[Celda 02 v2.0.4] Universe:\", universe)\n",
    "\n",
    "# -----------------------------\n",
    "# Detect layout: hive partition symbol=...\n",
    "# -----------------------------\n",
    "symbol_partitions = [d for d in M5_DIR.iterdir() if d.is_dir() and d.name.lower().startswith(\"symbol=\")]\n",
    "IS_HIVE = len(symbol_partitions) > 0\n",
    "print(\"\\n[Celda 02 v2.0.4] Layout detectado:\")\n",
    "print(\"  IS_HIVE(symbol=...):\", IS_HIVE)\n",
    "if IS_HIVE:\n",
    "    print(\"  sample partitions:\", [d.name for d in symbol_partitions[:5]])\n",
    "\n",
    "# -----------------------------\n",
    "# Cargar por símbolo + canonicalizar\n",
    "# -----------------------------\n",
    "required_cols = [\"time_utc\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"spread\",\"symbol\"]\n",
    "dfs: List[pl.DataFrame] = []\n",
    "qa_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for sym in universe:\n",
    "    print(\"\\n\" + \"-\"*100)\n",
    "    print(f\"[Celda 02 v2.0.4] Loading symbol={sym}\")\n",
    "\n",
    "    if IS_HIVE:\n",
    "        sym_dir = M5_DIR / f\"symbol={sym}\"\n",
    "        if not sym_dir.exists():\n",
    "            raise RuntimeError(f\"[Celda 02 v2.0.4] ERROR: No existe partición {sym_dir}\")\n",
    "        glob = str(sym_dir / \"**\" / \"*.parquet\")\n",
    "        lf = pl.scan_parquet(glob)\n",
    "    else:\n",
    "        glob = str(M5_DIR / \"**\" / \"*.parquet\")\n",
    "        lf = pl.scan_parquet(glob)\n",
    "\n",
    "    schema = lf.collect_schema()\n",
    "    cols = schema.names()\n",
    "    print(\"[Celda 02 v2.0.4] Columns(sample):\", cols[:20], (\"...\" if len(cols) > 20 else \"\"))\n",
    "\n",
    "    tcol = _pick_col(cols, TIME_CANDS)\n",
    "    ocol = _pick_col(cols, O_CANDS)\n",
    "    hcol = _pick_col(cols, H_CANDS)\n",
    "    lcol = _pick_col(cols, L_CANDS)\n",
    "    ccol = _pick_col(cols, C_CANDS)\n",
    "    vcol = _pick_col(cols, V_CANDS)\n",
    "\n",
    "    if tcol is None or ocol is None or hcol is None or lcol is None or ccol is None:\n",
    "        raise RuntimeError(\n",
    "            f\"[Celda 02 v2.0.4] ERROR: columnas OHLCV faltantes en {sym}. \"\n",
    "            f\"time={tcol}, open={ocol}, high={hcol}, low={lcol}, close={ccol}. cols={cols}\"\n",
    "        )\n",
    "\n",
    "    spread_col = next((c for c in cols if c.lower() == \"spread\"), None)\n",
    "    sym_col = next((c for c in cols if c.lower() == \"symbol\"), None)\n",
    "\n",
    "    # construir select canónico\n",
    "    time_expr = _coerce_time_expr(tcol, schema[tcol])\n",
    "\n",
    "    volume_expr = (pl.col(vcol).cast(pl.Float64).alias(\"volume\")) if vcol else pl.lit(0.0).cast(pl.Float64).alias(\"volume\")\n",
    "\n",
    "    if spread_col:\n",
    "        spread_expr = pl.col(spread_col).cast(pl.Float64).alias(\"spread\")\n",
    "    else:\n",
    "        spread_expr = pl.lit(0.0).cast(pl.Float64).alias(\"spread\")\n",
    "\n",
    "    if IS_HIVE:\n",
    "        # en hive particionado, puede no existir 'symbol' dentro del parquet\n",
    "        sym_expr = (pl.col(sym_col).cast(pl.Utf8).alias(\"symbol\")) if sym_col else pl.lit(sym).cast(pl.Utf8).alias(\"symbol\")\n",
    "    else:\n",
    "        # si no es hive, debería existir symbol o filtramos por columna si existe\n",
    "        sym_expr = (pl.col(sym_col).cast(pl.Utf8).alias(\"symbol\")) if sym_col else pl.lit(sym).cast(pl.Utf8).alias(\"symbol\")\n",
    "\n",
    "    lf2 = lf.select([\n",
    "        time_expr,\n",
    "        pl.col(ocol).cast(pl.Float64).alias(\"open\"),\n",
    "        pl.col(hcol).cast(pl.Float64).alias(\"high\"),\n",
    "        pl.col(lcol).cast(pl.Float64).alias(\"low\"),\n",
    "        pl.col(ccol).cast(pl.Float64).alias(\"close\"),\n",
    "        volume_expr,\n",
    "        spread_expr,\n",
    "        sym_expr,\n",
    "    ]).drop_nulls([\"time_utc\"])\n",
    "\n",
    "    if (not IS_HIVE) and sym_col:\n",
    "        lf2 = lf2.filter(pl.col(\"symbol\") == sym)\n",
    "    elif IS_HIVE:\n",
    "        lf2 = lf2.with_columns(pl.lit(sym).alias(\"symbol\"))\n",
    "\n",
    "    df = lf2.collect()\n",
    "    print(\"[Celda 02 v2.0.4] Loaded rows (raw):\", df.height)\n",
    "\n",
    "    # sort + dedup\n",
    "    n_before = df.height\n",
    "    df = df.sort(\"time_utc\").unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    n_after = df.height\n",
    "    dup_removed = n_before - n_after\n",
    "    print(\"[Celda 02 v2.0.4] After dedup rows:\", n_after, \"| dup_removed:\", dup_removed)\n",
    "\n",
    "    if n_after == 0:\n",
    "        raise RuntimeError(f\"[Celda 02 v2.0.4] ERROR: {sym} quedó vacío tras limpiar.\")\n",
    "\n",
    "    # monotonic sanity\n",
    "    min_dt = df.select(pl.col(\"time_utc\").diff().dt.total_seconds().min()).item()\n",
    "    if min_dt is not None and float(min_dt) < 0:\n",
    "        raise RuntimeError(f\"[Celda 02 v2.0.4] ERROR: {sym} no es monotónico (dt_min={min_dt}).\")\n",
    "\n",
    "    times = df[\"time_utc\"]\n",
    "    start = times.min()\n",
    "    end = times.max()\n",
    "\n",
    "    total_gaps = _gap_stats_total(times)\n",
    "    intraday_gaps = _gap_stats_intraday(df.select([\"time_utc\"]))\n",
    "\n",
    "    expected_intraday = _expected_bars_data_driven(df.select([\"time_utc\"]))\n",
    "    coverage_intraday_pct = float(n_after / expected_intraday * 100.0) if expected_intraday > 0 else 0.0\n",
    "\n",
    "    print(\"[Celda 02 v2.0.4] start_utc:\", start)\n",
    "    print(\"[Celda 02 v2.0.4] end_utc  :\", end)\n",
    "    print(\"[Celda 02 v2.0.4] intraday share_300s:\", intraday_gaps[\"share_300s\"])\n",
    "    print(\"[Celda 02 v2.0.4] coverage_intraday_pct:\", f\"{coverage_intraday_pct:.2f}%\")\n",
    "    print(\"[Celda 02 v2.0.4] gaps_total   :\", total_gaps)\n",
    "    print(\"[Celda 02 v2.0.4] gaps_intraday:\", intraday_gaps)\n",
    "\n",
    "    # Gate duro: intraday M5 consistente\n",
    "    if float(intraday_gaps[\"share_300s\"]) < 0.90:\n",
    "        raise RuntimeError(\n",
    "            f\"[Celda 02 v2.0.4] ERROR: {sym} intraday share_300s={intraday_gaps['share_300s']:.3f} < 0.90. \"\n",
    "            \"Tu dataset NO es M5 consistente intradía.\"\n",
    "        )\n",
    "\n",
    "    qa_rows.append({\n",
    "        \"symbol\": sym,\n",
    "        \"m5_dir\": str(M5_DIR),\n",
    "        \"layout_hive\": bool(IS_HIVE),\n",
    "        \"rows\": int(n_after),\n",
    "        \"dup_removed\": int(dup_removed),\n",
    "        \"start_utc\": str(start),\n",
    "        \"end_utc\": str(end),\n",
    "        \"coverage_intraday_pct\": float(coverage_intraday_pct),\n",
    "        \"gaps_total\": total_gaps,\n",
    "        \"gaps_intraday\": intraday_gaps,\n",
    "    })\n",
    "\n",
    "    dfs.append(df.select(required_cols))\n",
    "\n",
    "# Concatenar todo (schema canónico)\n",
    "ohlcv = pl.concat(dfs, how=\"vertical\").sort([\"symbol\", \"time_utc\"])\n",
    "\n",
    "missing_cols = [c for c in required_cols if c not in ohlcv.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"[Celda 02 v2.0.4] ERROR: dataset no canónico, faltan columnas: {missing_cols}\")\n",
    "\n",
    "# Persistir\n",
    "OUT_OHLCV.parent.mkdir(parents=True, exist_ok=True)\n",
    "ohlcv.write_parquet(OUT_OHLCV)\n",
    "\n",
    "qa_report = {\n",
    "    \"cell\": \"02 v2.0.4\",\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"run_id\": RUN[\"RUN_ID\"],\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"m5_dir\": str(M5_DIR),\n",
    "    \"m5_dir_mode\": M5_DIR_MODE,\n",
    "    \"layout_hive\": bool(IS_HIVE),\n",
    "    \"expected_bar_seconds\": EXPECTED_BAR_SECONDS,\n",
    "    \"n_rows_total\": int(ohlcv.height),\n",
    "    \"n_symbols\": int(ohlcv.select(pl.col(\"symbol\").n_unique()).item()),\n",
    "    \"per_symbol\": qa_rows,\n",
    "    \"notes\": [\n",
    "        \"QA intraday evita penalizar overnight/weekend.\",\n",
    "        \"Gate duro: share_300s >= 0.90 para consistencia M5 intradía.\",\n",
    "        \"Si no detecta M5_DIR, setea TREND_M5_M5_CLEAN_DIR explícitamente.\"\n",
    "    ],\n",
    "}\n",
    "OUT_QA.write_text(json.dumps(qa_report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[Celda 02 v2.0.4] OK — ohlcv_clean creado: {OUT_OHLCV} | exists: {OUT_OHLCV.exists()}\")\n",
    "print(f\"[Celda 02 v2.0.4] OK — data_qa_report creado: {OUT_QA} | exists: {OUT_QA.exists()}\")\n",
    "\n",
    "print(\"\\n--- Preview (head) ---\")\n",
    "print(ohlcv.head(5))\n",
    "\n",
    "print(\"\\n--- QA (resumen) ---\")\n",
    "print(pl.DataFrame(qa_rows).select([\"symbol\",\"rows\",\"dup_removed\",\"coverage_intraday_pct\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Celda 03 v2.0.1] Cache detectado. Usando cost model existente:\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\cost_model_snapshot.json\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\cost_model_v2.parquet\n",
      "\n",
      "--- Cost Model Snapshot (resumen) ---\n",
      "  BNBUSD: total_base_bps=8.78, total_stress_bps=21.59 (slip_base=0.78, slip_stress=5.59, gap_base=0.00, gap_stress=0.00)\n",
      "  BTCUSD: total_base_bps=8.61, total_stress_bps=20.56 (slip_base=0.61, slip_stress=4.56, gap_base=0.00, gap_stress=0.00)\n",
      "  LVMH: total_base_bps=14.75, total_stress_bps=35.64 (slip_base=0.75, slip_stress=4.64, gap_base=2.00, gap_stress=6.00)\n",
      "  XAUAUD: total_base_bps=5.26, total_stress_bps=12.64 (slip_base=0.26, slip_stress=1.64, gap_base=1.00, gap_stress=3.00)\n",
      "\n",
      "[Celda 03 v2.0.1] OK — cost model listo.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 03 v2.0.1 — Cost Model (base/stress + slippage proxy + gap proxy) [RETURNS POR SÍMBOLO OK]\n",
    "# Propósito:\n",
    "#   - Costos reproducibles net-of-costs:\n",
    "#       * base_cost_bps / stress_cost_bps (instrument_specs)\n",
    "#       * slippage proxy: spread (si existe) o proxy por volatilidad (abs-return) POR SÍMBOLO\n",
    "#       * gap proxy (equity/fx) como add-on conservador\n",
    "#   - Prints explícitos por símbolo (componentes y totales).\n",
    "#\n",
    "# Inputs:\n",
    "#   - RUN (Celda 00)\n",
    "#   - instrument_specs_v2.parquet (Celda 01)\n",
    "#   - ohlcv_clean_m5.parquet (Celda 02)\n",
    "#   - data_qa_report.json (Celda 02) [preferible v2.0.4]\n",
    "#\n",
    "# Outputs:\n",
    "#   - cost_model_snapshot.json\n",
    "#   - cost_model_v2.parquet\n",
    "#\n",
    "# ENV:\n",
    "#   - TREND_M5_FORCE_REBUILD_COST_MODEL=1 => fuerza rebuild\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# -----------------------------\n",
    "# Preflight\n",
    "# -----------------------------\n",
    "if \"RUN\" not in globals():\n",
    "    raise RuntimeError(\"[Celda 03 v2.0.1] ERROR: No existe RUN en memoria. Ejecuta primero Celda 00 v2.0.\")\n",
    "\n",
    "RUN_DIR: Path = RUN[\"RUN_DIR\"]\n",
    "ARTIFACTS: Dict[str, Path] = RUN[\"ARTIFACTS\"]\n",
    "\n",
    "INSTRUMENT_SPECS_PATH = RUN_DIR / \"instrument_specs_v2.parquet\"\n",
    "OHLCV_CLEAN_PATH = ARTIFACTS[\"ohlcv_clean\"]\n",
    "QA_REPORT_PATH = ARTIFACTS[\"data_qa_report\"]\n",
    "\n",
    "if not INSTRUMENT_SPECS_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 03 v2.0.1] ERROR: Falta instrument_specs_v2.parquet. Ejecuta Celda 01 v2.0.\")\n",
    "if not OHLCV_CLEAN_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 03 v2.0.1] ERROR: Falta ohlcv_clean_m5.parquet. Ejecuta Celda 02.\")\n",
    "\n",
    "OUT_COST_SNAPSHOT = ARTIFACTS[\"cost_model_snapshot\"]\n",
    "OUT_COST_TABLE = RUN_DIR / \"cost_model_v2.parquet\"\n",
    "\n",
    "FORCE_REBUILD_COST = os.getenv(\"TREND_M5_FORCE_REBUILD_COST_MODEL\", \"\").strip().lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "def _now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cache\n",
    "# -----------------------------\n",
    "if OUT_COST_SNAPSHOT.exists() and OUT_COST_TABLE.exists() and (not FORCE_REBUILD_COST):\n",
    "    print(f\"[Celda 03 v2.0.1] Cache detectado. Usando cost model existente:\\n  - {OUT_COST_SNAPSHOT}\\n  - {OUT_COST_TABLE}\")\n",
    "    snap = json.loads(OUT_COST_SNAPSHOT.read_text(encoding=\"utf-8\"))\n",
    "    print(\"\\n--- Cost Model Snapshot (resumen) ---\")\n",
    "    for r in snap.get(\"per_symbol\", []):\n",
    "        print(f\"  {r['symbol']}: total_base_bps={r['total_base_bps']:.2f}, total_stress_bps={r['total_stress_bps']:.2f} \"\n",
    "              f\"(slip_base={r['slippage_base_bps']:.2f}, slip_stress={r['slippage_stress_bps']:.2f}, gap_base={r['gap_base_bps']:.2f}, gap_stress={r['gap_stress_bps']:.2f})\")\n",
    "    print(\"\\n[Celda 03 v2.0.1] OK — cost model listo.\")\n",
    "else:\n",
    "    specs = pl.read_parquet(INSTRUMENT_SPECS_PATH)\n",
    "\n",
    "    # QA flag (no bloquea, solo imprime)\n",
    "    qa_session_aware = False\n",
    "    if QA_REPORT_PATH.exists():\n",
    "        try:\n",
    "            qa = json.loads(QA_REPORT_PATH.read_text(encoding=\"utf-8\"))\n",
    "            qa_session_aware = bool(qa.get(\"cell\", \"\").startswith(\"02 v2.\"))\n",
    "        except Exception:\n",
    "            qa_session_aware = False\n",
    "    print(f\"[Celda 03 v2.0.1] QA detected: {qa_session_aware} | QA path: {QA_REPORT_PATH}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Microstructure proxies (POR SÍMBOLO)\n",
    "    # -----------------------------\n",
    "    # Nota institucional:\n",
    "    # - spread_bps proxy depende de que spread exista.\n",
    "    # - abs_ret_bps proxy usa retornos close-to-close POR SÍMBOLO.\n",
    "    # - Si el archivo no estuviera ordenado por symbol/time, esto debería fallar en QA previo. Aquí imprimimos sanity.\n",
    "    sanity = (\n",
    "        pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "        .select([\"symbol\", \"time_utc\"])\n",
    "        .group_by(\"symbol\")\n",
    "        .agg([\n",
    "            (pl.col(\"time_utc\").diff().dt.total_seconds().min()).alias(\"min_dt_sec\"),\n",
    "            (pl.col(\"time_utc\").diff().dt.total_seconds().max()).alias(\"max_dt_sec\"),\n",
    "            pl.len().alias(\"n_rows\"),\n",
    "        ])\n",
    "        .collect()\n",
    "        .sort(\"symbol\")\n",
    "    )\n",
    "    print(\"\\n--- Sanity order/spacing (time_utc diff stats) ---\")\n",
    "    print(sanity)\n",
    "    bad_order = sanity.filter(pl.col(\"min_dt_sec\") < 0)\n",
    "    if bad_order.height > 0:\n",
    "        raise RuntimeError(f\"[Celda 03 v2.0.1] ERROR: time_utc no está ordenado (min_dt_sec<0) en: {bad_order.select('symbol').to_series().to_list()}\")\n",
    "\n",
    "    df_stats = (\n",
    "        pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "        .select([\"symbol\", \"time_utc\", \"close\", \"spread\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"close\").shift(1).over(\"symbol\").alias(\"close_prev\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"close_prev\").is_not_null() & (pl.col(\"close_prev\") > 0))\n",
    "              .then((pl.col(\"close\") / pl.col(\"close_prev\") - 1.0).abs())\n",
    "              .otherwise(None)\n",
    "              .alias(\"abs_ret\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"abs_ret\") * 10_000).alias(\"abs_ret_bps\"),\n",
    "            pl.when(pl.col(\"spread\").is_not_null() & (pl.col(\"close\") > 0))\n",
    "              .then((pl.col(\"spread\") / pl.col(\"close\")) * 10_000)\n",
    "              .otherwise(None)\n",
    "              .alias(\"spread_bps\"),\n",
    "        ])\n",
    "        .group_by(\"symbol\")\n",
    "        .agg([\n",
    "            pl.len().alias(\"n_rows\"),\n",
    "            pl.col(\"spread_bps\").drop_nulls().len().alias(\"n_spread_nonnull\"),\n",
    "            pl.col(\"spread_bps\").median().alias(\"spread_med_bps\"),\n",
    "            pl.col(\"spread_bps\").quantile(0.95, \"nearest\").alias(\"spread_p95_bps\"),\n",
    "            pl.col(\"abs_ret_bps\").median().alias(\"vol_med_absret_bps\"),\n",
    "            pl.col(\"abs_ret_bps\").quantile(0.95, \"nearest\").alias(\"vol_p95_absret_bps\"),\n",
    "        ])\n",
    "        .collect()\n",
    "        .sort(\"symbol\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Microstructure proxies (from OHLCV clean) ---\")\n",
    "    print(df_stats)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Construir cost model por símbolo (con prints explícitos)\n",
    "    # -----------------------------\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for r in specs.to_dicts():\n",
    "        sym = r[\"symbol\"]\n",
    "        asset_class = r[\"asset_class\"]\n",
    "        base_bps = float(r[\"base_cost_bps\"])\n",
    "        stress_bps = float(r[\"stress_cost_bps\"])\n",
    "\n",
    "        s = df_stats.filter(pl.col(\"symbol\") == sym)\n",
    "        if s.height != 1:\n",
    "            raise RuntimeError(f\"[Celda 03 v2.0.1] ERROR: no encuentro stats para {sym} en OHLCV clean.\")\n",
    "        srow = s.row(0, named=True)\n",
    "\n",
    "        n_spread_nonnull = int(srow[\"n_spread_nonnull\"])\n",
    "        n_rows = int(srow[\"n_rows\"])\n",
    "        spread_med = srow[\"spread_med_bps\"]\n",
    "        spread_p95 = srow[\"spread_p95_bps\"]\n",
    "        vol_med = srow[\"vol_med_absret_bps\"]\n",
    "        vol_p95 = srow[\"vol_p95_absret_bps\"]\n",
    "\n",
    "        spread_usable = (n_spread_nonnull / max(n_rows, 1)) >= 0.10\n",
    "\n",
    "        if spread_usable and (spread_med is not None):\n",
    "            slip_base = float(max(spread_med, 0.0)) * 0.50\n",
    "            slip_stress = float(max(spread_p95 or spread_med, 0.0)) * 0.75\n",
    "            slip_method = \"spread_bps_proxy\"\n",
    "        else:\n",
    "            slip_base = float(max(vol_med or 0.0, 0.0)) * 0.10\n",
    "            slip_stress = float(max(vol_p95 or vol_med or 0.0, 0.0)) * 0.15\n",
    "            slip_method = \"vol_absret_bps_proxy\"\n",
    "\n",
    "        if asset_class == \"equity\":\n",
    "            gap_base = 2.0\n",
    "            gap_stress = 6.0\n",
    "        elif asset_class == \"fx_metal\":\n",
    "            gap_base = 1.0\n",
    "            gap_stress = 3.0\n",
    "        else:\n",
    "            gap_base = 0.0\n",
    "            gap_stress = 0.0\n",
    "\n",
    "        total_base = base_bps + slip_base + gap_base\n",
    "        total_stress = stress_bps + slip_stress + gap_stress\n",
    "\n",
    "        # Print explícito por símbolo\n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "        print(f\"[Celda 03 v2.0.1] {sym} | asset_class={asset_class}\")\n",
    "        print(f\"  base_cost_bps={base_bps:.2f} | stress_cost_bps={stress_bps:.2f}\")\n",
    "        print(f\"  slippage_method={slip_method} | spread_coverage_pct={(n_spread_nonnull/max(n_rows,1))*100.0:.2f}%\")\n",
    "        print(f\"  spread_med_bps={spread_med} | spread_p95_bps={spread_p95}\")\n",
    "        print(f\"  vol_med_absret_bps={vol_med} | vol_p95_absret_bps={vol_p95}\")\n",
    "        print(f\"  slippage_base_bps={slip_base:.3f} | slippage_stress_bps={slip_stress:.3f}\")\n",
    "        print(f\"  gap_base_bps={gap_base:.2f} | gap_stress_bps={gap_stress:.2f}\")\n",
    "        print(f\"  >>> TOTAL_BASE_BPS={total_base:.3f} | TOTAL_STRESS_BPS={total_stress:.3f}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"asset_class\": asset_class,\n",
    "            \"base_cost_bps\": base_bps,\n",
    "            \"stress_cost_bps\": stress_bps,\n",
    "            \"slippage_base_bps\": slip_base,\n",
    "            \"slippage_stress_bps\": slip_stress,\n",
    "            \"gap_base_bps\": gap_base,\n",
    "            \"gap_stress_bps\": gap_stress,\n",
    "            \"total_base_bps\": total_base,\n",
    "            \"total_stress_bps\": total_stress,\n",
    "            \"slippage_method\": slip_method,\n",
    "            \"spread_med_bps\": float(spread_med) if spread_med is not None else None,\n",
    "            \"spread_p95_bps\": float(spread_p95) if spread_p95 is not None else None,\n",
    "            \"vol_med_absret_bps\": float(vol_med) if vol_med is not None else None,\n",
    "            \"vol_p95_absret_bps\": float(vol_p95) if vol_p95 is not None else None,\n",
    "            \"spread_coverage_pct\": float(n_spread_nonnull / max(n_rows, 1) * 100.0),\n",
    "        })\n",
    "\n",
    "    cost_table = pl.DataFrame(rows).sort(\"symbol\")\n",
    "\n",
    "    # Gates\n",
    "    bad = cost_table.filter(\n",
    "        (pl.col(\"total_base_bps\") <= 0) |\n",
    "        (pl.col(\"total_stress_bps\") <= 0) |\n",
    "        (pl.col(\"total_stress_bps\") < pl.col(\"total_base_bps\"))\n",
    "    )\n",
    "    if bad.height > 0:\n",
    "        raise RuntimeError(f\"[Celda 03 v2.0.1] ERROR: cost model inválido:\\n{bad}\")\n",
    "\n",
    "    OUT_COST_TABLE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cost_table.write_parquet(OUT_COST_TABLE)\n",
    "\n",
    "    snapshot = {\n",
    "        \"cell\": \"03 v2.0.1\",\n",
    "        \"created_utc\": _now_utc_iso(),\n",
    "        \"qa_detected\": qa_session_aware,\n",
    "        \"notes\": [\n",
    "            \"total_*_bps = base/stress (spec) + slippage proxy + gap proxy.\",\n",
    "            \"slippage proxy: usa spread si existe; caso contrario, proxy por abs-return POR SÍMBOLO.\",\n",
    "            \"gap proxy es add-on conservador; se refina más adelante si se requiere.\",\n",
    "        ],\n",
    "        \"per_symbol\": cost_table.to_dicts(),\n",
    "    }\n",
    "\n",
    "    OUT_COST_SNAPSHOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    OUT_COST_SNAPSHOT.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n[Celda 03 v2.0.1] OK — cost model guardado:\")\n",
    "    print(f\"  - {OUT_COST_SNAPSHOT}\")\n",
    "    print(f\"  - {OUT_COST_TABLE}\")\n",
    "\n",
    "    print(\"\\n--- Cost Model Table (v2.0.1) ---\")\n",
    "    print(cost_table)\n",
    "\n",
    "    warn = cost_table.filter(pl.col(\"slippage_method\") == \"vol_absret_bps_proxy\")\n",
    "    if warn.height > 0:\n",
    "        print(\"\\n[Celda 03 v2.0.1] AVISO: spread no utilizable; slippage estimado por proxy de volatilidad:\")\n",
    "        print(warn.select([\"symbol\", \"slippage_method\", \"spread_coverage_pct\", \"vol_med_absret_bps\", \"slippage_base_bps\", \"slippage_stress_bps\", \"total_base_bps\", \"total_stress_bps\"]))\n",
    "\n",
    "    print(\"\\n[Celda 03 v2.0.1] OK — costos listos para net-of-costs en baseline/alpha/engine.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Celda 04 v2.0.1] Universe: ['BNBUSD', 'BTCUSD', 'LVMH', 'XAUAUD']\n",
      "\n",
      "--- Data ranges (ohlcv_clean) ---\n",
      "shape: (4, 4)\n",
      "┌────────┬─────────────────────┬─────────────────────┬────────┐\n",
      "│ symbol ┆ start_utc           ┆ end_utc             ┆ n_rows │\n",
      "│ ---    ┆ ---                 ┆ ---                 ┆ ---    │\n",
      "│ str    ┆ datetime[ms]        ┆ datetime[ms]        ┆ u32    │\n",
      "╞════════╪═════════════════════╪═════════════════════╪════════╡\n",
      "│ BNBUSD ┆ 2021-11-19 00:00:00 ┆ 2025-12-02 05:50:00 ┆ 409320 │\n",
      "│ BTCUSD ┆ 2021-11-19 00:00:00 ┆ 2025-12-02 23:50:00 ┆ 337024 │\n",
      "│ LVMH   ┆ 2021-11-19 10:00:00 ┆ 2025-12-02 18:25:00 ┆ 104341 │\n",
      "│ XAUAUD ┆ 2021-11-19 01:05:00 ┆ 2025-12-02 05:50:00 ┆ 283032 │\n",
      "└────────┴─────────────────────┴─────────────────────┴────────┘\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Celda 04 v2.0.1] BNBUSD | asset_class=crypto\n",
      "  data: start=2021-11-19 00:00:00  end=2025-12-02 05:50:00  n_rows=409,320\n",
      "  chosen_cfg: IS=18m  OOS=3m  STEP=3m\n",
      "  embargo_days=7.00 | folds_possible=10 | mode=expanding\n",
      "  fold=01 | IS bars=151,744 days=546 | OOS bars=25,623 days=92\n",
      "  fold=02 | IS bars=177,367 days=638 | OOS bars=25,628 days=92\n",
      "  fold=03 | IS bars=202,995 days=730 | OOS bars=25,635 days=92\n",
      "  fold=04 | IS bars=228,630 days=822 | OOS bars=25,049 days=90\n",
      "  fold=05 | IS bars=253,681 days=912 | OOS bars=25,619 days=92\n",
      "  fold=06 | IS bars=279,298 days=1004 | OOS bars=25,632 days=92\n",
      "  fold=07 | IS bars=304,930 days=1096 | OOS bars=25,625 days=92\n",
      "  fold=08 | IS bars=330,555 days=1188 | OOS bars=24,793 days=89\n",
      "  fold=09 | IS bars=355,374 days=1277 | OOS bars=25,276 days=90\n",
      "  fold=10 | IS bars=380,733 days=1367 | OOS bars=24,906 days=90\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Celda 04 v2.0.1] BTCUSD | asset_class=crypto\n",
      "  data: start=2021-11-19 00:00:00  end=2025-12-02 23:50:00  n_rows=337,024\n",
      "  chosen_cfg: IS=18m  OOS=3m  STEP=3m\n",
      "  embargo_days=7.00 | folds_possible=10 | mode=expanding\n",
      "  fold=01 | IS bars=80,036 days=544 | OOS bars=25,865 days=92\n",
      "  fold=02 | IS bars=105,913 days=636 | OOS bars=25,696 days=92\n",
      "  fold=03 | IS bars=131,537 days=728 | OOS bars=25,700 days=92\n",
      "  fold=04 | IS bars=157,297 days=820 | OOS bars=25,164 days=90\n",
      "  fold=05 | IS bars=182,476 days=910 | OOS bars=25,091 days=92\n",
      "  fold=06 | IS bars=207,587 days=1002 | OOS bars=25,586 days=92\n",
      "  fold=07 | IS bars=233,138 days=1094 | OOS bars=25,772 days=92\n",
      "  fold=08 | IS bars=258,850 days=1186 | OOS bars=24,559 days=89\n",
      "  fold=09 | IS bars=283,421 days=1275 | OOS bars=24,731 days=90\n",
      "  fold=10 | IS bars=308,223 days=1365 | OOS bars=24,904 days=90\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Celda 04 v2.0.1] LVMH | asset_class=equity\n",
      "  data: start=2021-11-19 10:00:00  end=2025-12-02 18:25:00  n_rows=104,341\n",
      "  chosen_cfg: IS=18m  OOS=3m  STEP=3m\n",
      "  embargo_days=7.00 | folds_possible=10 | mode=expanding\n",
      "  fold=01 | IS bars=38,915 days=384 | OOS bars=6,732 days=66\n",
      "  fold=02 | IS bars=45,647 days=450 | OOS bars=6,559 days=65\n",
      "  fold=03 | IS bars=52,206 days=515 | OOS bars=6,283 days=62\n",
      "  fold=04 | IS bars=58,489 days=577 | OOS bars=6,267 days=62\n",
      "  fold=05 | IS bars=64,761 days=639 | OOS bars=6,452 days=64\n",
      "  fold=06 | IS bars=71,213 days=703 | OOS bars=6,656 days=66\n",
      "  fold=07 | IS bars=77,869 days=769 | OOS bars=6,070 days=61\n",
      "  fold=08 | IS bars=83,939 days=830 | OOS bars=6,060 days=60\n",
      "  fold=09 | IS bars=89,999 days=890 | OOS bars=6,666 days=66\n",
      "  fold=10 | IS bars=96,665 days=956 | OOS bars=6,666 days=66\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Celda 04 v2.0.1] XAUAUD | asset_class=fx_metal\n",
      "  data: start=2021-11-19 01:05:00  end=2025-12-02 05:50:00  n_rows=283,032\n",
      "  chosen_cfg: IS=18m  OOS=3m  STEP=3m\n",
      "  embargo_days=7.00 | folds_possible=10 | mode=expanding\n",
      "  fold=01 | IS bars=104,611 days=385 | OOS bars=17,919 days=66\n",
      "  fold=02 | IS bars=122,530 days=451 | OOS bars=17,652 days=65\n",
      "  fold=03 | IS bars=140,247 days=516 | OOS bars=17,124 days=63\n",
      "  fold=04 | IS bars=157,338 days=579 | OOS bars=17,472 days=64\n",
      "  fold=05 | IS bars=174,778 days=643 | OOS bars=17,661 days=65\n",
      "  fold=06 | IS bars=192,439 days=708 | OOS bars=17,978 days=66\n",
      "  fold=07 | IS bars=210,417 days=774 | OOS bars=17,324 days=64\n",
      "  fold=08 | IS bars=227,741 days=838 | OOS bars=16,925 days=62\n",
      "  fold=09 | IS bars=244,666 days=900 | OOS bars=17,916 days=66\n",
      "  fold=10 | IS bars=262,582 days=966 | OOS bars=17,989 days=66\n",
      "\n",
      "--- Folds por símbolo ---\n",
      "shape: (4, 2)\n",
      "┌────────┬─────────┐\n",
      "│ symbol ┆ n_folds │\n",
      "│ ---    ┆ ---     │\n",
      "│ str    ┆ u32     │\n",
      "╞════════╪═════════╡\n",
      "│ BNBUSD ┆ 10      │\n",
      "│ BTCUSD ┆ 10      │\n",
      "│ LVMH   ┆ 10      │\n",
      "│ XAUAUD ┆ 10      │\n",
      "└────────┴─────────┘\n",
      "\n",
      "[Celda 04 v2.0.1] OK — WFO folds guardados:\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\wfo_folds.parquet\n",
      "  - C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\wfo_folds_snapshot.json\n",
      "\n",
      "--- WFO Folds (preview) ---\n",
      "shape: (12, 18)\n",
      "┌────────┬─────────┬────────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ symbol ┆ fold_id ┆ asset_clas ┆ wfo_mode  ┆ … ┆ cfg_is_mon ┆ cfg_oos_mo ┆ cfg_step_m ┆ embargo_b │\n",
      "│ ---    ┆ ---     ┆ s          ┆ ---       ┆   ┆ ths        ┆ nths       ┆ onths      ┆ ars       │\n",
      "│ str    ┆ i64     ┆ ---        ┆ str       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│        ┆         ┆ str        ┆           ┆   ┆ i64        ┆ i64        ┆ i64        ┆ i64       │\n",
      "╞════════╪═════════╪════════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ BNBUSD ┆ 1       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 2       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 3       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 4       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 5       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ …      ┆ …       ┆ …          ┆ …         ┆ … ┆ …          ┆ …          ┆ …          ┆ …         │\n",
      "│ BNBUSD ┆ 8       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 9       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BNBUSD ┆ 10      ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BTCUSD ┆ 1       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "│ BTCUSD ┆ 2       ┆ crypto     ┆ expanding ┆ … ┆ 18         ┆ 3          ┆ 3          ┆ 2016      │\n",
      "└────────┴─────────┴────────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘\n",
      "\n",
      "[Celda 04 v2.0.1] OK — Se permite avanzar a Celda 05.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 04 v2.0.1 — WFO Builder (≥6 folds + embargo/purge) [SESSION-AWARE GATES]\n",
    "# Fix vs v2.0:\n",
    "#   - Gate de tamaño ahora es por asset_class y además por \"trading days\" (más defendible).\n",
    "#   - Evita bloquear equities session-only (LVMH) con thresholds 24/7.\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# -----------------------------\n",
    "# Preflight\n",
    "# -----------------------------\n",
    "if \"RUN\" not in globals():\n",
    "    raise RuntimeError(\"[Celda 04 v2.0.1] ERROR: No existe RUN en memoria. Ejecuta primero Celda 00 v2.0.\")\n",
    "\n",
    "RUN_DIR: Path = RUN[\"RUN_DIR\"]\n",
    "ARTIFACTS: Dict[str, Path] = RUN[\"ARTIFACTS\"]\n",
    "\n",
    "INSTRUMENT_SPECS_PATH = RUN_DIR / \"instrument_specs_v2.parquet\"\n",
    "OHLCV_CLEAN_PATH = ARTIFACTS[\"ohlcv_clean\"]\n",
    "QA_REPORT_PATH = ARTIFACTS[\"data_qa_report\"]\n",
    "\n",
    "if not INSTRUMENT_SPECS_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 04 v2.0.1] ERROR: Falta instrument_specs_v2.parquet. Ejecuta Celda 01 v2.0.\")\n",
    "if not OHLCV_CLEAN_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 04 v2.0.1] ERROR: Falta ohlcv_clean_m5.parquet. Ejecuta Celda 02.\")\n",
    "\n",
    "OUT_WFO_FOLDS = ARTIFACTS[\"wfo_folds\"]\n",
    "OUT_WFO_SNAPSHOT = ARTIFACTS[\"wfo_folds_snapshot\"]\n",
    "\n",
    "FORCE_REBUILD = os.getenv(\"TREND_M5_FORCE_REBUILD_WFO\", \"1\").strip().lower() in (\"1\", \"true\", \"yes\")  # default=1 aquí\n",
    "WFO_MODE = os.getenv(\"TREND_M5_WFO_MODE\", \"expanding\").strip().lower()\n",
    "if WFO_MODE not in (\"expanding\", \"rolling\"):\n",
    "    raise ValueError(\"[Celda 04 v2.0.1] ERROR: TREND_M5_WFO_MODE debe ser 'expanding' o 'rolling'.\")\n",
    "\n",
    "MIN_FOLDS = int(os.getenv(\"TREND_M5_MIN_FOLDS\", \"6\"))\n",
    "MAX_HOLD_BARS = int(os.getenv(\"TREND_M5_MAX_HOLD_BARS\", \"2016\"))  # ~1 semana\n",
    "EXPECTED_BAR_SECONDS = 300  # M5\n",
    "EMBARGO = timedelta(seconds=MAX_HOLD_BARS * EXPECTED_BAR_SECONDS)\n",
    "\n",
    "def _now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def add_months(dt: datetime, months: int) -> datetime:\n",
    "    y = dt.year\n",
    "    m = dt.month + months\n",
    "    while m > 12:\n",
    "        y += 1\n",
    "        m -= 12\n",
    "    while m < 1:\n",
    "        y -= 1\n",
    "        m += 12\n",
    "    last_day = calendar.monthrange(y, m)[1]\n",
    "    d = min(dt.day, last_day)\n",
    "    return dt.replace(year=y, month=m, day=d)\n",
    "\n",
    "@dataclass\n",
    "class WFOConfig:\n",
    "    is_months: int\n",
    "    oos_months: int\n",
    "    step_months: int\n",
    "\n",
    "CANDIDATE_CONFIGS_BY_ASSET = {\n",
    "    \"crypto\": [\n",
    "        WFOConfig(18, 3, 3),\n",
    "        WFOConfig(12, 3, 3),\n",
    "        WFOConfig(12, 2, 2),\n",
    "    ],\n",
    "    \"equity\": [\n",
    "        WFOConfig(18, 3, 3),  # se mantiene; el gate ahora es session-aware\n",
    "        WFOConfig(12, 3, 3),\n",
    "        WFOConfig(12, 2, 2),\n",
    "    ],\n",
    "    \"fx_metal\": [\n",
    "        WFOConfig(18, 3, 3),\n",
    "        WFOConfig(12, 3, 3),\n",
    "        WFOConfig(12, 2, 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Gates session-aware (por asset_class)\n",
    "# -----------------------------\n",
    "# Días son el gate principal (defendible). Barras son sanity.\n",
    "GATES = {\n",
    "    \"crypto\":   {\"min_is_days\": 365, \"min_oos_days\": 60, \"min_is_bars\": 70_000, \"min_oos_bars\": 20_000},\n",
    "    \"fx_metal\": {\"min_is_days\": 365, \"min_oos_days\": 60, \"min_is_bars\": 80_000, \"min_oos_bars\": 12_000},\n",
    "    \"equity\":   {\"min_is_days\": 250, \"min_oos_days\": 60, \"min_is_bars\": 35_000, \"min_oos_bars\": 6_000},\n",
    "    \"unknown\":  {\"min_is_days\": 250, \"min_oos_days\": 60, \"min_is_bars\": 35_000, \"min_oos_bars\": 6_000},\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Construcción (sin cache por defecto)\n",
    "# -----------------------------\n",
    "specs = pl.read_parquet(INSTRUMENT_SPECS_PATH)\n",
    "universe = specs.select(\"symbol\").to_series().to_list()\n",
    "spec_map = {r[\"symbol\"]: r for r in specs.to_dicts()}\n",
    "\n",
    "df_ranges = (\n",
    "    pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "    .group_by(\"symbol\")\n",
    "    .agg([\n",
    "        pl.min(\"time_utc\").alias(\"start_utc\"),\n",
    "        pl.max(\"time_utc\").alias(\"end_utc\"),\n",
    "        pl.len().alias(\"n_rows\"),\n",
    "    ])\n",
    "    .collect()\n",
    "    .sort(\"symbol\")\n",
    ")\n",
    "\n",
    "print(\"[Celda 04 v2.0.1] Universe:\", universe)\n",
    "print(\"\\n--- Data ranges (ohlcv_clean) ---\")\n",
    "print(df_ranges)\n",
    "\n",
    "qa_cell = None\n",
    "if QA_REPORT_PATH.exists():\n",
    "    try:\n",
    "        qa_cell = json.loads(QA_REPORT_PATH.read_text(encoding=\"utf-8\")).get(\"cell\")\n",
    "    except Exception:\n",
    "        qa_cell = None\n",
    "\n",
    "# Precompute daily calendar por símbolo (más eficiente que re-scan por fold)\n",
    "daily = (\n",
    "    pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "    .select([\n",
    "        pl.col(\"symbol\"),\n",
    "        pl.col(\"time_utc\").dt.truncate(\"1d\").alias(\"day\"),\n",
    "    ])\n",
    "    .unique()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "def count_bars(sym: str, t0: datetime, t1: datetime) -> int:\n",
    "    return int(\n",
    "        pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "        .filter((pl.col(\"symbol\") == sym) & (pl.col(\"time_utc\") >= t0) & (pl.col(\"time_utc\") < t1))\n",
    "        .select(pl.len())\n",
    "        .collect()\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "def count_days(sym: str, t0: datetime, t1: datetime) -> int:\n",
    "    return int(\n",
    "        daily.filter((pl.col(\"symbol\") == sym) & (pl.col(\"day\") >= t0.replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "                     & (pl.col(\"day\") <  t1.replace(hour=0, minute=0, second=0, microsecond=0)))\n",
    "        .select(pl.len())\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "def possible_fold_count(start: datetime, end: datetime, cfg: WFOConfig, embargo: timedelta) -> int:\n",
    "    is_start = start\n",
    "    is_end = add_months(is_start, cfg.is_months)\n",
    "    n = 0\n",
    "    while True:\n",
    "        oos_start = is_end + embargo\n",
    "        oos_end = add_months(oos_start, cfg.oos_months)\n",
    "        if oos_end > end:\n",
    "            break\n",
    "        n += 1\n",
    "        is_end = add_months(is_end, cfg.step_months)\n",
    "        if is_end >= end:\n",
    "            break\n",
    "    return n\n",
    "\n",
    "all_folds_rows: List[Dict[str, Any]] = []\n",
    "snapshot_per_symbol: List[Dict[str, Any]] = []\n",
    "\n",
    "for sym in universe:\n",
    "    r = df_ranges.filter(pl.col(\"symbol\") == sym)\n",
    "    start_dt = r.select(\"start_utc\").item()\n",
    "    end_dt = r.select(\"end_utc\").item()\n",
    "    n_rows = int(r.select(\"n_rows\").item())\n",
    "\n",
    "    asset_class = spec_map[sym].get(\"asset_class\", \"unknown\")\n",
    "    cfg_candidates = CANDIDATE_CONFIGS_BY_ASSET.get(asset_class, [WFOConfig(12, 3, 3)])\n",
    "\n",
    "    chosen_cfg: Optional[WFOConfig] = None\n",
    "    chosen_count = 0\n",
    "\n",
    "    for cfg in cfg_candidates:\n",
    "        cnt = possible_fold_count(start_dt, end_dt, cfg, EMBARGO)\n",
    "        if cnt >= MIN_FOLDS:\n",
    "            chosen_cfg = cfg\n",
    "            chosen_count = cnt\n",
    "            break\n",
    "        if cnt > chosen_count:\n",
    "            chosen_cfg = cfg\n",
    "            chosen_count = cnt\n",
    "\n",
    "    if chosen_cfg is None or chosen_count < 3:\n",
    "        raise RuntimeError(f\"[Celda 04 v2.0.1] ERROR: WFO indefendible para {sym}. folds_possible={chosen_count}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    print(f\"[Celda 04 v2.0.1] {sym} | asset_class={asset_class}\")\n",
    "    print(f\"  data: start={start_dt}  end={end_dt}  n_rows={n_rows:,}\")\n",
    "    print(f\"  chosen_cfg: IS={chosen_cfg.is_months}m  OOS={chosen_cfg.oos_months}m  STEP={chosen_cfg.step_months}m\")\n",
    "    print(f\"  embargo_days={EMBARGO.total_seconds()/86400.0:.2f} | folds_possible={chosen_count} | mode={WFO_MODE}\")\n",
    "\n",
    "    fold_id = 1\n",
    "    is_start = start_dt\n",
    "    is_end = add_months(is_start, chosen_cfg.is_months)\n",
    "\n",
    "    while True:\n",
    "        embargo_start = is_end\n",
    "        embargo_end = is_end + EMBARGO\n",
    "        oos_start = embargo_end\n",
    "        oos_end = add_months(oos_start, chosen_cfg.oos_months)\n",
    "\n",
    "        if oos_end > end_dt:\n",
    "            break\n",
    "\n",
    "        if WFO_MODE == \"rolling\":\n",
    "            is_start_eff = add_months(is_start, (fold_id - 1) * chosen_cfg.step_months)\n",
    "            is_end_eff = is_end\n",
    "        else:\n",
    "            is_start_eff = is_start\n",
    "            is_end_eff = is_end\n",
    "\n",
    "        is_bars = count_bars(sym, is_start_eff, is_end_eff)\n",
    "        oos_bars = count_bars(sym, oos_start, oos_end)\n",
    "\n",
    "        is_days = count_days(sym, is_start_eff, is_end_eff)\n",
    "        oos_days = count_days(sym, oos_start, oos_end)\n",
    "\n",
    "        all_folds_rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"fold_id\": int(fold_id),\n",
    "            \"asset_class\": asset_class,\n",
    "            \"wfo_mode\": WFO_MODE,\n",
    "            \"is_start_utc\": is_start_eff,\n",
    "            \"is_end_utc\": is_end_eff,\n",
    "            \"embargo_start_utc\": embargo_start,\n",
    "            \"embargo_end_utc\": embargo_end,\n",
    "            \"oos_start_utc\": oos_start,\n",
    "            \"oos_end_utc\": oos_end,\n",
    "            \"is_bars\": int(is_bars),\n",
    "            \"oos_bars\": int(oos_bars),\n",
    "            \"is_days\": int(is_days),\n",
    "            \"oos_days\": int(oos_days),\n",
    "            \"cfg_is_months\": int(chosen_cfg.is_months),\n",
    "            \"cfg_oos_months\": int(chosen_cfg.oos_months),\n",
    "            \"cfg_step_months\": int(chosen_cfg.step_months),\n",
    "            \"embargo_bars\": int(MAX_HOLD_BARS),\n",
    "        })\n",
    "\n",
    "        print(f\"  fold={fold_id:02d} | IS bars={is_bars:,} days={is_days} | OOS bars={oos_bars:,} days={oos_days}\")\n",
    "\n",
    "        fold_id += 1\n",
    "        is_end = add_months(is_end, chosen_cfg.step_months)\n",
    "        if is_end >= end_dt:\n",
    "            break\n",
    "\n",
    "    snapshot_per_symbol.append({\n",
    "        \"symbol\": sym,\n",
    "        \"asset_class\": asset_class,\n",
    "        \"config\": {\"is_months\": chosen_cfg.is_months, \"oos_months\": chosen_cfg.oos_months, \"step_months\": chosen_cfg.step_months},\n",
    "        \"wfo_mode\": WFO_MODE,\n",
    "        \"n_folds\": int(fold_id - 1),\n",
    "        \"embargo_bars\": int(MAX_HOLD_BARS),\n",
    "        \"embargo_days\": float(EMBARGO.total_seconds() / 86400.0),\n",
    "        \"data_start_utc\": str(start_dt),\n",
    "        \"data_end_utc\": str(end_dt),\n",
    "        \"n_rows\": int(n_rows),\n",
    "    })\n",
    "\n",
    "wfo_df = pl.DataFrame(all_folds_rows).sort([\"symbol\", \"fold_id\"])\n",
    "\n",
    "# Gate A: folds por símbolo\n",
    "folds_by_sym = wfo_df.group_by(\"symbol\").agg(pl.len().alias(\"n_folds\")).sort(\"symbol\")\n",
    "print(\"\\n--- Folds por símbolo ---\")\n",
    "print(folds_by_sym)\n",
    "\n",
    "too_few = folds_by_sym.filter(pl.col(\"n_folds\") < 3)\n",
    "if too_few.height > 0:\n",
    "    raise RuntimeError(f\"[Celda 04 v2.0.1] ERROR: símbolos con <3 folds: {too_few}\")\n",
    "\n",
    "# Gate B: session-aware (por asset_class)\n",
    "def gate_row(asset_class: str) -> Dict[str, int]:\n",
    "    g = GATES.get(asset_class, GATES[\"unknown\"])\n",
    "    return {k: int(v) for k, v in g.items()}\n",
    "\n",
    "bad_rows = []\n",
    "for row in wfo_df.iter_rows(named=True):\n",
    "    g = gate_row(row[\"asset_class\"])\n",
    "    if (row[\"is_days\"] < g[\"min_is_days\"]) or (row[\"oos_days\"] < g[\"min_oos_days\"]) or (row[\"is_bars\"] < g[\"min_is_bars\"]) or (row[\"oos_bars\"] < g[\"min_oos_bars\"]):\n",
    "        bad_rows.append({\n",
    "            \"symbol\": row[\"symbol\"],\n",
    "            \"fold_id\": row[\"fold_id\"],\n",
    "            \"asset_class\": row[\"asset_class\"],\n",
    "            \"is_bars\": row[\"is_bars\"], \"oos_bars\": row[\"oos_bars\"],\n",
    "            \"is_days\": row[\"is_days\"], \"oos_days\": row[\"oos_days\"],\n",
    "            **{f\"gate_{k}\": v for k, v in g.items()}\n",
    "        })\n",
    "\n",
    "if bad_rows:\n",
    "    bad_df = pl.DataFrame(bad_rows).sort([\"symbol\", \"fold_id\"])\n",
    "    print(\"\\n[Celda 04 v2.0.1] Detalle folds que NO pasan gates session-aware (se detiene):\")\n",
    "    print(bad_df)\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 04 v2.0.1] ERROR: Hay folds que no cumplen mínimos day-aware y bar-aware por asset_class. \"\n",
    "        \"Si esto pasa, tu data o tus ventanas son insuficientes para WFO defendible.\"\n",
    "    )\n",
    "\n",
    "# Gate C: no OOS overlap por símbolo\n",
    "bad_overlap = []\n",
    "for sym in wfo_df.select(\"symbol\").unique().to_series().to_list():\n",
    "    s = wfo_df.filter(pl.col(\"symbol\") == sym).sort(\"fold_id\")\n",
    "    prev_end = None\n",
    "    for row in s.iter_rows(named=True):\n",
    "        if prev_end is not None and row[\"oos_start_utc\"] < prev_end:\n",
    "            bad_overlap.append((sym, row[\"fold_id\"]))\n",
    "        prev_end = row[\"oos_end_utc\"]\n",
    "if bad_overlap:\n",
    "    raise RuntimeError(f\"[Celda 04 v2.0.1] ERROR: OOS overlap detectado: {bad_overlap}\")\n",
    "\n",
    "# Persistir\n",
    "OUT_WFO_FOLDS.parent.mkdir(parents=True, exist_ok=True)\n",
    "wfo_df.write_parquet(OUT_WFO_FOLDS)\n",
    "\n",
    "snapshot = {\n",
    "    \"cell\": \"04 v2.0.1\",\n",
    "    \"created_utc\": _now_utc_iso(),\n",
    "    \"qa_cell_detected\": qa_cell,\n",
    "    \"wfo_mode\": WFO_MODE,\n",
    "    \"min_folds_target\": MIN_FOLDS,\n",
    "    \"max_hold_bars\": MAX_HOLD_BARS,\n",
    "    \"embargo_days\": float(EMBARGO.total_seconds() / 86400.0),\n",
    "    \"gates\": GATES,\n",
    "    \"notes\": [\n",
    "        \"Gates cambiados a day-aware + bar-aware por asset_class (session-aware).\",\n",
    "        \"Esto evita bloquear equities session-only con umbrales 24/7.\",\n",
    "    ],\n",
    "    \"per_symbol\": snapshot_per_symbol,\n",
    "    \"folds_path\": str(OUT_WFO_FOLDS),\n",
    "}\n",
    "\n",
    "OUT_WFO_SNAPSHOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_WFO_SNAPSHOT.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[Celda 04 v2.0.1] OK — WFO folds guardados:\")\n",
    "print(f\"  - {OUT_WFO_FOLDS}\")\n",
    "print(f\"  - {OUT_WFO_SNAPSHOT}\")\n",
    "\n",
    "print(\"\\n--- WFO Folds (preview) ---\")\n",
    "print(wfo_df.head(12))\n",
    "\n",
    "print(\"\\n[Celda 04 v2.0.1] OK — Se permite avanzar a Celda 05.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da23dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"ret\"; valid columns: [\"symbol\", \"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"spread\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nSORT BY [col(\"symbol\"), col(\"time_utc\")]\n  SELECT [col(\"symbol\"), col(\"time_utc\"), col(\"open\"), col(\"high\"), col(\"low\"), col(\"close\"), col(\"volume\"), col(\"spread\")]\n    Parquet SCAN [C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\ohlcv_clean_m5.parquet]\n    PROJECT */8 COLUMNS\n    ESTIMATED ROWS: 1133717",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 193\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Final select (canónico)\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    172\u001b[39m lf_feat = (\n\u001b[32m    173\u001b[39m     lf4.select([\n\u001b[32m    174\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m     .sort([\u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtime_utc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    191\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m df_feat = \u001b[43mlf_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# QA / sanity\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    198\u001b[39m mono = (\n\u001b[32m    199\u001b[39m     df_feat.group_by(\u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m     .agg(pl.col(\u001b[33m\"\u001b[39m\u001b[33mtime_utc\u001b[39m\u001b[33m\"\u001b[39m).is_sorted().alias(\u001b[33m\"\u001b[39m\u001b[33mis_sorted\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    201\u001b[39m     .sort(\u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    202\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2422\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2420\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2421\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: unable to find column \"ret\"; valid columns: [\"symbol\", \"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"spread\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nSORT BY [col(\"symbol\"), col(\"time_utc\")]\n  SELECT [col(\"symbol\"), col(\"time_utc\"), col(\"open\"), col(\"high\"), col(\"low\"), col(\"close\"), col(\"volume\"), col(\"spread\")]\n    Parquet SCAN [C:\\Quant\\MT5_Data_Extraction\\artifacts\\v2\\run_20251222_161231_844455a0\\ohlcv_clean_m5.parquet]\n    PROJECT */8 COLUMNS\n    ESTIMATED ROWS: 1133717"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Celda 05 v2.0.3 — Feature Set (Causal): Trendiness + Direction [FIX nested windows]\n",
    "# Fix vs v2.0.2:\n",
    "#   - Elimina \"window dentro de rolling/window\": primero materializa columnas base (ret, true_range, abs_diff),\n",
    "#     luego aplica rollings/EMAs usando pl.col(\"...\") (sin anidar ventanas).\n",
    "#   - Mantiene Lazy-friendly y sin ColumnNotFound.\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# -----------------------------\n",
    "# Preflight\n",
    "# -----------------------------\n",
    "if \"RUN\" not in globals():\n",
    "    raise RuntimeError(\"[Celda 05 v2.0.3] ERROR: No existe RUN en memoria. Ejecuta primero Celda 00 v2.0.\")\n",
    "\n",
    "RUN_DIR: Path = RUN[\"RUN_DIR\"]\n",
    "ARTIFACTS: Dict[str, Path] = RUN[\"ARTIFACTS\"]\n",
    "\n",
    "OHLCV_CLEAN_PATH = ARTIFACTS.get(\"ohlcv_clean\", RUN_DIR / \"ohlcv_clean_m5.parquet\")\n",
    "QA_REPORT_PATH = ARTIFACTS.get(\"data_qa_report\", RUN_DIR / \"data_qa_report.json\")\n",
    "WFO_FOLDS_PATH = ARTIFACTS.get(\"wfo_folds\", RUN_DIR / \"wfo_folds.parquet\")\n",
    "\n",
    "if not OHLCV_CLEAN_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 05 v2.0.3] ERROR: Falta ohlcv_clean_m5.parquet. Ejecuta Celda 02.\")\n",
    "if not WFO_FOLDS_PATH.exists():\n",
    "    raise RuntimeError(\"[Celda 05 v2.0.3] ERROR: Falta wfo_folds.parquet. Ejecuta Celda 04.\")\n",
    "\n",
    "OUT_FEATURES = RUN_DIR / \"features_m5_v2.parquet\"\n",
    "OUT_SNAPSHOT = RUN_DIR / \"features_snapshot_v2.json\"\n",
    "\n",
    "RUN[\"ARTIFACTS\"][\"features_m5\"] = OUT_FEATURES\n",
    "RUN[\"ARTIFACTS\"][\"features_snapshot\"] = OUT_SNAPSHOT\n",
    "\n",
    "FORCE_REBUILD = os.getenv(\"TREND_M5_FORCE_REBUILD_FEATURES\", \"\").strip().lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "def _now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "# -----------------------------\n",
    "# Parámetros\n",
    "# -----------------------------\n",
    "EXPECTED_BAR_SECONDS = 300\n",
    "\n",
    "ER_WIN = int(os.getenv(\"TREND_M5_ER_WIN\", \"288\"))\n",
    "VOL_WIN = int(os.getenv(\"TREND_M5_VOL_WIN\", \"288\"))\n",
    "MOM_WIN = int(os.getenv(\"TREND_M5_MOM_WIN\", \"288\"))\n",
    "ATR_WIN = int(os.getenv(\"TREND_M5_ATR_WIN\", \"96\"))\n",
    "\n",
    "EMA_FAST = int(os.getenv(\"TREND_M5_EMA_FAST\", \"200\"))\n",
    "EMA_SLOW = int(os.getenv(\"TREND_M5_EMA_SLOW\", \"600\"))\n",
    "SLOPE_WIN = int(os.getenv(\"TREND_M5_SLOPE_WIN\", \"50\"))\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# -----------------------------\n",
    "# Cache\n",
    "# -----------------------------\n",
    "if OUT_FEATURES.exists() and OUT_SNAPSHOT.exists() and (not FORCE_REBUILD):\n",
    "    print(f\"[Celda 05 v2.0.3] Cache detectado. Usando features existentes:\\n  - {OUT_FEATURES}\\n  - {OUT_SNAPSHOT}\")\n",
    "    snap = json.loads(OUT_SNAPSHOT.read_text(encoding=\"utf-8\"))\n",
    "    print(\"\\n--- Features Snapshot (resumen) ---\")\n",
    "    print(\"  params:\", snap.get(\"params\", {}))\n",
    "    print(\"  symbols:\", snap.get(\"symbols\", []))\n",
    "    print(\"  schema_cols(sample):\", snap.get(\"schema_cols\", [])[:20], \"...\")\n",
    "    print(\"\\n[Celda 05 v2.0.3] OK — features listos.\")\n",
    "else:\n",
    "    lf0 = (\n",
    "        pl.scan_parquet(OHLCV_CLEAN_PATH)\n",
    "        .select([\"symbol\", \"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"spread\"])\n",
    "        .sort([\"symbol\", \"time_utc\"])\n",
    "    )\n",
    "\n",
    "    cols = lf0.collect_schema().names()\n",
    "    required = [\"symbol\", \"time_utc\", \"open\", \"high\", \"low\", \"close\"]\n",
    "    missing = [c for c in required if c not in cols]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Celda 05 v2.0.3] ERROR: faltan columnas en ohlcv_clean: {missing}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 1: columnas base (permitido usar .over aquí)\n",
    "    #   - IMPORTANT: no usar estas expresiones dentro de rolling posteriormente.\n",
    "    # ============================================================\n",
    "    close_prev = pl.col(\"close\").shift(1).over(\"symbol\")\n",
    "\n",
    "    ret_expr = (\n",
    "        pl.when(close_prev.is_not_null() & (close_prev > 0))\n",
    "        .then(pl.col(\"close\") / close_prev - 1.0)\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "    # abs_diff por símbolo (evita diff().over, y evita nested windows)\n",
    "    abs_diff_expr = (pl.col(\"close\") - pl.col(\"close\").shift(1).over(\"symbol\")).abs()\n",
    "\n",
    "    # true range base (por símbolo)\n",
    "    tr_expr = pl.max_horizontal([\n",
    "        (pl.col(\"high\") - pl.col(\"low\")),\n",
    "        (pl.col(\"high\") - pl.col(\"close\").shift(1).over(\"symbol\")).abs(),\n",
    "        (pl.col(\"low\")  - pl.col(\"close\").shift(1).over(\"symbol\")).abs(),\n",
    "    ])\n",
    "\n",
    "    lf1 = (\n",
    "        lf0.with_columns([\n",
    "            close_prev.alias(\"close_prev\"),\n",
    "            ret_expr.alias(\"ret\"),\n",
    "            pl.col(\"ret\").abs().alias(\"abs_ret\"),\n",
    "            abs_diff_expr.alias(\"abs_diff\"),\n",
    "            tr_expr.alias(\"true_range\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 2: rollings sobre columnas materializadas (NO nested windows)\n",
    "    # ============================================================\n",
    "    lf2 = (\n",
    "        lf1.with_columns([\n",
    "            (pl.col(\"ret\").rolling_std(window_size=VOL_WIN, min_samples=VOL_WIN).over(\"symbol\") * 10_000)\n",
    "                .alias(f\"vol_bps_{VOL_WIN}\"),\n",
    "\n",
    "            (pl.col(\"true_range\").rolling_mean(window_size=ATR_WIN, min_samples=ATR_WIN).over(\"symbol\") / pl.col(\"close\") * 10_000)\n",
    "                .alias(f\"atr_bps_{ATR_WIN}\"),\n",
    "\n",
    "            ((pl.col(\"close\") / pl.col(\"close\").shift(MOM_WIN).over(\"symbol\") - 1.0) * 10_000)\n",
    "                .alias(f\"mom_bps_{MOM_WIN}\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    lf3 = (\n",
    "        lf2.with_columns([\n",
    "            (pl.col(f\"mom_bps_{MOM_WIN}\").abs() / (pl.col(f\"vol_bps_{VOL_WIN}\") + EPS))\n",
    "                .alias(f\"mom_eff_{MOM_WIN}\"),\n",
    "\n",
    "            ((pl.col(\"close\") - pl.col(\"close\").shift(ER_WIN).over(\"symbol\")).abs() /\n",
    "             (pl.col(\"abs_diff\").rolling_sum(window_size=ER_WIN, min_samples=ER_WIN).over(\"symbol\") + EPS))\n",
    "                .alias(f\"er_{ER_WIN}\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 3: EMAs y dirección (columnas materializadas => combinaciones seguras)\n",
    "    # ============================================================\n",
    "    lf4 = (\n",
    "        lf3.with_columns([\n",
    "            pl.col(\"close\").ewm_mean(span=EMA_FAST, adjust=False).over(\"symbol\").alias(f\"ema_{EMA_FAST}\"),\n",
    "            pl.col(\"close\").ewm_mean(span=EMA_SLOW, adjust=False).over(\"symbol\").alias(f\"ema_{EMA_SLOW}\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(f\"ema_{EMA_FAST}\") > pl.col(f\"ema_{EMA_SLOW}\")).then(1)\n",
    "              .when(pl.col(f\"ema_{EMA_FAST}\") < pl.col(f\"ema_{EMA_SLOW}\")).then(-1)\n",
    "              .otherwise(0)\n",
    "              .alias(\"trend_dir\"),\n",
    "\n",
    "            (((pl.col(f\"ema_{EMA_FAST}\") - pl.col(f\"ema_{EMA_SLOW}\")).abs() / pl.col(\"close\")) * 10_000)\n",
    "              .alias(\"trend_strength_bps\"),\n",
    "\n",
    "            (((pl.col(f\"ema_{EMA_SLOW}\") / pl.col(f\"ema_{EMA_SLOW}\").shift(SLOPE_WIN).over(\"symbol\")) - 1.0) * 10_000)\n",
    "              .alias(f\"trend_slope_bps_{SLOPE_WIN}\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final select (canónico)\n",
    "    # -----------------------------\n",
    "    lf_feat = (\n",
    "        lf4.select([\n",
    "            \"symbol\",\n",
    "            \"time_utc\",\n",
    "            \"open\", \"high\", \"low\", \"close\",\n",
    "            \"volume\", \"spread\",\n",
    "            \"ret\",\n",
    "            f\"vol_bps_{VOL_WIN}\",\n",
    "            f\"atr_bps_{ATR_WIN}\",\n",
    "            f\"mom_bps_{MOM_WIN}\",\n",
    "            f\"mom_eff_{MOM_WIN}\",\n",
    "            f\"er_{ER_WIN}\",\n",
    "            f\"ema_{EMA_FAST}\",\n",
    "            f\"ema_{EMA_SLOW}\",\n",
    "            \"trend_dir\",\n",
    "            \"trend_strength_bps\",\n",
    "            f\"trend_slope_bps_{SLOPE_WIN}\",\n",
    "        ])\n",
    "        .sort([\"symbol\", \"time_utc\"])\n",
    "    )\n",
    "\n",
    "    df_feat = lf_feat.collect()\n",
    "\n",
    "    # -----------------------------\n",
    "    # QA / sanity\n",
    "    # -----------------------------\n",
    "    mono = (\n",
    "        df_feat.group_by(\"symbol\")\n",
    "        .agg(pl.col(\"time_utc\").is_sorted().alias(\"is_sorted\"))\n",
    "        .sort(\"symbol\")\n",
    "    )\n",
    "    if mono.filter(pl.col(\"is_sorted\") == False).height > 0:\n",
    "        raise RuntimeError(f\"[Celda 05 v2.0.3] ERROR: time_utc no está ordenado en features:\\n{mono}\")\n",
    "\n",
    "    key_cols = [\n",
    "        f\"er_{ER_WIN}\",\n",
    "        f\"vol_bps_{VOL_WIN}\",\n",
    "        f\"atr_bps_{ATR_WIN}\",\n",
    "        f\"mom_bps_{MOM_WIN}\",\n",
    "        f\"ema_{EMA_FAST}\",\n",
    "        f\"ema_{EMA_SLOW}\",\n",
    "    ]\n",
    "    null_report = (\n",
    "        df_feat.group_by(\"symbol\")\n",
    "        .agg([(pl.col(c).is_null().mean() * 100.0).alias(f\"null_pct_{c}\") for c in key_cols])\n",
    "        .sort(\"symbol\")\n",
    "    )\n",
    "\n",
    "    OUT_FEATURES.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_feat.write_parquet(OUT_FEATURES)\n",
    "\n",
    "    qa_cell = None\n",
    "    if QA_REPORT_PATH.exists():\n",
    "        try:\n",
    "            qa_cell = json.loads(QA_REPORT_PATH.read_text(encoding=\"utf-8\")).get(\"cell\")\n",
    "        except Exception:\n",
    "            qa_cell = None\n",
    "\n",
    "    snapshot = {\n",
    "        \"cell\": \"05 v2.0.3\",\n",
    "        \"created_utc\": _now_utc_iso(),\n",
    "        \"qa_cell_detected\": qa_cell,\n",
    "        \"params\": {\n",
    "            \"ER_WIN\": ER_WIN,\n",
    "            \"VOL_WIN\": VOL_WIN,\n",
    "            \"MOM_WIN\": MOM_WIN,\n",
    "            \"ATR_WIN\": ATR_WIN,\n",
    "            \"EMA_FAST\": EMA_FAST,\n",
    "            \"EMA_SLOW\": EMA_SLOW,\n",
    "            \"SLOPE_WIN\": SLOPE_WIN,\n",
    "            \"EXPECTED_BAR_SECONDS\": EXPECTED_BAR_SECONDS,\n",
    "        },\n",
    "        \"schema_cols\": df_feat.columns,\n",
    "        \"symbols\": df_feat.select(\"symbol\").unique().to_series().to_list(),\n",
    "        \"notes\": [\n",
    "            \"Fix nested windows: primero columnas base, luego rollings/EMAs sobre pl.col(...) materializadas.\",\n",
    "            \"Features causales (<=t) alineadas con entrada t+1.\",\n",
    "        ],\n",
    "    }\n",
    "    OUT_SNAPSHOT.write_text(json.dumps(snapshot, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[Celda 05 v2.0.3] OK — features guardados:\")\n",
    "    print(f\"  - {OUT_FEATURES}\")\n",
    "    print(f\"  - {OUT_SNAPSHOT}\")\n",
    "\n",
    "    print(\"\\n--- Features preview ---\")\n",
    "    print(df_feat.head(8))\n",
    "\n",
    "    print(\"\\n--- Monotonicidad time_utc por símbolo ---\")\n",
    "    print(mono)\n",
    "\n",
    "    print(\"\\n--- Null% (warmup) en features clave ---\")\n",
    "    print(null_report)\n",
    "\n",
    "    dist = (\n",
    "        df_feat.group_by([\"symbol\", \"trend_dir\"])\n",
    "        .agg(pl.len().alias(\"n\"))\n",
    "        .sort([\"symbol\", \"trend_dir\"])\n",
    "    )\n",
    "    print(\"\\n--- Distribución trend_dir (sanity) ---\")\n",
    "    print(dist)\n",
    "\n",
    "    print(\"\\n[Celda 05 v2.0.3] OK — Se permite avanzar a Celda 06 (Regime Gate ON/OFF + hysteresis).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
