{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae2a4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe: True\n",
      "TamaÃ±o aproximado: 857.50 MB\n",
      "\n",
      "--- HEAD (primeras 5 lÃ­neas) ---\n",
      "1: PARQUET INVENTORY (GLOBAL)\n",
      "2: ================================================================================\n",
      "3: ROOT: C:\\Quant\\MT5_Data_Extraction\n",
      "4: Generated: 20251207_142248\n",
      "5: Total parquets: 1327124\n",
      "\n",
      "--- TAIL (Ãºltimas ~5 lÃ­neas) ---\n",
      "1:     - ok_spreadpct: Boolean\n",
      "2:     - ok_all: Boolean\n",
      "3: \n",
      "4: ================================================================================\n",
      "5: FINISHED. Files with errors: 0\n",
      "\n",
      "Conteo de delimitadores en lÃ­nea candidata: {',': 0, '\\t': 0, ';': 0, '|': 0}\n",
      "âš ï¸ No se detecta delimitador claro. PodrÃ­a ser JSONL u otro formato.\n",
      "\n",
      "âœ… Celda 0 OK: inspecciÃ³n sin reventar RAM.\n"
     ]
    }
   ],
   "source": [
    "# Celda 0: InspecciÃ³n ultra-ligera del TXT gigante\n",
    "# Objetivo:\n",
    "#   - Ver tamaÃ±o real\n",
    "#   - Leer primeras/Ãºltimas lÃ­neas sin cargar todo\n",
    "#   - Inferir delimitador probable (coma, tab, ;, |)\n",
    "#   - Detectar si parece CSV/TSV o JSONL\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "INVENTORY_PATH = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\parquet_inventory_global_20251207_142248.txt\"\n",
    "\n",
    "p = Path(INVENTORY_PATH)\n",
    "print(\"Existe:\", p.exists())\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro el archivo: {p}\")\n",
    "\n",
    "size_mb = p.stat().st_size / (1024**2)\n",
    "print(f\"TamaÃ±o aproximado: {size_mb:,.2f} MB\")\n",
    "\n",
    "# Leer primeras lÃ­neas\n",
    "print(\"\\n--- HEAD (primeras 5 lÃ­neas) ---\")\n",
    "with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    head_lines = [next(f) for _ in range(5)]\n",
    "for i, line in enumerate(head_lines, 1):\n",
    "    print(f\"{i}: {line[:300].rstrip()}\")\n",
    "\n",
    "# Leer Ãºltimas lÃ­neas sin cargar todo (seek desde el final)\n",
    "print(\"\\n--- TAIL (Ãºltimas ~5 lÃ­neas) ---\")\n",
    "with open(p, \"rb\") as f:\n",
    "    f.seek(0, os.SEEK_END)\n",
    "    end = f.tell()\n",
    "    block = 4096\n",
    "    data = b\"\"\n",
    "    while end > 0 and data.count(b\"\\n\") < 10:\n",
    "        read_size = min(block, end)\n",
    "        end -= read_size\n",
    "        f.seek(end)\n",
    "        data = f.read(read_size) + data\n",
    "tail_text = data.decode(\"utf-8\", errors=\"replace\").splitlines()[-5:]\n",
    "for i, line in enumerate(tail_text, 1):\n",
    "    print(f\"{i}: {line[:300].rstrip()}\")\n",
    "\n",
    "# HeurÃ­stica de delimitador usando la primera lÃ­nea \"larga\"\n",
    "candidate = max(head_lines, key=len)\n",
    "delims = [\",\", \"\\t\", \";\", \"|\"]\n",
    "counts = {d: candidate.count(d) for d in delims}\n",
    "print(\"\\nConteo de delimitadores en lÃ­nea candidata:\", counts)\n",
    "\n",
    "best_delim = max(counts, key=counts.get)\n",
    "if counts[best_delim] == 0:\n",
    "    print(\"âš ï¸ No se detecta delimitador claro. PodrÃ­a ser JSONL u otro formato.\")\n",
    "else:\n",
    "    print(f\"Delimitador probable: {repr(best_delim)}\")\n",
    "\n",
    "print(\"\\nâœ… Celda 0 OK: inspecciÃ³n sin reventar RAM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528df2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando leer 50k filas para inferir esquema...\n"
     ]
    },
    {
     "ename": "ComputeError",
     "evalue": "found more fields than defined in 'Schema'\n\nConsider setting 'truncate_ragged_lines=True'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m OUT_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIntentando leer 50k filas para inferir esquema...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mINVENTORY_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDELIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mColumnas detectadas:\u001b[39m\u001b[33m\"\u001b[39m, df.columns)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShape sample:\u001b[39m\u001b[33m\"\u001b[39m, df.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\io\\csv\\functions.py:549\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[32m    543\u001b[39m         source,\n\u001b[32m    544\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         storage_options=storage_options,\n\u001b[32m    548\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m         df = \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8-lossy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m            \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m            \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Quant\\MT5_Data_Extraction\\venv1\\Lib\\site-packages\\polars\\io\\csv\\functions.py:697\u001b[39m, in \u001b[36m_read_csv_impl\u001b[39m\u001b[34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    693\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    695\u001b[39m projection, columns = parse_columns_arg(columns)\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m pydf = \u001b[43mPyDataFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[31mComputeError\u001b[39m: found more fields than defined in 'Schema'\n\nConsider setting 'truncate_ragged_lines=True'."
     ]
    }
   ],
   "source": [
    "# Celda 2: Convertir el inventario a Parquet ZSTD + versiÃ³n lite\n",
    "# Objetivo:\n",
    "#   - Leer en modo eficiente\n",
    "#   - Detectar columnas\n",
    "#   - Guardar:\n",
    "#       * inventory_sample.parquet (muestra grande)\n",
    "#       * inventory_lite.parquet (solo columnas clave si existen)\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "INVENTORY_PATH = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\parquet_inventory_global_20251207_142248.txt\"\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\")\n",
    "\n",
    "# Si en Celda 0 saliÃ³ otro delimitador, cÃ¡mbialo aquÃ­:\n",
    "DELIM = \",\"  # o \"\\t\" o \";\" o \"|\"\n",
    "\n",
    "OUT_SAMPLE_PARQUET = OUT_DIR / \"inventory_sample_50k.parquet\"\n",
    "OUT_LITE_PARQUET   = OUT_DIR / \"inventory_lite.parquet\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Intentando leer 50k filas para inferir esquema...\")\n",
    "df = pl.read_csv(\n",
    "    INVENTORY_PATH,\n",
    "    separator=DELIM,\n",
    "    infer_schema_length=5000,\n",
    "    n_rows=50_000,\n",
    "    ignore_errors=True,\n",
    ")\n",
    "\n",
    "print(\"Columnas detectadas:\", df.columns)\n",
    "print(\"Shape sample:\", df.shape)\n",
    "\n",
    "# Guardar sample parquet comprimido\n",
    "df.write_parquet(str(OUT_SAMPLE_PARQUET), compression=\"zstd\")\n",
    "print(\"âœ… Sample parquet:\", OUT_SAMPLE_PARQUET, \"| MB:\", OUT_SAMPLE_PARQUET.stat().st_size / (1024**2))\n",
    "\n",
    "# HeurÃ­stica de columnas clave tÃ­picas de inventarios\n",
    "lower_map = {c.lower(): c for c in df.columns}\n",
    "preferred = [\n",
    "    \"file_path\", \"path\", \"parquet_path\",\n",
    "    \"symbol\", \"dataset\", \"timeframe\",\n",
    "    \"start\", \"end\", \"start_utc\", \"end_utc\",\n",
    "    \"rows\", \"n_rows\", \"cols\", \"n_cols\",\n",
    "    \"size\", \"size_bytes\", \"size_mb\",\n",
    "    \"hash\", \"md5\", \"sha1\",\n",
    "]\n",
    "\n",
    "lite_cols = []\n",
    "for key in preferred:\n",
    "    if key in lower_map:\n",
    "        lite_cols.append(lower_map[key])\n",
    "\n",
    "# Si no encontramos columnas estÃ¡ndar, al menos guardamos primeras 8\n",
    "if not lite_cols:\n",
    "    lite_cols = df.columns[: min(8, len(df.columns))]\n",
    "\n",
    "df_lite = df.select(lite_cols).unique()\n",
    "\n",
    "df_lite.write_parquet(str(OUT_LITE_PARQUET), compression=\"zstd\")\n",
    "print(\"âœ… Lite parquet:\", OUT_LITE_PARQUET, \"| MB:\", OUT_LITE_PARQUET.stat().st_size / (1024**2))\n",
    "print(\"Lite cols:\", lite_cols)\n",
    "\n",
    "print(\"\\nâœ… Celda 2 OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partiendo archivo...\n",
      "âœ… Chunk 1 creado\n",
      "âœ… Chunk 2 creado\n",
      "âœ… Chunk 3 creado\n",
      "âœ… Chunk 4 creado\n",
      "âœ… Chunk 5 creado\n",
      "âœ… Chunk 6 creado\n",
      "âœ… Chunk 7 creado\n",
      "âœ… Chunk 8 creado\n",
      "âœ… Chunk 9 creado\n",
      "âœ… Chunk 10 creado\n",
      "âœ… Chunk 11 creado\n",
      "âœ… Chunk 12 creado\n",
      "âœ… Chunk 13 creado\n",
      "âœ… Chunk 14 creado\n",
      "âœ… Chunk 15 creado\n",
      "âœ… Chunk 16 creado\n",
      "Chunks en: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_chunks\n",
      "\n",
      "âœ… Celda 3 OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Partir TXT gigante por tamaÃ±o aproximado\n",
    "# Objetivo:\n",
    "#   - Crear chunks para inspecciÃ³n o archivado\n",
    "#   - No toca lÃ³gica estadÃ­stica, solo gestiÃ³n de archivo\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "INVENTORY_PATH = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\parquet_inventory_global_20251207_142248.txt\"\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_chunks\")\n",
    "\n",
    "CHUNK_MB = 50\n",
    "\n",
    "p = Path(INVENTORY_PATH)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chunk_bytes = CHUNK_MB * 1024 * 1024\n",
    "chunk_idx = 1\n",
    "written = 0\n",
    "\n",
    "print(\"Partiendo archivo...\")\n",
    "with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as fin:\n",
    "    fout = open(OUT_DIR / f\"inventory_part_{chunk_idx:03d}.txt\", \"w\", encoding=\"utf-8\")\n",
    "    for line in fin:\n",
    "        fout.write(line)\n",
    "        written += len(line.encode(\"utf-8\", errors=\"replace\"))\n",
    "\n",
    "        if written >= chunk_bytes:\n",
    "            fout.close()\n",
    "            print(f\"âœ… Chunk {chunk_idx} creado\")\n",
    "            chunk_idx += 1\n",
    "            written = 0\n",
    "            fout = open(OUT_DIR / f\"inventory_part_{chunk_idx:03d}.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "print(\"Chunks en:\", OUT_DIR)\n",
    "print(\"\\nâœ… Celda 3 OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c5f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe: True\n",
      "TamaÃ±o MB: 857.4952621459961\n",
      "Head lines capturadas: 200\n",
      "Tail lines capturadas: 200\n",
      "LÃ­neas que contienen '.parquet': 2,654,248\n",
      "âœ… Creado: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_paths.parquet | MB: 7.420282363891602\n",
      "âœ… Creado: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_head_tail.parquet | MB: 0.0023736953735351562\n",
      "\n",
      "ğŸ‘‰ Sube estos 2 archivos aquÃ­:\n",
      "   1) inventory_paths.parquet\n",
      "   2) inventory_head_tail.parquet\n",
      "\n",
      "âœ… Celda A OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda A: Extraer lÃ­neas crÃ­ticas del TXT gigante a PARQUET (subible)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Evitar el bloqueo por \"demasiado contenido de texto\"\n",
    "#   - Extraer:\n",
    "#       1) Todas las lÃ­neas que contengan \".parquet\"\n",
    "#       2) Un bloque pequeÃ±o del encabezado\n",
    "#       3) Un bloque pequeÃ±o del cierre\n",
    "#   - Guardar en:\n",
    "#       * inventory_paths.parquet\n",
    "#       * inventory_head_tail.parquet\n",
    "#\n",
    "# Resultado esperado:\n",
    "#   - Archivos binarios livianos, fÃ¡ciles de subir aquÃ­.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "INVENTORY_PATH = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\parquet_inventory_global_20251207_142248.txt\"\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "p = Path(INVENTORY_PATH)\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(f\"No encuentro: {p}\")\n",
    "\n",
    "print(\"Existe:\", p.exists())\n",
    "print(\"TamaÃ±o MB:\", p.stat().st_size / (1024**2))\n",
    "\n",
    "# --- Helper: leer head ---\n",
    "HEAD_N = 200\n",
    "head_lines = []\n",
    "with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    for _ in range(HEAD_N):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        head_lines.append(line.rstrip(\"\\n\"))\n",
    "\n",
    "print(f\"Head lines capturadas: {len(head_lines)}\")\n",
    "\n",
    "# --- Helper: leer tail sin cargar todo ---\n",
    "def read_tail_lines(path: Path, approx_bytes=200_000):\n",
    "    # lee un bloque al final y corta a lÃ­neas\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        end = f.tell()\n",
    "        start = max(0, end - approx_bytes)\n",
    "        f.seek(start)\n",
    "        data = f.read(end - start)\n",
    "    text = data.decode(\"utf-8\", errors=\"replace\")\n",
    "    lines = text.splitlines()\n",
    "    # tomamos Ãºltimos 200 o menos\n",
    "    return lines[-200:] if len(lines) > 200 else lines\n",
    "\n",
    "tail_lines = read_tail_lines(p)\n",
    "print(f\"Tail lines capturadas: {len(tail_lines)}\")\n",
    "\n",
    "# --- Extraer lÃ­neas con \".parquet\" ---\n",
    "parquet_line_regex = re.compile(r\"\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "parquet_lines = []\n",
    "with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    for line in f:\n",
    "        if parquet_line_regex.search(line):\n",
    "            parquet_lines.append(line.rstrip(\"\\n\"))\n",
    "\n",
    "print(f\"LÃ­neas que contienen '.parquet': {len(parquet_lines):,}\")\n",
    "\n",
    "# --- Guardar a Parquet binario ---\n",
    "paths_df = pl.DataFrame({\"line\": parquet_lines})\n",
    "head_tail_df = pl.DataFrame({\n",
    "    \"section\": [\"head\"] * len(head_lines) + [\"tail\"] * len(tail_lines),\n",
    "    \"line\": head_lines + tail_lines\n",
    "})\n",
    "\n",
    "OUT_PATHS = OUT_DIR / \"inventory_paths.parquet\"\n",
    "OUT_HEAD_TAIL = OUT_DIR / \"inventory_head_tail.parquet\"\n",
    "\n",
    "paths_df.write_parquet(str(OUT_PATHS), compression=\"zstd\")\n",
    "head_tail_df.write_parquet(str(OUT_HEAD_TAIL), compression=\"zstd\")\n",
    "\n",
    "print(\"âœ… Creado:\", OUT_PATHS, \"| MB:\", OUT_PATHS.stat().st_size / (1024**2))\n",
    "print(\"âœ… Creado:\", OUT_HEAD_TAIL, \"| MB:\", OUT_HEAD_TAIL.stat().st_size / (1024**2))\n",
    "\n",
    "print(\"\\nğŸ‘‰ Sube estos 2 archivos aquÃ­:\")\n",
    "print(\"   1) inventory_paths.parquet\")\n",
    "print(\"   2) inventory_head_tail.parquet\")\n",
    "print(\"\\nâœ… Celda A OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43573e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe inventory_paths.parquet: True\n",
      "Leyendo inventory_paths.parquet...\n",
      "Rows lÃ­neas: 2654248\n",
      "Abs detectados: 0\n",
      "Rel detectados: 1,327,124\n",
      "Paths finales vÃ¡lidos: 1,327,124\n",
      "\n",
      "Ejemplos rel_path (primeros 5):\n",
      "shape: (5, 1)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ rel_path                        â”‚\n",
      "â”‚ ---                             â”‚\n",
      "â”‚ str                             â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ data\\bulk_data\\m5_raw\\symbol=Aâ€¦ â”‚\n",
      "â”‚ data\\bulk_data\\m5_raw\\symbol=Aâ€¦ â”‚\n",
      "â”‚ data\\bulk_data\\m5_raw\\symbol=Aâ€¦ â”‚\n",
      "â”‚ data\\bulk_data\\m5_raw\\symbol=Aâ€¦ â”‚\n",
      "â”‚ data\\bulk_data\\m5_raw\\symbol=Aâ€¦ â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Paths extraÃ­dos (bruto): 1327124\n",
      "Paths Ãºnicos: 1327124\n",
      "\n",
      "âœ… OUTPUT lite   â†’ C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_lite.parquet | rows=1327124\n",
      "âœ… OUTPUT folder â†’ C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_folder_stats.parquet | rows=48803\n",
      "âœ… OUTPUT root   â†’ C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_root_stats.parquet | rows=5\n",
      "\n",
      "Top 10 carpetas por #parquets:\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ folder                          â”† parquet_count â”‚\n",
      "â”‚ ---                             â”† ---           â”‚\n",
      "â”‚ str                             â”† u32           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â”‚ C:\\Quant\\MT5_Data_Extraction\\dâ€¦ â”† 189           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Top buckets bajo ROOT:\n",
      "shape: (5, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ root_bucket              â”† parquet_count â”‚\n",
      "â”‚ ---                      â”† ---           â”‚\n",
      "â”‚ str                      â”† u32           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ data                     â”† 1326899       â”‚\n",
      "â”‚ venv1                    â”† 210           â”‚\n",
      "â”‚ outputs                  â”† 13            â”‚\n",
      "â”‚ respaldo                 â”† 1             â”‚\n",
      "â”‚ filtered_symbols.parquet â”† 1             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… Celda C v3 OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda C v3: ConstrucciÃ³n profesional del inventario LITE (robusta + FIX regex ROOT)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Tomar \"inventory_paths.parquet\" (lÃ­neas con .parquet)\n",
    "#   - Extraer paths reales:\n",
    "#       * abs_path: C:\\...\\*.parquet\n",
    "#       * rel_path: prioriza formato \"REL : xxx.parquet\"\n",
    "#   - Normalizar, deduplicar\n",
    "#   - Generar:\n",
    "#       1) inventory_lite.parquet\n",
    "#       2) inventory_folder_stats.parquet\n",
    "#       3) inventory_root_stats.parquet\n",
    "#\n",
    "# Fix crÃ­tico:\n",
    "#   - Polars str.replace usa REGEX -> hay que escapar ROOT (Windows backslashes)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "IN_PATHS = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_paths.parquet\"\n",
    "ROOT = r\"C:\\Quant\\MT5_Data_Extraction\"\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_LITE   = OUT_DIR / \"inventory_lite.parquet\"\n",
    "OUT_FSTATS = OUT_DIR / \"inventory_folder_stats.parquet\"\n",
    "OUT_RSTATS = OUT_DIR / \"inventory_root_stats.parquet\"\n",
    "\n",
    "p = Path(IN_PATHS)\n",
    "print(\"Existe inventory_paths.parquet:\", p.exists())\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"No encuentro inventory_paths.parquet. Revisa la ruta.\")\n",
    "\n",
    "print(\"Leyendo inventory_paths.parquet...\")\n",
    "df_lines = pl.read_parquet(IN_PATHS)\n",
    "\n",
    "if \"line\" not in df_lines.columns:\n",
    "    raise RuntimeError(\"inventory_paths.parquet no tiene columna 'line'.\")\n",
    "\n",
    "print(\"Rows lÃ­neas:\", df_lines.height)\n",
    "\n",
    "# NormalizaciÃ³n de lÃ­neas\n",
    "df0 = df_lines.with_columns(\n",
    "    pl.col(\"line\")\n",
    "      .cast(pl.Utf8)\n",
    "      .str.replace_all(\"/\", \"\\\\\")\n",
    "      .alias(\"line_norm\")\n",
    ")\n",
    "\n",
    "# Patrones\n",
    "abs_pattern = r'([A-Za-z]:\\\\[^\"\\s]+?\\.parquet)'\n",
    "\n",
    "# PRIORIDAD: lÃ­neas del inventario parecen tener formato:\n",
    "#   \"REL : data\\bulk_data\\m5_raw\\symbol=...\\part=YYYYMMDD.parquet\"\n",
    "rel_pattern = r'REL\\s*:\\s*([^\\r\\n]+?\\.parquet)'\n",
    "\n",
    "# Fallback genÃ©rico:\n",
    "any_parquet_pattern = r'([^\"\\s]+?\\.parquet)'\n",
    "\n",
    "df1 = df0.with_columns([\n",
    "    pl.col(\"line_norm\").str.extract(abs_pattern, 1).alias(\"abs_path\"),\n",
    "    pl.col(\"line_norm\").str.extract(rel_pattern, 1).alias(\"rel_path_fmt\"),\n",
    "    pl.col(\"line_norm\").str.extract(any_parquet_pattern, 1).alias(\"any_path\"),\n",
    "])\n",
    "\n",
    "# Construir rel_path:\n",
    "#   - si rel_path_fmt existe -> Ãºsalo\n",
    "#   - si no, usar any_path solo si parece path real (contiene \"\\\")\n",
    "df1 = df1.with_columns(\n",
    "    pl.when(pl.col(\"rel_path_fmt\").is_not_null())\n",
    "      .then(pl.col(\"rel_path_fmt\"))\n",
    "      .when(pl.col(\"abs_path\").is_null() & pl.col(\"any_path\").str.contains(r\"\\\\\"))\n",
    "      .then(pl.col(\"any_path\"))\n",
    "      .otherwise(None)\n",
    "      .alias(\"rel_path\")\n",
    ")\n",
    "\n",
    "# Normalizar ROOT\n",
    "root_norm = ROOT.replace(\"/\", \"\\\\\").rstrip(\"\\\\\") + \"\\\\\"\n",
    "root_pattern = re.escape(root_norm)  # âœ… FIX regex\n",
    "\n",
    "# Construir parquet_path final:\n",
    "df_paths = (\n",
    "    df1\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"abs_path\").is_not_null())\n",
    "          .then(pl.col(\"abs_path\"))\n",
    "          .when(pl.col(\"rel_path\").is_not_null())\n",
    "          .then(\n",
    "              pl.concat_str([\n",
    "                  pl.lit(root_norm),\n",
    "                  pl.col(\"rel_path\").str.replace_all(r\"^[\\\\]+\", \"\")\n",
    "              ])\n",
    "          )\n",
    "          .otherwise(None)\n",
    "          .alias(\"parquet_path\")\n",
    "    )\n",
    "    .select([\"line_norm\", \"abs_path\", \"rel_path\", \"parquet_path\"])\n",
    ")\n",
    "\n",
    "# DiagnÃ³stico duro\n",
    "abs_count   = df_paths.filter(pl.col(\"abs_path\").is_not_null()).height\n",
    "rel_count   = df_paths.filter(pl.col(\"rel_path\").is_not_null()).height\n",
    "final_count = df_paths.filter(pl.col(\"parquet_path\").is_not_null()).height\n",
    "\n",
    "print(f\"Abs detectados: {abs_count:,}\")\n",
    "print(f\"Rel detectados: {rel_count:,}\")\n",
    "print(f\"Paths finales vÃ¡lidos: {final_count:,}\")\n",
    "\n",
    "print(\"\\nEjemplos rel_path (primeros 5):\")\n",
    "print(df_paths.filter(pl.col(\"rel_path\").is_not_null()).select(\"rel_path\").head(5))\n",
    "\n",
    "# Quedarnos con paths vÃ¡lidos\n",
    "df_valid = df_paths.select(\"parquet_path\").drop_nulls()\n",
    "print(\"Paths extraÃ­dos (bruto):\", df_valid.height)\n",
    "\n",
    "# DeduplicaciÃ³n\n",
    "df_lite = df_valid.unique()\n",
    "print(\"Paths Ãºnicos:\", df_lite.height)\n",
    "\n",
    "# Carpeta\n",
    "df_lite = df_lite.with_columns(\n",
    "    pl.col(\"parquet_path\")\n",
    "      .str.replace(r\"\\\\[^\\\\]+\\.parquet$\", \"\")\n",
    "      .alias(\"folder\")\n",
    ")\n",
    "\n",
    "# Stats por carpeta\n",
    "folder_stats = (\n",
    "    df_lite\n",
    "    .group_by(\"folder\")\n",
    "    .agg(pl.len().alias(\"parquet_count\"))\n",
    "    .sort(\"parquet_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Root bucket (usa replace con ROOT escapado)\n",
    "df_lite = df_lite.with_columns(\n",
    "    pl.when(pl.col(\"parquet_path\").str.starts_with(root_norm))\n",
    "      .then(\n",
    "          pl.col(\"parquet_path\")\n",
    "            .str.replace(root_pattern, \"\")  # âœ… FIX: patrÃ³n seguro\n",
    "            .str.split(\"\\\\\")\n",
    "            .list.first()\n",
    "      )\n",
    "      .otherwise(pl.lit(\"OUTSIDE_ROOT\"))\n",
    "      .alias(\"root_bucket\")\n",
    ")\n",
    "\n",
    "root_stats = (\n",
    "    df_lite\n",
    "    .group_by(\"root_bucket\")\n",
    "    .agg(pl.len().alias(\"parquet_count\"))\n",
    "    .sort(\"parquet_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Guardar outputs\n",
    "df_lite.select([\"parquet_path\", \"folder\", \"root_bucket\"]).write_parquet(str(OUT_LITE), compression=\"zstd\")\n",
    "folder_stats.write_parquet(str(OUT_FSTATS), compression=\"zstd\")\n",
    "root_stats.write_parquet(str(OUT_RSTATS), compression=\"zstd\")\n",
    "\n",
    "print(f\"\\nâœ… OUTPUT lite   â†’ {OUT_LITE} | rows={df_lite.height}\")\n",
    "print(f\"âœ… OUTPUT folder â†’ {OUT_FSTATS} | rows={folder_stats.height}\")\n",
    "print(f\"âœ… OUTPUT root   â†’ {OUT_RSTATS} | rows={root_stats.height}\")\n",
    "\n",
    "print(\"\\nTop 10 carpetas por #parquets:\")\n",
    "print(folder_stats.head(10))\n",
    "\n",
    "print(\"\\nTop buckets bajo ROOT:\")\n",
    "print(root_stats)\n",
    "\n",
    "print(\"\\nâœ… Celda C v3 OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8993e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Creado: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_folder_stats_top200.csv | KB: 23.9326171875\n",
      "âœ… Creado: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_inventory_mini_report.txt | KB: 0.3544921875\n",
      "\n",
      "ğŸ‘‰ Sube estos dos archivos aquÃ­:\n",
      "   1) inventory_folder_stats_top200.csv\n",
      "   2) inventory_inventory_mini_report.txt\n",
      "\n",
      "âœ… Celda D OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda D: Exportar resumen ultra pequeÃ±o para subir al chat\n",
    "# -----------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Crear CSV muy liviano con las 200 carpetas mÃ¡s grandes\n",
    "#   - Crear TXT diagnÃ³stico mÃ­nimo\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\")\n",
    "\n",
    "IN_FSTATS = OUT_DIR / \"inventory_folder_stats.parquet\"\n",
    "IN_RSTATS = OUT_DIR / \"inventory_root_stats.parquet\"\n",
    "\n",
    "OUT_CSV_TOP200 = OUT_DIR / \"inventory_folder_stats_top200.csv\"\n",
    "OUT_TXT_MINI = OUT_DIR / \"inventory_inventory_mini_report.txt\"\n",
    "\n",
    "if not IN_FSTATS.exists():\n",
    "    raise FileNotFoundError(\"No encuentro inventory_folder_stats.parquet. Ejecuta Celda C primero.\")\n",
    "\n",
    "folder_stats = pl.read_parquet(str(IN_FSTATS))\n",
    "root_stats = pl.read_parquet(str(IN_RSTATS)) if IN_RSTATS.exists() else None\n",
    "\n",
    "top200 = folder_stats.head(200)\n",
    "top200.write_csv(str(OUT_CSV_TOP200))\n",
    "\n",
    "lines = []\n",
    "lines.append(\"INVENTORY MINI REPORT\")\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(f\"folders_total = {folder_stats.height}\")\n",
    "lines.append(f\"top200_csv    = {str(OUT_CSV_TOP200)}\")\n",
    "\n",
    "if root_stats is not None:\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"ROOT BUCKETS:\")\n",
    "    for row in root_stats.iter_rows(named=True):\n",
    "        lines.append(f\"  - {row['root_bucket']}: {row['parquet_count']}\")\n",
    "\n",
    "OUT_TXT_MINI.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Creado:\", OUT_CSV_TOP200, \"| KB:\", OUT_CSV_TOP200.stat().st_size / 1024)\n",
    "print(\"âœ… Creado:\", OUT_TXT_MINI, \"| KB:\", OUT_TXT_MINI.stat().st_size / 1024)\n",
    "\n",
    "print(\"\\nğŸ‘‰ Sube estos dos archivos aquÃ­:\")\n",
    "print(\"   1) inventory_folder_stats_top200.csv\")\n",
    "print(\"   2) inventory_inventory_mini_report.txt\")\n",
    "\n",
    "print(\"\\nâœ… Celda D OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b51849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe inventory_lite.parquet: True\n",
      "Rows inventory_lite: 1327124\n",
      "\n",
      "ROOT level1 stats:\n",
      "shape: (5, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ level1                   â”† parquet_count â”‚\n",
      "â”‚ ---                      â”† ---           â”‚\n",
      "â”‚ str                      â”† u32           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ data                     â”† 1326899       â”‚\n",
      "â”‚ venv1                    â”† 210           â”‚\n",
      "â”‚ outputs                  â”† 13            â”‚\n",
      "â”‚ respaldo                 â”† 1             â”‚\n",
      "â”‚ filtered_symbols.parquet â”† 1             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… OUTPUT: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_data_level1_stats.parquet\n",
      "âœ… OUTPUT: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_data_level2_stats.parquet\n",
      "âœ… CSV TOP50: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_data_level1_top50.csv\n",
      "âœ… CSV TOP50: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_data_level2_top50.csv\n",
      "\n",
      "Top 10 data_level1:\n",
      "shape: (6, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ data_level1       â”† parquet_count â”‚\n",
      "â”‚ ---               â”† ---           â”‚\n",
      "â”‚ str               â”† u32           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ restore           â”† 1073448       â”‚\n",
      "â”‚ bulk_data         â”† 121457        â”‚\n",
      "â”‚ historical_data   â”† 103917        â”‚\n",
      "â”‚ processed_data    â”† 28054         â”‚\n",
      "â”‚ metadata          â”† 22            â”‚\n",
      "â”‚ ea_params.parquet â”† 1             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Top 10 data_level2:\n",
      "shape: (10, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ data_level2                     â”† parquet_count â”‚\n",
      "â”‚ ---                             â”† ---           â”‚\n",
      "â”‚ str                             â”† u32           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ restore\\restore_20251120_11422â€¦ â”† 140765        â”‚\n",
      "â”‚ restore\\restore_20251202_23225â€¦ â”† 131993        â”‚\n",
      "â”‚ restore\\restore_20251201_22084â€¦ â”† 130298        â”‚\n",
      "â”‚ restore\\restore_20251121_16133â€¦ â”† 128530        â”‚\n",
      "â”‚ restore\\restore_20251120_05260â€¦ â”† 128416        â”‚\n",
      "â”‚ restore\\restore_20251119_20385â€¦ â”† 126334        â”‚\n",
      "â”‚ restore\\restore_20251121_10314â€¦ â”† 125871        â”‚\n",
      "â”‚ bulk_data\\m5_raw                â”† 121457        â”‚\n",
      "â”‚ historical_data\\m5_clean        â”† 103917        â”‚\n",
      "â”‚ restore\\restore_20251120_22471â€¦ â”† 98604         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… Celda F OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda F: Breakdown profesional de inventario bajo ROOT/data\n",
    "# -----------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Usar inventory_lite.parquet\n",
    "#   - Calcular conteos por:\n",
    "#       * nivel1 bajo ROOT (root_bucket ya lo tienes, pero lo recalculamos seguro)\n",
    "#       * nivel1 y nivel2 especÃ­ficamente dentro de \"data\"\n",
    "#   - Generar CSV pequeÃ±os top50 para subir\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import re\n",
    "\n",
    "IN_LITE = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_lite.parquet\"\n",
    "ROOT = r\"C:\\Quant\\MT5_Data_Extraction\"\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DATA_L1 = OUT_DIR / \"inventory_data_level1_stats.parquet\"\n",
    "OUT_DATA_L2 = OUT_DIR / \"inventory_data_level2_stats.parquet\"\n",
    "OUT_DATA_L1_CSV = OUT_DIR / \"inventory_data_level1_top50.csv\"\n",
    "OUT_DATA_L2_CSV = OUT_DIR / \"inventory_data_level2_top50.csv\"\n",
    "\n",
    "p = Path(IN_LITE)\n",
    "print(\"Existe inventory_lite.parquet:\", p.exists())\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"No encuentro inventory_lite.parquet. Ejecuta Celda C v3 primero.\")\n",
    "\n",
    "df = pl.read_parquet(IN_LITE)\n",
    "print(\"Rows inventory_lite:\", df.height)\n",
    "\n",
    "root_norm = ROOT.replace(\"/\", \"\\\\\").rstrip(\"\\\\\") + \"\\\\\"\n",
    "root_pattern = re.escape(root_norm)\n",
    "\n",
    "# Extraer path relativo bajo ROOT\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"parquet_path\").str.starts_with(root_norm))\n",
    "      .then(pl.col(\"parquet_path\").str.replace(root_pattern, \"\"))\n",
    "      .otherwise(None)\n",
    "      .alias(\"rel_under_root\")\n",
    ")\n",
    "\n",
    "# Nivel 1 bajo ROOT\n",
    "df = df.with_columns(\n",
    "    pl.col(\"rel_under_root\")\n",
    "      .str.split(\"\\\\\")\n",
    "      .list.first()\n",
    "      .alias(\"level1\")\n",
    ")\n",
    "\n",
    "root_l1_stats = (\n",
    "    df.filter(pl.col(\"level1\").is_not_null())\n",
    "      .group_by(\"level1\")\n",
    "      .agg(pl.len().alias(\"parquet_count\"))\n",
    "      .sort(\"parquet_count\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"\\nROOT level1 stats:\")\n",
    "print(root_l1_stats)\n",
    "\n",
    "# Filtrar solo data\n",
    "df_data = df.filter(pl.col(\"level1\") == \"data\").with_columns(\n",
    "    pl.col(\"rel_under_root\")\n",
    "      .str.replace(r\"^data\\\\\", \"\")\n",
    "      .alias(\"rel_under_data\")\n",
    ")\n",
    "\n",
    "# Nivel 1 dentro de data\n",
    "df_data = df_data.with_columns(\n",
    "    pl.col(\"rel_under_data\").str.split(\"\\\\\").list.first().alias(\"data_level1\")\n",
    ")\n",
    "\n",
    "data_l1_stats = (\n",
    "    df_data.group_by(\"data_level1\")\n",
    "           .agg(pl.len().alias(\"parquet_count\"))\n",
    "           .sort(\"parquet_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Nivel 2 dentro de data\n",
    "df_data = df_data.with_columns(\n",
    "    pl.when(pl.col(\"rel_under_data\").str.contains(r\"\\\\\"))\n",
    "      .then(\n",
    "          pl.col(\"rel_under_data\").str.split(\"\\\\\").list.slice(0, 2).list.join(\"\\\\\")\n",
    "      )\n",
    "      .otherwise(pl.col(\"data_level1\"))\n",
    "      .alias(\"data_level2\")\n",
    ")\n",
    "\n",
    "data_l2_stats = (\n",
    "    df_data.group_by(\"data_level2\")\n",
    "           .agg(pl.len().alias(\"parquet_count\"))\n",
    "           .sort(\"parquet_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "data_l1_stats.write_parquet(str(OUT_DATA_L1), compression=\"zstd\")\n",
    "data_l2_stats.write_parquet(str(OUT_DATA_L2), compression=\"zstd\")\n",
    "\n",
    "# CSV top50 subibles\n",
    "data_l1_stats.head(50).write_csv(str(OUT_DATA_L1_CSV))\n",
    "data_l2_stats.head(50).write_csv(str(OUT_DATA_L2_CSV))\n",
    "\n",
    "print(\"\\nâœ… OUTPUT:\", OUT_DATA_L1)\n",
    "print(\"âœ… OUTPUT:\", OUT_DATA_L2)\n",
    "print(\"âœ… CSV TOP50:\", OUT_DATA_L1_CSV)\n",
    "print(\"âœ… CSV TOP50:\", OUT_DATA_L2_CSV)\n",
    "\n",
    "print(\"\\nTop 10 data_level1:\")\n",
    "print(data_l1_stats.head(10))\n",
    "\n",
    "print(\"\\nTop 10 data_level2:\")\n",
    "print(data_l2_stats.head(10))\n",
    "\n",
    "print(\"\\nâœ… Celda F OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e078ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 1327124\n",
      "Rows data-only: 1327109\n",
      "âœ… OUTPUT: C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_lite_data_only.parquet\n",
      "\n",
      "âœ… Celda G OK.\n"
     ]
    }
   ],
   "source": [
    "# Celda G: Construir inventario limpio SOLO de data (trading-only)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Eliminar ruido accidental del inventario global:\n",
    "#       venv1, outputs, respaldo, archivos en raÃ­z\n",
    "#   - Guardar:\n",
    "#       inventory_lite_data_only.parquet\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IN_LITE = r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\\inventory_lite.parquet\"\n",
    "OUT_DIR = Path(r\"C:\\Quant\\MT5_Data_Extraction\\diagnostics_global\\inventory_extracts\")\n",
    "OUT_DATA_ONLY = OUT_DIR / \"inventory_lite_data_only.parquet\"\n",
    "\n",
    "df = pl.read_parquet(IN_LITE)\n",
    "\n",
    "df_data = df.filter(\n",
    "    pl.col(\"parquet_path\").str.contains(r\"\\\\data\\\\\")\n",
    ")\n",
    "\n",
    "df_data.write_parquet(str(OUT_DATA_ONLY), compression=\"zstd\")\n",
    "\n",
    "print(\"Rows total:\", df.height)\n",
    "print(\"Rows data-only:\", df_data.height)\n",
    "print(\"âœ… OUTPUT:\", OUT_DATA_ONLY)\n",
    "print(\"\\nâœ… Celda G OK.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
