{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb3f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Celda 00 (Bootstrap del ER_FILTER_5M_V2)‚Ä¶\n",
      "[Celda 00] cwd detectado = C:\\Quant\\MT5_Data_Extraction\n",
      "[Celda 00] PROJECT_ROOT = C:\\Quant\\MT5_Data_Extraction\n",
      "[Celda 00] TZ de c√°lculo: UTC | TZ de impresi√≥n: America/Guayaquil\n",
      "[Celda 00] now_utc   = 2025-12-18T19:08:10+00:00\n",
      "[Celda 00] now_local = 2025-12-18T14:08:10-05:00\n",
      "[Celda 00] RUN_ID = 20251218_190810\n",
      "[Celda 00] RANDOM_SEED = 32919305 | NUMPY_SEED = 32919305\n",
      "[Celda 00] Creando estructura de carpetas de salida del ER Filter‚Ä¶\n",
      "[Celda 00] Sincronizando config maestro ‚Üí config del RUN‚Ä¶\n",
      "[Celda 00] Config maestro encontrado y cargado desde: C:\\Quant\\MT5_Data_Extraction\\config\\er_filter_5m.json\n",
      "[Celda 00] Config maestro  ‚Üí C:\\Quant\\MT5_Data_Extraction\\config\\er_filter_5m.json\n",
      "[Celda 00] Config del RUN ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json\n",
      "[Celda 00] üìÅ OUT_ROOT      ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\n",
      "[Celda 00] üìÅ logs/        ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\logs\n",
      "[Celda 00] üìÅ diagnostics/ ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\n",
      "[Celda 00] üìÅ metrics/     ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\n",
      "[Celda 00] üìÅ events/      ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\n",
      "[Celda 00] üìÅ stability/   ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\n",
      "[Celda 00] üìÅ scores/      ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "[Celda 00] üìÅ baskets/     ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\n",
      "[Celda 00] üìÅ exports/     ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\exports\n",
      "[Celda 00] üìÅ reports/     ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\reports\n",
      "[Celda 00] üìÅ config_run/  ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json\n",
      "[Celda 00] üíæ run_metadata ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\logs\\run_metadata.json (OK, bytes=2363)\n",
      "[Celda 00] Resumen: RUN_ID=20251218_190810 | RANDOM_SEED=32919305 | Python=3.11.9 | polars=1.35.1 | pandas=2.3.3 | numpy=2.3.4 | scipy=1.16.3 | pyarrow=22.0.0\n",
      ">>> Celda 00 :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 00 ‚Äî Bootstrap, Versionado, Semillas y Rutas Base (ER_FILTER_5M_V2)\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Inicializar RUN_ID, semillas aleatorias y zonas horarias.\n",
    "#   - Detectar PROJECT_ROOT de forma robusta.\n",
    "#   - Construir las rutas SOLO de salida del filtro:\n",
    "#         outputs/er_filter_5m/<RUN_ID>/...\n",
    "#   - Inicializar GLOBAL_STATE con:\n",
    "#         - project_root, run_id, tz_calc, tz_print, random_seed\n",
    "#         - paths (rutas de salida del ER Filter + config del RUN)\n",
    "#         - config (cargado desde config/er_filter_5m.json)\n",
    "#         - flags b√°sicos y estructuras para inputs/metrics/exports, etc.\n",
    "#\n",
    "# NOTA IMPORTANTE (conceptual):\n",
    "#   Este notebook NO genera datos de mercado:\n",
    "#   asume que el Data Engine (MT5_DE_5M_V1) ya ejecut√≥ y dej√≥ sus PADs listos.\n",
    "#   Las rutas del Data Engine (DATA_ROOT, M5_CLEAN_DIR, METADATA_DIR, etc.)\n",
    "#   se resolver√°n en la Celda 01 a partir de PROJECT_ROOT y del config_snapshot.json.\n",
    "#   Aqu√≠ SOLO se definen rutas de salida propias del ER Filter\n",
    "#   y se sincroniza el config maestro ‚Üí config del RUN.\n",
    "#\n",
    "# NOTA OPERATIVA:\n",
    "#   Aunque el encabezado indique V2, este bootstrap mantiene el contrato\n",
    "#   compatible con el pipeline base y NO altera nombres/paths de artefactos.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import platform\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------\n",
    "# Impresiones iniciales obligatorias\n",
    "# -------------------------------\n",
    "print(\"Inicializando Celda 00 (Bootstrap del ER_FILTER_5M_V2)‚Ä¶\")\n",
    "\n",
    "# cwd y deducci√≥n de PROJECT_ROOT (b√∫squeda ascendente hasta 5 niveles)\n",
    "cwd = Path.cwd().resolve()\n",
    "print(f\"[Celda 00] cwd detectado = {str(cwd)}\")\n",
    "\n",
    "def _has_project_markers(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Devuelve True si el path contiene marcadores m√≠nimos de proyecto.\n",
    "    NOTA: Aqu√≠ solo buscamos estructura general; los detalles de DATA_ROOT\n",
    "          se resolver√°n en Celda 01 (handshake con Data Engine).\n",
    "    \"\"\"\n",
    "    markers = [\"bulk_data\", \"processed_data\", \"outputs\"]\n",
    "    try:\n",
    "        return any((path / m).exists() for m in markers)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "PROJECT_ROOT = cwd\n",
    "if not _has_project_markers(PROJECT_ROOT):\n",
    "    found = False\n",
    "    probe = PROJECT_ROOT\n",
    "    for _ in range(5):\n",
    "        probe = probe.parent\n",
    "        if _has_project_markers(probe):\n",
    "            PROJECT_ROOT = probe\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        # Se mantiene como comportamiento no bloqueante, pero se registra flag expl√≠cito.\n",
    "        print(\"[Celda 00][WARN] No se encontraron marcadores de proyecto; PROJECT_ROOT se deja en cwd\")\n",
    "\n",
    "PROJECT_ROOT = PROJECT_ROOT.resolve()\n",
    "print(f\"[Celda 00] PROJECT_ROOT = {str(PROJECT_ROOT)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Zonas horarias\n",
    "# -------------------------------\n",
    "TZ_CALC = \"UTC\"\n",
    "TZ_PRINT = \"America/Guayaquil\"\n",
    "print(f\"[Celda 00] TZ de c√°lculo: {TZ_CALC} | TZ de impresi√≥n: {TZ_PRINT}\")\n",
    "\n",
    "now_utc = datetime.datetime.now(datetime.timezone.utc).replace(microsecond=0)\n",
    "\n",
    "# zoneinfo/pytz fallback para hora local de referencia\n",
    "_local_tz = None\n",
    "local_iso = None\n",
    "try:\n",
    "    import pytz  # type: ignore\n",
    "    _local_tz = pytz.timezone(TZ_PRINT)\n",
    "    now_local = now_utc.astimezone(_local_tz)\n",
    "    local_iso = now_local.isoformat()\n",
    "except Exception:\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # py>=3.9\n",
    "        _local_tz = ZoneInfo(TZ_PRINT)\n",
    "        now_local = now_utc.astimezone(_local_tz)\n",
    "        local_iso = now_local.isoformat()\n",
    "    except Exception:\n",
    "        now_local = now_utc\n",
    "        local_iso = now_local.isoformat()\n",
    "        print(\"[Celda 00][WARN] No se pudo aplicar zona horaria de impresi√≥n; usando UTC como fallback\")\n",
    "\n",
    "print(f\"[Celda 00] now_utc   = {now_utc.isoformat()}\")\n",
    "print(f\"[Celda 00] now_local = {local_iso}\")\n",
    "\n",
    "# -------------------------------\n",
    "# RUN_ID (UTC) y RANDOM_SEED\n",
    "# -------------------------------\n",
    "RUN_ID = now_utc.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"[Celda 00] RUN_ID = {RUN_ID}\")\n",
    "\n",
    "env_seed = os.getenv(\"ER_FILTER_RANDOM_SEED\", \"\").strip()\n",
    "if env_seed != \"\":\n",
    "    try:\n",
    "        RANDOM_SEED = int(env_seed)\n",
    "    except Exception:\n",
    "        raise RuntimeError(f\"[Celda 00][ERROR] Variable de entorno ER_FILTER_RANDOM_SEED inv√°lida: {env_seed!r}\")\n",
    "else:\n",
    "    # SEED derivada del RUN_ID -> reproducible por run\n",
    "    h = hashlib.sha256(RUN_ID.encode(\"utf-8\")).digest()\n",
    "    RANDOM_SEED = int.from_bytes(h[:4], \"big\")  # 32 bits reproducibles por RUN_ID\n",
    "\n",
    "# Semillas reproducibles (random + numpy si est√° disponible)\n",
    "random.seed(RANDOM_SEED)\n",
    "try:\n",
    "    import numpy as np  # type: ignore\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    print(f\"[Celda 00] RANDOM_SEED = {RANDOM_SEED} | NUMPY_SEED = {RANDOM_SEED}\")\n",
    "except Exception:\n",
    "    print(f\"[Celda 00] RANDOM_SEED = {RANDOM_SEED}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Estructura de carpetas de salida del ER Filter\n",
    "# -------------------------------\n",
    "# IMPORTANTE:\n",
    "#   Estas rutas pertenecen EXCLUSIVAMENTE al ER_FILTER_5M_V2.\n",
    "#   NO son rutas del Data Engine; las rutas del Data Engine se obtendr√°n\n",
    "#   en la Celda 01 v√≠a PROJECT_ROOT + config_snapshot.json.\n",
    "OUT_ROOT        = (PROJECT_ROOT / \"outputs\" / \"er_filter_5m\" / RUN_ID).resolve()\n",
    "OUT_LOGS_DIR    = (OUT_ROOT / \"logs\").resolve()\n",
    "OUT_DIAG_DIR    = (OUT_ROOT / \"diagnostics\").resolve()\n",
    "OUT_METRICS_DIR = (OUT_ROOT / \"metrics\").resolve()\n",
    "OUT_EVENTS_DIR  = (OUT_ROOT / \"events\").resolve()\n",
    "OUT_STAB_DIR    = (OUT_ROOT / \"stability\").resolve()\n",
    "OUT_SCORES_DIR  = (OUT_ROOT / \"scores\").resolve()\n",
    "OUT_BASKETS_DIR = (OUT_ROOT / \"baskets\").resolve()\n",
    "OUT_EXPORTS_DIR = (OUT_ROOT / \"exports\").resolve()\n",
    "OUT_REPORTS_DIR = (OUT_ROOT / \"reports\").resolve()\n",
    "\n",
    "_dirs = [\n",
    "    OUT_ROOT,\n",
    "    OUT_LOGS_DIR,\n",
    "    OUT_DIAG_DIR,\n",
    "    OUT_METRICS_DIR,\n",
    "    OUT_EVENTS_DIR,\n",
    "    OUT_STAB_DIR,\n",
    "    OUT_SCORES_DIR,\n",
    "    OUT_BASKETS_DIR,\n",
    "    OUT_EXPORTS_DIR,\n",
    "    OUT_REPORTS_DIR,\n",
    "]\n",
    "\n",
    "print(\"[Celda 00] Creando estructura de carpetas de salida del ER Filter‚Ä¶\")\n",
    "for d in _dirs:\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Celda 00][ERROR] Cannot create directory: {str(d)} | reason={e}\")\n",
    "\n",
    "for d in _dirs:\n",
    "    if not d.exists() or not d.is_dir():\n",
    "        raise RuntimeError(f\"[Celda 00][ERROR] Directory check failed: {str(d)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# GLOBAL_STATE (m√≠nimo requerido + estructura base)\n",
    "# -------------------------------\n",
    "# NOTA:\n",
    "#   - GLOBAL_STATE[\"paths\"] contiene SOLO rutas de salida del ER Filter + config del RUN.\n",
    "#   - Las rutas del Data Engine (DATA_ROOT, M5_CLEAN_DIR, METADATA_DIR, etc.)\n",
    "#     se rellenar√°n en Celda 01 dentro de GLOBAL_STATE[\"inputs\"].\n",
    "GLOBAL_STATE = {\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"tz_calc\": TZ_CALC,\n",
    "    \"tz_print\": TZ_PRINT,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {\n",
    "        \"out_root\": str(OUT_ROOT),\n",
    "        \"logs\": str(OUT_LOGS_DIR),\n",
    "        \"diagnostics\": str(OUT_DIAG_DIR),\n",
    "        \"metrics\": str(OUT_METRICS_DIR),\n",
    "        \"events\": str(OUT_EVENTS_DIR),\n",
    "        \"stability\": str(OUT_STAB_DIR),\n",
    "        \"scores\": str(OUT_SCORES_DIR),\n",
    "        \"baskets\": str(OUT_BASKETS_DIR),\n",
    "        \"exports\": str(OUT_EXPORTS_DIR),\n",
    "        \"reports\": str(OUT_REPORTS_DIR),\n",
    "        # \"config\" se a√±adir√° justo despu√©s de sincronizar el maestro ‚Üí RUN\n",
    "    },\n",
    "    # Config del ER Filter (se cargar√° aqu√≠ desde config/er_filter_5m.json)\n",
    "    \"config\": {},\n",
    "    # Flags generales de estado de ejecuci√≥n del notebook\n",
    "    \"flags\": {\n",
    "        \"initialized\": True,               # Celda 00 completada\n",
    "        \"handshake_done\": False,           # Se pondr√° True en Celda 01\n",
    "        \"project_markers_found\": _has_project_markers(PROJECT_ROOT),\n",
    "        \"config_master_autogenerated\": False,\n",
    "    },\n",
    "    # Estructuras que se ir√°n rellenando en celdas posteriores\n",
    "    \"inputs\": {},    # Handshake con Data Engine (Celda 01)\n",
    "    \"tables\": {},    # Tablas intermedias del ER Filter\n",
    "    \"symbols\": {},   # Listas de s√≠mbolos (eligible_m5, universe, watchlist, etc.)\n",
    "    \"metrics\": {},   # Rutas de m√©tricas clave (ER, PD, estabilidad, scores‚Ä¶)\n",
    "    \"exports\": {},   # Artefactos preparados para EAs\n",
    "    \"baskets\": {},   # Info de cestas decorrelacionadas\n",
    "    \"reports\": {},   # Rutas a reportes HTML/otros\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Sincronizar config maestro ‚Üí config del RUN\n",
    "#   - Si no existe config/er_filter_5m.json, se crea un config por defecto (PC \"virgen\").\n",
    "# -------------------------------\n",
    "print(\"[Celda 00] Sincronizando config maestro ‚Üí config del RUN‚Ä¶\")\n",
    "\n",
    "CONFIG_DIR = (PROJECT_ROOT / \"config\").resolve()\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_cfg_path = CONFIG_DIR / \"er_filter_5m.json\"\n",
    "run_cfg_path = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "def _build_default_config() -> dict:\n",
    "    \"\"\"\n",
    "    Construye un config m√≠nimo pero funcional para una m√°quina 'virgen'.\n",
    "    Los valores son conservadores y deber√°n ajustarse m√°s adelante,\n",
    "    pero permiten que el notebook se ejecute de principio a fin.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"policy\": {\n",
    "            # Umbrales gen√©ricos; se ajustan en runs reales\n",
    "            \"min_TR_after_cost_trend\": 0.55,\n",
    "            \"min_TR_after_cost_range\": 0.55,\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"alpha_strict\": 0.01,\n",
    "            \"alpha_balanced\": 0.05,\n",
    "            \"alpha_loose\": 0.10,\n",
    "            \"n_eff_cap_PD\": 200,\n",
    "            \"n_eff_cap_p_event\": 200,\n",
    "            \"n_eff_cap_ER\": 200,\n",
    "        },\n",
    "        \"ER\": {\n",
    "            \"window\": 50,\n",
    "        },\n",
    "        \"PD\": {\n",
    "            \"window\": 50,\n",
    "        },\n",
    "        \"hysteresis\": {\n",
    "            \"pctile_delta\": 5.0,\n",
    "        },\n",
    "        \"decay\": {\n",
    "            # par√°metros de alpha decay se pueden completar m√°s adelante\n",
    "        },\n",
    "        \"corr\": {\n",
    "            \"threshold\": 0.8,\n",
    "        },\n",
    "        \"folds\": {\n",
    "            \"n_folds\": 4,\n",
    "        },\n",
    "        \"baskets\": {\n",
    "            \"size_max\": 10,\n",
    "        },\n",
    "        \"selection\": {\n",
    "            # thresholds de SCORE_FINAL, EstabScore, etc. se afinan luego\n",
    "        },\n",
    "        \"session\": {\n",
    "            # par√°metros de sesiones/ventanas horarias si aplica\n",
    "        },\n",
    "        \"meta\": {\n",
    "            \"config_version\": \"auto_default_v1\",\n",
    "            \"created_utc\": now_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"notes\": \"Config generado autom√°ticamente por Celda 00 en ausencia de er_filter_5m.json\",\n",
    "        },\n",
    "        \"structure\": {\n",
    "            \"enabled\": True,\n",
    "            \"min_events_per_symbol\": 100,\n",
    "        },\n",
    "        \"stability\": {\n",
    "            \"EstabScore_min\": 0.4,\n",
    "        },\n",
    "        \"viability\": {\n",
    "            \"score_viability_min\": 0.3,\n",
    "        },\n",
    "    }\n",
    "\n",
    "if not base_cfg_path.exists():\n",
    "    # PC virgen ‚Üí generar config maestro por defecto\n",
    "    print(f\"[Celda 00][WARN] Config maestro no encontrado: {base_cfg_path}\")\n",
    "    print(\"[Celda 00] Generando config maestro por defecto (auto_default_v1)‚Ä¶\")\n",
    "    base_cfg = _build_default_config()\n",
    "    GLOBAL_STATE[\"flags\"][\"config_master_autogenerated\"] = True\n",
    "    try:\n",
    "        base_cfg_path.write_text(json.dumps(base_cfg, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"[Celda 00] Config maestro por defecto escrito en: {str(base_cfg_path)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"[Celda 00][ERROR] No se pudo escribir config maestro por defecto en {base_cfg_path} | reason={e}\"\n",
    "        )\n",
    "else:\n",
    "    # *** FIX UTF-8 BOM ***\n",
    "    try:\n",
    "        # utf-8-sig elimina el BOM si existe (como en archivos escritos por PowerShell con UTF8)\n",
    "        base_cfg_text = base_cfg_path.read_text(encoding=\"utf-8-sig\")\n",
    "        base_cfg = json.loads(base_cfg_text)\n",
    "        print(f\"[Celda 00] Config maestro encontrado y cargado desde: {str(base_cfg_path)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Celda 00][ERROR] No se pudo leer/parsear el config maestro {base_cfg_path} | reason={e}\")\n",
    "\n",
    "# Escribir config del RUN (copia exacta del maestro en este momento)\n",
    "try:\n",
    "    run_cfg_path.write_text(json.dumps(base_cfg, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"[Celda 00][ERROR] No se pudo escribir config del RUN en {run_cfg_path} | reason={e}\")\n",
    "\n",
    "if not run_cfg_path.exists() or run_cfg_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(\"[Celda 00][ERROR] config.json del RUN no existe o est√° vac√≠o tras la escritura\")\n",
    "\n",
    "GLOBAL_STATE[\"paths\"][\"config\"] = str(run_cfg_path)\n",
    "GLOBAL_STATE[\"config\"] = base_cfg\n",
    "\n",
    "print(f\"[Celda 00] Config maestro  ‚Üí {str(base_cfg_path)}\")\n",
    "print(f\"[Celda 00] Config del RUN ‚Üí {str(run_cfg_path)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaci√≥n de GLOBAL_STATE[\"paths\"]\n",
    "# -------------------------------\n",
    "required_path_keys = {\n",
    "    \"out_root\",\n",
    "    \"logs\",\n",
    "    \"diagnostics\",\n",
    "    \"metrics\",\n",
    "    \"events\",\n",
    "    \"stability\",\n",
    "    \"scores\",\n",
    "    \"baskets\",\n",
    "    \"exports\",\n",
    "    \"reports\",\n",
    "    \"config\",\n",
    "}\n",
    "paths_dict = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "missing_keys = required_path_keys - set(paths_dict.keys())\n",
    "if missing_keys:\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 00][ERROR] GLOBAL_STATE['paths'] es incompleto; faltan claves: {sorted(missing_keys)}\"\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Persistir logs/run_metadata.json\n",
    "# -------------------------------\n",
    "run_metadata = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"created_utc\": now_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"created_local\": local_iso,\n",
    "    \"tz_calc\": TZ_CALC,\n",
    "    \"tz_print\": TZ_PRINT,\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"out_root\": str(OUT_ROOT),\n",
    "    \"paths\": GLOBAL_STATE[\"paths\"],\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": {\n",
    "        \"system\": platform.system(),\n",
    "        \"release\": platform.release(),\n",
    "        \"machine\": platform.machine(),\n",
    "        \"python_implementation\": platform.python_implementation(),\n",
    "    },\n",
    "    \"packages\": {\n",
    "        \"polars\": None,   # se rellenan m√°s abajo\n",
    "        \"pandas\": None,\n",
    "        \"numpy\": None,\n",
    "        \"scipy\": None,\n",
    "        \"pyarrow\": None,\n",
    "    },\n",
    "    \"notes\": {\n",
    "        \"role\": \"ER_FILTER_5M_V2 bootstrap\",\n",
    "        \"data_engine_dependency\": (\n",
    "            \"Este notebook NO genera datos de mercado; depende del Data Engine \"\n",
    "            \"MT5_DE_5M_V1 y de sus PADs (config_snapshot, √≠ndices M5, etc.).\"\n",
    "        ),\n",
    "        \"config_master\": str(base_cfg_path),\n",
    "        \"config_run\": str(run_cfg_path),\n",
    "        \"config_master_autogenerated\": bool(GLOBAL_STATE[\"flags\"].get(\"config_master_autogenerated\", False)),\n",
    "        \"project_markers_found\": bool(GLOBAL_STATE[\"flags\"].get(\"project_markers_found\", False)),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Versiones de librer√≠as clave (se rellenan ahora)\n",
    "def _safe_version(modname: str):\n",
    "    try:\n",
    "        mod = __import__(modname)\n",
    "        ver = getattr(mod, \"__version__\", None)\n",
    "        return ver if isinstance(ver, str) else str(ver) if ver is not None else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "ver_polars = _safe_version(\"polars\")\n",
    "ver_pandas = _safe_version(\"pandas\")\n",
    "ver_numpy = _safe_version(\"numpy\")\n",
    "ver_scipy = _safe_version(\"scipy\")\n",
    "ver_pyarrow = _safe_version(\"pyarrow\")\n",
    "\n",
    "run_metadata[\"packages\"][\"polars\"] = ver_polars\n",
    "run_metadata[\"packages\"][\"pandas\"] = ver_pandas\n",
    "run_metadata[\"packages\"][\"numpy\"] = ver_numpy\n",
    "run_metadata[\"packages\"][\"scipy\"] = ver_scipy\n",
    "run_metadata[\"packages\"][\"pyarrow\"] = ver_pyarrow\n",
    "\n",
    "metadata_path = OUT_LOGS_DIR / \"run_metadata.json\"\n",
    "try:\n",
    "    txt = json.dumps(run_metadata, ensure_ascii=False, indent=2, sort_keys=True)\n",
    "    metadata_path.write_text(txt, encoding=\"utf-8\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"[Celda 00][ERROR] No se pudo escribir run_metadata.json en {str(metadata_path)} | reason={e}\")\n",
    "\n",
    "if not metadata_path.exists():\n",
    "    raise RuntimeError(\"[Celda 00][ERROR] run_metadata.json no existe tras la escritura\")\n",
    "size_bytes = metadata_path.stat().st_size\n",
    "if size_bytes <= 0:\n",
    "    raise RuntimeError(\"[Celda 00][ERROR] run_metadata.json tiene tama√±o 0\")\n",
    "try:\n",
    "    _ = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"[Celda 00][ERROR] run_metadata.json no es legible como JSON | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Impresiones finales obligatorias\n",
    "# -------------------------------\n",
    "print(f\"[Celda 00] üìÅ OUT_ROOT      ‚Üí {str(OUT_ROOT)}\")\n",
    "print(f\"[Celda 00] üìÅ logs/        ‚Üí {str(OUT_LOGS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ diagnostics/ ‚Üí {str(OUT_DIAG_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ metrics/     ‚Üí {str(OUT_METRICS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ events/      ‚Üí {str(OUT_EVENTS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ stability/   ‚Üí {str(OUT_STAB_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ scores/      ‚Üí {str(OUT_SCORES_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ baskets/     ‚Üí {str(OUT_BASKETS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ exports/     ‚Üí {str(OUT_EXPORTS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ reports/     ‚Üí {str(OUT_REPORTS_DIR)}\")\n",
    "print(f\"[Celda 00] üìÅ config_run/  ‚Üí {str(run_cfg_path)}\")\n",
    "print(f\"[Celda 00] üíæ run_metadata ‚Üí {str(metadata_path)} (OK, bytes={size_bytes})\")\n",
    "\n",
    "py_short = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
    "print(\n",
    "    \"[Celda 00] Resumen: \"\n",
    "    f\"RUN_ID={RUN_ID} | RANDOM_SEED={RANDOM_SEED} | Python={py_short} | \"\n",
    "    f\"polars={ver_polars} | pandas={ver_pandas} | numpy={ver_numpy} | \"\n",
    "    f\"scipy={ver_scipy} | pyarrow={ver_pyarrow}\"\n",
    ")\n",
    "\n",
    "print(\">>> Celda 00 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddd7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando Celda 01 (Handshake con Data Engine)‚Ä¶\n",
      "[Celda 01] PROJECT_ROOT (desde GLOBAL_STATE) = C:\\Quant\\MT5_Data_Extraction\n",
      "[Celda 01][INFO] 'stats.volume_min_notional' no encontrado en config; inicializado a 0.0 (sin filtro m√≠nimo de volumen/notional). Ajusta este valor en config/er_filter_5m.json si deseas un umbral institucional.\n",
      "[Celda 01][INFO] 'stats.volatility_band_refs' no encontrado en config; inicializado con placeholders {low, medium, high} = None. Define bandas num√©ricas en config/er_filter_5m.json para usarlas en filtros suaves de volatilidad.\n",
      "[Celda 01][INFO] 'stats.coverage_pct_min' no encontrado en config; inicializado a 30.0 (% de d√≠as con datos). Ajusta en er_filter_5m.json si lo deseas.\n",
      "[Celda 01][INFO] 'stats.coverage_min_symbols' no encontrado en config; inicializado a 50 s√≠mbolos. Ajusta en er_filter_5m.json si lo deseas.\n",
      "[Celda 01][INFO] 'stats.max_spread_p99_pts' no encontrado en config; inicializado a 99.0 puntos. Ajusta este umbral de spreads en er_filter_5m.json.\n",
      "[Celda 01][INFO] 'stats.max_tick_anomalies_pct' no encontrado en config; inicializado a 5.0 (%). Ajusta este umbral de anomal√≠as de ticks en er_filter_5m.json.\n",
      "[Celda 01][INFO] 'stats.max_data_age_days' no encontrado en config; inicializado a 0 (sin filtro duro de frescura; solo diagn√≥stico si lo configuras > 0).\n",
      "[Celda 01] Secciones de config detectadas: policy=True, stats=True, scores=True, baskets=True\n",
      "[Celda 01] stats.volume_min_notional = 0.0, stats.volatility_band_refs = {'low': None, 'medium': None, 'high': None}\n",
      "[Celda 01] QA thresholds ‚Üí coverage_pct_min=30.0%, coverage_min_symbols=50, max_spread_p99_pts=99.0, max_tick_anomalies_pct=5.0%, max_data_age_days=0\n",
      "[Celda 01] ‚úÖ config_snapshot.json detectado en: C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\config_snapshot.json\n",
      "[Celda 01] Rutas derivadas de config_snapshot.json:\n",
      "  ‚Ä¢ DATA_ROOT          = C:\\Quant\\MT5_Data_Extraction\\data\n",
      "  ‚Ä¢ METADATA_DIR       = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\n",
      "  ‚Ä¢ M5_CLEAN_DIR       = C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "  ‚Ä¢ PROCESSED_DATA_DIR = C:\\Quant\\MT5_Data_Extraction\\data\\processed_data\n",
      "  ‚Ä¢ M5_RAW_DIR         = C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\\m5_raw\n",
      ">>> Celda 01 :: Handshake con Data Engine\n",
      "üìÅ INPUT m5_raw ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\\m5_raw (parquet_files=121457, partitioned=true)\n",
      "üìÅ Layout M5_CLEAN_DIR ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean (exists=true, parquet_files=103917)\n",
      "üìÅ Layout RESTORE m5   ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\restore\\restore_20251202_232253\\historical_data\\m5_clean (exists=true, parquet_files=103917)\n",
      "üìÅ META ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\symbol_index_m5.parquet (found=true)\n",
      "üìÅ META ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\day_index_m5.parquet (found=true)\n",
      "üìÅ META ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\window_catalog_m5.parquet (found=true)\n",
      "üìÅ META ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\m5_manifest.parquet (found=true)\n",
      "üìÅ META ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\dataset_catalog.parquet (found=true)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\processed_data\\corr_matrix_5m.csv (found=false)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\processed_data\\universe_ranked.parquet (found=false)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\ea_watchlist.parquet (found=false, fallback_csv=false)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\ea_params.parquet (found=true, fallback_csv=false)\n",
      "‚ö†Ô∏è FALLBACK ‚Üí correlaci√≥n M5 ser√° reconstruida\n",
      "üîé Fuente M5 seleccionada ‚Üí type=partitioned_m5_clean | root=C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean | parquet_files=103917\n",
      "üßæ S√≠mbolos detectados (antes de QA) = 92 | muestra=['AAPL', 'AAVUSD', 'ADAUSD', 'AIRF', 'ALGUSD', 'ALVG', 'AMZN', 'AUDCAD', 'AUDCHF', 'AUDJPY']\n",
      "[Celda 01][INFO] ticks_recent_qc_summary.json no encontrado. QA ticks opcional no aplicado.\n",
      "üìä Cobertura: 87 s√≠mbolos con cobertura >= 30% de d√≠as con datos desde 2021-01-01\n",
      "[Celda 01] QA timestamps ‚Üí min_age=15.8d, median_age=736.8d, max_age=1490.8d\n",
      "üßæ S√≠mbolos finales post-QA = 87 | muestra=['AAPL', 'AAVUSD', 'ADAUSD', 'AIRF', 'ALGUSD', 'ALVG', 'AMZN', 'AUDCAD', 'AUDCHF', 'AUDJPY']\n",
      "[Celda 01][INFO] Watchlist no encontrada (parquet/csv). Se generar√° m√°s adelante si el pipeline la requiere.\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\handshake_summary.json (OK, bytes=3788)\n",
      "[Celda 01] GLOBAL_STATE['inputs'] actualizado con rutas de Data Engine y fuente M5.\n",
      "[Celda 01] GLOBAL_STATE['paths']['metadata'] = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\n",
      "[Celda 01] GLOBAL_STATE['data_engine'] = {'m5_source_type': 'partitioned_m5_clean', 'm5_root_dir': 'C:\\\\Quant\\\\MT5_Data_Extraction\\\\data\\\\historical_data\\\\m5_clean', 'metadata_dir': 'C:\\\\Quant\\\\MT5_Data_Extraction\\\\data\\\\metadata', 'has_universe_ranked': False, 'has_corr_matrix': False, 'handshake_summary': 'C:\\\\Quant\\\\MT5_Data_Extraction\\\\outputs\\\\er_filter_5m\\\\20251218_190810\\\\diagnostics\\\\handshake_summary.json'}\n",
      ">>> Celda 01 :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 01 ‚Äî Handshake con Data Engine (PAD-aware + config_snapshot.json) ‚Äî ER_FILTER_5M_V2\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   - Leer PROJECT_ROOT y config desde GLOBAL_STATE (Celda 00).\n",
    "#   - Validar y estructurar GLOBAL_STATE[\"config\"] en secciones:\n",
    "#       * policy  ‚Üí reglas de negocio (spreads m√°ximos, comisiones, sesiones, etc.).\n",
    "#       * stats   ‚Üí par√°metros estad√≠sticos (alphas, caps de n_eff, cuantiles IS/OOS, etc.).\n",
    "#       * scores  ‚Üí pesos para scoring y selecci√≥n.\n",
    "#       * baskets ‚Üí reglas/parametrizaci√≥n de cestas.\n",
    "#   - TODOS los thresholds de p-values, caps de n_eff, bandas de oportunidad, etc.\n",
    "#     deben leerse SIEMPRE desde GLOBAL_STATE[\"config\"][\"stats\"] en celdas posteriores.\n",
    "#   - Intentar leer data/metadata/config_snapshot.json generado por MT5_DE_5M_V1.\n",
    "#   - A partir de ese snapshot, obtener rutas can√≥nicas:\n",
    "#         DATA_ROOT, METADATA_DIR, M5_CLEAN_DIR, PROCESSED_DATA_DIR, M5_RAW_DIR (si existe).\n",
    "#     Si no existe el snapshot, usar layout por defecto PROJECT_ROOT/data/...\n",
    "#   - Detectar la fuente M5 con prioridad:\n",
    "#         1) historical_data/m5_clean (si tiene parquet)  ‚Üí GOLD\n",
    "#         2) bulk_data/m5_raw\n",
    "#         3) restore/*/historical_data/m5_clean (√∫ltimo restore)\n",
    "#   - Hacer QA b√°sico: ticks, cobertura, frescura de timestamps (SOLO DIAGN√ìSTICO).\n",
    "#   - Persistir diagnostics/handshake_summary.json.\n",
    "#   - Actualizar GLOBAL_STATE[\"inputs\"] con:\n",
    "#         DATA_ROOT, METADATA_DIR, M5_CLEAN_DIR, PROCESSED_DATA_DIR,\n",
    "#         M5_SOURCE_TYPE, M5_ROOT_DIR, PADs y artefactos auxiliares.\n",
    "#   - Actualizar GLOBAL_STATE[\"paths\"][\"metadata\"] para que el ER Filter pueda\n",
    "#     localizar artefactos del Data Engine (data_quality, universe_snapshot, etc.).\n",
    "#   - Registrar un resumen compacto en GLOBAL_STATE[\"data_engine\"].\n",
    "#\n",
    "# Cambios estructurales aplicados (versi√≥n final):\n",
    "#   1) Mensajes de ausencia de artefactos OPCIONALES (ticks QC, watchlist/params)\n",
    "#      pasan de [WARN] ‚Üí [INFO] para evitar falsos ‚Äúalertas‚Äù en un flujo sano.\n",
    "#   2) Se mantiene toda la l√≥gica de selecci√≥n y QA sin cambiar contratos ni paths.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import polars as pl  # Para lecturas de parquet en checks\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones de estado previo\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 01] GLOBAL_STATE no existe. Debes ejecutar primero la Celda 00 (Bootstrap).\")\n",
    "\n",
    "if \"project_root\" not in GLOBAL_STATE or \"paths\" not in GLOBAL_STATE:\n",
    "    raise RuntimeError(\"[Celda 01] GLOBAL_STATE incompleto: faltan claves 'project_root' o 'paths'.\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(GLOBAL_STATE[\"paths\"][\"diagnostics\"]).resolve()\n",
    "\n",
    "print(\"Inicializando Celda 01 (Handshake con Data Engine)‚Ä¶\")\n",
    "print(f\"[Celda 01] PROJECT_ROOT (desde GLOBAL_STATE) = {str(PROJECT_ROOT)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Lectura y validaci√≥n de config del ER Filter\n",
    "# -------------------------------\n",
    "cfg = GLOBAL_STATE.get(\"config\", {})\n",
    "if not isinstance(cfg, dict) or not cfg:\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 01] GLOBAL_STATE['config'] no est√° inicializado o no es un dict. \"\n",
    "        \"Revisa la Celda 00 y el archivo config/er_filter_5m.json.\"\n",
    "    )\n",
    "\n",
    "# Asegurar secciones principales\n",
    "cfg_policy  = cfg.setdefault(\"policy\", {})    # reglas de negocio: spreads, comisiones, sesiones, etc.\n",
    "cfg_stats   = cfg.setdefault(\"stats\", {})     # par√°metros estad√≠sticos: alphas, caps n_eff, cuantiles, etc.\n",
    "cfg_scores  = cfg.setdefault(\"scores\", {})    # pesos para scoring y selecci√≥n\n",
    "cfg_baskets = cfg.setdefault(\"baskets\", {})   # reglas/parametrizaci√≥n de cestas\n",
    "\n",
    "# Claves base para vol√∫menes y volatilidad\n",
    "# NOTA:\n",
    "#   En un entorno \"production-grade\", estas claves deben venir expl√≠citas\n",
    "#   en config/er_filter_5m.json. Si faltan, se inicializan de forma conservadora.\n",
    "if \"volume_min_notional\" not in cfg_stats:\n",
    "    cfg_stats[\"volume_min_notional\"] = 0.0\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.volume_min_notional' no encontrado en config; \"\n",
    "        \"inicializado a 0.0 (sin filtro m√≠nimo de volumen/notional). \"\n",
    "        \"Ajusta este valor en config/er_filter_5m.json si deseas un umbral institucional.\"\n",
    "    )\n",
    "\n",
    "if \"volatility_band_refs\" not in cfg_stats:\n",
    "    cfg_stats[\"volatility_band_refs\"] = {\"low\": None, \"medium\": None, \"high\": None}\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.volatility_band_refs' no encontrado en config; \"\n",
    "        \"inicializado con placeholders {low, medium, high} = None. \"\n",
    "        \"Define bandas num√©ricas en config/er_filter_5m.json para usarlas en filtros suaves de volatilidad.\"\n",
    "    )\n",
    "\n",
    "# Umbrales de cobertura y QA b√°sicos (centralizados en config.stats)\n",
    "if \"coverage_pct_min\" not in cfg_stats:\n",
    "    cfg_stats[\"coverage_pct_min\"] = 30.0\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.coverage_pct_min' no encontrado en config; \"\n",
    "        \"inicializado a 30.0 (% de d√≠as con datos). Ajusta en er_filter_5m.json si lo deseas.\"\n",
    "    )\n",
    "\n",
    "if \"coverage_min_symbols\" not in cfg_stats:\n",
    "    cfg_stats[\"coverage_min_symbols\"] = 50\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.coverage_min_symbols' no encontrado en config; \"\n",
    "        \"inicializado a 50 s√≠mbolos. Ajusta en er_filter_5m.json si lo deseas.\"\n",
    "    )\n",
    "\n",
    "if \"max_spread_p99_pts\" not in cfg_stats:\n",
    "    cfg_stats[\"max_spread_p99_pts\"] = 99.0\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.max_spread_p99_pts' no encontrado en config; \"\n",
    "        \"inicializado a 99.0 puntos. Ajusta este umbral de spreads en er_filter_5m.json.\"\n",
    "    )\n",
    "\n",
    "if \"max_tick_anomalies_pct\" not in cfg_stats:\n",
    "    cfg_stats[\"max_tick_anomalies_pct\"] = 5.0\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.max_tick_anomalies_pct' no encontrado en config; \"\n",
    "        \"inicializado a 5.0 (%). Ajusta este umbral de anomal√≠as de ticks en er_filter_5m.json.\"\n",
    "    )\n",
    "\n",
    "# üî¥ Filtro de frescura: por defecto desactivado (0 d√≠as) ‚Üí SOLO diagn√≥stico\n",
    "if \"max_data_age_days\" not in cfg_stats:\n",
    "    cfg_stats[\"max_data_age_days\"] = 0\n",
    "    print(\n",
    "        \"[Celda 01][INFO] 'stats.max_data_age_days' no encontrado en config; \"\n",
    "        \"inicializado a 0 (sin filtro duro de frescura; solo diagn√≥stico si lo configuras > 0).\"\n",
    "    )\n",
    "\n",
    "# Persistir cambios en memoria\n",
    "GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "print(\n",
    "    \"[Celda 01] Secciones de config detectadas: \"\n",
    "    f\"policy={bool(cfg_policy)}, stats={bool(cfg_stats)}, \"\n",
    "    f\"scores={bool(cfg_scores)}, baskets={bool(cfg_baskets)}\"\n",
    ")\n",
    "print(\n",
    "    \"[Celda 01] stats.volume_min_notional = \"\n",
    "    f\"{cfg_stats.get('volume_min_notional')}, \"\n",
    "    \"stats.volatility_band_refs = \"\n",
    "    f\"{cfg_stats.get('volatility_band_refs')}\"\n",
    ")\n",
    "print(\n",
    "    \"[Celda 01] QA thresholds ‚Üí \"\n",
    "    f\"coverage_pct_min={cfg_stats.get('coverage_pct_min')}%, \"\n",
    "    f\"coverage_min_symbols={cfg_stats.get('coverage_min_symbols')}, \"\n",
    "    f\"max_spread_p99_pts={cfg_stats.get('max_spread_p99_pts')}, \"\n",
    "    f\"max_tick_anomalies_pct={cfg_stats.get('max_tick_anomalies_pct')}%, \"\n",
    "    f\"max_data_age_days={cfg_stats.get('max_data_age_days')}\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Paso 0: Leer config_snapshot.json si existe\n",
    "# -------------------------------\n",
    "CONFIG_SNAPSHOT_PATH = (PROJECT_ROOT / \"data\" / \"metadata\" / \"config_snapshot.json\").resolve()\n",
    "\n",
    "# Layout por defecto (fallback) si no hay snapshot\n",
    "DATA_ROOT          = (PROJECT_ROOT / \"data\").resolve()\n",
    "METADATA_DIR       = (DATA_ROOT / \"metadata\").resolve()\n",
    "M5_CLEAN_DIR       = (DATA_ROOT / \"historical_data\" / \"m5_clean\").resolve()\n",
    "PROCESSED_DATA_DIR = (DATA_ROOT / \"processed_data\").resolve()\n",
    "M5_RAW_DIR         = (DATA_ROOT / \"bulk_data\" / \"m5_raw\").resolve()\n",
    "\n",
    "if CONFIG_SNAPSHOT_PATH.exists() and CONFIG_SNAPSHOT_PATH.is_file():\n",
    "    print(f\"[Celda 01] ‚úÖ config_snapshot.json detectado en: {str(CONFIG_SNAPSHOT_PATH)}\")\n",
    "    try:\n",
    "        snap = json.loads(CONFIG_SNAPSHOT_PATH.read_text(encoding=\"utf-8\"))\n",
    "        paths_cfg = snap.get(\"dataset\", {}).get(\"paths\", {})\n",
    "\n",
    "        if \"DATA_ROOT\" in paths_cfg:\n",
    "            DATA_ROOT = Path(paths_cfg[\"DATA_ROOT\"]).resolve()\n",
    "        if \"METADATA_DIR\" in paths_cfg:\n",
    "            METADATA_DIR = Path(paths_cfg[\"METADATA_DIR\"]).resolve()\n",
    "        if \"M5_CLEAN_DIR\" in paths_cfg:\n",
    "            M5_CLEAN_DIR = Path(paths_cfg[\"M5_CLEAN_DIR\"]).resolve()\n",
    "        if \"PROCESSED_DATA_DIR\" in paths_cfg:\n",
    "            PROCESSED_DATA_DIR = Path(paths_cfg[\"PROCESSED_DATA_DIR\"]).resolve()\n",
    "        if \"M5_RAW_DIR\" in paths_cfg:\n",
    "            M5_RAW_DIR = Path(paths_cfg[\"M5_RAW_DIR\"]).resolve()\n",
    "\n",
    "        print(\"[Celda 01] Rutas derivadas de config_snapshot.json:\")\n",
    "        print(f\"  ‚Ä¢ DATA_ROOT          = {str(DATA_ROOT)}\")\n",
    "        print(f\"  ‚Ä¢ METADATA_DIR       = {str(METADATA_DIR)}\")\n",
    "        print(f\"  ‚Ä¢ M5_CLEAN_DIR       = {str(M5_CLEAN_DIR)}\")\n",
    "        print(f\"  ‚Ä¢ PROCESSED_DATA_DIR = {str(PROCESSED_DATA_DIR)}\")\n",
    "        print(f\"  ‚Ä¢ M5_RAW_DIR         = {str(M5_RAW_DIR)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 01][WARN] Error leyendo config_snapshot.json ({e!r}).\")\n",
    "        print(\"             Se usar√° layout por defecto PROJECT_ROOT/data/...\")\n",
    "else:\n",
    "    print(\"[Celda 01][WARN] config_snapshot.json NO encontrado.\")\n",
    "    print(\"             Se utilizar√° el layout por defecto PROJECT_ROOT/data/...\")\n",
    "\n",
    "# -------------------------------\n",
    "# Definici√≥n de rutas candidatas (M5), alineadas con MT5_DE\n",
    "# -------------------------------\n",
    "DE_M5_RAW_DIR         = M5_RAW_DIR.resolve()\n",
    "PAD_HIST_CLEAN_DIR    = M5_CLEAN_DIR.resolve()\n",
    "PAD_RESTORE_ROOT      = (DATA_ROOT / \"restore\").resolve()\n",
    "PAD_META_DIR          = METADATA_DIR.resolve()\n",
    "PAD_DAY_INDEX         = (PAD_META_DIR / \"day_index_m5.parquet\").resolve()\n",
    "PAD_SYMBOL_INDEX      = (PAD_META_DIR / \"symbol_index_m5.parquet\").resolve()\n",
    "PAD_WINDOW_CATALOG    = (PAD_META_DIR / \"window_catalog_m5.parquet\").resolve()\n",
    "PAD_MANIFEST          = (PAD_META_DIR / \"m5_manifest.parquet\").resolve()\n",
    "PAD_DATASET_CATALOG   = (PAD_META_DIR / \"dataset_catalog.parquet\").resolve()\n",
    "TICKS_QC_SUMMARY_JSON = (PAD_META_DIR / \"ticks_recent_qc_summary.json\").resolve()\n",
    "\n",
    "DE_CORR_M5_FILE           = (PROCESSED_DATA_DIR / \"corr_matrix_5m.csv\").resolve()\n",
    "DE_UNIVERSE_FILE          = (PROCESSED_DATA_DIR / \"universe_ranked.parquet\").resolve()\n",
    "DE_WATCHLIST_FILE         = (DATA_ROOT / \"ea_watchlist.parquet\").resolve()\n",
    "DE_WATCHLIST_FALLBACK_CSV = (DATA_ROOT / \"ea_watchlist.csv\").resolve()\n",
    "DE_PARAMS_FILE            = (DATA_ROOT / \"ea_params.parquet\").resolve()\n",
    "DE_PARAMS_FALLBACK_CSV    = (DATA_ROOT / \"ea_params.csv\").resolve()\n",
    "\n",
    "# -------------------------------\n",
    "# Utilidades\n",
    "# -------------------------------\n",
    "def _bool(b) -> str:\n",
    "    return \"true\" if bool(b) else \"false\"\n",
    "\n",
    "def _count_parquet_recursive(dir_path: Path, max_files: int = 10_000_000) -> int:\n",
    "    if not dir_path.exists() or not dir_path.is_dir():\n",
    "        return 0\n",
    "    n = 0\n",
    "    for p in dir_path.rglob(\"*.parquet\"):\n",
    "        if p.is_file():\n",
    "            n += 1\n",
    "            if n >= max_files:\n",
    "                break\n",
    "    return n\n",
    "\n",
    "def _latest_restore_m5_clean(restore_root: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Devuelve el subdirectorio 'historical_data/m5_clean' dentro del restore m√°s reciente,\n",
    "    si existe alguno con ese layout.\n",
    "    \"\"\"\n",
    "    if not restore_root.exists() or not restore_root.is_dir():\n",
    "        return None\n",
    "    restore_dirs = [d for d in restore_root.iterdir() if d.is_dir() and d.name.lower().startswith(\"restore_\")]\n",
    "    if not restore_dirs:\n",
    "        return None\n",
    "    restore_dirs.sort(key=lambda d: d.stat().st_mtime, reverse=True)\n",
    "    for d in restore_dirs:\n",
    "        candidate = d / \"historical_data\" / \"m5_clean\"\n",
    "        if candidate.exists() and candidate.is_dir():\n",
    "            return candidate.resolve()\n",
    "    return None\n",
    "\n",
    "def _detect_partitioned_symbols(m5_root: Path, max_symbols: int = 10_000) -> list[str]:\n",
    "    symbols: list[str] = []\n",
    "    if m5_root.exists() and m5_root.is_dir():\n",
    "        for sub in m5_root.glob(\"symbol=*\"):\n",
    "            if sub.is_dir():\n",
    "                sym = sub.name.split(\"symbol=\", 1)[-1].strip()\n",
    "                if sym:\n",
    "                    symbols.append(sym)\n",
    "                    if len(symbols) >= max_symbols:\n",
    "                        break\n",
    "    return symbols\n",
    "\n",
    "def _detect_flat_symbols(m5_dir: Path, max_symbols: int = 100_000) -> list[str]:\n",
    "    symbols: list[str] = []\n",
    "    if m5_dir.exists() and m5_dir.is_dir():\n",
    "        for p in m5_dir.glob(\"*.parquet\"):\n",
    "            if p.is_file():\n",
    "                sym = p.stem.strip()\n",
    "                if sym:\n",
    "                    symbols.append(sym)\n",
    "                    if len(symbols) >= max_symbols:\n",
    "                        break\n",
    "    return symbols\n",
    "\n",
    "# -------------------------------\n",
    "# Handshake\n",
    "# -------------------------------\n",
    "print(\">>> Celda 01 :: Handshake con Data Engine\")\n",
    "\n",
    "# 1) Reportar layout m5_raw (detecta flat/partitioned)\n",
    "m5_raw_exists         = DE_M5_RAW_DIR.exists() and DE_M5_RAW_DIR.is_dir()\n",
    "m5_raw_is_partitioned = m5_raw_exists and len(list(DE_M5_RAW_DIR.glob(\"symbol=*\"))) > 0\n",
    "m5_raw_parquet_count  = _count_parquet_recursive(DE_M5_RAW_DIR) if m5_raw_exists else 0\n",
    "print(\n",
    "    f\"üìÅ INPUT m5_raw ‚Üí {str(DE_M5_RAW_DIR)} \"\n",
    "    f\"(parquet_files={m5_raw_parquet_count}, partitioned={_bool(m5_raw_is_partitioned)})\"\n",
    ")\n",
    "\n",
    "# 2) PADs reales (partitionado y restore) ‚Üí GOLD principal\n",
    "part_exists           = PAD_HIST_CLEAN_DIR.exists() and PAD_HIST_CLEAN_DIR.is_dir()\n",
    "part_parquet_count    = _count_parquet_recursive(PAD_HIST_CLEAN_DIR) if part_exists else 0\n",
    "latest_restore_m5     = _latest_restore_m5_clean(PAD_RESTORE_ROOT)\n",
    "restore_exists        = latest_restore_m5 is not None\n",
    "restore_parquet_count = _count_parquet_recursive(latest_restore_m5) if restore_exists else 0\n",
    "\n",
    "print(\n",
    "    f\"üìÅ Layout M5_CLEAN_DIR ‚Üí {str(PAD_HIST_CLEAN_DIR)} \"\n",
    "    f\"(exists={_bool(part_exists)}, parquet_files={part_parquet_count})\"\n",
    ")\n",
    "print(\n",
    "    f\"üìÅ Layout RESTORE m5   ‚Üí {str(latest_restore_m5) if restore_exists else '(none)'} \"\n",
    "    f\"(exists={_bool(restore_exists)}, parquet_files={restore_parquet_count})\"\n",
    ")\n",
    "\n",
    "# 3) Metadata PADs\n",
    "PAD_DAY_INDEX      = PAD_DAY_INDEX.resolve()\n",
    "PAD_SYMBOL_INDEX   = PAD_SYMBOL_INDEX.resolve()\n",
    "PAD_WINDOW_CATALOG = PAD_WINDOW_CATALOG.resolve()\n",
    "\n",
    "day_index_found      = PAD_DAY_INDEX.exists() and PAD_DAY_INDEX.is_file()\n",
    "symbol_index_found   = PAD_SYMBOL_INDEX.exists() and PAD_SYMBOL_INDEX.is_file()\n",
    "window_catalog_found = PAD_WINDOW_CATALOG.exists() and PAD_WINDOW_CATALOG.is_file()\n",
    "manifest_found       = PAD_MANIFEST.exists() and PAD_MANIFEST.is_file()\n",
    "catalog_found        = PAD_DATASET_CATALOG.exists() and PAD_DATASET_CATALOG.is_file()\n",
    "\n",
    "print(f\"üìÅ META ‚Üí {str(PAD_SYMBOL_INDEX)} (found={_bool(symbol_index_found)})\")\n",
    "print(f\"üìÅ META ‚Üí {str(PAD_DAY_INDEX)} (found={_bool(day_index_found)})\")\n",
    "print(f\"üìÅ META ‚Üí {str(PAD_WINDOW_CATALOG)} (found={_bool(window_catalog_found)})\")\n",
    "print(f\"üìÅ META ‚Üí {str(PAD_MANIFEST)} (found={_bool(manifest_found)})\")\n",
    "print(f\"üìÅ META ‚Üí {str(PAD_DATASET_CATALOG)} (found={_bool(catalog_found)})\")\n",
    "\n",
    "# 4) Artefactos auxiliares\n",
    "corr_found     = DE_CORR_M5_FILE.exists() and DE_CORR_M5_FILE.is_file()\n",
    "universe_found = DE_UNIVERSE_FILE.exists() and DE_UNIVERSE_FILE.is_file()\n",
    "print(f\"üìÅ INPUT ‚Üí {str(DE_CORR_M5_FILE)} (found={_bool(corr_found)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí {str(DE_UNIVERSE_FILE)} (found={_bool(universe_found)})\")\n",
    "\n",
    "watchlist_found     = DE_WATCHLIST_FILE.exists() and DE_WATCHLIST_FILE.is_file()\n",
    "watchlist_csv_found = DE_WATCHLIST_FALLBACK_CSV.exists() and DE_WATCHLIST_FALLBACK_CSV.is_file()\n",
    "print(\n",
    "    f\"üìÅ INPUT ‚Üí {str(DE_WATCHLIST_FILE)} \"\n",
    "    f\"(found={_bool(watchlist_found)}, fallback_csv={_bool(watchlist_csv_found)})\"\n",
    ")\n",
    "\n",
    "params_found     = DE_PARAMS_FILE.exists() and DE_PARAMS_FILE.is_file()\n",
    "params_csv_found = DE_PARAMS_FALLBACK_CSV.exists() and DE_PARAMS_FALLBACK_CSV.is_file()\n",
    "print(\n",
    "    f\"üìÅ INPUT ‚Üí {str(DE_PARAMS_FILE)} \"\n",
    "    f\"(found={_bool(params_found)}, fallback_csv={_bool(params_csv_found)})\"\n",
    ")\n",
    "\n",
    "if not corr_found:\n",
    "    print(\"‚ö†Ô∏è FALLBACK ‚Üí correlaci√≥n M5 ser√° reconstruida\")\n",
    "\n",
    "# -------------------------------\n",
    "# Selecci√≥n determin√≠stica de fuente M5\n",
    "# -------------------------------\n",
    "M5_SOURCE_TYPE         = None\n",
    "M5_ROOT_DIR            = None\n",
    "symbols_detected: list[str] = []\n",
    "parquet_count_selected = 0\n",
    "\n",
    "if part_exists and part_parquet_count > 0:\n",
    "    M5_SOURCE_TYPE = \"partitioned_m5_clean\"\n",
    "    M5_ROOT_DIR    = PAD_HIST_CLEAN_DIR\n",
    "    symbols_detected = _detect_partitioned_symbols(PAD_HIST_CLEAN_DIR)\n",
    "    parquet_count_selected = part_parquet_count\n",
    "elif m5_raw_exists and m5_raw_parquet_count > 0:\n",
    "    if m5_raw_is_partitioned:\n",
    "        M5_SOURCE_TYPE = \"partitioned_m5_raw\"\n",
    "        symbols_detected = _detect_partitioned_symbols(DE_M5_RAW_DIR)\n",
    "    else:\n",
    "        M5_SOURCE_TYPE = \"flat_m5_raw\"\n",
    "        symbols_detected = _detect_flat_symbols(DE_M5_RAW_DIR)\n",
    "    M5_ROOT_DIR = DE_M5_RAW_DIR\n",
    "    parquet_count_selected = m5_raw_parquet_count\n",
    "elif restore_exists and restore_parquet_count > 0:\n",
    "    M5_SOURCE_TYPE = \"restore_partitioned_m5_clean\"\n",
    "    M5_ROOT_DIR    = latest_restore_m5\n",
    "    symbols_detected = _detect_partitioned_symbols(latest_restore_m5)\n",
    "    parquet_count_selected = restore_parquet_count\n",
    "\n",
    "# Validaci√≥n m√≠nima: si no hay ninguna fuente, fallar de forma amigable\n",
    "if M5_SOURCE_TYPE is None or M5_ROOT_DIR is None:\n",
    "    print(\"[Celda 01][ERROR] No se encontr√≥ ninguna fuente M5 v√°lida.\")\n",
    "    print(f\"  ‚Ä¢ M5_CLEAN_DIR: exists={_bool(part_exists)}, parquet_files={part_parquet_count}\")\n",
    "    print(f\"  ‚Ä¢ m5_raw:       exists={_bool(m5_raw_exists)}, parquet_files={m5_raw_parquet_count}\")\n",
    "    print(f\"  ‚Ä¢ RESTORE m5:   exists={_bool(restore_exists)}, parquet_files={restore_parquet_count}\")\n",
    "    raise RuntimeError(\n",
    "        \"No se encontr√≥ ninguna fuente M5 v√°lida. \"\n",
    "        \"Verifica que MT5_DE_5M_V1 haya generado historical_data/m5_clean, \"\n",
    "        \"o que exista bulk_data/m5_raw, o un restore reciente.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"üîé Fuente M5 seleccionada ‚Üí type={M5_SOURCE_TYPE} | \"\n",
    "    f\"root={str(M5_ROOT_DIR)} | parquet_files={parquet_count_selected}\"\n",
    ")\n",
    "n_syms      = len(symbols_detected)\n",
    "sample_syms = symbols_detected[:10]\n",
    "print(f\"üßæ S√≠mbolos detectados (antes de QA) = {n_syms} | muestra={sample_syms}\")\n",
    "\n",
    "# -------------------------------\n",
    "# QA ticks: spreads y anomal√≠as\n",
    "# -------------------------------\n",
    "spread_threshold = float(cfg_stats.get(\"max_spread_p99_pts\", 99.0))\n",
    "max_anom_pct     = float(cfg_stats.get(\"max_tick_anomalies_pct\", 5.0))\n",
    "\n",
    "if TICKS_QC_SUMMARY_JSON.exists():\n",
    "    try:\n",
    "        qc_data = json.loads(TICKS_QC_SUMMARY_JSON.read_text(encoding=\"utf-8\"))\n",
    "        bad_symbols = set()\n",
    "\n",
    "        # top_symbols_by_median_spread_pts puede ser lista de listas/tuplas o lista de dicts\n",
    "        items = qc_data.get(\"top_symbols_by_median_spread_pts\", []) or []\n",
    "        for item in items:\n",
    "            sym = None\n",
    "            spread_val = None\n",
    "            if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
    "                sym, spread_raw = item[0], item[1]\n",
    "                try:\n",
    "                    spread_val = float(spread_raw)\n",
    "                except Exception:\n",
    "                    spread_val = None\n",
    "            elif isinstance(item, dict):\n",
    "                sym = item.get(\"symbol\")\n",
    "                spread_raw = item.get(\"median_spread_pts\")\n",
    "                try:\n",
    "                    spread_val = float(spread_raw) if spread_raw is not None else None\n",
    "                except Exception:\n",
    "                    spread_val = None\n",
    "\n",
    "            if sym and spread_val is not None and spread_val > spread_threshold:\n",
    "                bad_symbols.add(sym)\n",
    "\n",
    "        anomalies   = qc_data.get(\"anomalies_ask_lt_bid_total\")\n",
    "        total_ticks = qc_data.get(\"n_ticks_total\")\n",
    "        if anomalies is not None and total_ticks:\n",
    "            try:\n",
    "                anomalies_f   = float(anomalies)\n",
    "                total_ticks_f = float(total_ticks)\n",
    "                pct_anomalies = (anomalies_f / total_ticks_f) * 100.0\n",
    "                if pct_anomalies > max_anom_pct:\n",
    "                    print(\n",
    "                        f\"[Celda 01][WARN] Anomal√≠as globales ask<bid = {pct_anomalies:.2f}% \"\n",
    "                        f\"(umbral={max_anom_pct:.2f}%). Considera revisar datos.\"\n",
    "                    )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if bad_symbols:\n",
    "            symbols_detected = [sym for sym in symbols_detected if sym not in bad_symbols]\n",
    "        print(f\"üõ°Ô∏è QA ticks: Excluidos {len(bad_symbols)} s√≠mbolos con spread_p99 > {spread_threshold}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 01][WARN] Error al leer QA ticks: {e}. Saltando exclusi√≥n.\")\n",
    "else:\n",
    "    # ‚úÖ Cambiado a INFO: artefacto opcional del Data Engine\n",
    "    print(\"[Celda 01][INFO] ticks_recent_qc_summary.json no encontrado. QA ticks opcional no aplicado.\")\n",
    "\n",
    "# -------------------------------\n",
    "# QA cobertura m√≠nima (dataset_catalog / day_index)\n",
    "# -------------------------------\n",
    "COVERAGE_PCT_MIN  = float(cfg_stats.get(\"coverage_pct_min\", 30.0))\n",
    "MIN_VALID_SYMBOLS = int(cfg_stats.get(\"coverage_min_symbols\", 50))\n",
    "\n",
    "if catalog_found or day_index_found:\n",
    "    try:\n",
    "        if catalog_found:\n",
    "            catalog_df = pl.read_parquet(PAD_DATASET_CATALOG)\n",
    "        else:\n",
    "            catalog_df = pl.DataFrame()\n",
    "\n",
    "        if catalog_df.is_empty() or \"coverage_pct\" not in catalog_df.columns:\n",
    "            # Reconstruir a partir de day_index si es posible\n",
    "            if day_index_found:\n",
    "                day_df = pl.read_parquet(PAD_DAY_INDEX)\n",
    "                if not day_df.is_empty():\n",
    "                    start_date = datetime(2021, 1, 1, tzinfo=timezone.utc)\n",
    "                    total_days_expected = (datetime.now(timezone.utc) - start_date).days\n",
    "                    catalog_df = (\n",
    "                        day_df\n",
    "                        .group_by(\"symbol\")\n",
    "                        .agg(pl.len().alias(\"days_covered\"))\n",
    "                        .with_columns(\n",
    "                            (pl.col(\"days_covered\") / total_days_expected * 100.0)\n",
    "                            .alias(\"coverage_pct\")\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        if not catalog_df.is_empty() and \"coverage_pct\" in catalog_df.columns:\n",
    "            valid_symbols = (\n",
    "                catalog_df\n",
    "                .filter(pl.col(\"coverage_pct\") >= COVERAGE_PCT_MIN)[\"symbol\"]\n",
    "                .to_list()\n",
    "            )\n",
    "            symbols_detected = [sym for sym in symbols_detected if sym in valid_symbols]\n",
    "            n_valid = len(symbols_detected)\n",
    "            print(\n",
    "                f\"üìä Cobertura: {n_valid} s√≠mbolos con cobertura >= {COVERAGE_PCT_MIN:.0f}% \"\n",
    "                \"de d√≠as con datos desde 2021-01-01\"\n",
    "            )\n",
    "            if n_valid < MIN_VALID_SYMBOLS:\n",
    "                print(\n",
    "                    f\"[Celda 01][WARN] Solo {n_valid} s√≠mbolos superan coverage_pct_min \"\n",
    "                    f\"(>= {COVERAGE_PCT_MIN:.0f}%) vs coverage_min_symbols={MIN_VALID_SYMBOLS}. \"\n",
    "                    \"Continuando, pero datos podr√≠an ser pobres.\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                \"[Celda 01][WARN] No se pudo obtener 'coverage_pct' ni de dataset_catalog.parquet \"\n",
    "                \"ni de day_index_m5.parquet. Saltando check de cobertura.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 01][WARN] Error en check de cobertura: {e}. Saltando filtrado.\")\n",
    "else:\n",
    "    print(\"[Celda 01][WARN] Ni dataset_catalog.parquet ni day_index_m5.parquet disponibles. Saltando cobertura.\")\n",
    "\n",
    "# -------------------------------\n",
    "# QA frescura de timestamps (manifest) ‚Äî SOLO DIAGN√ìSTICO (no filtra)\n",
    "# -------------------------------\n",
    "MAX_AGE_DAYS = int(cfg_stats.get(\"max_data_age_days\", 0))\n",
    "\n",
    "if manifest_found:\n",
    "    try:\n",
    "        manifest_df = pl.read_parquet(PAD_MANIFEST)\n",
    "        now_ms      = int(datetime.now(timezone.utc).timestamp() * 1000)\n",
    "\n",
    "        # age_days = cu√°ntos d√≠as han pasado desde el √∫ltimo timestamp por s√≠mbolo\n",
    "        manifest_df = manifest_df.with_columns(\n",
    "            ((now_ms - pl.col(\"last_ts_utc\")) / (86400 * 1000)).alias(\"age_days\")\n",
    "        )\n",
    "\n",
    "        age_stats = (\n",
    "            manifest_df\n",
    "            .select(\n",
    "                pl.col(\"age_days\").min().alias(\"min_age_days\"),\n",
    "                pl.col(\"age_days\").median().alias(\"median_age_days\"),\n",
    "                pl.col(\"age_days\").max().alias(\"max_age_days\"),\n",
    "            )\n",
    "            .to_dicts()[0]\n",
    "        )\n",
    "        print(\n",
    "            \"[Celda 01] QA timestamps ‚Üí \"\n",
    "            f\"min_age={age_stats['min_age_days']:.1f}d, \"\n",
    "            f\"median_age={age_stats['median_age_days']:.1f}d, \"\n",
    "            f\"max_age={age_stats['max_age_days']:.1f}d\"\n",
    "        )\n",
    "\n",
    "        if MAX_AGE_DAYS > 0:\n",
    "            fresh_list = (\n",
    "                manifest_df\n",
    "                .filter(pl.col(\"age_days\") <= MAX_AGE_DAYS)[\"symbol\"]\n",
    "                .to_list()\n",
    "            )\n",
    "            print(\n",
    "                f\"[Celda 01] QA timestamps (diagn√≥stico): \"\n",
    "                f\"{len(fresh_list)} s√≠mbolos con age_days <= {MAX_AGE_DAYS}\"\n",
    "            )\n",
    "\n",
    "        # IMPORTANTE: NO tocamos symbols_detected aqu√≠.\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 01][WARN] Error al validar timestamps: {e}. Saltando diagn√≥stico.\")\n",
    "else:\n",
    "    print(\"[Celda 01][INFO] m5_manifest.parquet no encontrado. Saltando diagn√≥stico de timestamps.\")\n",
    "\n",
    "# Estado final de s√≠mbolos detectados (tras QA ticks + cobertura; SIN filtro de frescura)\n",
    "n_syms      = len(symbols_detected)\n",
    "sample_syms = symbols_detected[:10]\n",
    "print(f\"üßæ S√≠mbolos finales post-QA = {n_syms} | muestra={sample_syms}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Elegir watchlist/params efectivos con fallback\n",
    "# -------------------------------\n",
    "watchlist_selected_path = (\n",
    "    DE_WATCHLIST_FILE if watchlist_found\n",
    "    else (DE_WATCHLIST_FALLBACK_CSV if watchlist_csv_found else Path(\"\"))\n",
    ")\n",
    "watchlist_selected_fmt = \"parquet\" if watchlist_found else (\"csv\" if watchlist_csv_found else \"none\")\n",
    "\n",
    "params_selected_path = (\n",
    "    DE_PARAMS_FILE if params_found\n",
    "    else (DE_PARAMS_FALLBACK_CSV if params_csv_found else Path(\"\"))\n",
    ")\n",
    "params_selected_fmt = \"parquet\" if params_found else (\"csv\" if params_csv_found else \"none\")\n",
    "\n",
    "# ‚úÖ Cambiado a INFO: artefactos opcionales del Data Engine\n",
    "if watchlist_selected_fmt == \"none\":\n",
    "    print(\"[Celda 01][INFO] Watchlist no encontrada (parquet/csv). Se generar√° m√°s adelante si el pipeline la requiere.\")\n",
    "if params_selected_fmt == \"none\":\n",
    "    print(\"[Celda 01][INFO] ea_params no encontrado (parquet/csv). Se generar√° m√°s adelante si el pipeline lo requiere.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Persistir diagnostics/handshake_summary.json (PAD-aware)\n",
    "# -------------------------------\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "handshake_path = OUT_DIAG_DIR / \"handshake_summary.json\"\n",
    "\n",
    "summary = {\n",
    "    \"timestamp_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"metadata_dir\": str(METADATA_DIR),\n",
    "    \"m5_clean_dir\": str(M5_CLEAN_DIR),\n",
    "    \"m5_raw_dir\": str(M5_RAW_DIR),\n",
    "    \"processed_data_dir\": str(PROCESSED_DATA_DIR),\n",
    "    \"m5_source\": {\n",
    "        \"type\": M5_SOURCE_TYPE,\n",
    "        \"root_dir\": str(M5_ROOT_DIR),\n",
    "        \"parquet_count\": parquet_count_selected,\n",
    "        \"symbols_count\": n_syms,\n",
    "        \"symbols_sample\": sample_syms,\n",
    "        \"m5_raw_layout\": {\n",
    "            \"path\": str(DE_M5_RAW_DIR),\n",
    "            \"exists\": m5_raw_exists,\n",
    "            \"parquet_count_recursive\": m5_raw_parquet_count,\n",
    "            \"is_partitioned\": m5_raw_is_partitioned,\n",
    "        },\n",
    "        \"partitioned_layout\": {\n",
    "            \"path\": str(PAD_HIST_CLEAN_DIR),\n",
    "            \"exists\": part_exists,\n",
    "            \"parquet_count_recursive\": part_parquet_count,\n",
    "        },\n",
    "        \"restore_layout\": {\n",
    "            \"restore_root\": str(PAD_RESTORE_ROOT),\n",
    "            \"m5_clean_latest\": str(latest_restore_m5) if restore_exists else \"\",\n",
    "            \"exists\": restore_exists,\n",
    "            \"parquet_count_recursive\": restore_parquet_count,\n",
    "        },\n",
    "    },\n",
    "    \"metadata_pads\": {\n",
    "        \"symbol_index\": {\"path\": str(PAD_SYMBOL_INDEX), \"exists\": symbol_index_found},\n",
    "        \"day_index\": {\"path\": str(PAD_DAY_INDEX), \"exists\": day_index_found},\n",
    "        \"window_catalog\": {\"path\": str(PAD_WINDOW_CATALOG), \"exists\": window_catalog_found},\n",
    "        \"manifest\": {\"path\": str(PAD_MANIFEST), \"exists\": manifest_found},\n",
    "        \"dataset_catalog\": {\"path\": str(PAD_DATASET_CATALOG), \"exists\": catalog_found},\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"corr_matrix_5m_csv\": {\"path\": str(DE_CORR_M5_FILE), \"exists\": corr_found},\n",
    "        \"universe_ranked_parquet\": {\"path\": str(DE_UNIVERSE_FILE), \"exists\": universe_found},\n",
    "        \"watchlist\": {\n",
    "            \"parquet_path\": str(DE_WATCHLIST_FILE),\n",
    "            \"exists_parquet\": bool(watchlist_found),\n",
    "            \"csv_path\": str(DE_WATCHLIST_FALLBACK_CSV),\n",
    "            \"exists_csv\": bool(watchlist_csv_found),\n",
    "            \"selected_path\": str(watchlist_selected_path),\n",
    "            \"selected_format\": watchlist_selected_fmt,\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"parquet_path\": str(DE_PARAMS_FILE),\n",
    "            \"exists_parquet\": bool(params_found),\n",
    "            \"csv_path\": str(DE_PARAMS_FALLBACK_CSV),\n",
    "            \"exists_csv\": bool(params_csv_found),\n",
    "            \"selected_path\": str(params_selected_path),\n",
    "            \"selected_format\": params_selected_fmt,\n",
    "        },\n",
    "    },\n",
    "    \"flags\": {\n",
    "        \"fallback_corr\": (not corr_found),\n",
    "        \"using_partitioned\": (M5_SOURCE_TYPE not in (\"flat_m5_raw\",)),\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"has_policy\": bool(cfg_policy),\n",
    "        \"has_stats\": bool(cfg_stats),\n",
    "        \"has_scores\": bool(cfg_scores),\n",
    "        \"has_baskets\": bool(cfg_baskets),\n",
    "        \"stats_volume_min_notional\": cfg_stats.get(\"volume_min_notional\"),\n",
    "        \"stats_volatility_band_refs\": cfg_stats.get(\"volatility_band_refs\"),\n",
    "        \"coverage_pct_min\": cfg_stats.get(\"coverage_pct_min\"),\n",
    "        \"coverage_min_symbols\": cfg_stats.get(\"coverage_min_symbols\"),\n",
    "        \"max_spread_p99_pts\": cfg_stats.get(\"max_spread_p99_pts\"),\n",
    "        \"max_tick_anomalies_pct\": cfg_stats.get(\"max_tick_anomalies_pct\"),\n",
    "        \"max_data_age_days\": cfg_stats.get(\"max_data_age_days\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "txt = json.dumps(summary, ensure_ascii=False, indent=2, sort_keys=True)\n",
    "handshake_path.write_text(txt, encoding=\"utf-8\")\n",
    "size_bytes = handshake_path.stat().st_size\n",
    "if size_bytes <= 0:\n",
    "    raise RuntimeError(\"[Celda 01] handshake_summary.json no escrito correctamente (size=0)\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(handshake_path)} (OK, bytes={size_bytes})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Actualizar GLOBAL_STATE en memoria\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"inputs\", {})\n",
    "GLOBAL_STATE.setdefault(\"flags\", {})\n",
    "GLOBAL_STATE.setdefault(\"symbols\", {})\n",
    "GLOBAL_STATE.setdefault(\"paths\", {})\n",
    "\n",
    "GLOBAL_STATE[\"inputs\"].update({\n",
    "    # Layout base del Data Engine\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "    \"METADATA_DIR\": str(METADATA_DIR),\n",
    "    \"M5_CLEAN_DIR\": str(M5_CLEAN_DIR),\n",
    "    \"M5_RAW_DIR\": str(M5_RAW_DIR),\n",
    "    \"PROCESSED_DATA_DIR\": str(PROCESSED_DATA_DIR),\n",
    "    # Fuentes M5\n",
    "    \"M5_SOURCE_TYPE\": M5_SOURCE_TYPE,\n",
    "    \"M5_ROOT_DIR\": str(M5_ROOT_DIR),\n",
    "    \"DE_M5_RAW_DIR\": str(DE_M5_RAW_DIR),\n",
    "    \"PAD_HIST_CLEAN_DIR\": str(PAD_HIST_CLEAN_DIR),\n",
    "    \"PAD_RESTORE_ROOT\": str(PAD_RESTORE_ROOT),\n",
    "    # Metadata PADs\n",
    "    \"PAD_SYMBOL_INDEX\": str(PAD_SYMBOL_INDEX),\n",
    "    \"PAD_DAY_INDEX\": str(PAD_DAY_INDEX),\n",
    "    \"PAD_WINDOW_CATALOG\": str(PAD_WINDOW_CATALOG),\n",
    "    \"PAD_MANIFEST\": str(PAD_MANIFEST),\n",
    "    \"PAD_DATASET_CATALOG\": str(PAD_DATASET_CATALOG),\n",
    "    # Artefactos auxiliares\n",
    "    \"DE_CORR_M5_FILE\": str(DE_CORR_M5_FILE),\n",
    "    \"DE_UNIVERSE_FILE\": str(DE_UNIVERSE_FILE),\n",
    "    \"DE_WATCHLIST_FILE\": str(DE_WATCHLIST_FILE),\n",
    "    \"DE_WATCHLIST_FALLBACK_CSV\": str(DE_WATCHLIST_FALLBACK_CSV),\n",
    "    \"DE_PARAMS_FILE\": str(DE_PARAMS_FILE),\n",
    "    \"DE_PARAMS_FALLBACK_CSV\": str(DE_PARAMS_FALLBACK_CSV),\n",
    "    \"WATCHLIST_SELECTED_PATH\": str(watchlist_selected_path),\n",
    "    \"WATCHLIST_SELECTED_FORMAT\": watchlist_selected_fmt,\n",
    "    \"PARAMS_SELECTED_PATH\": str(params_selected_path),\n",
    "    \"PARAMS_SELECTED_FORMAT\": params_selected_fmt,\n",
    "})\n",
    "\n",
    "# Propagar METADATA_DIR al espacio de rutas del ER Filter (para Celda 02+)\n",
    "GLOBAL_STATE[\"paths\"][\"metadata\"] = str(METADATA_DIR)\n",
    "\n",
    "# Flags y s√≠mbolos detectados (universo M5 post-QA ticks/cobertura)\n",
    "GLOBAL_STATE[\"flags\"][\"fallback_corr\"]        = not corr_found\n",
    "GLOBAL_STATE[\"flags\"][\"using_partitioned\"]    = (M5_SOURCE_TYPE not in (\"flat_m5_raw\",))\n",
    "GLOBAL_STATE[\"flags\"][\"handshake_done\"]       = True\n",
    "GLOBAL_STATE[\"flags\"][\"data_engine_attached\"] = True\n",
    "\n",
    "GLOBAL_STATE[\"symbols\"][\"detected\"] = symbols_detected  # lista (se filtrar√° en celdas posteriores)\n",
    "\n",
    "# Resumen compacto de Data Engine en GLOBAL_STATE[\"data_engine\"]\n",
    "GLOBAL_STATE[\"data_engine\"] = {\n",
    "    \"m5_source_type\": M5_SOURCE_TYPE,\n",
    "    \"m5_root_dir\": str(M5_ROOT_DIR),\n",
    "    \"metadata_dir\": str(METADATA_DIR),\n",
    "    \"has_universe_ranked\": bool(universe_found),\n",
    "    \"has_corr_matrix\": bool(corr_found),\n",
    "    \"handshake_summary\": str(handshake_path),\n",
    "}\n",
    "\n",
    "print(\"[Celda 01] GLOBAL_STATE['inputs'] actualizado con rutas de Data Engine y fuente M5.\")\n",
    "print(\"[Celda 01] GLOBAL_STATE['paths']['metadata'] =\", GLOBAL_STATE[\"paths\"][\"metadata\"])\n",
    "print(\"[Celda 01] GLOBAL_STATE['data_engine'] =\", GLOBAL_STATE[\"data_engine\"])\n",
    "print(\">>> Celda 01 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848dbe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 02 :: Carga de artifacts (versi√≥n SIN HUMO)\n",
      "‚úÖ Integridad √≠ndices PAD: day_index=92, symbol_index=92 (discrepancia=0.00%)\n",
      "[Celda 02] S√≠mbolos detectados en rates M5 (UNIVERSO GOLD M5 base) = 87\n",
      "üõ°Ô∏è Corte anti-look-ahead (informativo): datos hasta 2025-11-18\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\watchlist_autogen.parquet (rows=92, cols=4, format=generated)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\ea_params.parquet (rows=107, cols=4, format=parquet)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\processed_data\\universe_ranked.parquet (found=false)\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\processed_data\\corr_matrix_5m.csv (found=false)\n",
      "‚ö†Ô∏è FALLBACK ‚Üí correlaci√≥n M5 ser√° reconstruida m√°s adelante si el pipeline la requiere.\n",
      "[Celda 02] n_watchlist=92, n_params=107, n_universe_raw=0, n_rates_m5=87\n",
      "üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params)             = 89\n",
      "üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params, universe)  = 0\n",
      "üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params, rates_m5)  = 84\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\data_quality_summary.parquet (rows=108, cols=13, format=parquet)\n",
      "üìä DATA QUALITY ‚Üí n_symbols=108\n",
      "üìÅ INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\universe_snapshot_latest.parquet (rows=107, cols=50, format=parquet)\n",
      "üìä UNIVERSE SNAPSHOT ‚Üí n_symbols=107\n",
      "[Celda 02] UNIVERSO EFECTIVO M5 (s√≠mbolos con datos + config) = 84\n",
      "üìä Distribuci√≥n n_days_total ‚Üí min=1423, median=1475.0, max=1475\n",
      "üìä Distribuci√≥n coverage_days ‚Üí min=0.686, median=0.711, max=0.997\n",
      "üßæ Muestra universe_effective (primeros s√≠mbolos):\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ first_date_m5 ‚îÜ last_date_m5 ‚îÜ n_days_total ‚îÜ n_days_with_bars ‚îÜ coverage_days ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---           ‚îÜ ---          ‚îÜ ---          ‚îÜ ---              ‚îÜ ---           ‚îÇ\n",
      "‚îÇ str    ‚îÜ date          ‚îÜ date         ‚îÜ i32          ‚îÜ u32              ‚îÜ f64           ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19    ‚îÜ 2025-12-01   ‚îÜ 1474         ‚îÜ 1011             ‚îÜ 0.685889      ‚îÇ\n",
      "‚îÇ AAVUSD ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1471             ‚îÜ 0.997288      ‚îÇ\n",
      "‚îÇ ADAUSD ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1469             ‚îÜ 0.995932      ‚îÇ\n",
      "‚îÇ ALGUSD ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1467             ‚îÜ 0.994576      ‚îÇ\n",
      "‚îÇ ALVG   ‚îÜ 2021-11-19    ‚îÜ 2025-12-01   ‚îÜ 1474         ‚îÜ 1030             ‚îÜ 0.698779      ‚îÇ\n",
      "‚îÇ AMZN   ‚îÜ 2021-11-19    ‚îÜ 2025-12-01   ‚îÜ 1474         ‚îÜ 1011             ‚îÜ 0.685889      ‚îÇ\n",
      "‚îÇ AUDCAD ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1048             ‚îÜ 0.710508      ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1048             ‚îÜ 0.710508      ‚îÇ\n",
      "‚îÇ AUDJPY ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1048             ‚îÜ 0.710508      ‚îÇ\n",
      "‚îÇ AUDNZD ‚îÜ 2021-11-19    ‚îÜ 2025-12-02   ‚îÜ 1475         ‚îÜ 1048             ‚îÜ 0.710508      ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 02] Universo efectivo M5 FINAL ‚Üí n_symbols=84 (de los cuales 0 ten√≠an metadatos en universe_raw)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\artifacts_summary.json (OK, bytes=14259)\n",
      "üìå Resumen Celda 02 ‚Üí n_universe_raw=0, n_universe_efectivo_M5=84, n_watchlist=92, n_params=107, corr_matrix_disponible=False, data_quality_disponible=True, universe_snapshot_disponible=True\n",
      ">>> Celda 02 :: OK (universo GOLD base + universo efectivo con coverage/flags, sin mirar edge)\n"
     ]
    }
   ],
   "source": [
    "# Celda 02 ‚Äî Universo efectivo M5 (GOLD-based, sin humo) ‚Äî ER_FILTER_5M_V2\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Objetivo (versi√≥n \"SIN HUMO\"):\n",
    "#   - Cargar TODOS los artefactos necesarios para definir el entorno de trabajo M5,\n",
    "#     pero SIN filtrar agresivamente el universo aqu√≠.\n",
    "#\n",
    "#   - IMPORTANTE (conceptual):\n",
    "#       Esta celda NO mira el edge (ER, PD, p_event ni performance).\n",
    "#       Solo verifica elegibilidad de datos y configuraci√≥n m√≠nima.\n",
    "#       El edge, la significancia estad√≠stica y la estabilidad temporal\n",
    "#       se medir√°n en celdas posteriores del pipeline (08/09/10).\n",
    "#\n",
    "#   - Artefactos que se consumen:\n",
    "#       * Universe (Data Engine):\n",
    "#           - Ranking global de activos (universe_ranked.parquet).\n",
    "#           - Se usa como METADATO (rank, familia, etc.), NO como filtro duro.\n",
    "#\n",
    "#       * Corr matrix:\n",
    "#           - Matriz de correlaciones M5 (corr_matrix_5m.csv).\n",
    "#           - Si existe, validamos que sea legible.\n",
    "#           - Si NO existe, marcamos flag y dejamos reconstrucci√≥n para m√°s adelante.\n",
    "#\n",
    "#       * Watchlist:\n",
    "#           - Lista operativa de s√≠mbolos (manual o autogenerada desde PADs).\n",
    "#\n",
    "#       * Params:\n",
    "#           - Par√°metros por s√≠mbolo para EAs (comisi√≥n, spread, preset, etc.).\n",
    "#\n",
    "#       * Data Quality (opcional):\n",
    "#           - data_quality_summary.parquet (del Data Engine).\n",
    "#\n",
    "#       * Universe Snapshot por RUN_ID (opcional):\n",
    "#           - universe_snapshot_<RUN_ID>.parquet (del Data Engine),\n",
    "#             o universe_snapshot_latest.parquet si existe.\n",
    "#\n",
    "#   - UNIVERSO GOLD M5 vs UNIVERSO EFECTIVO M5:\n",
    "#       - Universo GOLD M5 (base)         ‚Üí rates_syms\n",
    "#           = todos los s√≠mbolos con datos M5 limpios\n",
    "#             que sobreviven a los filtros de Celda 01\n",
    "#             (QA ticks, cobertura; frescura solo diagn√≥stica si as√≠ qued√≥ en 01).\n",
    "#\n",
    "#       - Universo EFECTIVO M5 (con config) ‚Üí universe_effective_syms\n",
    "#           = s√≠mbolos que:\n",
    "#               ¬∑ Est√°n en GOLD (rates_syms)\n",
    "#               ¬∑ Y adem√°s tienen watchlist y params\n",
    "#\n",
    "#         Implementaci√≥n:\n",
    "#             base_syms = watchlist ‚à© params ‚à© rates_m5\n",
    "#         (y si eso queda vac√≠o, se relaja a parejas/union con rates_m5).\n",
    "#\n",
    "#       - universe_ranked / universe_snapshot se usan solo para a√±adir metadata\n",
    "#         (rank, familia, etc.), NUNCA para expulsar s√≠mbolos.\n",
    "#\n",
    "#   - Esta celda S√ç construye:\n",
    "#       * first_date_m5, last_date_m5\n",
    "#       * n_days_total, n_days_with_bars, coverage_days\n",
    "#       * flags: short_history_flag, poor_day_coverage\n",
    "#     PERO NO mata s√≠mbolos por estos flags; solo marca.\n",
    "#\n",
    "#   - GLOBAL_STATE (salida):\n",
    "#       - GLOBAL_STATE[\"tables\"][\"watchlist\"]\n",
    "#       - GLOBAL_STATE[\"tables\"][\"params\"]\n",
    "#       - GLOBAL_STATE[\"tables\"][\"universe_raw\"]        ‚Üí universe del Data Engine (tal cual, si existe)\n",
    "#       - GLOBAL_STATE[\"tables\"][\"universe\"]            ‚Üí universo EFECTIVO M5 (con coverage y flags)\n",
    "#       - GLOBAL_STATE[\"tables\"][\"corr_matrix_path\"]\n",
    "#       - GLOBAL_STATE[\"tables\"][\"data_quality\"]        (opcional)\n",
    "#       - GLOBAL_STATE[\"tables\"][\"universe_snapshot\"]   (opcional)\n",
    "#\n",
    "#       - GLOBAL_STATE[\"symbols\"][\"rates_m5\"]           ‚Üí UNIVERSO GOLD M5 (base)\n",
    "#       - GLOBAL_STATE[\"symbols\"][\"wl\"]\n",
    "#       - GLOBAL_STATE[\"symbols\"][\"pr\"]\n",
    "#       - GLOBAL_STATE[\"symbols\"][\"un_raw\"]             ‚Üí s√≠mbolos del universe_raw\n",
    "#       - GLOBAL_STATE[\"symbols\"][\"universe_effective\"] ‚Üí UNIVERSO EFECTIVO M5 (GOLD + config)\n",
    "#\n",
    "#       - GLOBAL_STATE[\"flags\"][\"corr_matrix_available\"]\n",
    "#       - GLOBAL_STATE[\"flags\"][\"data_quality_available\"]\n",
    "#       - GLOBAL_STATE[\"flags\"][\"universe_snapshot_available\"]\n",
    "#\n",
    "#   - artifacts_summary.json:\n",
    "#       - Registra rutas, formatos, shapes y conjuntos de s√≠mbolos,\n",
    "#         para auditar cualquier filtrado extra√±o en pasos posteriores.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Se requiere 'polars' para la Celda 02 | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Utilidades\n",
    "# -------------------------------\n",
    "def norm_symbol(x: str) -> str:\n",
    "    \"\"\"Normalizaci√≥n determin√≠stica de s√≠mbolo: may√∫sculas, sin espacios.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    return str(x).strip().upper().replace(\" \", \"\")\n",
    "\n",
    "def _load_table(path: Path, expected_name: str, fmt_hint: Optional[str] = None) -> Tuple[pl.DataFrame, str]:\n",
    "    \"\"\"Carga tabla parquet/csv en Polars, con control de errores y formato devuelto.\"\"\"\n",
    "    if fmt_hint is not None:\n",
    "        fmt = str(fmt_hint).lower()\n",
    "        if fmt == \"parquet\":\n",
    "            if not path.exists() or not path.is_file():\n",
    "                raise RuntimeError(f\"{expected_name}: archivo parquet no encontrado: {str(path)}\")\n",
    "            try:\n",
    "                return pl.read_parquet(str(path)), \"parquet\"\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"{expected_name}: error leyendo parquet {str(path)} | reason={e}\")\n",
    "        if fmt == \"csv\":\n",
    "            if not path.exists() or not path.is_file():\n",
    "                raise RuntimeError(f\"{expected_name}: archivo csv no encontrado: {str(path)}\")\n",
    "            try:\n",
    "                return pl.read_csv(str(path)), \"csv\"\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"{expected_name}: error leyendo csv {str(path)} | reason={e}\")\n",
    "        if fmt == \"none\":\n",
    "            raise RuntimeError(f\"{expected_name}: formato 'none' sin ruta seleccionada (no existe ni parquet ni csv)\")\n",
    "        raise RuntimeError(f\"{expected_name}: fmt_hint inv√°lido: {fmt_hint!r}\")\n",
    "\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".parquet\":\n",
    "        try:\n",
    "            return pl.read_parquet(str(path)), \"parquet\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"{expected_name}: error leyendo parquet {str(path)} | reason={e}\")\n",
    "    if suf == \".csv\":\n",
    "        try:\n",
    "            return pl.read_csv(str(path)), \"csv\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"{expected_name}: error leyendo csv {str(path)} | reason={e}\")\n",
    "\n",
    "    pq = path.with_suffix(\".parquet\")\n",
    "    if pq.exists():\n",
    "        try:\n",
    "            return pl.read_parquet(str(pq)), \"parquet\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"{expected_name}: error leyendo parquet {str(pq)} | reason={e}\")\n",
    "    cs = path.with_suffix(\".csv\")\n",
    "    if cs.exists():\n",
    "        try:\n",
    "            return pl.read_csv(str(cs)), \"csv\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"{expected_name}: error leyendo csv {str(cs)} | reason={e}\")\n",
    "    raise RuntimeError(f\"{expected_name}: no se pudo inferir formato ni encontrar archivo: {str(path)}\")\n",
    "\n",
    "def _extract_symbols_from_partitioned(root: Path, max_symbols: int = 200_000) -> list[str]:\n",
    "    \"\"\"Detecta s√≠mbolos en layout particionado (symbol=SY...).\"\"\"\n",
    "    syms: list[str] = []\n",
    "    if root.exists() and root.is_dir():\n",
    "        for sub in root.glob(\"symbol=*\"):\n",
    "            if sub.is_dir():\n",
    "                sym = sub.name.split(\"symbol=\", 1)[-1].strip()\n",
    "                if sym:\n",
    "                    syms.append(norm_symbol(sym))\n",
    "                    if len(syms) >= max_symbols:\n",
    "                        break\n",
    "    return syms\n",
    "\n",
    "def _extract_symbols_from_flat(rates_dir: Path, max_symbols: int = 200_000) -> list[str]:\n",
    "    \"\"\"Detecta s√≠mbolos por archivos *.parquet en layout plano.\"\"\"\n",
    "    syms: list[str] = []\n",
    "    if rates_dir.exists() and rates_dir.is_dir():\n",
    "        for p in rates_dir.glob(\"*.parquet\"):\n",
    "            if p.is_file():\n",
    "                sym = norm_symbol(p.stem)\n",
    "                if sym:\n",
    "                    syms.append(sym)\n",
    "                    if len(syms) >= max_symbols:\n",
    "                        break\n",
    "    return syms\n",
    "\n",
    "def _autogen_watchlist_from_pads(\n",
    "    out_dir: Path,\n",
    "    pad_symbol_index: Path,\n",
    "    m5_root: Path,\n",
    "    source_type: str\n",
    ") -> Tuple[pl.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    Autogenera una watchlist si no existe:\n",
    "      - Rama preferente: PAD_SYMBOL_INDEX (col 'symbol').\n",
    "      - Si no existe / falla: detecci√≥n por layout M5 (partitioned/flat).\n",
    "    Escribe un parquet en diagnostics para trazabilidad.\n",
    "    \"\"\"\n",
    "    syms: list[str] = []\n",
    "    if pad_symbol_index.exists() and pad_symbol_index.is_file():\n",
    "        try:\n",
    "            df = pl.read_parquet(str(pad_symbol_index))\n",
    "            df = df.rename({c: c.lower() for c in df.columns})\n",
    "            if \"symbol\" in df.columns:\n",
    "                syms = [norm_symbol(s) for s in df.get_column(\"symbol\").to_list()]\n",
    "        except Exception:\n",
    "            syms = []\n",
    "\n",
    "    if not syms:\n",
    "        if source_type == \"flat_m5_raw\":\n",
    "            syms = _extract_symbols_from_flat(m5_root)\n",
    "        else:\n",
    "            syms = _extract_symbols_from_partitioned(m5_root)\n",
    "\n",
    "    syms = sorted(set([s for s in syms if s]))\n",
    "    wl_df = pl.DataFrame(\n",
    "        {\n",
    "            \"symbol\": syms,\n",
    "            \"enabled\": [True] * len(syms),\n",
    "            \"category\": [None] * len(syms),\n",
    "            \"min_spread\": [None] * len(syms),\n",
    "        }\n",
    "    )\n",
    "    out_path = out_dir / \"watchlist_autogen.parquet\"\n",
    "    wl_df.write_parquet(str(out_path))\n",
    "    return wl_df, out_path\n",
    "\n",
    "def _autogen_params_from_pads(\n",
    "    out_dir: Path,\n",
    "    pad_day_index: Path,\n",
    "    wl_syms: list[str]\n",
    ") -> Tuple[pl.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    Autogenera params si no existe:\n",
    "      - commission = 0.0 (conservador; se ajustar√° m√°s adelante).\n",
    "      - spread_est = mediana de 'spread_points' del √∫ltimo d√≠a disponible por s√≠mbolo,\n",
    "                     si hay path usable en PAD_DAY_INDEX; si no, None.\n",
    "    Escribe parquet en diagnostics para trazabilidad.\n",
    "    \"\"\"\n",
    "    symbols_out: list[str] = []\n",
    "    spread_est_list: list[Optional[float]] = []\n",
    "    commission_list: list[float] = []\n",
    "\n",
    "    last_path_map: dict[str, Path] = {}\n",
    "    if pad_day_index.exists() and pad_day_index.is_file():\n",
    "        try:\n",
    "            di = pl.read_parquet(str(pad_day_index))\n",
    "            di = di.rename({c: c.lower() for c in di.columns})\n",
    "            if {\"symbol\", \"date\", \"path\"}.issubset(set(di.columns)):\n",
    "                di = di.with_columns(pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\"))\n",
    "                di_sorted = di.sort(by=[\"symbol\", \"date\"], descending=[False, True])\n",
    "                last = di_sorted.group_by(\"symbol\").agg(pl.first(\"path\").alias(\"last_path\"))\n",
    "                for row in last.iter_rows(named=True):\n",
    "                    lp = row.get(\"last_path\")\n",
    "                    if lp:\n",
    "                        last_path_map[str(row[\"symbol\"])] = Path(lp)\n",
    "        except Exception:\n",
    "            last_path_map = {}\n",
    "\n",
    "    for sym in wl_syms:\n",
    "        median_spread = None\n",
    "        p = last_path_map.get(sym)\n",
    "        if p and p.exists() and p.is_file():\n",
    "            try:\n",
    "                lf = pl.scan_parquet(str(p))\n",
    "                cols = lf.collect_schema().names()\n",
    "                if \"spread_points\" in cols:\n",
    "                    median_spread = (\n",
    "                        lf.select(\n",
    "                            pl.col(\"spread_points\")\n",
    "                            .cast(pl.Float64)\n",
    "                            .median()\n",
    "                            .alias(\"median_spread\")\n",
    "                        )\n",
    "                        .collect()[\"median_spread\"][0]\n",
    "                    )\n",
    "            except Exception:\n",
    "                median_spread = None\n",
    "\n",
    "        symbols_out.append(sym)\n",
    "        commission_list.append(0.0)\n",
    "        spread_est_list.append(median_spread)\n",
    "\n",
    "    pr_df = pl.DataFrame(\n",
    "        {\n",
    "            \"symbol\": symbols_out,\n",
    "            \"commission\": commission_list,\n",
    "            \"spread_rule\": [None] * len(symbols_out),\n",
    "            \"spread_est\": spread_est_list,\n",
    "        }\n",
    "    )\n",
    "    out_path = out_dir / \"params_autogen.parquet\"\n",
    "    pr_df.write_parquet(str(out_path))\n",
    "    return pr_df, out_path\n",
    "\n",
    "def _print_input_loaded(path: Path, rows: int, cols: int, fmt: str):\n",
    "    print(f\"üìÅ INPUT ‚Üí {str(path)} (rows={rows}, cols={cols}, format={fmt})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones previas de entorno\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero la Celda 00 y 01.\")\n",
    "\n",
    "for k in (\"inputs\", \"paths\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE incompleto: falta clave '{k}'.\")\n",
    "\n",
    "inputs = GLOBAL_STATE[\"inputs\"]\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "required_inputs = [\n",
    "    \"M5_SOURCE_TYPE\", \"M5_ROOT_DIR\",\n",
    "    \"DE_M5_RAW_DIR\", \"PAD_HIST_CLEAN_DIR\", \"PAD_RESTORE_ROOT\",\n",
    "    \"DE_CORR_M5_FILE\", \"DE_UNIVERSE_FILE\",\n",
    "    \"WATCHLIST_SELECTED_PATH\", \"WATCHLIST_SELECTED_FORMAT\",\n",
    "    \"PARAMS_SELECTED_PATH\", \"PARAMS_SELECTED_FORMAT\",\n",
    "    \"PAD_SYMBOL_INDEX\", \"PAD_DAY_INDEX\",\n",
    "]\n",
    "for k in required_inputs:\n",
    "    if k not in inputs:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE['inputs']['{k}'] no est√° definido. Revisa Celda 01.\")\n",
    "\n",
    "# Paths seleccionados / PADs\n",
    "WATCHLIST_SELECTED_PATH = Path(inputs[\"WATCHLIST_SELECTED_PATH\"]) if inputs[\"WATCHLIST_SELECTED_PATH\"] else Path(\"\")\n",
    "WATCHLIST_SELECTED_FORMAT = inputs[\"WATCHLIST_SELECTED_FORMAT\"]\n",
    "PARAMS_SELECTED_PATH = Path(inputs[\"PARAMS_SELECTED_PATH\"]) if inputs[\"PARAMS_SELECTED_PATH\"] else Path(\"\")\n",
    "PARAMS_SELECTED_FORMAT = inputs[\"PARAMS_SELECTED_FORMAT\"]\n",
    "\n",
    "DE_UNIVERSE_FILE = Path(inputs[\"DE_UNIVERSE_FILE\"]).resolve()\n",
    "DE_CORR_M5_FILE = Path(inputs[\"DE_CORR_M5_FILE\"]).resolve()\n",
    "\n",
    "M5_SOURCE_TYPE = inputs[\"M5_SOURCE_TYPE\"]\n",
    "M5_ROOT_DIR = Path(inputs[\"M5_ROOT_DIR\"]).resolve()\n",
    "DE_M5_RAW_DIR = Path(inputs[\"DE_M5_RAW_DIR\"]).resolve()\n",
    "PAD_HIST_CLEAN_DIR = Path(inputs[\"PAD_HIST_CLEAN_DIR\"]).resolve()\n",
    "PAD_RESTORE_ROOT = Path(inputs[\"PAD_RESTORE_ROOT\"]).resolve()\n",
    "PAD_SYMBOL_INDEX = Path(inputs[\"PAD_SYMBOL_INDEX\"]).resolve()\n",
    "PAD_DAY_INDEX = Path(inputs[\"PAD_DAY_INDEX\"]).resolve()\n",
    "\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "\n",
    "# Directorio de metadata (opcional, para data_quality y universe_snapshot)\n",
    "METADATA_DIR = None\n",
    "_metadata_raw = paths.get(\"metadata\")\n",
    "if _metadata_raw:\n",
    "    try:\n",
    "        METADATA_DIR = Path(_metadata_raw).resolve()\n",
    "    except Exception:\n",
    "        METADATA_DIR = None\n",
    "\n",
    "# Paths opcionales a artefactos del Data Engine\n",
    "DQ_FILE_RAW = inputs.get(\"DE_DATA_QUALITY_FILE\") or inputs.get(\"DATA_QUALITY_SUMMARY_FILE\") or \"\"\n",
    "UNIV_SNAP_RAW = inputs.get(\"DE_UNIVERSE_SNAPSHOT_FILE\") or inputs.get(\"UNIVERSE_SNAPSHOT_FILE\") or \"\"\n",
    "\n",
    "DQ_FILE = Path(DQ_FILE_RAW).resolve() if DQ_FILE_RAW else None\n",
    "UNIV_SNAP_FILE = Path(UNIV_SNAP_RAW).resolve() if UNIV_SNAP_RAW else None\n",
    "\n",
    "# Fallbacks desde METADATA_DIR\n",
    "if DQ_FILE is None and METADATA_DIR is not None:\n",
    "    candidate = METADATA_DIR / \"data_quality_summary.parquet\"\n",
    "    if candidate.exists() and candidate.is_file():\n",
    "        DQ_FILE = candidate\n",
    "\n",
    "if UNIV_SNAP_FILE is None and METADATA_DIR is not None:\n",
    "    # 1) Prioridad absoluta: universe_snapshot_latest.parquet (si existe)\n",
    "    latest_named = METADATA_DIR / \"universe_snapshot_latest.parquet\"\n",
    "    if latest_named.exists() and latest_named.is_file():\n",
    "        UNIV_SNAP_FILE = latest_named\n",
    "    else:\n",
    "        # 2) RUN_ID si existe\n",
    "        run_id = (\n",
    "            GLOBAL_STATE.get(\"RUN_ID\")\n",
    "            or GLOBAL_STATE.get(\"run_id\")\n",
    "            or (GLOBAL_STATE.get(\"RUN\", {}) or {}).get(\"id\")\n",
    "        )\n",
    "        candidate = None\n",
    "        if run_id:\n",
    "            cand = METADATA_DIR / f\"universe_snapshot_{run_id}.parquet\"\n",
    "            if cand.exists() and cand.is_file():\n",
    "                candidate = cand\n",
    "\n",
    "        # 3) √öltimo snapshot por mtime\n",
    "        if candidate is None:\n",
    "            try:\n",
    "                snaps = [p for p in METADATA_DIR.glob(\"universe_snapshot_*.parquet\") if p.is_file()]\n",
    "                if snaps:\n",
    "                    snaps = sorted(snaps, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "                    candidate = snaps[0]\n",
    "            except Exception:\n",
    "                candidate = None\n",
    "\n",
    "        if candidate is not None:\n",
    "            UNIV_SNAP_FILE = candidate\n",
    "\n",
    "# Persistir detecciones a inputs para consistencia de celdas posteriores\n",
    "if DQ_FILE is not None:\n",
    "    inputs[\"DATA_QUALITY_SUMMARY_FILE\"] = str(DQ_FILE)\n",
    "if UNIV_SNAP_FILE is not None:\n",
    "    inputs[\"UNIVERSE_SNAPSHOT_FILE\"] = str(UNIV_SNAP_FILE)\n",
    "\n",
    "# Flags locales\n",
    "wl_autogen = False\n",
    "pr_autogen = False\n",
    "\n",
    "print(\">>> Celda 02 :: Carga de artifacts (versi√≥n SIN HUMO)\")\n",
    "\n",
    "# -------------------------------\n",
    "# Check de integridad: day_index vs symbol_index (solo aviso)\n",
    "# -------------------------------\n",
    "day_index_found = PAD_DAY_INDEX.exists() and PAD_DAY_INDEX.is_file()\n",
    "symbol_index_found = PAD_SYMBOL_INDEX.exists() and PAD_SYMBOL_INDEX.is_file()\n",
    "\n",
    "if day_index_found and symbol_index_found:\n",
    "    try:\n",
    "        day_df = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "        sym_df = pl.read_parquet(str(PAD_SYMBOL_INDEX))\n",
    "        day_df = day_df.rename({c: c.lower() for c in day_df.columns})\n",
    "        sym_df = sym_df.rename({c: c.lower() for c in sym_df.columns})\n",
    "\n",
    "        day_sym_count = day_df[\"symbol\"].n_unique() if \"symbol\" in day_df.columns else 0\n",
    "        sym_count = sym_df[\"symbol\"].n_unique() if \"symbol\" in sym_df.columns else 0\n",
    "\n",
    "        discrepancy_pct = abs(day_sym_count - sym_count) / max(day_sym_count, 1) * 100\n",
    "        if discrepancy_pct > 1:\n",
    "            print(\n",
    "                f\"[Celda 02][WARN] Discrepancia >1% entre day_index ({day_sym_count} syms) \"\n",
    "                f\"y symbol_index ({sym_count} syms). NO se rompe ejecuci√≥n; s√≥lo aviso.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"‚úÖ Integridad √≠ndices PAD: day_index={day_sym_count}, \"\n",
    "                f\"symbol_index={sym_count} (discrepancia={discrepancy_pct:.2f}%)\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 02][WARN] Error en check de integridad √≠ndices PAD: {e}\")\n",
    "else:\n",
    "    print(\"[Celda 02][WARN] √çndices PAD no completos (day_index/symbol_index). Saltando check de integridad.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Derivar s√≠mbolos base de M5 (rates_syms = UNIVERSO GOLD M5 base)\n",
    "# -------------------------------\n",
    "rates_syms: list[str] = []\n",
    "if (\n",
    "    \"symbols\" in GLOBAL_STATE\n",
    "    and isinstance(GLOBAL_STATE[\"symbols\"].get(\"detected\"), list)\n",
    "    and GLOBAL_STATE[\"symbols\"][\"detected\"]\n",
    "):\n",
    "    rates_syms = [norm_symbol(s) for s in GLOBAL_STATE[\"symbols\"][\"detected\"]]\n",
    "else:\n",
    "    if M5_SOURCE_TYPE == \"flat_m5_raw\":\n",
    "        rates_syms = _extract_symbols_from_flat(DE_M5_RAW_DIR)\n",
    "    else:\n",
    "        root = M5_ROOT_DIR if M5_ROOT_DIR.exists() else PAD_HIST_CLEAN_DIR\n",
    "        rates_syms = _extract_symbols_from_partitioned(root)\n",
    "\n",
    "rates_syms = sorted(set([s for s in rates_syms if s]))\n",
    "print(f\"[Celda 02] S√≠mbolos detectados en rates M5 (UNIVERSO GOLD M5 base) = {len(rates_syms)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Corte anti-look-ahead (informativo)\n",
    "# -------------------------------\n",
    "cutoff_date = datetime.now(timezone.utc) - timedelta(days=30)\n",
    "print(f\"üõ°Ô∏è Corte anti-look-ahead (informativo): datos hasta {cutoff_date.date()}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar o AUTOGENERAR WATCHLIST\n",
    "# -------------------------------\n",
    "if str(WATCHLIST_SELECTED_PATH) == \"\" or WATCHLIST_SELECTED_FORMAT.lower() == \"none\":\n",
    "    wl_df, wl_path = _autogen_watchlist_from_pads(\n",
    "        OUT_DIAG_DIR, PAD_SYMBOL_INDEX, M5_ROOT_DIR, M5_SOURCE_TYPE\n",
    "    )\n",
    "    _print_input_loaded(wl_path, wl_df.height, wl_df.width, \"generated\")\n",
    "    wl_autogen = True\n",
    "    inputs[\"WATCHLIST_SELECTED_PATH\"] = str(wl_path)\n",
    "    inputs[\"WATCHLIST_SELECTED_FORMAT\"] = \"parquet\"\n",
    "else:\n",
    "    wl_df, wl_fmt = _load_table(WATCHLIST_SELECTED_PATH, \"watchlist\", WATCHLIST_SELECTED_FORMAT)\n",
    "    wl_df = wl_df.rename({c: c.lower() for c in wl_df.columns})\n",
    "    if \"symbol\" not in wl_df.columns:\n",
    "        raise RuntimeError(\"Missing required column 'symbol' in watchlist\")\n",
    "    wl_df = wl_df.with_columns(pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\"))\n",
    "    for opt_col in [\"enabled\", \"category\", \"min_spread\"]:\n",
    "        if opt_col not in wl_df.columns:\n",
    "            wl_df = wl_df.with_columns(pl.lit(None).alias(opt_col))\n",
    "    _print_input_loaded(WATCHLIST_SELECTED_PATH, wl_df.height, wl_df.width, wl_fmt)\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar o AUTOGENERAR PARAMS\n",
    "# -------------------------------\n",
    "if str(PARAMS_SELECTED_PATH) == \"\" or PARAMS_SELECTED_FORMAT.lower() == \"none\":\n",
    "    wl_syms_local = [norm_symbol(s) for s in wl_df.get_column(\"symbol\").to_list()]\n",
    "    pr_df, pr_path = _autogen_params_from_pads(OUT_DIAG_DIR, PAD_DAY_INDEX, wl_syms_local)\n",
    "    _print_input_loaded(pr_path, pr_df.height, pr_df.width, \"generated\")\n",
    "    pr_autogen = True\n",
    "    inputs[\"PARAMS_SELECTED_PATH\"] = str(pr_path)\n",
    "    inputs[\"PARAMS_SELECTED_FORMAT\"] = \"parquet\"\n",
    "else:\n",
    "    pr_df, pr_fmt = _load_table(PARAMS_SELECTED_PATH, \"params\", PARAMS_SELECTED_FORMAT)\n",
    "    pr_df = pr_df.rename({c: c.lower() for c in pr_df.columns})\n",
    "    if \"symbol\" not in pr_df.columns:\n",
    "        raise RuntimeError(\"Missing required column 'symbol' in params\")\n",
    "    pr_df = pr_df.with_columns(pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\"))\n",
    "\n",
    "    has_commission = \"commission\" in pr_df.columns\n",
    "    has_spread_rule = \"spread_rule\" in pr_df.columns\n",
    "    has_spread_est = \"spread_est\" in pr_df.columns\n",
    "    if not has_commission or (not has_spread_rule and not has_spread_est):\n",
    "        raise RuntimeError(\n",
    "            \"Params missing required cost columns \"\n",
    "            \"(commission y al menos uno de spread_rule/spread_est)\"\n",
    "        )\n",
    "    _print_input_loaded(PARAMS_SELECTED_PATH, pr_df.height, pr_df.width, pr_fmt)\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar UNIVERSE (Data Engine) ‚Äî SOLO como metadato\n",
    "# -------------------------------\n",
    "un_df = None\n",
    "if DE_UNIVERSE_FILE.exists() and DE_UNIVERSE_FILE.is_file():\n",
    "    try:\n",
    "        un_df = pl.read_parquet(str(DE_UNIVERSE_FILE))\n",
    "        un_df = un_df.rename({c: c.lower() for c in un_df.columns})\n",
    "        if \"symbol\" not in un_df.columns:\n",
    "            raise RuntimeError(\"Missing required column 'symbol' in universe\")\n",
    "        un_df = un_df.with_columns(pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\"))\n",
    "        if \"rank\" not in un_df.columns:\n",
    "            un_df = un_df.with_columns(pl.lit(None).alias(\"rank\"))\n",
    "        _print_input_loaded(DE_UNIVERSE_FILE, un_df.height, un_df.width, \"parquet\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"universe: error leyendo/parsing {str(DE_UNIVERSE_FILE)} | reason={e}\")\n",
    "else:\n",
    "    print(f\"üìÅ INPUT ‚Üí {str(DE_UNIVERSE_FILE)} (found=false)\")\n",
    "\n",
    "# -------------------------------\n",
    "# Correlaci√≥n (existencia/legible si existe; NO gatea si falta)\n",
    "# -------------------------------\n",
    "corr_found = DE_CORR_M5_FILE.exists() and DE_CORR_M5_FILE.is_file()\n",
    "if corr_found:\n",
    "    try:\n",
    "        size_corr = DE_CORR_M5_FILE.stat().st_size\n",
    "        if size_corr <= 0:\n",
    "            raise RuntimeError(\"corr_matrix_5m.csv size=0\")\n",
    "        with DE_CORR_M5_FILE.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            _ = fh.readline()\n",
    "        print(f\"üìÅ INPUT ‚Üí {str(DE_CORR_M5_FILE)} (found=true, readable=true, bytes={size_corr})\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"corr_matrix_5m.csv no legible | reason={e}\")\n",
    "else:\n",
    "    print(f\"üìÅ INPUT ‚Üí {str(DE_CORR_M5_FILE)} (found=false)\")\n",
    "    print(\"‚ö†Ô∏è FALLBACK ‚Üí correlaci√≥n M5 ser√° reconstruida m√°s adelante si el pipeline la requiere.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Conjuntos de s√≠mbolos base\n",
    "# -------------------------------\n",
    "wl_syms = sorted(set(wl_df.get_column(\"symbol\").to_list()))\n",
    "pr_syms = sorted(set(pr_df.get_column(\"symbol\").to_list()))\n",
    "un_syms = sorted(set(un_df.get_column(\"symbol\").to_list())) if un_df is not None else []\n",
    "\n",
    "set_wl, set_pr, set_un, set_rates = set(wl_syms), set(pr_syms), set(un_syms), set(rates_syms)\n",
    "\n",
    "print(f\"[Celda 02] n_watchlist={len(set_wl)}, n_params={len(set_pr)}, n_universe_raw={len(set_un)}, n_rates_m5={len(set_rates)}\")\n",
    "\n",
    "inter_wl_pr = sorted(set_wl & set_pr)\n",
    "inter_wl_pr_un = sorted(set_wl & set_pr & set_un) if un_syms else []\n",
    "inter_wl_pr_rates = sorted(set_wl & set_pr & set_rates)\n",
    "\n",
    "print(f\"üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params)             = {len(inter_wl_pr)}\")\n",
    "print(f\"üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params, universe)  = {len(inter_wl_pr_un)}\")\n",
    "print(f\"üìä RESUMEN ‚Üí intersecci√≥n(watchlist, params, rates_m5)  = {len(inter_wl_pr_rates)}\")\n",
    "\n",
    "if len(inter_wl_pr) == 0:\n",
    "    raise RuntimeError(\"Intersection(watchlist, params) is empty (esto s√≠ es cr√≠tico).\")\n",
    "\n",
    "# -------------------------------\n",
    "# DATA QUALITY SUMMARY (opcional)\n",
    "# -------------------------------\n",
    "df_data_quality = None\n",
    "dq_syms: list[str] = []\n",
    "\n",
    "if DQ_FILE is not None and DQ_FILE.exists() and DQ_FILE.is_file():\n",
    "    try:\n",
    "        df_data_quality = pl.read_parquet(str(DQ_FILE))\n",
    "        df_data_quality = df_data_quality.rename({c: c.lower() for c in df_data_quality.columns})\n",
    "        if \"symbol\" in df_data_quality.columns:\n",
    "            df_data_quality = df_data_quality.with_columns(\n",
    "                pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\")\n",
    "            )\n",
    "            dq_syms = sorted(set(df_data_quality.get_column(\"symbol\").to_list()))\n",
    "        _print_input_loaded(DQ_FILE, df_data_quality.height, df_data_quality.width, \"parquet\")\n",
    "        print(f\"üìä DATA QUALITY ‚Üí n_symbols={len(dq_syms)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 02][WARN] No se pudo leer data_quality_summary.parquet desde {DQ_FILE} | reason={e}\")\n",
    "        df_data_quality = None\n",
    "        dq_syms = []\n",
    "else:\n",
    "    if DQ_FILE is not None:\n",
    "        print(f\"[Celda 02][WARN] data_quality_summary.parquet no encontrado en {DQ_FILE}\")\n",
    "    else:\n",
    "        print(\"[Celda 02][INFO] DATA_QUALITY_SUMMARY_FILE no definido o no detectable; se omite data_quality.\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNIVERSE SNAPSHOT (opcional)\n",
    "# -------------------------------\n",
    "df_universe_snapshot = None\n",
    "usnap_syms: list[str] = []\n",
    "\n",
    "if UNIV_SNAP_FILE is not None and UNIV_SNAP_FILE.exists() and UNIV_SNAP_FILE.is_file():\n",
    "    try:\n",
    "        df_universe_snapshot = pl.read_parquet(str(UNIV_SNAP_FILE))\n",
    "        df_universe_snapshot = df_universe_snapshot.rename({c: c.lower() for c in df_universe_snapshot.columns})\n",
    "        if \"symbol\" in df_universe_snapshot.columns:\n",
    "            df_universe_snapshot = df_universe_snapshot.with_columns(\n",
    "                pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\")\n",
    "            )\n",
    "            usnap_syms = sorted(set(df_universe_snapshot.get_column(\"symbol\").to_list()))\n",
    "        _print_input_loaded(UNIV_SNAP_FILE, df_universe_snapshot.height, df_universe_snapshot.width, \"parquet\")\n",
    "        print(f\"üìä UNIVERSE SNAPSHOT ‚Üí n_symbols={len(usnap_syms)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 02][WARN] No se pudo leer universe_snapshot desde {UNIV_SNAP_FILE} | reason={e}\")\n",
    "        df_universe_snapshot = None\n",
    "        usnap_syms = []\n",
    "else:\n",
    "    if UNIV_SNAP_FILE is not None:\n",
    "        print(f\"[Celda 02][WARN] universe_snapshot parquet no encontrado en {UNIV_SNAP_FILE}\")\n",
    "    else:\n",
    "        print(\"[Celda 02][INFO] UNIVERSE_SNAPSHOT_FILE no definido o no detectable; se omite universe_snapshot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Construcci√≥n de UNIVERSO EFECTIVO M5 (GOLD + config)\n",
    "# -------------------------------\n",
    "base_syms = set_wl & set_pr & set_rates\n",
    "\n",
    "if not base_syms:\n",
    "    print(\"[Celda 02][WARN] watchlist ‚à© params ‚à© rates_m5 est√° vac√≠o. Relajando criterio:\")\n",
    "    base_syms = (set_wl & set_rates) or (set_pr & set_rates) or set_rates\n",
    "\n",
    "base_syms = sorted(base_syms)\n",
    "print(f\"[Celda 02] UNIVERSO EFECTIVO M5 (s√≠mbolos con datos + config) = {len(base_syms)}\")\n",
    "\n",
    "# Construir DataFrame de universo efectivo, a√±adiendo metadata de universe_raw si existe\n",
    "if un_df is not None:\n",
    "    un_eff = un_df.filter(pl.col(\"symbol\").is_in(base_syms))\n",
    "    syms_in_un_raw = set(un_eff.get_column(\"symbol\").to_list()) if un_eff.height > 0 else set()\n",
    "    missing_syms = [s for s in base_syms if s not in syms_in_un_raw]\n",
    "    if missing_syms:\n",
    "        extra_df = pl.DataFrame({\"symbol\": missing_syms, \"rank\": [None] * len(missing_syms)})\n",
    "        un_eff = pl.concat([un_eff, extra_df], how=\"diagonal_relaxed\")\n",
    "else:\n",
    "    un_eff = pl.DataFrame({\"symbol\": base_syms, \"rank\": [None] * len(base_syms)})\n",
    "\n",
    "universe_effective = un_eff\n",
    "\n",
    "# -------------------------------\n",
    "# A√±adir first_date_m5, last_date_m5, n_days_total, coverage_days, flags\n",
    "# -------------------------------\n",
    "cfg = GLOBAL_STATE.get(\"config\", {})\n",
    "cfg_stats = cfg.get(\"stats\", {}) if isinstance(cfg, dict) else {}\n",
    "\n",
    "# Defaults suaves para coherencia interna de flags\n",
    "MIN_HISTORY_DAYS = int(cfg_stats.get(\"min_history_days\", 90))\n",
    "POOR_DAY_COVERAGE = float(cfg_stats.get(\"poor_day_coverage\", 0.5))\n",
    "\n",
    "if day_index_found:\n",
    "    try:\n",
    "        day_idx = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "        day_idx = day_idx.rename({c: c.lower() for c in day_idx.columns})\n",
    "\n",
    "        if {\"symbol\", \"date\"}.issubset(set(day_idx.columns)):\n",
    "            day_idx = day_idx.with_columns(\n",
    "                pl.col(\"symbol\").map_elements(norm_symbol).alias(\"symbol\")\n",
    "            )\n",
    "\n",
    "            dt_dtype = day_idx.schema.get(\"date\")\n",
    "            if dt_dtype == pl.Utf8:\n",
    "                day_idx = day_idx.with_columns(\n",
    "                    pl.col(\"date\")\n",
    "                    .str.strptime(pl.Date, format=\"%Y%m%d\", strict=False)\n",
    "                    .alias(\"date\")\n",
    "                )\n",
    "            else:\n",
    "                day_idx = day_idx.with_columns(\n",
    "                    pl.col(\"date\").cast(pl.Date).alias(\"date\")\n",
    "                )\n",
    "\n",
    "            day_idx = day_idx.filter(pl.col(\"date\").is_not_null())\n",
    "\n",
    "            universe_effective_syms_tmp = universe_effective[\"symbol\"].to_list()\n",
    "            day_idx_eff = day_idx.filter(pl.col(\"symbol\").is_in(universe_effective_syms_tmp))\n",
    "\n",
    "            if day_idx_eff.height == 0:\n",
    "                print(\"[Celda 02][WARN] day_index_m5 no tiene filas para universe_effective; no se a√±aden fechas/coverage.\")\n",
    "            else:\n",
    "                un_stats = (\n",
    "                    day_idx_eff\n",
    "                    .group_by(\"symbol\")\n",
    "                    .agg(\n",
    "                        [\n",
    "                            pl.col(\"date\").min().alias(\"first_date_m5\"),\n",
    "                            pl.col(\"date\").max().alias(\"last_date_m5\"),\n",
    "                            pl.len().alias(\"n_days_with_bars\"),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                un_stats = un_stats.with_columns(\n",
    "                    [\n",
    "                        pl.col(\"first_date_m5\").cast(pl.Date).alias(\"first_date_m5\"),\n",
    "                        pl.col(\"last_date_m5\").cast(pl.Date).alias(\"last_date_m5\"),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                un_stats = un_stats.with_columns(\n",
    "                    (\n",
    "                        pl.col(\"last_date_m5\").cast(pl.Int32)\n",
    "                        - pl.col(\"first_date_m5\").cast(pl.Int32)\n",
    "                        + 1\n",
    "                    ).alias(\"n_days_total\")\n",
    "                )\n",
    "\n",
    "                un_stats = un_stats.with_columns(\n",
    "                    (pl.col(\"n_days_with_bars\") / pl.col(\"n_days_total\")).alias(\"coverage_days\")\n",
    "                )\n",
    "\n",
    "                un_stats = un_stats.with_columns(\n",
    "                    [\n",
    "                        (pl.col(\"n_days_total\") < MIN_HISTORY_DAYS).alias(\"short_history_flag\"),\n",
    "                        (pl.col(\"coverage_days\") < POOR_DAY_COVERAGE).alias(\"poor_day_coverage\"),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                universe_effective = universe_effective.join(un_stats, on=\"symbol\", how=\"left\")\n",
    "\n",
    "                if \"n_days_total\" in universe_effective.columns and \"coverage_days\" in universe_effective.columns:\n",
    "                    dist = (\n",
    "                        universe_effective\n",
    "                        .select(\n",
    "                            pl.col(\"n_days_total\").min().alias(\"min_days_total\"),\n",
    "                            pl.col(\"n_days_total\").median().alias(\"median_days_total\"),\n",
    "                            pl.col(\"n_days_total\").max().alias(\"max_days_total\"),\n",
    "                            pl.col(\"coverage_days\").min().alias(\"min_coverage\"),\n",
    "                            pl.col(\"coverage_days\").median().alias(\"median_coverage\"),\n",
    "                            pl.col(\"coverage_days\").max().alias(\"max_coverage\"),\n",
    "                        )\n",
    "                        .to_dicts()[0]\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"üìä Distribuci√≥n n_days_total ‚Üí \"\n",
    "                        f\"min={dist['min_days_total']}, \"\n",
    "                        f\"median={dist['median_days_total']}, \"\n",
    "                        f\"max={dist['max_days_total']}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"üìä Distribuci√≥n coverage_days ‚Üí \"\n",
    "                        f\"min={dist['min_coverage']:.3f}, \"\n",
    "                        f\"median={dist['median_coverage']:.3f}, \"\n",
    "                        f\"max={dist['max_coverage']:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    sample_rows = (\n",
    "                        universe_effective\n",
    "                        .select(\n",
    "                            [\n",
    "                                \"symbol\",\n",
    "                                \"first_date_m5\",\n",
    "                                \"last_date_m5\",\n",
    "                                \"n_days_total\",\n",
    "                                \"n_days_with_bars\",\n",
    "                                \"coverage_days\",\n",
    "                            ]\n",
    "                        )\n",
    "                        .head(10)\n",
    "                    )\n",
    "                    print(\"üßæ Muestra universe_effective (primeros s√≠mbolos):\")\n",
    "                    print(sample_rows)\n",
    "        else:\n",
    "            print(\"[Celda 02][WARN] day_index_m5.parquet no tiene columnas esperadas {'symbol','date'}; no se a√±aden fechas/coverage.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 02][WARN] Error construyendo universe_effective con coverage: {e}\")\n",
    "else:\n",
    "    print(\"[Celda 02][WARN] day_index_m5.parquet no disponible; universe_effective sin fechas ni coverage.\")\n",
    "\n",
    "# Lista final del universo efectivo\n",
    "universe_effective_syms = universe_effective[\"symbol\"].to_list()\n",
    "\n",
    "print(\n",
    "    f\"[Celda 02] Universo efectivo M5 FINAL ‚Üí n_symbols={len(universe_effective_syms)} \"\n",
    "    f\"(de los cuales {len(set(universe_effective_syms) & set_un)} ten√≠an metadatos en universe_raw)\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Persistir diagnostics/artifacts_summary.json\n",
    "# -------------------------------\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "artifacts_path = OUT_DIAG_DIR / \"artifacts_summary.json\"\n",
    "\n",
    "artifacts_summary = {\n",
    "    \"paths\": {\n",
    "        \"watchlist\": str(inputs.get(\"WATCHLIST_SELECTED_PATH\", \"\")),\n",
    "        \"params\": str(inputs.get(\"PARAMS_SELECTED_PATH\", \"\")),\n",
    "        \"universe_raw\": str(DE_UNIVERSE_FILE),\n",
    "        \"corr_matrix_csv\": str(DE_CORR_M5_FILE),\n",
    "        \"m5_source_type\": M5_SOURCE_TYPE,\n",
    "        \"m5_root_dir\": str(M5_ROOT_DIR),\n",
    "        \"pad_symbol_index\": str(PAD_SYMBOL_INDEX),\n",
    "        \"pad_day_index\": str(PAD_DAY_INDEX),\n",
    "        \"data_quality_summary\": (str(DQ_FILE) if DQ_FILE is not None else \"\"),\n",
    "        \"universe_snapshot\": (str(UNIV_SNAP_FILE) if UNIV_SNAP_FILE is not None else \"\"),\n",
    "    },\n",
    "    \"formats\": {\n",
    "        \"watchlist\": (\"generated\" if wl_autogen else WATCHLIST_SELECTED_FORMAT),\n",
    "        \"params\": (\"generated\" if pr_autogen else PARAMS_SELECTED_FORMAT),\n",
    "        \"universe_raw\": (\"parquet\" if un_df is not None else \"none\"),\n",
    "        \"corr_matrix_csv\": (\"present\" if corr_found else \"absent\"),\n",
    "        \"data_quality_summary\": (\"parquet\" if df_data_quality is not None else \"none\"),\n",
    "        \"universe_snapshot\": (\"parquet\" if df_universe_snapshot is not None else \"none\"),\n",
    "    },\n",
    "    \"shapes\": {\n",
    "        \"watchlist\": {\"rows\": wl_df.height, \"cols\": wl_df.width},\n",
    "        \"params\": {\"rows\": pr_df.height, \"cols\": pr_df.width},\n",
    "        \"universe_raw\": {\n",
    "            \"rows\": (un_df.height if un_df is not None else 0),\n",
    "            \"cols\": (un_df.width if un_df is not None else 0),\n",
    "        },\n",
    "        \"universe_effective\": {\n",
    "            \"rows\": universe_effective.height,\n",
    "            \"cols\": universe_effective.width,\n",
    "        },\n",
    "        \"data_quality_summary\": {\n",
    "            \"rows\": (df_data_quality.height if df_data_quality is not None else 0),\n",
    "            \"cols\": (df_data_quality.width if df_data_quality is not None else 0),\n",
    "        },\n",
    "        \"universe_snapshot\": {\n",
    "            \"rows\": (df_universe_snapshot.height if df_universe_snapshot is not None else 0),\n",
    "            \"cols\": (df_universe_snapshot.width if df_universe_snapshot is not None else 0),\n",
    "        },\n",
    "    },\n",
    "    \"columns\": {\n",
    "        \"watchlist\": wl_df.columns,\n",
    "        \"params\": pr_df.columns,\n",
    "        \"universe_raw\": (un_df.columns if un_df is not None else []),\n",
    "        \"universe_effective\": universe_effective.columns,\n",
    "        \"data_quality_summary\": (df_data_quality.columns if df_data_quality is not None else []),\n",
    "        \"universe_snapshot\": (df_universe_snapshot.columns if df_universe_snapshot is not None else []),\n",
    "    },\n",
    "    \"counts\": {\n",
    "        \"wl\": len(wl_syms),\n",
    "        \"pr\": len(pr_syms),\n",
    "        \"un_raw\": len(un_syms),\n",
    "        \"rates_m5\": len(rates_syms),\n",
    "        \"universe_effective\": len(universe_effective_syms),\n",
    "        \"inter_wl_pr\": len(inter_wl_pr),\n",
    "        \"inter_wl_pr_un_raw\": len(inter_wl_pr_un),\n",
    "        \"inter_wl_pr_rates\": len(inter_wl_pr_rates),\n",
    "        \"data_quality_symbols\": len(dq_syms),\n",
    "        \"universe_snapshot_symbols\": len(usnap_syms),\n",
    "    },\n",
    "    \"symbols\": {\n",
    "        \"wl\": wl_syms[:1000],\n",
    "        \"pr\": pr_syms[:1000],\n",
    "        \"un_raw\": un_syms[:1000] if un_syms else [],\n",
    "        \"rates_m5\": rates_syms[:1000],\n",
    "        \"data_quality\": dq_syms[:1000],\n",
    "        \"universe_snapshot\": usnap_syms[:1000],\n",
    "        \"universe_effective\": universe_effective_syms[:1000],\n",
    "    },\n",
    "    \"autogen\": {\n",
    "        \"watchlist\": wl_autogen,\n",
    "        \"params\": pr_autogen,\n",
    "    },\n",
    "}\n",
    "\n",
    "artifacts_path.write_text(\n",
    "    json.dumps(artifacts_summary, ensure_ascii=False, indent=2, sort_keys=True),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "size_bytes = artifacts_path.stat().st_size\n",
    "if size_bytes <= 0:\n",
    "    raise RuntimeError(\"artifacts_summary.json no escrito correctamente (size=0)\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(artifacts_path)} (OK, bytes={size_bytes})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Guardar en memoria (GLOBAL_STATE)\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"tables\", {})\n",
    "GLOBAL_STATE[\"tables\"][\"watchlist\"] = wl_df\n",
    "GLOBAL_STATE[\"tables\"][\"params\"] = pr_df\n",
    "GLOBAL_STATE[\"tables\"][\"universe_raw\"] = un_df if un_df is not None else None\n",
    "GLOBAL_STATE[\"tables\"][\"universe\"] = universe_effective\n",
    "GLOBAL_STATE[\"tables\"][\"corr_matrix_path\"] = str(DE_CORR_M5_FILE)\n",
    "GLOBAL_STATE[\"tables\"][\"data_quality\"] = df_data_quality\n",
    "GLOBAL_STATE[\"tables\"][\"universe_snapshot\"] = df_universe_snapshot\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"symbols\", {})\n",
    "GLOBAL_STATE[\"symbols\"][\"rates_m5\"] = rates_syms\n",
    "GLOBAL_STATE[\"symbols\"][\"wl\"] = wl_syms\n",
    "GLOBAL_STATE[\"symbols\"][\"pr\"] = pr_syms\n",
    "GLOBAL_STATE[\"symbols\"][\"un_raw\"] = un_syms\n",
    "GLOBAL_STATE[\"symbols\"][\"universe_effective\"] = universe_effective_syms\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"flags\", {})\n",
    "GLOBAL_STATE[\"flags\"][\"corr_matrix_available\"] = bool(corr_found)\n",
    "GLOBAL_STATE[\"flags\"][\"data_quality_available\"] = bool(df_data_quality is not None)\n",
    "GLOBAL_STATE[\"flags\"][\"universe_snapshot_available\"] = bool(df_universe_snapshot is not None)\n",
    "\n",
    "# Refrescar inputs en GLOBAL_STATE tras posibles autogen/detecciones\n",
    "GLOBAL_STATE[\"inputs\"] = inputs\n",
    "\n",
    "print(\n",
    "    f\"üìå Resumen Celda 02 ‚Üí \"\n",
    "    f\"n_universe_raw={len(un_syms)}, \"\n",
    "    f\"n_universe_efectivo_M5={len(universe_effective_syms)}, \"\n",
    "    f\"n_watchlist={len(wl_syms)}, \"\n",
    "    f\"n_params={len(pr_syms)}, \"\n",
    "    f\"corr_matrix_disponible={corr_found}, \"\n",
    "    f\"data_quality_disponible={df_data_quality is not None}, \"\n",
    "    f\"universe_snapshot_disponible={df_universe_snapshot is not None}\"\n",
    ")\n",
    "\n",
    "print(\">>> Celda 02 :: OK (universo GOLD base + universo efectivo con coverage/flags, sin mirar edge)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11735918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 03 :: QA Cobertura M5 + Data Gate\n",
      "[Celda 03] M5_ROOT_DIR = C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 03] Config QA ‚Üí min_bars_m5=20000, tol_seconds=¬±5, min_pct_valid_transitions=0.900, day_full_threshold_pct=0.750, min_pct_days_full_required=0.750, expected_bars_per_day=72, max_gap_minutes=120, min_history_years=1.0\n",
      "[Celda 03] Legacy detectado: qa.min_pct_days_full=0.750\n",
      "[Celda 03] PAD_DAY_INDEX = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\day_index_m5.parquet\n",
      "[Celda 03] day_index_m5 ‚Üí filas=98570, rango_fechas=[2021-11-19 ‚Üí 2025-12-02]\n",
      "[Celda 03] S√≠mbolos iniciales (universe_effective) = 84\n",
      "[Celda 03] Ventana aplicada: TODOS los d√≠as disponibles\n",
      "[Celda 03] ... procesados 20/84 s√≠mbolos\n",
      "[Celda 03] ... procesados 40/84 s√≠mbolos\n",
      "[Celda 03] ... procesados 60/84 s√≠mbolos\n",
      "[Celda 03] ... procesados 80/84 s√≠mbolos\n",
      "[Celda 03] ... procesados 84/84 s√≠mbolos\n",
      "[Celda 03] DataQuality: s√≠mbolos con info=108\n",
      "[Celda 03] S√≠mbolos en √≠ndice (tras filtros) = 84\n",
      "[Celda 03] eligible_m5 (cobertura) = 84\n",
      "[Celda 03] passed_data_gate (combinado) = 84\n",
      "[Celda 03] Parametr√≠a efectiva ‚Üí min_bars=20000, min_pct_valid_transitions=0.900, day_full_threshold_pct=0.750, min_pct_days_full_required=0.750, min_history_years=1.0\n",
      "[Celda 03] Top 10 por pct_valid_m5_bars:\n",
      "       BNBUSD | 0.9991302648294733\n",
      "       XLMUSD | 0.9991225046991218\n",
      "       ETCUSD | 0.99911762477299\n",
      "       LNKUSD | 0.9991151740320688\n",
      "       UNIUSD | 0.9991151740320688\n",
      "       ALGUSD | 0.9990999519320378\n",
      "       VECUSD | 0.999093058105396\n",
      "       BCHUSD | 0.9990784446492346\n",
      "       AVAUSD | 0.9990760047226426\n",
      "       EURAUD | 0.9964758739331585\n",
      "[Celda 03] Bottom 10 por pct_valid_m5_bars:\n",
      "       USDILS | 0.9709634046940638\n",
      "          IBE | 0.9765728930276234\n",
      "       USDHKD | 0.9788990982083496\n",
      "      DASHUSD | 0.9854656967197237\n",
      "       LTCUSD | 0.9856574533147285\n",
      "       XMRUSD | 0.9856847680231798\n",
      "       ETHUSD | 0.9856921875603086\n",
      "      DOGEUSD | 0.9856929096922447\n",
      "       XRPUSD | 0.9856953689573473\n",
      "       BTCUSD | 0.985695380744398\n",
      "[Celda 03] Top 10 por pct_days_full:\n",
      "       GBPUSD | 1.0\n",
      "       XAGUSD | 1.0\n",
      "       AUDCAD | 0.9990458015267175\n",
      "       AUDCHF | 0.9990458015267175\n",
      "       AUDJPY | 0.9990458015267175\n",
      "       AUDNZD | 0.9990458015267175\n",
      "       AUDUSD | 0.9990458015267175\n",
      "       CADCHF | 0.9990458015267175\n",
      "       CADJPY | 0.9990458015267175\n",
      "       CHFJPY | 0.9990458015267175\n",
      "[Celda 03] Bottom 10 por pct_days_full:\n",
      "      DASHUSD | 0.7950987066031314\n",
      "       ADAUSD | 0.7971409121851599\n",
      "      DOGEUSD | 0.7971409121851599\n",
      "       DOTUSD | 0.7971409121851599\n",
      "       ETHUSD | 0.7971409121851599\n",
      "       LTCUSD | 0.7971409121851599\n",
      "       XMRUSD | 0.7971409121851599\n",
      "       XRPUSD | 0.7971409121851599\n",
      "       BTCUSD | 0.7978216473791695\n",
      "       AAVUSD | 0.840924541128484\n",
      "[Celda 03] Top 10 por pct_valid_transitions:\n",
      "       BNBUSD | 0.9991302627046387\n",
      "       XLMUSD | 0.9991225025542753\n",
      "       ETCUSD | 0.9991176226162367\n",
      "       LNKUSD | 0.9991151718693094\n",
      "       UNIUSD | 0.9991151718693094\n",
      "       ALGUSD | 0.9990999497247122\n",
      "       VECUSD | 0.9990930558882919\n",
      "       BCHUSD | 0.9990784423965386\n",
      "       AVAUSD | 0.9990760024639934\n",
      "       EURAUD | 0.996475862161115\n",
      "[Celda 03] Bottom 10 por pct_valid_transitions:\n",
      "       USDILS | 0.9709633035512131\n",
      "          IBE | 0.9765726638690808\n",
      "       USDHKD | 0.978899026266263\n",
      "      DASHUSD | 0.9854656534048577\n",
      "       LTCUSD | 0.9856574107336363\n",
      "       XMRUSD | 0.9856847255249478\n",
      "       ETHUSD | 0.9856921450796893\n",
      "      DOGEUSD | 0.9856928672159133\n",
      "       XRPUSD | 0.9856953264868042\n",
      "       BTCUSD | 0.9856953383003534\n",
      "[Celda 03] Distribuci√≥n data_quality_flag_norm:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ data_quality_flag_norm ‚îÜ n   ‚îÇ\n",
      "‚îÇ ---                    ‚îÜ --- ‚îÇ\n",
      "‚îÇ str                    ‚îÜ u32 ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ OK                     ‚îÜ 84  ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 03] Resumen hist_years:\n",
      "shape: (1, 4)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ min      ‚îÜ median   ‚îÜ max      ‚îÜ nulls ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---      ‚îÜ ---      ‚îÜ ---   ‚îÇ\n",
      "‚îÇ f64      ‚îÜ f64      ‚îÜ f64      ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 3.890486 ‚îÜ 4.035592 ‚îÜ 4.035592 ‚îÜ 0     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      ">>> Celda 03 :: OK (eligible_m5=84, passed_data_gate=84)\n"
     ]
    }
   ],
   "source": [
    "# Celda 03 ‚Äî QA de Cobertura M5 (elegibilidad por historial M5 sano + Data Gate)\n",
    "# -------------------------------------------------------------------\n",
    "# Enfoque de trading:\n",
    "#   - Esta celda garantiza que NO se usen s√≠mbolos con historial M5 roto\n",
    "#     y aplica adem√°s un ‚ÄúData Gate‚Äù combinado:\n",
    "#       * Cobertura M5 (barras, d√≠as completos, cadencia)  ‚Üí eligible_m5\n",
    "#       * Calidad de datos agregada (data_quality_flag)     ‚Üí OK / WARNING / BAD\n",
    "#       * Historial m√≠nimo en a√±os                          ‚Üí hist_years >= qa.min_history_years\n",
    "#\n",
    "#   - Se apoya en:\n",
    "#       1) PAD_DAY_INDEX creado por el Data Engine (MT5_DE_5M_V1),\n",
    "#          que contiene, por s√≠mbolo y d√≠a, la ruta al archivo diario de M5.\n",
    "#       2) df_data_quality cargado en Celda 02 desde:\n",
    "#              metadata/data_quality_summary.parquet\n",
    "#\n",
    "#   - Elegibilidad base de cobertura (eligible_m5) por s√≠mbolo:\n",
    "#       1) N√∫mero m√≠nimo de barras M5 v√°lidas en todo el periodo (qa.min_bars_m5).\n",
    "#       2) Porcentaje de d√≠as \"completos\" (legacy qa.min_pct_days_full):\n",
    "#            * Un d√≠a ‚Äúcompleto‚Äù SE DEFINE POR S√çMBOLO:\n",
    "#              - Se calcula la mediana de barras/d√≠a de ese s√≠mbolo.\n",
    "#              - D√≠a completo = n¬∫_barras_d√≠a >= mediana_barras_d√≠a * threshold_pct\n",
    "#              ‚Üí Esto hace el QA justo para:\n",
    "#                    - Cripto 24/7 (~288 barras/d√≠a).\n",
    "#                    - Acciones US (~78 barras/d√≠a).\n",
    "#                    - Cualquier activo con ventana horaria limitada.\n",
    "#\n",
    "#          IMPORTANTE (mejora estructural):\n",
    "#            En esta versi√≥n se separan dos conceptos:\n",
    "#              - qa.day_full_threshold_pct       (define \"d√≠a completo\")\n",
    "#              - qa.min_pct_days_full_required  (m√≠nimo exigido para cobertura)\n",
    "#            Si no existen en config, ambos heredan qa.min_pct_days_full.\n",
    "#\n",
    "#       3) Porcentaje de transiciones v√°lidas (qa.min_pct_valid_transitions),\n",
    "#          es decir, transiciones de 5 minutos dentro de 300¬±qa.tol_seconds\n",
    "#          y sin grandes gaps.\n",
    "#\n",
    "#   - Data Gate (passed_data_gate) a nivel de s√≠mbolo:\n",
    "#       - Se basa en:\n",
    "#           * data_quality_flag:\n",
    "#               \"OK\"       ‚Üí permite pasar (si historial OK).\n",
    "#               \"WARNING\"  ‚Üí permite pasar (pero con matices; se identifica por flag).\n",
    "#               \"BAD\"      ‚Üí NO pasa el data gate.\n",
    "#               NULL/otras ‚Üí se trata como \"UNKNOWN\" (mismo trato que WARNING).\n",
    "#           * Historial m√≠nimo (hist_years) calculado desde first_ts_utc / last_ts_utc:\n",
    "#               hist_years ‚âà (last_ts - first_ts) / 365.25 d√≠as\n",
    "#               y se compara con qa.min_history_years (por defecto 1.0).\n",
    "#\n",
    "#       - Regla combinada final:\n",
    "#           passed_data_gate = eligible_m5 AND gate_hist_quality\n",
    "#           donde gate_hist_quality = False si:\n",
    "#               * hist_years es NULL, o\n",
    "#               * hist_years < qa.min_history_years, o\n",
    "#               * data_quality_flag_norm == \"BAD\"\n",
    "#             True en otro caso.\n",
    "#\n",
    "# Configuraci√≥n:\n",
    "#   - Todos los umbrales salen de un JSON de config (config.json en diagnostics),\n",
    "#     o usan defaults razonables si no est√°n definidos:\n",
    "#         qa.min_bars_m5\n",
    "#         qa.min_pct_days_full                 (legacy)\n",
    "#         qa.day_full_threshold_pct            (nuevo)\n",
    "#         qa.min_pct_days_full_required        (nuevo)\n",
    "#         qa.expected_bars_per_day     ‚Üí Fallback global SOLO si falla la mediana por s√≠mbolo.\n",
    "#         qa.min_pct_valid_transitions\n",
    "#         qa.tol_seconds\n",
    "#         qa.max_gap_minutes\n",
    "#         qa.min_history_years\n",
    "#         qa.window_days               ‚Üí None / null = usar TODO el GOLD\n",
    "#\n",
    "# Base de s√≠mbolos:\n",
    "#   - Si existe GLOBAL_STATE[\"symbols\"][\"universe_effective\"], se usa como universo base.\n",
    "#   - En caso contrario, se usa GLOBAL_STATE[\"symbols\"][\"rates_m5\"] (UNIVERSO GOLD M5 base).\n",
    "#\n",
    "# Salidas:\n",
    "#   - coverage_table_5m.parquet / .csv en diagnostics con m√©tricas por s√≠mbolo:\n",
    "#       * bars_total, bars_valid_m5, pct_valid_m5_bars\n",
    "#       * pct_valid_transitions, avg_dt_seconds_overall, avg_dt_seconds_valid\n",
    "#       * pct_gaps_over_max, days_total, days_full, pct_days_full, etc.\n",
    "#       * first_ts_utc, last_ts_utc, hist_days, hist_years\n",
    "#       * eligible_m5, exclusion_reason\n",
    "#       * data_quality_flag, data_quality_flag_norm (si se pudo cargar)\n",
    "#       * passed_data_gate, data_gate_reason\n",
    "#\n",
    "#   - GLOBAL_STATE:\n",
    "#       GLOBAL_STATE[\"symbols\"][\"eligible_m5\"]                    -> lista de s√≠mbolos elegibles por cobertura.\n",
    "#       GLOBAL_STATE[\"symbols\"][\"passed_data_gate\"]               -> lista de s√≠mbolos que pasan el data gate combinado.\n",
    "#       GLOBAL_STATE[\"metrics\"][\"qa_coverage_path\"]               -> ruta del parquet de QA.\n",
    "#\n",
    "#       # Naming expl√≠cito de ‚ÄúUniverso Gold‚Äù:\n",
    "#       #   - universe_gold                  ‚Üí s√≠mbolos que pasan TODO el Data Gate.\n",
    "#       #   - universe_gold_coverage_only    ‚Üí s√≠mbolos elegibles solo por cobertura.\n",
    "#       GLOBAL_STATE[\"symbols\"][\"universe_gold\"]\n",
    "#       GLOBAL_STATE[\"symbols\"][\"universe_gold_coverage_only\"]\n",
    "#\n",
    "#   - Adem√°s, esta celda actualiza contadores de puerta en:\n",
    "#       GLOBAL_STATE[\"report_stats\"][\"c03\"] = {\n",
    "#           \"n_initial\", \"n_idx_coverage\", \"n_eligible_m5\", \"n_passed_data_gate\"\n",
    "#       }\n",
    "#\n",
    "#   - Prints:\n",
    "#       * Datos de entrada: ruta PAD_DAY_INDEX, n¬∫ filas, rango de fechas.\n",
    "#       * Datos de salida: n¬∫ total de s√≠mbolos en √≠ndice, n¬∫ que pasan QA cobertura,\n",
    "#                         n¬∫ que pasan Data Gate, top/bottom 10 por cobertura.\n",
    "#       * Marca final: \">>> Celda 03 :: OK (eligible_m5=..., passed_data_gate=...)\". \n",
    "#\n",
    "#   - Nota importante:\n",
    "#       Un activo con cobertura sub√≥ptima puede seguir siendo interesante,\n",
    "#       pero se le penalizar√° luego en score_stability y en flags de data.\n",
    "#       Aqu√≠ no se filtra agresivamente por edge, solo por historial claramente roto.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Se requiere 'polars' para la Celda 03 | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Utilidades de configuraci√≥n\n",
    "# -------------------------------\n",
    "def _norm_symbol(x: str) -> str:\n",
    "    return \"\" if x is None else str(x).strip().upper().replace(\" \", \"\")\n",
    "\n",
    "def _get_cfg_int(cfg: dict, dotted_key: str, default: Optional[int]) -> Optional[int]:\n",
    "    \"\"\"Lee un entero de config usando clave con notaci√≥n 'a.b.c'. Si falta, usa default.\"\"\"\n",
    "    if not isinstance(cfg, dict):\n",
    "        return default\n",
    "    cur = cfg\n",
    "    for k in dotted_key.split(\".\"):\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    try:\n",
    "        return int(cur)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _get_cfg_float(cfg: dict, dotted_key: str, default: Optional[float]) -> Optional[float]:\n",
    "    \"\"\"Lee un float de config usando clave con notaci√≥n 'a.b.c'. Si falta, usa default.\"\"\"\n",
    "    if not isinstance(cfg, dict):\n",
    "        return default\n",
    "    cur = cfg\n",
    "    for k in dotted_key.split(\".\"):\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    try:\n",
    "        return float(cur)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _load_config(out_diag_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Carga config.json desde diagnostics.\n",
    "    NOTA: la intenci√≥n es que Celda 04 (u otra) sea la que genere/actualice este config.\n",
    "    \"\"\"\n",
    "    cfg_path = out_diag_dir / \"config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# Conversi√≥n robusta a Datetime(us)\n",
    "# -------------------------------\n",
    "def _guess_epoch_unit_from_value(sample_val: float) -> str:\n",
    "    # Heur√≠stica por magnitud\n",
    "    try:\n",
    "        v = float(sample_val)\n",
    "    except Exception:\n",
    "        return \"s\"\n",
    "    if v > 1e17:  # ns\n",
    "        return \"ns\"\n",
    "    if v > 1e14:  # us\n",
    "        return \"us\"\n",
    "    if v > 1e11:  # ms\n",
    "        return \"ms\"\n",
    "    return \"s\"    # s\n",
    "\n",
    "def _to_datetime_series(col: pl.Series) -> Optional[pl.Series]:\n",
    "    if col is None or col.len() == 0:\n",
    "        return None\n",
    "\n",
    "    dtype = col.dtype\n",
    "\n",
    "    # Datetime ‚Üí normalizar a us\n",
    "    if str(dtype).startswith(\"Datetime\"):\n",
    "        try:\n",
    "            return col.cast(pl.Datetime(\"us\"))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Num√©rico ‚Üí √©poca\n",
    "    if dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Float64, pl.Float32):\n",
    "        try:\n",
    "            sample = col.drop_nulls()\n",
    "            if sample.len() == 0:\n",
    "                return None\n",
    "            sample_val = float(sample.quantile(0.5, interpolation=\"nearest\"))\n",
    "        except Exception:\n",
    "            sample_val = None\n",
    "\n",
    "        unit0 = _guess_epoch_unit_from_value(sample_val if sample_val is not None else 0.0)\n",
    "        for u in (unit0, \"ns\", \"us\", \"ms\", \"s\"):\n",
    "            try:\n",
    "                return pl.from_epoch(col.cast(pl.Float64), time_unit=u).cast(pl.Datetime(\"us\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    # Texto ISO ‚Üí parseo laxo, con fallback a num√©rico\n",
    "    if dtype == pl.Utf8:\n",
    "        try:\n",
    "            s = col.str.strptime(pl.Datetime, strict=False, time_unit=\"us\")\n",
    "            if s is not None and s.null_count() < s.len():\n",
    "                return s.cast(pl.Datetime(\"us\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            s_num = col.str.replace_all(r\"[^\\d\\.]\", \"\").cast(pl.Float64)\n",
    "            sample = s_num.drop_nulls()\n",
    "            if sample.len() == 0:\n",
    "                return None\n",
    "            unit = _guess_epoch_unit_from_value(float(sample.quantile(0.5, interpolation=\"nearest\")))\n",
    "            return pl.from_epoch(s_num, time_unit=unit).cast(pl.Datetime(\"us\"))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # √öltimo recurso\n",
    "    try:\n",
    "        return col.cast(pl.Datetime(\"us\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# Lectura diaria (por archivo)\n",
    "# -------------------------------\n",
    "def _read_daily_ts(parquet_path: Path) -> Optional[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Lee un archivo diario de m5_clean y devuelve DataFrame con 'ts' (Datetime(us)).\n",
    "    No hay unificaci√≥n multi-archivo aqu√≠ (eso se hace a nivel de s√≠mbolo).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not parquet_path.exists() or not parquet_path.is_file():\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    for colname in (\"timestamp_utc\", \"time\", \"datetime\", \"timestamp\"):\n",
    "        try:\n",
    "            df = pl.read_parquet(str(parquet_path), columns=[colname])\n",
    "            if colname not in df.columns:\n",
    "                continue\n",
    "            col = df.get_column(colname)\n",
    "            s = _to_datetime_series(col)\n",
    "            if s is None or s.len() == 0:\n",
    "                continue\n",
    "            return pl.DataFrame({\"ts\": s}).drop_nulls().sort(\"ts\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# M√©tricas de cadencia (con medias y gaps)\n",
    "# -------------------------------\n",
    "def _compute_symbol_metrics(\n",
    "    df_ts: pl.DataFrame,\n",
    "    tol_seconds: int,\n",
    "    max_gap_seconds: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas de cadencia, devolviendo:\n",
    "      - bars_total, bars_valid_m5, pct_valid_m5_bars\n",
    "      - pct_valid_transitions\n",
    "      - avg_dt_seconds_overall (sobre todos los deltas)\n",
    "      - avg_dt_seconds_valid (solo transiciones v√°lidas 300¬±tol)\n",
    "      - pct_gaps_over_max (proporci√≥n de transiciones con gap > max_gap_seconds)\n",
    "      - first_ts_utc, last_ts_utc, dups\n",
    "      - hist_days, hist_years\n",
    "    \"\"\"\n",
    "    empty_payload = {\n",
    "        \"bars_total\": 0,\n",
    "        \"bars_valid_m5\": 0,\n",
    "        \"pct_valid_m5_bars\": 0.0,\n",
    "        \"pct_valid_transitions\": 0.0,\n",
    "        \"avg_dt_seconds_overall\": None,\n",
    "        \"avg_dt_seconds_valid\": None,\n",
    "        \"pct_gaps_over_max\": None,\n",
    "        \"first_ts_utc\": None,\n",
    "        \"last_ts_utc\": None,\n",
    "        \"dups\": 0,\n",
    "        \"hist_days\": None,\n",
    "        \"hist_years\": None,\n",
    "    }\n",
    "\n",
    "    if df_ts is None or getattr(df_ts, \"is_empty\", lambda: True)():\n",
    "        return empty_payload\n",
    "\n",
    "    df = df_ts.drop_nulls().sort(\"ts\")\n",
    "    n_total = df.height\n",
    "    if n_total == 0:\n",
    "        return empty_payload\n",
    "\n",
    "    try:\n",
    "        n_unique = int(df.select(pl.col(\"ts\").n_unique()).item())\n",
    "    except Exception:\n",
    "        n_unique = n_total\n",
    "\n",
    "    dups = int(max(0, n_total - n_unique))\n",
    "    df = df.unique(subset=[\"ts\"], keep=\"first\").sort(\"ts\")\n",
    "    n = df.height\n",
    "    if n == 0:\n",
    "        return empty_payload\n",
    "\n",
    "    # deltas en segundos\n",
    "    df = df.with_columns((pl.col(\"ts\").diff().dt.total_seconds()).alias(\"dt_s\"))\n",
    "    deltas = df.get_column(\"dt_s\").drop_nulls()\n",
    "    dcount = deltas.len()\n",
    "\n",
    "    if dcount == 0:\n",
    "        avg_dt_overall = None\n",
    "        avg_dt_valid = None\n",
    "        valid_transitions = 0\n",
    "        pct_valid_transitions = 0.0\n",
    "        bars_valid = 1\n",
    "        pct_valid_bars = float(bars_valid / n) if n > 0 else 0.0\n",
    "        pct_gaps_over_max = 0.0\n",
    "    else:\n",
    "        avg_dt_overall = float(deltas.mean())\n",
    "\n",
    "        valid_mask = (deltas >= (300 - tol_seconds)) & (deltas <= (300 + tol_seconds))\n",
    "        valid_transitions = int(valid_mask.sum())\n",
    "        pct_valid_transitions = float(valid_transitions / dcount) if dcount > 0 else 0.0\n",
    "\n",
    "        # barras v√°lidas aproximadas seg√∫n transiciones v√°lidas\n",
    "        bars_valid = int(valid_transitions + 1)\n",
    "        pct_valid_bars = float(bars_valid / n) if n > 0 else 0.0\n",
    "\n",
    "        avg_dt_valid = float(deltas.filter(valid_mask).mean()) if valid_transitions > 0 else None\n",
    "\n",
    "        gaps_over = int((deltas > max_gap_seconds).sum())\n",
    "        pct_gaps_over_max = float(gaps_over / dcount) if dcount > 0 else 0.0\n",
    "\n",
    "    ts_series = df.get_column(\"ts\")\n",
    "    first_ts = ts_series.min()\n",
    "    last_ts = ts_series.max()\n",
    "\n",
    "    def _iso_no_micro(tsv) -> Optional[str]:\n",
    "        if tsv is None:\n",
    "            return None\n",
    "        s = str(tsv)\n",
    "        if \".\" in s:\n",
    "            s = s.split(\".\", 1)[0]\n",
    "        if \"+\" in s:\n",
    "            s = s.split(\"+\", 1)[0]\n",
    "        if s.endswith(\"Z\"):\n",
    "            s = s[:-1]\n",
    "        return s\n",
    "\n",
    "    hist_days = None\n",
    "    hist_years = None\n",
    "    try:\n",
    "        if isinstance(first_ts, datetime) and isinstance(last_ts, datetime):\n",
    "            delta = last_ts - first_ts\n",
    "            hist_days = int(delta.days)\n",
    "            hist_years = float(hist_days) / 365.25 if hist_days is not None else None\n",
    "    except Exception:\n",
    "        hist_days = None\n",
    "        hist_years = None\n",
    "\n",
    "    return {\n",
    "        \"bars_total\": int(n),\n",
    "        \"bars_valid_m5\": int(bars_valid),\n",
    "        \"pct_valid_m5_bars\": float(pct_valid_bars),\n",
    "        \"pct_valid_transitions\": float(pct_valid_transitions),\n",
    "        \"avg_dt_seconds_overall\": float(avg_dt_overall) if avg_dt_overall is not None else None,\n",
    "        \"avg_dt_seconds_valid\": float(avg_dt_valid) if avg_dt_valid is not None else None,\n",
    "        \"pct_gaps_over_max\": float(pct_gaps_over_max) if pct_gaps_over_max is not None else None,\n",
    "        \"first_ts_utc\": _iso_no_micro(first_ts),\n",
    "        \"last_ts_utc\": _iso_no_micro(last_ts),\n",
    "        \"dups\": int(dups),\n",
    "        \"hist_days\": int(hist_days) if hist_days is not None else None,\n",
    "        \"hist_years\": float(hist_years) if hist_years is not None else None,\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers de output\n",
    "# -------------------------------\n",
    "def _ensure_dir(p: Path) -> Path:\n",
    "    try:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return p\n",
    "\n",
    "def _print_top_bottom(df: pl.DataFrame, col: str, n: int = 10):\n",
    "    if df is None or df.is_empty() or col not in df.columns:\n",
    "        return\n",
    "    try:\n",
    "        top = df.select([\"symbol\", col]).sort(col, descending=True).head(n)\n",
    "        bot = df.select([\"symbol\", col]).sort(col, descending=False).head(n)\n",
    "        print(f\"[Celda 03] Top {n} por {col}:\")\n",
    "        for sym, v in top.iter_rows():\n",
    "            print(f\"   {sym:>10s} | {v}\")\n",
    "        print(f\"[Celda 03] Bottom {n} por {col}:\")\n",
    "        for sym, v in bot.iter_rows():\n",
    "            print(f\"   {sym:>10s} | {v}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 03][WARN] No se pudo imprimir top/bottom por {col} | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones de entorno\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero Celdas 00/01/02.\")\n",
    "\n",
    "for key in (\"inputs\", \"paths\", \"symbols\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE incompleto: falta clave '{key}'.\")\n",
    "\n",
    "inputs = GLOBAL_STATE[\"inputs\"]\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "symbols_state = GLOBAL_STATE[\"symbols\"]\n",
    "tables_state = GLOBAL_STATE.get(\"tables\", {})\n",
    "\n",
    "OUT_DIAG_DIR = _ensure_dir(Path(paths[\"diagnostics\"]).resolve())\n",
    "M5_ROOT_DIR = Path(inputs[\"M5_ROOT_DIR\"]).resolve()\n",
    "PAD_DAY_INDEX = Path(inputs[\"PAD_DAY_INDEX\"]).resolve()\n",
    "\n",
    "# Check ligero de congruencia run/diagnostics (solo aviso)\n",
    "run_id = GLOBAL_STATE.get(\"RUN_ID\") or GLOBAL_STATE.get(\"run_id\")\n",
    "if run_id:\n",
    "    try:\n",
    "        if str(run_id) not in str(OUT_DIAG_DIR):\n",
    "            print(f\"[Celda 03][WARN] diagnostics no parece estar scopiado por RUN_ID | run_id={run_id} | dir={OUT_DIAG_DIR}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Universo base: universe_effective si existe, si no rates_m5\n",
    "universe_effective_syms = symbols_state.get(\"universe_effective\", [])\n",
    "use_universe_effective = isinstance(universe_effective_syms, list) and len(universe_effective_syms) > 0\n",
    "\n",
    "if use_universe_effective:\n",
    "    base_syms_raw = universe_effective_syms\n",
    "    base_source = \"universe_effective\"\n",
    "else:\n",
    "    rates_syms = symbols_state.get(\"rates_m5\", [])\n",
    "    if not isinstance(rates_syms, list) or len(rates_syms) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No hay s√≠mbolos detectados ni en 'universe_effective' ni en 'rates_m5'. \"\n",
    "            \"Revisa Celdas 01 y 02.\"\n",
    "        )\n",
    "    base_syms_raw = rates_syms\n",
    "    base_source = \"rates_m5\"\n",
    "\n",
    "base_syms = sorted({_norm_symbol(s) for s in base_syms_raw if s})\n",
    "base_syms_norm = sorted({s for s in base_syms if s})\n",
    "\n",
    "# df_data_quality (opcional) desde Celda 02\n",
    "df_data_quality = tables_state.get(\"data_quality\", None)\n",
    "dq_available = isinstance(df_data_quality, pl.DataFrame) and not df_data_quality.is_empty()\n",
    "\n",
    "# -------------------------------\n",
    "# Config (tolerancias y umbrales)\n",
    "# -------------------------------\n",
    "cfg = _load_config(OUT_DIAG_DIR)\n",
    "\n",
    "min_bars = _get_cfg_int(cfg, \"qa.min_bars_m5\", 20_000)\n",
    "tol_seconds = _get_cfg_int(cfg, \"qa.tol_seconds\", 5)\n",
    "window_days = _get_cfg_int(cfg, \"qa.window_days\", None)  # None / null = TODOS los d√≠as de GOLD\n",
    "\n",
    "# Defaults relajados (endurecibles por config)\n",
    "min_pct_valid_transitions = _get_cfg_float(cfg, \"qa.min_pct_valid_transitions\", 0.90)\n",
    "\n",
    "# Legacy (para backward compatibility)\n",
    "min_pct_days_full_legacy = _get_cfg_float(cfg, \"qa.min_pct_days_full\", 0.75)\n",
    "\n",
    "# Nuevos par√°metros (con fallback al legacy)\n",
    "day_full_threshold_pct = _get_cfg_float(cfg, \"qa.day_full_threshold_pct\", min_pct_days_full_legacy)\n",
    "min_pct_days_full_required = _get_cfg_float(cfg, \"qa.min_pct_days_full_required\", min_pct_days_full_legacy)\n",
    "\n",
    "expected_bars_per_day = _get_cfg_int(cfg, \"qa.expected_bars_per_day\", 72)\n",
    "max_gap_minutes = _get_cfg_int(cfg, \"qa.max_gap_minutes\", 120)\n",
    "min_history_years = _get_cfg_float(cfg, \"qa.min_history_years\", 1.0)\n",
    "\n",
    "min_bars = int(min_bars or 0)\n",
    "tol_seconds = int(tol_seconds or 0)\n",
    "expected_bars_per_day = int(expected_bars_per_day or 0)\n",
    "max_gap_minutes = int(max_gap_minutes or 0)\n",
    "max_gap_seconds = int((max_gap_minutes if max_gap_minutes > 0 else 120) * 60)\n",
    "min_history_years = float(min_history_years or 0.0)\n",
    "\n",
    "day_full_threshold_pct = float(day_full_threshold_pct or 0.75)\n",
    "min_pct_days_full_required = float(min_pct_days_full_required or 0.75)\n",
    "min_pct_valid_transitions = float(min_pct_valid_transitions or 0.0)\n",
    "\n",
    "print(\">>> Celda 03 :: QA Cobertura M5 + Data Gate\")\n",
    "print(f\"[Celda 03] M5_ROOT_DIR = {str(M5_ROOT_DIR)}\")\n",
    "print(\n",
    "    f\"[Celda 03] Config QA ‚Üí \"\n",
    "    f\"min_bars_m5={min_bars}, \"\n",
    "    f\"tol_seconds=¬±{tol_seconds}, \"\n",
    "    f\"min_pct_valid_transitions={min_pct_valid_transitions:.3f}, \"\n",
    "    f\"day_full_threshold_pct={day_full_threshold_pct:.3f}, \"\n",
    "    f\"min_pct_days_full_required={min_pct_days_full_required:.3f}, \"\n",
    "    f\"expected_bars_per_day={expected_bars_per_day}, \"\n",
    "    f\"max_gap_minutes={max_gap_minutes}, \"\n",
    "    f\"min_history_years={min_history_years}\"\n",
    ")\n",
    "try:\n",
    "    if isinstance(cfg.get(\"qa\", None), dict) and \"min_pct_days_full\" in cfg.get(\"qa\", {}):\n",
    "        print(f\"[Celda 03] Legacy detectado: qa.min_pct_days_full={float(min_pct_days_full_legacy or 0.75):.3f}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar √≠ndice diario (PAD_DAY_INDEX)\n",
    "# -------------------------------\n",
    "if not PAD_DAY_INDEX.exists():\n",
    "    raise RuntimeError(f\"PAD √≠ndice diario no encontrado: {str(PAD_DAY_INDEX)}\")\n",
    "\n",
    "try:\n",
    "    di = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo leer {str(PAD_DAY_INDEX)} | reason={e}\")\n",
    "\n",
    "# normalizar columnas\n",
    "di = di.rename({c: c.lower() for c in di.columns})\n",
    "required_cols = {\"symbol\", \"date\", \"path\"}\n",
    "missing = required_cols - set(di.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan columnas en day_index_m5.parquet: {missing}\")\n",
    "\n",
    "# normalizar symbols y filtrar universo\n",
    "di = di.with_columns(pl.col(\"symbol\").map_elements(_norm_symbol).alias(\"symbol\"))\n",
    "di = di.filter(pl.col(\"symbol\").is_in(base_syms_norm))\n",
    "\n",
    "# Parseo de date robusto: soporta Utf8 y enteros YYYYMMDD\n",
    "try:\n",
    "    dt = di.schema.get(\"date\")\n",
    "    if dt == pl.Utf8:\n",
    "        di = di.with_columns(\n",
    "            pl.col(\"date\").str.strptime(pl.Date, format=\"%Y%m%d\", strict=False).alias(\"date\")\n",
    "        )\n",
    "    elif dt in (pl.Int32, pl.Int64, pl.UInt32, pl.UInt64):\n",
    "        di = di.with_columns(\n",
    "            pl.col(\"date\").cast(pl.Utf8).str.strptime(pl.Date, format=\"%Y%m%d\", strict=False).alias(\"date\")\n",
    "        )\n",
    "except Exception:\n",
    "    try:\n",
    "        if di.schema.get(\"date\") == pl.Utf8:\n",
    "            di = di.with_columns(pl.col(\"date\").str.strptime(pl.Date, strict=False).alias(\"date\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Info de entrada: filas y rango de fechas\n",
    "n_rows_di = di.height\n",
    "date_min, date_max = None, None\n",
    "if n_rows_di > 0:\n",
    "    try:\n",
    "        date_min = str(di[\"date\"].min())\n",
    "        date_max = str(di[\"date\"].max())\n",
    "    except Exception:\n",
    "        date_min, date_max = None, None\n",
    "\n",
    "print(f\"[Celda 03] PAD_DAY_INDEX = {str(PAD_DAY_INDEX)}\")\n",
    "print(f\"[Celda 03] day_index_m5 ‚Üí filas={n_rows_di}, rango_fechas=[{date_min} ‚Üí {date_max}]\")\n",
    "print(f\"[Celda 03] S√≠mbolos iniciales ({base_source}) = {len(base_syms_norm)}\")\n",
    "\n",
    "# Ventana opcional de d√≠as por s√≠mbolo\n",
    "if window_days is not None and int(window_days) > 0:\n",
    "    try:\n",
    "        di = (\n",
    "            di.sort(by=[\"symbol\", \"date\"], descending=[False, True])\n",
    "              .group_by(\"symbol\")\n",
    "              .head(int(window_days))\n",
    "        )\n",
    "        print(f\"[Celda 03] Ventana aplicada: √∫ltimos {int(window_days)} d√≠as por s√≠mbolo\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 03][WARN] No se pudo aplicar window_days | reason={e}\")\n",
    "        print(\"[Celda 03] Ventana aplicada: TODOS los d√≠as disponibles\")\n",
    "else:\n",
    "    print(\"[Celda 03] Ventana aplicada: TODOS los d√≠as disponibles\")\n",
    "\n",
    "# Conteo s√≠mbolos efectivos en √≠ndice\n",
    "try:\n",
    "    n_idx_coverage = int(di.select(pl.col(\"symbol\").n_unique()).item())\n",
    "except Exception:\n",
    "    n_idx_coverage = 0\n",
    "\n",
    "# Map: symbol ‚Üí list(paths)\n",
    "paths_series = di.select([\"symbol\", \"path\"]) if not di.is_empty() else pl.DataFrame({\"symbol\": [], \"path\": []})\n",
    "sym_to_paths: Dict[str, List[Path]] = {}\n",
    "\n",
    "for r in paths_series.iter_rows(named=True):\n",
    "    sym = r.get(\"symbol\")\n",
    "    p_raw = r.get(\"path\")\n",
    "    if not sym or not p_raw:\n",
    "        continue\n",
    "    try:\n",
    "        p = Path(p_raw)\n",
    "        if not p.is_absolute():\n",
    "            p = M5_ROOT_DIR / p\n",
    "        sym_to_paths.setdefault(sym, []).append(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# -------------------------------\n",
    "# Procesamiento por s√≠mbolo\n",
    "# -------------------------------\n",
    "rows: List[Dict[str, Any]] = []\n",
    "excluded_msgs: List[str] = []\n",
    "n_syms = len(base_syms_norm)\n",
    "processed = 0\n",
    "\n",
    "for sym in base_syms_norm:\n",
    "    raw_list = sym_to_paths.get(sym, [])\n",
    "\n",
    "    # Deduplicar paths manteniendo orden\n",
    "    file_list: List[Path] = []\n",
    "    seen = set()\n",
    "    for p in raw_list:\n",
    "        sp = str(p)\n",
    "        if sp in seen:\n",
    "            continue\n",
    "        seen.add(sp)\n",
    "        file_list.append(p)\n",
    "\n",
    "    if not file_list:\n",
    "        rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"bars_total\": 0,\n",
    "            \"bars_valid_m5\": 0,\n",
    "            \"pct_valid_m5_bars\": 0.0,\n",
    "            \"pct_valid_transitions\": 0.0,\n",
    "            \"avg_dt_seconds_overall\": None,\n",
    "            \"avg_dt_seconds_valid\": None,\n",
    "            \"pct_gaps_over_max\": None,\n",
    "            \"days_total\": 0,\n",
    "            \"days_full\": 0,\n",
    "            \"pct_days_full\": 0.0,\n",
    "            \"first_ts_utc\": None,\n",
    "            \"last_ts_utc\": None,\n",
    "            \"dups\": 0,\n",
    "            \"hist_days\": None,\n",
    "            \"hist_years\": None,\n",
    "            \"eligible_m5\": False,\n",
    "            \"exclusion_reason\": \"no_index_paths\",\n",
    "        })\n",
    "        if len(excluded_msgs) < 10:\n",
    "            excluded_msgs.append(f\"EXCLUIDO symbol={sym} motivo=no_index_paths\")\n",
    "        processed += 1\n",
    "        if processed % 20 == 0 or processed == n_syms:\n",
    "            print(f\"[Celda 03] ... procesados {processed}/{n_syms} s√≠mbolos\")\n",
    "        continue\n",
    "\n",
    "    daily_frames: List[pl.DataFrame] = []\n",
    "    bars_per_day: List[int] = []\n",
    "\n",
    "    for p in file_list:\n",
    "        df_ts = _read_daily_ts(p)\n",
    "        if df_ts is None or df_ts.is_empty():\n",
    "            continue\n",
    "        daily_frames.append(df_ts)\n",
    "        bars_per_day.append(int(df_ts.height))\n",
    "\n",
    "    if not daily_frames:\n",
    "        rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"bars_total\": 0,\n",
    "            \"bars_valid_m5\": 0,\n",
    "            \"pct_valid_m5_bars\": 0.0,\n",
    "            \"pct_valid_transitions\": 0.0,\n",
    "            \"avg_dt_seconds_overall\": None,\n",
    "            \"avg_dt_seconds_valid\": None,\n",
    "            \"pct_gaps_over_max\": None,\n",
    "            \"days_total\": int(len(file_list)),\n",
    "            \"days_full\": 0,\n",
    "            \"pct_days_full\": 0.0,\n",
    "            \"first_ts_utc\": None,\n",
    "            \"last_ts_utc\": None,\n",
    "            \"dups\": 0,\n",
    "            \"hist_days\": None,\n",
    "            \"hist_years\": None,\n",
    "            \"eligible_m5\": False,\n",
    "            \"exclusion_reason\": \"no_data\",\n",
    "        })\n",
    "        if len(excluded_msgs) < 10:\n",
    "            excluded_msgs.append(f\"EXCLUIDO symbol={sym} motivo=no_data\")\n",
    "        processed += 1\n",
    "        if processed % 20 == 0 or processed == n_syms:\n",
    "            print(f\"[Celda 03] ... procesados {processed}/{n_syms} s√≠mbolos\")\n",
    "        continue\n",
    "\n",
    "    # Unificar ts de todos los d√≠as del s√≠mbolo\n",
    "    df_sym = pl.concat(daily_frames, how=\"vertical_relaxed\").drop_nulls().sort(\"ts\")\n",
    "\n",
    "    m = _compute_symbol_metrics(df_sym, tol_seconds=tol_seconds, max_gap_seconds=max_gap_seconds)\n",
    "\n",
    "    # D√≠as completos seg√∫n mediana por s√≠mbolo\n",
    "    days_total = int(len(bars_per_day))\n",
    "    if days_total > 0:\n",
    "        try:\n",
    "            median_bars = float(pl.Series(bars_per_day).median())\n",
    "        except Exception:\n",
    "            median_bars = 0.0\n",
    "\n",
    "        per_sym_expected = median_bars if median_bars and median_bars > 0 else float(expected_bars_per_day or 288)\n",
    "\n",
    "        # Umbral de d√≠a \"completo\" (definici√≥n)\n",
    "        full_threshold = int(per_sym_expected * day_full_threshold_pct)\n",
    "        if full_threshold < 1:\n",
    "            full_threshold = 1\n",
    "\n",
    "        days_full = int(sum(1 for n_bars in bars_per_day if n_bars >= full_threshold))\n",
    "        pct_days_full = float(days_full / days_total) if days_total > 0 else 0.0\n",
    "    else:\n",
    "        days_full = 0\n",
    "        pct_days_full = 0.0\n",
    "\n",
    "    eligible = True\n",
    "    reason = None\n",
    "\n",
    "    pct_valid_transitions_val = float(m.get(\"pct_valid_transitions\") or 0.0)\n",
    "    if pct_valid_transitions_val < min_pct_valid_transitions:\n",
    "        eligible = False\n",
    "        reason = \"low_pct_valid_transitions\"\n",
    "\n",
    "    # Exigencia m√≠nima de d√≠as completos (gate)\n",
    "    if eligible and pct_days_full < min_pct_days_full_required:\n",
    "        eligible = False\n",
    "        reason = \"low_pct_days_full\"\n",
    "\n",
    "    if eligible and int(m.get(\"bars_valid_m5\") or 0) < int(min_bars):\n",
    "        eligible = False\n",
    "        reason = \"few_bars\"\n",
    "\n",
    "    rows.append({\n",
    "        \"symbol\": sym,\n",
    "        **m,\n",
    "        \"days_total\": int(days_total),\n",
    "        \"days_full\": int(days_full),\n",
    "        \"pct_days_full\": float(pct_days_full),\n",
    "        \"eligible_m5\": bool(eligible),\n",
    "        \"exclusion_reason\": (reason if not eligible else None),\n",
    "    })\n",
    "\n",
    "    if not eligible and len(excluded_msgs) < 10:\n",
    "        excluded_msgs.append(f\"EXCLUIDO symbol={sym} motivo={reason}\")\n",
    "\n",
    "    processed += 1\n",
    "    if processed % 20 == 0 or processed == n_syms:\n",
    "        print(f\"[Celda 03] ... procesados {processed}/{n_syms} s√≠mbolos\")\n",
    "\n",
    "# -------------------------------\n",
    "# Construcci√≥n base de coverage_df\n",
    "# -------------------------------\n",
    "if rows:\n",
    "    coverage_df = pl.DataFrame(rows)\n",
    "else:\n",
    "    coverage_df = pl.DataFrame({\n",
    "        \"symbol\": pl.Series([], dtype=pl.Utf8),\n",
    "        \"bars_total\": pl.Series([], dtype=pl.Int64),\n",
    "        \"bars_valid_m5\": pl.Series([], dtype=pl.Int64),\n",
    "        \"pct_valid_m5_bars\": pl.Series([], dtype=pl.Float64),\n",
    "        \"pct_valid_transitions\": pl.Series([], dtype=pl.Float64),\n",
    "        \"avg_dt_seconds_overall\": pl.Series([], dtype=pl.Float64),\n",
    "        \"avg_dt_seconds_valid\": pl.Series([], dtype=pl.Float64),\n",
    "        \"pct_gaps_over_max\": pl.Series([], dtype=pl.Float64),\n",
    "        \"days_total\": pl.Series([], dtype=pl.Int64),\n",
    "        \"days_full\": pl.Series([], dtype=pl.Int64),\n",
    "        \"pct_days_full\": pl.Series([], dtype=pl.Float64),\n",
    "        \"first_ts_utc\": pl.Series([], dtype=pl.Utf8),\n",
    "        \"last_ts_utc\": pl.Series([], dtype=pl.Utf8),\n",
    "        \"dups\": pl.Series([], dtype=pl.Int64),\n",
    "        \"hist_days\": pl.Series([], dtype=pl.Int64),\n",
    "        \"hist_years\": pl.Series([], dtype=pl.Float64),\n",
    "        \"eligible_m5\": pl.Series([], dtype=pl.Boolean),\n",
    "        \"exclusion_reason\": pl.Series([], dtype=pl.Utf8),\n",
    "    })\n",
    "\n",
    "# Asegurar dtypes clave\n",
    "try:\n",
    "    coverage_df = coverage_df.with_columns(\n",
    "        pl.col(\"symbol\").cast(pl.Utf8),\n",
    "        pl.col(\"eligible_m5\").cast(pl.Boolean),\n",
    "        pl.col(\"bars_total\").cast(pl.Int64),\n",
    "        pl.col(\"bars_valid_m5\").cast(pl.Int64),\n",
    "        pl.col(\"pct_valid_m5_bars\").cast(pl.Float64),\n",
    "        pl.col(\"pct_valid_transitions\").cast(pl.Float64),\n",
    "        pl.col(\"pct_days_full\").cast(pl.Float64),\n",
    "        pl.col(\"hist_years\").cast(pl.Float64),\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------\n",
    "# Integraci√≥n con df_data_quality\n",
    "# -------------------------------\n",
    "if dq_available:\n",
    "    try:\n",
    "        dq_df = df_data_quality.rename({c: c.lower() for c in df_data_quality.columns})\n",
    "\n",
    "        if \"symbol\" in dq_df.columns:\n",
    "            dq_df = dq_df.with_columns(\n",
    "                pl.col(\"symbol\").map_elements(_norm_symbol).alias(\"symbol\")\n",
    "            )\n",
    "            if \"data_quality_flag\" in dq_df.columns:\n",
    "                dq_df = dq_df.with_columns(\n",
    "                    pl.col(\"data_quality_flag\").cast(pl.Utf8).str.to_uppercase().alias(\"data_quality_flag\")\n",
    "                )\n",
    "                dq_sym = (\n",
    "                    dq_df.unique(subset=[\"symbol\"], keep=\"last\")\n",
    "                         .select([\"symbol\", \"data_quality_flag\"])\n",
    "                )\n",
    "\n",
    "                coverage_df = coverage_df.join(dq_sym, on=\"symbol\", how=\"left\")\n",
    "                print(f\"[Celda 03] DataQuality: s√≠mbolos con info={dq_sym.height}\")\n",
    "            else:\n",
    "                print(\"[Celda 03][WARN] df_data_quality no tiene columna 'data_quality_flag'. Se omite gate por flag.\")\n",
    "        else:\n",
    "            print(\"[Celda 03][WARN] df_data_quality no tiene columna 'symbol'. Se omite gate por data_quality.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 03][WARN] Error integrando df_data_quality | reason={e}\")\n",
    "else:\n",
    "    print(\"[Celda 03] df_data_quality no disponible en GLOBAL_STATE['tables']['data_quality'].\")\n",
    "\n",
    "# Garantizar que exista columna data_quality_flag\n",
    "if \"data_quality_flag\" not in coverage_df.columns:\n",
    "    coverage_df = coverage_df.with_columns(pl.lit(None).cast(pl.Utf8).alias(\"data_quality_flag\"))\n",
    "\n",
    "# Normalizaci√≥n de flag\n",
    "coverage_df = coverage_df.with_columns(\n",
    "    pl.col(\"data_quality_flag\").cast(pl.Utf8).str.to_uppercase().alias(\"_dq_up\")\n",
    ").with_columns(\n",
    "    pl.when(pl.col(\"_dq_up\").is_in([\"OK\", \"WARNING\", \"BAD\"]))\n",
    "      .then(pl.col(\"_dq_up\"))\n",
    "      .otherwise(pl.lit(\"UNKNOWN\"))\n",
    "      .alias(\"data_quality_flag_norm\")\n",
    ").drop(\"_dq_up\")\n",
    "\n",
    "# -------------------------------\n",
    "# Data Gate (historial + flag) + combinaci√≥n final con cobertura\n",
    "# -------------------------------\n",
    "coverage_df = coverage_df.with_columns(\n",
    "    pl.when(pl.col(\"hist_years\").is_null())\n",
    "      .then(False)\n",
    "      .when(pl.col(\"hist_years\") < float(min_history_years))\n",
    "      .then(False)\n",
    "      .when(pl.col(\"data_quality_flag_norm\") == \"BAD\")\n",
    "      .then(False)\n",
    "      .otherwise(True)\n",
    "      .alias(\"_gate_hist_quality\")\n",
    ")\n",
    "\n",
    "coverage_df = coverage_df.with_columns(\n",
    "    (pl.col(\"eligible_m5\") & pl.col(\"_gate_hist_quality\")).alias(\"passed_data_gate\")\n",
    ")\n",
    "\n",
    "coverage_df = coverage_df.with_columns(\n",
    "    pl.when(~pl.col(\"eligible_m5\"))\n",
    "      .then(pl.lit(\"failed_coverage\"))\n",
    "      .when(pl.col(\"hist_years\").is_null())\n",
    "      .then(pl.lit(\"no_hist_years\"))\n",
    "      .when(pl.col(\"hist_years\") < float(min_history_years))\n",
    "      .then(pl.lit(\"short_history\"))\n",
    "      .when(pl.col(\"data_quality_flag_norm\") == \"BAD\")\n",
    "      .then(pl.lit(\"bad_data_quality_flag\"))\n",
    "      .otherwise(pl.lit(None))\n",
    "      .alias(\"data_gate_reason\")\n",
    ").drop(\"_gate_hist_quality\")\n",
    "\n",
    "# -------------------------------\n",
    "# Persistencia de resultados (parquet obligatorio)\n",
    "# -------------------------------\n",
    "out_parquet = OUT_DIAG_DIR / \"coverage_table_5m.parquet\"\n",
    "out_csv = OUT_DIAG_DIR / \"coverage_table_5m.csv\"\n",
    "\n",
    "coverage_df.write_parquet(str(out_parquet))\n",
    "\n",
    "try:\n",
    "    coverage_df.write_csv(str(out_csv))\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 03][WARN] No se pudo escribir coverage_table_5m.csv | reason={e}\")\n",
    "\n",
    "try:\n",
    "    if not out_parquet.exists() or out_parquet.stat().st_size == 0:\n",
    "        raise RuntimeError(\"coverage_table_5m.parquet no fue escrito correctamente.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(str(e))\n",
    "\n",
    "# -------------------------------\n",
    "# Listas y estado global\n",
    "# -------------------------------\n",
    "eligible_list = (\n",
    "    coverage_df.filter(pl.col(\"eligible_m5\") == True)\n",
    "               .select(\"symbol\")\n",
    "               .to_series()\n",
    "               .to_list()\n",
    ") if not coverage_df.is_empty() else []\n",
    "\n",
    "gold_list = (\n",
    "    coverage_df.filter(pl.col(\"passed_data_gate\") == True)\n",
    "               .select(\"symbol\")\n",
    "               .to_series()\n",
    "               .to_list()\n",
    ") if not coverage_df.is_empty() else []\n",
    "\n",
    "symbols_state[\"eligible_m5\"] = eligible_list\n",
    "symbols_state[\"passed_data_gate\"] = gold_list\n",
    "\n",
    "# Naming expl√≠cito\n",
    "symbols_state[\"universe_gold\"] = gold_list\n",
    "symbols_state[\"universe_gold_coverage_only\"] = eligible_list\n",
    "\n",
    "# M√©tricas\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"qa_coverage_path\"] = str(out_parquet)\n",
    "\n",
    "# Report stats\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c03\"] = {\n",
    "    \"n_initial\": int(len(base_syms_norm)),\n",
    "    \"n_idx_coverage\": int(n_idx_coverage),\n",
    "    \"n_eligible_m5\": int(len(eligible_list)),\n",
    "    \"n_passed_data_gate\": int(len(gold_list)),\n",
    "}\n",
    "\n",
    "# Guardar tabla en estado si deseas reutilizar\n",
    "GLOBAL_STATE.setdefault(\"tables\", {})\n",
    "GLOBAL_STATE[\"tables\"][\"coverage_5m\"] = coverage_df\n",
    "\n",
    "# -------------------------------\n",
    "# Prints de salida\n",
    "# -------------------------------\n",
    "print(f\"[Celda 03] S√≠mbolos en √≠ndice (tras filtros) = {n_idx_coverage}\")\n",
    "print(f\"[Celda 03] eligible_m5 (cobertura) = {len(eligible_list)}\")\n",
    "print(f\"[Celda 03] passed_data_gate (combinado) = {len(gold_list)}\")\n",
    "print(\n",
    "    f\"[Celda 03] Parametr√≠a efectiva ‚Üí \"\n",
    "    f\"min_bars={min_bars}, \"\n",
    "    f\"min_pct_valid_transitions={min_pct_valid_transitions:.3f}, \"\n",
    "    f\"day_full_threshold_pct={day_full_threshold_pct:.3f}, \"\n",
    "    f\"min_pct_days_full_required={min_pct_days_full_required:.3f}, \"\n",
    "    f\"min_history_years={min_history_years}\"\n",
    ")\n",
    "\n",
    "if excluded_msgs:\n",
    "    print(\"[Celda 03] Ejemplos de exclusi√≥n:\")\n",
    "    for msg in excluded_msgs:\n",
    "        print(f\"   - {msg}\")\n",
    "\n",
    "_print_top_bottom(coverage_df, \"pct_valid_m5_bars\", n=10)\n",
    "_print_top_bottom(coverage_df, \"pct_days_full\", n=10)\n",
    "_print_top_bottom(coverage_df, \"pct_valid_transitions\", n=10)\n",
    "\n",
    "# Diagn√≥stico breve adicional para evitar falsos OK silenciosos\n",
    "try:\n",
    "    print(\"[Celda 03] Distribuci√≥n data_quality_flag_norm:\")\n",
    "    print(\n",
    "        coverage_df.group_by(\"data_quality_flag_norm\")\n",
    "                   .agg(pl.len().alias(\"n\"))\n",
    "                   .sort(\"n\", descending=True)\n",
    "    )\n",
    "\n",
    "    print(\"[Celda 03] Resumen hist_years:\")\n",
    "    print(\n",
    "        coverage_df.select(\n",
    "            pl.col(\"hist_years\").min().alias(\"min\"),\n",
    "            pl.col(\"hist_years\").median().alias(\"median\"),\n",
    "            pl.col(\"hist_years\").max().alias(\"max\"),\n",
    "            pl.col(\"hist_years\").null_count().alias(\"nulls\")\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 03][WARN] Diagn√≥stico resumido no disponible | reason={e}\")\n",
    "\n",
    "print(\n",
    "    f\">>> Celda 03 :: OK \"\n",
    "    f\"(eligible_m5={len(eligible_list)}, passed_data_gate={len(gold_list)})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6065ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 04 :: Lectura de configuraci√≥n (policy + stats + par√°metros clave)\n",
      "[Celda 04] Ruta config del RUN ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json\n",
      "[Celda 04] RESUMEN CONFIG ‚Üí ER.window=34, PD.window=34, hysteresis.pctile_delta=0.02, min_TR_after_cost(global)=0.25, alpha=0.05/0.1/0.2, p0_event=0.2, pvalue_combiner=fisher, decay_penalty=0.0, corr_threshold=0.85, folds.granularity=symbol, folds.min_folds=5, baskets.size_max=12, selection.mode=topN, m=5, n=5, topN=20\n",
      "[Celda 04] CAPS n_eff ‚Üí n_eff_cap_PD=50, n_eff_cap_ER=50, n_eff_cap_p_event=50, n_eff_cap_p_event_bars=200000, min_cap_p_event_bars=10000\n",
      "[Celda 04] structure.enabled=True, min_events_per_symbol=100\n",
      "[Celda 04] UMBRALES TR POR FAMILIA/PRESET ‚Üí TREND(strict/balanced/loose)=0.3/0.25/0.2, RANGE(strict/balanced/loose)=0.3/0.25/0.2\n",
      "[Celda 04] UMBRALES ESTABILIDAD/VIABILIDAD ‚Üí EstabScore_min=None, score_viability_min=None\n",
      "[Celda 04] PESOS DE SCORE (can√≥nico) ‚Üí significance=0.25, opportunity=0.2, stability=0.25, viability=0.2, structure=0.1, sum=0.9999999999999999, schema=new\n",
      "[Celda 04] Versi√≥n l√≥gica de pvalue_engine = bucket_stat_score_v1\n",
      "\n",
      "[Celda 04] Pol√≠tica de uso de p-values (significancia estad√≠stica)\n",
      "  - p < alpha_strict  (0.05)  ‚Üí edge muy claro.\n",
      "  - p < alpha_balanced (0.1) ‚Üí edge razonable.\n",
      "  - p < alpha_loose    (0.2) ‚Üí edge d√©bil pero no ruido puro.\n",
      "  - p ‚â• alpha_loose    (0.2) ‚Üí sin evidencia clara de edge.\n",
      "\n",
      "[Celda 04] Nota: los caps de n_eff limitan el n√∫mero efectivo de tests al combinar p-values y evitan sobreconfianza por particiones redundantes.\n",
      "\n",
      "[Celda 04] Puente Celda 03 ‚Üí universe_gold=84, universe_gold_coverage_only=84\n",
      ">>> Celda 04 :: OK (config verificada, pol√≠tica y pesos de score resueltos)\n"
     ]
    }
   ],
   "source": [
    "# Celda 04 ‚Äî Lectura de configuraci√≥n y resumen de pol√≠tica (ER_FILTER_5M_V1)\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Verifica y centraliza la configuraci√≥n del filtro ER 5M.\n",
    "# Lee el config del RUN, expone secciones clave en GLOBAL_STATE y construye\n",
    "# un resumen num√©rico de la pol√≠tica (alphas, caps de n_eff, pesos de score, etc.).\n",
    "#\n",
    "# Cambios clave (parche):\n",
    "#   1) NO se sobrescribe cfg[\"structure\"] (se evita perder structure.score.*).\n",
    "#   2) Se soportan nombres nuevos y legacy de pesos de score.\n",
    "#   3) Se reportan caps p_event modernos basados en barras.\n",
    "#   4) Se imprime un puente expl√≠cito con m√©tricas de Celda 03 si existen.\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\">>> Celda 04 :: Lectura de configuraci√≥n (policy + stats + par√°metros clave)\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones de GLOBAL_STATE\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] GLOBAL_STATE no existe. Ejecuta primero la Celda 00.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {})\n",
    "if not isinstance(paths, dict):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] GLOBAL_STATE['paths'] no es un dict. Revisa Celda 00.\")\n",
    "\n",
    "config_run_path_str = paths.get(\"config\")\n",
    "if not config_run_path_str:\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] GLOBAL_STATE['paths']['config'] no est√° definido. Revisa Celda 00.\")\n",
    "\n",
    "config_run_path = Path(config_run_path_str).resolve()\n",
    "print(f\"[Celda 04] Ruta config del RUN ‚Üí {config_run_path}\")\n",
    "\n",
    "if not config_run_path.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 04][ERROR] El archivo de configuraci√≥n del RUN no existe: {config_run_path}\\n\"\n",
    "        \"Revisa que la Celda 00 haya copiado el config maestro correctamente.\"\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Utilidades simples\n",
    "# -------------------------------\n",
    "def _is_number(x) -> bool:\n",
    "    return isinstance(x, (int, float))\n",
    "\n",
    "def _deep_get(d: dict, keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def _resolve_score_weights(scores_block: dict):\n",
    "    \"\"\"\n",
    "    Soporta dos esquemas:\n",
    "      - Nuevo: w_significance, w_opportunity, w_stability, w_viability, w_structure\n",
    "      - Legacy: score_stat, score_opportunity, score_stability, score_viability, score_structure\n",
    "\n",
    "    Devuelve un dict can√≥nico:\n",
    "      { \"significance\", \"opportunity\", \"stability\", \"viability\", \"structure\", \"sum\" }\n",
    "    \"\"\"\n",
    "    if not isinstance(scores_block, dict):\n",
    "        scores_block = {}\n",
    "\n",
    "    # Nuevo esquema\n",
    "    w_sig   = scores_block.get(\"w_significance\", None)\n",
    "    w_opp   = scores_block.get(\"w_opportunity\", None)\n",
    "    w_stab  = scores_block.get(\"w_stability\", None)\n",
    "    w_viab  = scores_block.get(\"w_viability\", None)\n",
    "    w_str   = scores_block.get(\"w_structure\", None)\n",
    "\n",
    "    # Legacy esquema\n",
    "    lw_stat = scores_block.get(\"score_stat\", None)\n",
    "    lw_opp  = scores_block.get(\"score_opportunity\", None)\n",
    "    lw_stab = scores_block.get(\"score_stability\", None)\n",
    "    lw_viab = scores_block.get(\"score_viability\", None)\n",
    "    lw_str  = scores_block.get(\"score_structure\", None)\n",
    "\n",
    "    # Si hay valores del nuevo esquema, priorizarlos\n",
    "    use_new = any(_is_number(x) for x in (w_sig, w_opp, w_stab, w_viab, w_str))\n",
    "\n",
    "    if use_new:\n",
    "        sig  = float(w_sig)  if _is_number(w_sig)  else None\n",
    "        opp  = float(w_opp)  if _is_number(w_opp)  else None\n",
    "        stab = float(w_stab) if _is_number(w_stab) else None\n",
    "        viab = float(w_viab) if _is_number(w_viab) else None\n",
    "        stru = float(w_str)  if _is_number(w_str)  else None\n",
    "    else:\n",
    "        # Legacy: no existe \"significance\" expl√≠cito; se asume que score_stat ~ significance\n",
    "        sig  = float(lw_stat) if _is_number(lw_stat) else None\n",
    "        opp  = float(lw_opp)  if _is_number(lw_opp)  else None\n",
    "        stab = float(lw_stab) if _is_number(lw_stab) else None\n",
    "        viab = float(lw_viab) if _is_number(lw_viab) else None\n",
    "        stru = float(lw_str)  if _is_number(lw_str)  else None\n",
    "\n",
    "    weights = [x for x in (sig, opp, stab, viab, stru) if _is_number(x)]\n",
    "    s = float(sum(weights)) if weights else None\n",
    "\n",
    "    return {\n",
    "        \"significance\": sig,\n",
    "        \"opportunity\": opp,\n",
    "        \"stability\": stab,\n",
    "        \"viability\": viab,\n",
    "        \"structure\": stru,\n",
    "        \"sum\": s,\n",
    "        \"using_new_schema\": use_new\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Lectura de config (preferir GLOBAL_STATE, fallback a disco)\n",
    "# -------------------------------\n",
    "cfg = GLOBAL_STATE.get(\"config\", {})\n",
    "\n",
    "if not isinstance(cfg, dict) or not cfg:\n",
    "    print(\"[Celda 04][WARN] GLOBAL_STATE['config'] vac√≠o o inv√°lido. Leyendo config desde disco‚Ä¶\")\n",
    "    txt = config_run_path.read_text(encoding=\"utf-8\")\n",
    "    try:\n",
    "        cfg = json.loads(txt)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[Celda 04][ERROR] No se pudo parsear config.json del RUN | reason={e}\")\n",
    "\n",
    "if not isinstance(cfg, dict):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] Config cargado no es un dict. Revisa el JSON maestro.\")\n",
    "\n",
    "# Reflejar lo que usamos en GLOBAL_STATE por si se actualiz√≥ desde disco\n",
    "GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "# -------------------------------\n",
    "# Extraer bloques principales\n",
    "# -------------------------------\n",
    "policy     = cfg.get(\"policy\", {})       # min_TR_after_cost, min_tr_after_cost_trend_*, ...\n",
    "stats      = cfg.get(\"stats\", {})        # alpha_*, p0_event, pvalue_combiner, n_eff_cap_*\n",
    "er_block   = cfg.get(\"ER\", {})\n",
    "pd_block   = cfg.get(\"PD\", {})\n",
    "hyst       = cfg.get(\"hysteresis\", {})\n",
    "decay      = cfg.get(\"decay\", {})\n",
    "corr       = cfg.get(\"corr\", {})\n",
    "folds      = cfg.get(\"folds\", {})\n",
    "baskets    = cfg.get(\"baskets\", {})\n",
    "selection  = cfg.get(\"selection\", {})\n",
    "session    = cfg.get(\"session\", {})\n",
    "meta       = cfg.get(\"meta\", {})\n",
    "structure  = cfg.get(\"structure\", {}) or {}\n",
    "stability  = cfg.get(\"stability\", {}) or {}\n",
    "viability  = cfg.get(\"viability\", {}) or {}\n",
    "scores     = cfg.get(\"scores\", {}) or {}\n",
    "\n",
    "STRUCT_ENABLED        = bool(structure.get(\"enabled\", False))\n",
    "STRUCT_MIN_EVENTS_SYM = int(structure.get(\"min_events_per_symbol\", 100))\n",
    "\n",
    "# Exponer bloques en GLOBAL_STATE (sin mutar cfg)\n",
    "GLOBAL_STATE[\"config_sections\"] = {\n",
    "    \"policy\": policy,\n",
    "    \"stats\": stats,\n",
    "    \"ER\": er_block,\n",
    "    \"PD\": pd_block,\n",
    "    \"hysteresis\": hyst,\n",
    "    \"decay\": decay,\n",
    "    \"corr\": corr,\n",
    "    \"folds\": folds,\n",
    "    \"baskets\": baskets,\n",
    "    \"selection\": selection,\n",
    "    \"session\": session,\n",
    "    \"meta\": meta,\n",
    "    \"structure\": structure,\n",
    "    \"stability\": stability,\n",
    "    \"viability\": viability,\n",
    "    \"scores\": scores,\n",
    "}\n",
    "\n",
    "# Bloque resolved separado (NO sobrescribe cfg[\"structure\"])\n",
    "GLOBAL_STATE[\"structure_resolved\"] = {\n",
    "    \"enabled\": STRUCT_ENABLED,\n",
    "    \"min_events_per_symbol\": STRUCT_MIN_EVENTS_SYM,\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones m√≠nimas (hard checks)\n",
    "# -------------------------------\n",
    "er_w = _deep_get(cfg, [\"ER\", \"window\"])\n",
    "pd_w = _deep_get(cfg, [\"PD\", \"window\"])\n",
    "if not isinstance(er_w, int) or er_w <= 1:\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] ER.window inv√°lido (debe ser entero > 1). Corrige er_filter_5m.json.\")\n",
    "if not isinstance(pd_w, int) or pd_w <= 1:\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] PD.window inv√°lido (debe ser entero > 1). Corrige er_filter_5m.json.\")\n",
    "\n",
    "pct_delta = _deep_get(cfg, [\"hysteresis\", \"pctile_delta\"])\n",
    "if not _is_number(pct_delta) or float(pct_delta) <= 0:\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] hysteresis.pctile_delta inv√°lido. Corrige er_filter_5m.json.\")\n",
    "\n",
    "alpha_s = _deep_get(cfg, [\"stats\", \"alpha_strict\"])\n",
    "alpha_b = _deep_get(cfg, [\"stats\", \"alpha_balanced\"])\n",
    "alpha_l = _deep_get(cfg, [\"stats\", \"alpha_loose\"])\n",
    "if not all(_is_number(x) for x in (alpha_s, alpha_b, alpha_l)):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] stats.alpha_* inv√°lidos. Corrige er_filter_5m.json.\")\n",
    "if not (0.0 < float(alpha_s) < float(alpha_b) < float(alpha_l) < 1.0):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] stats.alpha_* fuera de rango u orden incorrecto. Corrige er_filter_5m.json.\")\n",
    "\n",
    "corr_thr = _deep_get(cfg, [\"corr\", \"threshold\"])\n",
    "if not _is_number(corr_thr) or not (0.0 < float(corr_thr) < 1.0):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] corr.threshold inv√°lido (debe estar en (0,1)). Corrige er_filter_5m.json.\")\n",
    "\n",
    "basket_sz = _deep_get(cfg, [\"baskets\", \"size_max\"])\n",
    "if not isinstance(basket_sz, int) or basket_sz <= 0:\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] baskets.size_max inv√°lido (debe ser entero > 0). Corrige er_filter_5m.json.\")\n",
    "\n",
    "# Validaci√≥n suave de policy.min_TR_after_cost (global)\n",
    "min_tr_global = policy.get(\"min_TR_after_cost\", None)\n",
    "if min_tr_global is not None and not _is_number(min_tr_global):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] policy.min_TR_after_cost no num√©rico. Corrige er_filter_5m.json.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Caps de n_eff (compatibilidad)\n",
    "# -------------------------------\n",
    "n_eff_cap_PD = stats.get(\"n_eff_cap_PD\", stats.get(\"n_eff_cap_pd\", None))\n",
    "n_eff_cap_ER = stats.get(\"n_eff_cap_ER\", stats.get(\"n_eff_cap_er\", None))\n",
    "\n",
    "# p_event moderno por barras\n",
    "n_eff_cap_p_event = stats.get(\"n_eff_cap_p_event\", None)\n",
    "n_eff_cap_p_event_bars = stats.get(\"n_eff_cap_p_event_bars\", None)\n",
    "min_cap_p_event_bars = stats.get(\"min_cap_p_event_bars\", None)\n",
    "\n",
    "for name, val in [\n",
    "    (\"stats.n_eff_cap_PD\", n_eff_cap_PD),\n",
    "    (\"stats.n_eff_cap_ER\", n_eff_cap_ER),\n",
    "    (\"stats.n_eff_cap_p_event\", n_eff_cap_p_event),\n",
    "    (\"stats.n_eff_cap_p_event_bars\", n_eff_cap_p_event_bars),\n",
    "    (\"stats.min_cap_p_event_bars\", min_cap_p_event_bars),\n",
    "]:\n",
    "    if val is not None and not _is_number(val):\n",
    "        raise RuntimeError(f\"[Celda 04][ERROR] {name} no num√©rico. Corrige er_filter_5m.json.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Umbrales de EstabScore y score_viability (si existen)\n",
    "# -------------------------------\n",
    "estab_min = (\n",
    "    stability.get(\"EstabScore_min\")\n",
    "    or stability.get(\"estabscore_min\")\n",
    "    or selection.get(\"EstabScore_min\")\n",
    "    or selection.get(\"estabscore_min\")\n",
    ")\n",
    "\n",
    "score_viab_min = (\n",
    "    viability.get(\"score_viability_min\")\n",
    "    or selection.get(\"score_viability_min\")\n",
    ")\n",
    "\n",
    "if estab_min is not None and not _is_number(estab_min):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] Umbral de EstabScore no num√©rico. Corrige er_filter_5m.json.\")\n",
    "if score_viab_min is not None and not _is_number(score_viab_min):\n",
    "    raise RuntimeError(\"[Celda 04][ERROR] Umbral de score_viability no num√©rico. Corrige er_filter_5m.json.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Pesos de score (nuevo + legacy)\n",
    "# -------------------------------\n",
    "resolved_weights = _resolve_score_weights(scores)\n",
    "\n",
    "# Warning suave si la suma parece rara\n",
    "if resolved_weights[\"sum\"] is not None:\n",
    "    s = resolved_weights[\"sum\"]\n",
    "    if not (0.90 <= s <= 1.10):\n",
    "        print(f\"[Celda 04][WARN] Suma de pesos de score fuera de rango esperado (~1.0): sum={s}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Resolver y guardar pol√≠tica num√©rica principal\n",
    "# -------------------------------\n",
    "GLOBAL_STATE[\"policy_resolved\"] = {\n",
    "    \"alpha_strict\": float(alpha_s),\n",
    "    \"alpha_balanced\": float(alpha_b),\n",
    "    \"alpha_loose\": float(alpha_l),\n",
    "\n",
    "    \"n_eff_cap_PD\": (float(n_eff_cap_PD) if _is_number(n_eff_cap_PD) else None),\n",
    "    \"n_eff_cap_ER\": (float(n_eff_cap_ER) if _is_number(n_eff_cap_ER) else None),\n",
    "\n",
    "    # p_event: mantener ambos formatos\n",
    "    \"n_eff_cap_p_event\": (float(n_eff_cap_p_event) if _is_number(n_eff_cap_p_event) else None),\n",
    "    \"n_eff_cap_p_event_bars\": (float(n_eff_cap_p_event_bars) if _is_number(n_eff_cap_p_event_bars) else None),\n",
    "    \"min_cap_p_event_bars\": (float(min_cap_p_event_bars) if _is_number(min_cap_p_event_bars) else None),\n",
    "\n",
    "    \"EstabScore_min\": (float(estab_min) if _is_number(estab_min) else None),\n",
    "    \"score_viability_min\": (float(score_viab_min) if _is_number(score_viab_min) else None),\n",
    "\n",
    "    \"score_weights\": {\n",
    "        \"significance\": resolved_weights[\"significance\"],\n",
    "        \"opportunity\": resolved_weights[\"opportunity\"],\n",
    "        \"stability\": resolved_weights[\"stability\"],\n",
    "        \"viability\": resolved_weights[\"viability\"],\n",
    "        \"structure\": resolved_weights[\"structure\"],\n",
    "        \"sum\": resolved_weights[\"sum\"],\n",
    "        \"using_new_schema\": bool(resolved_weights[\"using_new_schema\"]),\n",
    "    },\n",
    "\n",
    "    \"structure\": {\n",
    "        \"enabled\": STRUCT_ENABLED,\n",
    "        \"min_events_per_symbol\": STRUCT_MIN_EVENTS_SYM,\n",
    "    }\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Impresi√≥n de RESUMEN de pol√≠tica\n",
    "# -------------------------------\n",
    "p0_event      = stats.get(\"p0_event\", None)\n",
    "combiner      = stats.get(\"pvalue_combiner\", None)\n",
    "decay_penalty = decay.get(\"penalty_lambda\", None)\n",
    "folds_gran    = folds.get(\"granularity\", None)\n",
    "folds_min     = folds.get(\"min_folds\", None)\n",
    "\n",
    "sel_mode = selection.get(\"mode\", None)\n",
    "sel_m    = selection.get(\"m\", None)\n",
    "sel_n    = selection.get(\"n\", None)\n",
    "sel_topN = selection.get(\"topN\", None)\n",
    "\n",
    "min_tr_ts  = policy.get(\"min_tr_after_cost_trend_strict\",   None)\n",
    "min_tr_tb  = policy.get(\"min_tr_after_cost_trend_balanced\", None)\n",
    "min_tr_tl  = policy.get(\"min_tr_after_cost_trend_loose\",    None)\n",
    "min_tr_rs  = policy.get(\"min_tr_after_cost_range_strict\",   None)\n",
    "min_tr_rb  = policy.get(\"min_tr_after_cost_range_balanced\", None)\n",
    "min_tr_rl  = policy.get(\"min_tr_after_cost_range_loose\",    None)\n",
    "\n",
    "print(\n",
    "    \"[Celda 04] RESUMEN CONFIG ‚Üí \"\n",
    "    f\"ER.window={er_w}, \"\n",
    "    f\"PD.window={pd_w}, \"\n",
    "    f\"hysteresis.pctile_delta={pct_delta}, \"\n",
    "    f\"min_TR_after_cost(global)={min_tr_global}, \"\n",
    "    f\"alpha={alpha_s}/{alpha_b}/{alpha_l}, \"\n",
    "    f\"p0_event={p0_event}, \"\n",
    "    f\"pvalue_combiner={combiner}, \"\n",
    "    f\"decay_penalty={decay_penalty}, \"\n",
    "    f\"corr_threshold={corr_thr}, \"\n",
    "    f\"folds.granularity={folds_gran}, folds.min_folds={folds_min}, \"\n",
    "    f\"baskets.size_max={basket_sz}, \"\n",
    "    f\"selection.mode={sel_mode}, m={sel_m}, n={sel_n}, topN={sel_topN}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[Celda 04] CAPS n_eff ‚Üí \"\n",
    "    f\"n_eff_cap_PD={n_eff_cap_PD}, \"\n",
    "    f\"n_eff_cap_ER={n_eff_cap_ER}, \"\n",
    "    f\"n_eff_cap_p_event={n_eff_cap_p_event}, \"\n",
    "    f\"n_eff_cap_p_event_bars={n_eff_cap_p_event_bars}, \"\n",
    "    f\"min_cap_p_event_bars={min_cap_p_event_bars}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[Celda 04] structure.enabled={STRUCT_ENABLED}, \"\n",
    "    f\"min_events_per_symbol={STRUCT_MIN_EVENTS_SYM}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[Celda 04] UMBRALES TR POR FAMILIA/PRESET ‚Üí \"\n",
    "    f\"TREND(strict/balanced/loose)={min_tr_ts}/{min_tr_tb}/{min_tr_tl}, \"\n",
    "    f\"RANGE(strict/balanced/loose)={min_tr_rs}/{min_tr_rb}/{min_tr_rl}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[Celda 04] UMBRALES ESTABILIDAD/VIABILIDAD ‚Üí \"\n",
    "    f\"EstabScore_min={estab_min}, \"\n",
    "    f\"score_viability_min={score_viab_min}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"[Celda 04] PESOS DE SCORE (can√≥nico) ‚Üí \"\n",
    "    f\"significance={resolved_weights['significance']}, \"\n",
    "    f\"opportunity={resolved_weights['opportunity']}, \"\n",
    "    f\"stability={resolved_weights['stability']}, \"\n",
    "    f\"viability={resolved_weights['viability']}, \"\n",
    "    f\"structure={resolved_weights['structure']}, \"\n",
    "    f\"sum={resolved_weights['sum']}, \"\n",
    "    f\"schema={'new' if resolved_weights['using_new_schema'] else 'legacy'}\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Versi√≥n l√≥gica del motor de p-values / stat engine\n",
    "# -------------------------------\n",
    "pvalue_engine = (\n",
    "    stats.get(\"pvalue_engine\")\n",
    "    or meta.get(\"pvalue_engine\")\n",
    "    or combiner\n",
    "    or \"bucket_stat_score_v1\"\n",
    ")\n",
    "print(f\"[Celda 04] Versi√≥n l√≥gica de pvalue_engine = {pvalue_engine}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Bloque explicativo de pol√≠tica de p-values\n",
    "# -------------------------------\n",
    "print(\"\\n[Celda 04] Pol√≠tica de uso de p-values (significancia estad√≠stica)\")\n",
    "print(f\"  - p < alpha_strict  ({alpha_s})  ‚Üí edge muy claro.\")\n",
    "print(f\"  - p < alpha_balanced ({alpha_b}) ‚Üí edge razonable.\")\n",
    "print(f\"  - p < alpha_loose    ({alpha_l}) ‚Üí edge d√©bil pero no ruido puro.\")\n",
    "print(f\"  - p ‚â• alpha_loose    ({alpha_l}) ‚Üí sin evidencia clara de edge.\")\n",
    "\n",
    "print(\n",
    "    \"\\n[Celda 04] Nota: los caps de n_eff limitan el n√∫mero efectivo de tests \"\n",
    "    \"al combinar p-values y evitan sobreconfianza por particiones redundantes.\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Puente expl√≠cito con Celda 03 (si existe)\n",
    "# -------------------------------\n",
    "symbols_state = GLOBAL_STATE.get(\"symbols\", {})\n",
    "gold = symbols_state.get(\"universe_gold\", symbols_state.get(\"passed_data_gate\", [])) or []\n",
    "gold_cov = symbols_state.get(\"universe_gold_coverage_only\", symbols_state.get(\"eligible_m5\", [])) or []\n",
    "\n",
    "try:\n",
    "    n_gold = len(gold) if isinstance(gold, list) else 0\n",
    "    n_gold_cov = len(gold_cov) if isinstance(gold_cov, list) else 0\n",
    "    if n_gold or n_gold_cov:\n",
    "        print(\n",
    "            f\"\\n[Celda 04] Puente Celda 03 ‚Üí \"\n",
    "            f\"universe_gold={n_gold}, \"\n",
    "            f\"universe_gold_coverage_only={n_gold_cov}\"\n",
    "        )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------\n",
    "# run_metadata: resumen de pol√≠tica para el RUN\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"run_metadata\", {})\n",
    "GLOBAL_STATE[\"run_metadata\"][\"policy_summary\"] = {\n",
    "    \"timestamp_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"alpha_strict\": float(alpha_s),\n",
    "    \"alpha_balanced\": float(alpha_b),\n",
    "    \"alpha_loose\": float(alpha_l),\n",
    "    \"n_eff_caps\": {\n",
    "        \"PD\": (float(n_eff_cap_PD) if _is_number(n_eff_cap_PD) else None),\n",
    "        \"ER\": (float(n_eff_cap_ER) if _is_number(n_eff_cap_ER) else None),\n",
    "        \"p_event\": (float(n_eff_cap_p_event) if _is_number(n_eff_cap_p_event) else None),\n",
    "        \"p_event_bars\": (float(n_eff_cap_p_event_bars) if _is_number(n_eff_cap_p_event_bars) else None),\n",
    "        \"min_p_event_bars\": (float(min_cap_p_event_bars) if _is_number(min_cap_p_event_bars) else None),\n",
    "    },\n",
    "    \"EstabScore_min\": (float(estab_min) if _is_number(estab_min) else None),\n",
    "    \"score_viability_min\": (float(score_viab_min) if _is_number(score_viab_min) else None),\n",
    "    \"score_weights\": GLOBAL_STATE[\"policy_resolved\"][\"score_weights\"],\n",
    "    \"selection\": {\n",
    "        \"mode\": sel_mode,\n",
    "        \"m\": sel_m,\n",
    "        \"n\": sel_n,\n",
    "        \"topN\": sel_topN,\n",
    "    },\n",
    "    \"baskets_size_max\": basket_sz,\n",
    "    \"pvalue_combiner\": combiner,\n",
    "    \"pvalue_engine\": pvalue_engine,\n",
    "    \"structure_resolved\": GLOBAL_STATE.get(\"structure_resolved\", {}),\n",
    "    \"meta_version\": meta.get(\"version\", None),\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Report stats Celda 04\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c04\"] = {\n",
    "    \"ER_window\": int(er_w),\n",
    "    \"PD_window\": int(pd_w),\n",
    "    \"alpha_strict\": float(alpha_s),\n",
    "    \"alpha_balanced\": float(alpha_b),\n",
    "    \"alpha_loose\": float(alpha_l),\n",
    "    \"corr_threshold\": float(corr_thr),\n",
    "    \"baskets_size_max\": int(basket_sz),\n",
    "    \"structure_enabled\": bool(STRUCT_ENABLED),\n",
    "    \"min_events_per_symbol\": int(STRUCT_MIN_EVENTS_SYM),\n",
    "    \"score_weights_sum\": (float(resolved_weights[\"sum\"]) if _is_number(resolved_weights[\"sum\"]) else None),\n",
    "    \"n_universe_gold\": (len(gold) if isinstance(gold, list) else None),\n",
    "    \"n_universe_gold_coverage_only\": (len(gold_cov) if isinstance(gold_cov, list) else None),\n",
    "    \"meta_version\": meta.get(\"version\", None),\n",
    "}\n",
    "\n",
    "print(\">>> Celda 04 :: OK (config verificada, pol√≠tica y pesos de score resueltos)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f982a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 05 :: ER & PD (M5)\n",
      "[Celda 05] RUN_ID           = 20251218_190810\n",
      "[Celda 05] Universo estad√≠stico para ER/PD/vol = 'universe_gold'\n",
      "[Celda 05] n_symbols = 84 | muestra = ['AAPL', 'AAVUSD', 'ADAUSD', 'ALGUSD', 'ALVG', 'AMZN', 'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD']\n",
      "[Celda 05] M5_SOURCE_TYPE   = partitioned_m5_clean\n",
      "[Celda 05] M5_ROOT_DIR      = C:\\Quant\\MT5_Data_Extraction\\data\\historical_data\\m5_clean\n",
      "[Celda 05] PAD_DAY_INDEX    = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\day_index_m5.parquet\n",
      "[Celda 05] policy.IS_OOS_split_q = 0.7\n",
      "[Celda 05] NO se usar√° bulk_data/rates_5m (solo PAD_DAY_INDEX / m5_clean/m5_raw).\n",
      "[Celda 05] progreso: 5/84 (procesados=5, omitidos=0)\n",
      "[Celda 05] progreso: 10/84 (procesados=10, omitidos=0)\n",
      "[Celda 05] progreso: 15/84 (procesados=15, omitidos=0)\n",
      "[Celda 05] progreso: 20/84 (procesados=20, omitidos=0)\n",
      "[Celda 05] progreso: 25/84 (procesados=25, omitidos=0)\n",
      "[Celda 05] progreso: 30/84 (procesados=30, omitidos=0)\n",
      "[Celda 05] progreso: 35/84 (procesados=35, omitidos=0)\n",
      "[Celda 05] progreso: 40/84 (procesados=40, omitidos=0)\n",
      "[Celda 05] progreso: 45/84 (procesados=45, omitidos=0)\n",
      "[Celda 05] progreso: 50/84 (procesados=50, omitidos=0)\n",
      "[Celda 05] progreso: 55/84 (procesados=55, omitidos=0)\n",
      "[Celda 05] progreso: 60/84 (procesados=60, omitidos=0)\n",
      "[Celda 05] progreso: 65/84 (procesados=65, omitidos=0)\n",
      "[Celda 05] progreso: 70/84 (procesados=70, omitidos=0)\n",
      "[Celda 05] progreso: 75/84 (procesados=75, omitidos=0)\n",
      "[Celda 05] progreso: 80/84 (procesados=80, omitidos=0)\n",
      "[Celda 05] progreso: 84/84 (procesados=84, omitidos=0)\n",
      "[Celda 05] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\er_series.parquet (OK, rows=24522466, cols=5)\n",
      "[Celda 05] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\pd_series.parquet (OK, rows=24522466, cols=5)\n",
      "[Celda 05] n_symbols (universo stats) = 84\n",
      "[Celda 05] ER: symbols=84, date_range=[2021-11-19 00:00:00+00:00 ‚Üí 2025-12-02 23:50:00+00:00] (procesados=84, omitidos=0)\n",
      "[Celda 05] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_volatility_summary.parquet (OK, rows=84, cols=5)\n",
      "[Celda 05] Rango vol_long_term (s√≠mbolos): [0.000038 ‚Üí 0.004807]\n",
      "[Celda 05] Top-3 m√°s vol√°tiles (vol_long_term):\n",
      "  symbol=GALUSD vol_long_term=0.004807 vol_p90_abs_ret=0.005274 flag=ALTA\n",
      "  symbol=IMXUSD vol_long_term=0.004304 vol_p90_abs_ret=0.005271 flag=ALTA\n",
      "  symbol=TSLA vol_long_term=0.004251 vol_p90_abs_ret=0.005106 flag=ALTA\n",
      "[Celda 05] Top-3 menos vol√°tiles (vol_long_term):\n",
      "  symbol=USDHKD vol_long_term=0.000038 vol_p90_abs_ret=0.000049 flag=BAJA\n",
      "  symbol=AUDNZD vol_long_term=0.000225 vol_p90_abs_ret=0.000318 flag=BAJA\n",
      "  symbol=EURGBP vol_long_term=0.000239 vol_p90_abs_ret=0.000339 flag=BAJA\n",
      "[Celda 05] Rango ER_P40/ER_P60 (FULL, legacy): ER_P40‚àà[0.1001, 0.1379], ER_P60‚àà[0.1624, 0.2183]\n",
      "[Celda 05] Rango PD_P40/PD_P60 (FULL, legacy): PD_P40‚àà[0.7817, 0.8376], PD_P60‚àà[0.8621, 0.8999]\n",
      "[Celda 05] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\er_pd_percentiles_summary.json (OK, bytes=56896)\n",
      "[Celda 05] Top 10 s√≠mbolos por ER_P60 (FULL legacy):\n",
      "  rank=1 symbol=TSLA ER_P60=0.2183\n",
      "  rank=2 symbol=GOOG ER_P60=0.2126\n",
      "  rank=3 symbol=AMZN ER_P60=0.2126\n",
      "  rank=4 symbol=AAPL ER_P60=0.2105\n",
      "  rank=5 symbol=USDILS ER_P60=0.2105\n",
      "  rank=6 symbol=NVDA ER_P60=0.2105\n",
      "  rank=7 symbol=MSFT ER_P60=0.2092\n",
      "  rank=8 symbol=LVMH ER_P60=0.2048\n",
      "  rank=9 symbol=WMT ER_P60=0.2000\n",
      "  rank=10 symbol=ALVG ER_P60=0.2000\n",
      "[Celda 05] Top 10 s√≠mbolos por PD_P60 (FULL legacy):\n",
      "  rank=1 symbol=AUDNZD PD_P60=0.8999\n",
      "  rank=2 symbol=EURNOK PD_P60=0.8997\n",
      "  rank=3 symbol=GBPCHF PD_P60=0.8980\n",
      "  rank=4 symbol=EURCHF PD_P60=0.8978\n",
      "  rank=5 symbol=EURGBP PD_P60=0.8974\n",
      "  rank=6 symbol=GBPCAD PD_P60=0.8961\n",
      "  rank=7 symbol=CADCHF PD_P60=0.8947\n",
      "  rank=8 symbol=GBPNZD PD_P60=0.8945\n",
      "  rank=9 symbol=GBPAUD PD_P60=0.8945\n",
      "  rank=10 symbol=ETCUSD PD_P60=0.8941\n",
      "[Celda 05] time epoch unit = s\n",
      ">>> Celda 05 :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 05 ‚Äî C√°lculo de ER (Kaufman), PD (Price Density) y volatilidad base SOLO M5\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rol de esta celda:\n",
    "#  - Tomar las series M5 (solo close) de los s√≠mbolos del universo estad√≠stico,\n",
    "#    definido de forma ROBUSTA a partir de GLOBAL_STATE[\"symbols\"]:\n",
    "#       * Prioridad: config.UNIVERSE_KEY_DEFAULT (si existe y no est√° vac√≠o)\n",
    "#       * Si no, se prueban: universe_gold, passed_data_gate, universe_effective,\n",
    "#         universe_gold_coverage_only, eligible_m5, rates_m5.\n",
    "#  - Normalizar a:\n",
    "#       * time_utc  -> datetime con tz=UTC\n",
    "#       * close     -> float64\n",
    "#  - Calcular:\n",
    "#       * ER_t (Kaufman) con ventana ER.window\n",
    "#       * PD_t = 1.0 - ER_t  (Price Density operativa)\n",
    "#  - Calcular, a NIVEL S√çMBOLO, una volatilidad base con retornos logar√≠tmicos 5M:\n",
    "#       * vol_long_term      = std(ret_5m)\n",
    "#       * vol_p90_abs_ret    = percentil 90 de |ret_5m|\n",
    "#       * vol_regime_flag    = BAJA / MEDIA / ALTA (seg√∫n terciles de vol_long_term)\n",
    "#  - Guardar tablas:\n",
    "#       * er_series.parquet:                     [symbol, time_utc, ER, ER_kaufman, window]\n",
    "#       * pd_series.parquet:                     [symbol, time_utc, PD, PD_kaufman, window]\n",
    "#       * regime_volatility_summary.parquet:     [symbol, vol_long_term, vol_p90_abs_ret, n_ret, vol_regime_flag]\n",
    "#       * er_pd_percentiles_summary.json         (percentiles P40/P60 FULL y SOLO-IS por s√≠mbolo)\n",
    "#\n",
    "# Anti-leak (CR√çTICO):\n",
    "#  - Los percentiles usados por el detector de r√©gimen (Celda 06) se calculan SOLO con IS:\n",
    "#       * IS/OOS split por s√≠mbolo con policy.IS_OOS_split_q\n",
    "#       * ts_cut_int = quantile(ts_int, q) con interpolation=\"nearest\"\n",
    "#       * segment = IS si ts_int <= ts_cut_int, si no OOS\n",
    "#\n",
    "# Precondiciones:\n",
    "#  - Celdas 00‚Äì04 ejecutadas y GLOBAL_STATE poblado.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Se requiere 'polars' para la Celda 05 | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones de estado previo\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas previas (00‚Äì04).\")\n",
    "\n",
    "required_keys = (\"project_root\", \"run_id\", \"paths\", \"symbols\", \"config\", \"inputs\")\n",
    "missing = [k for k in required_keys if k not in GLOBAL_STATE]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"GLOBAL_STATE incompleto; faltan claves: {missing}\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "RUN_ID = GLOBAL_STATE[\"run_id\"]\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "inputs = GLOBAL_STATE[\"inputs\"]\n",
    "symbols_state = GLOBAL_STATE[\"symbols\"]\n",
    "config = GLOBAL_STATE[\"config\"]\n",
    "\n",
    "if \"metrics\" not in paths or \"diagnostics\" not in paths:\n",
    "    raise RuntimeError(\"Faltan rutas en GLOBAL_STATE['paths']: se requieren 'metrics' y 'diagnostics'.\")\n",
    "\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar config.json (para policy.IS_OOS_split_q consistente con Celda 08IS)\n",
    "# -------------------------------\n",
    "config_path = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "def _safe_load_json(p: Path) -> dict:\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "CONFIG_FILE = _safe_load_json(config_path)\n",
    "\n",
    "# Unificamos config en GLOBAL_STATE (sin romper compatibilidad)\n",
    "if isinstance(CONFIG_FILE, dict) and CONFIG_FILE:\n",
    "    GLOBAL_STATE[\"config\"] = CONFIG_FILE\n",
    "    config = GLOBAL_STATE[\"config\"]\n",
    "\n",
    "policy_cfg = (config.get(\"policy\", {}) or {}) if isinstance(config, dict) else {}\n",
    "def _cfg_float(section: dict, key: str, default: float) -> float:\n",
    "    try:\n",
    "        v = section.get(key, default)\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "# Mismo default que Celda 08IS\n",
    "IS_OOS_split_q = _cfg_float(policy_cfg, \"IS_OOS_split_q\", 0.70)\n",
    "if not (0.50 <= IS_OOS_split_q <= 0.95):\n",
    "    print(f\"[Celda 05][WARN] policy.IS_OOS_split_q fuera de rango [{IS_OOS_split_q}]. Se fuerza a 0.70.\")\n",
    "    IS_OOS_split_q = 0.70\n",
    "\n",
    "# -------------------------------\n",
    "# Selector de universo estad√≠stico (robusto)\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"config\", {})\n",
    "symbols_dict = GLOBAL_STATE.get(\"symbols\", {})\n",
    "\n",
    "preferred_keys: List[str] = []\n",
    "cfg_universe = GLOBAL_STATE[\"config\"].get(\"UNIVERSE_KEY_DEFAULT\")\n",
    "if cfg_universe:\n",
    "    preferred_keys.append(str(cfg_universe))\n",
    "\n",
    "preferred_keys.extend([\n",
    "    \"universe_gold\",\n",
    "    \"passed_data_gate\",\n",
    "    \"universe_effective\",\n",
    "    \"universe_gold_coverage_only\",\n",
    "    \"eligible_m5\",\n",
    "    \"rates_m5\",\n",
    "])\n",
    "\n",
    "seen = set()\n",
    "preferred_keys = [k for k in preferred_keys if not (k in seen or seen.add(k))]\n",
    "\n",
    "UNIVERSE_KEY_DEFAULT: Optional[str] = None\n",
    "syms_for_stats: List[str] = []\n",
    "\n",
    "for k in preferred_keys:\n",
    "    v = symbols_dict.get(k)\n",
    "    if isinstance(v, list) and len(v) > 0:\n",
    "        UNIVERSE_KEY_DEFAULT = k\n",
    "        syms_for_stats = v\n",
    "        break\n",
    "\n",
    "if not syms_for_stats or UNIVERSE_KEY_DEFAULT is None:\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 05] No se pudo encontrar un universo estad√≠stico no vac√≠o en GLOBAL_STATE['symbols']. \"\n",
    "        f\"Prob√© claves={preferred_keys}. Revisa Celda 03 / construcci√≥n de universe_effective / passed_data_gate.\"\n",
    "    )\n",
    "\n",
    "GLOBAL_STATE[\"config\"][\"UNIVERSE_KEY_DEFAULT\"] = UNIVERSE_KEY_DEFAULT\n",
    "\n",
    "# -------------------------------\n",
    "# Config r√©gimen (ventanas ER / PD)\n",
    "# -------------------------------\n",
    "ER_window = int(config.get(\"ER\", {}).get(\"window\", 34))\n",
    "PD_window = int(config.get(\"PD\", {}).get(\"window\", ER_window))\n",
    "\n",
    "if ER_window < 2:\n",
    "    raise RuntimeError(\"ER.window debe ser >= 2\")\n",
    "\n",
    "# PD es derivado directo de ER\n",
    "if PD_window != ER_window:\n",
    "    print(\n",
    "        f\"[Celda 05][WARN] PD.window ({PD_window}) != ER.window ({ER_window}). \"\n",
    "        \"PD se definir√° como 1-ER usando ER.window.\"\n",
    "    )\n",
    "    PD_window = ER_window\n",
    "\n",
    "# Handshake: tipo y ra√≠z de M5\n",
    "M5_SOURCE_TYPE = inputs.get(\"M5_SOURCE_TYPE\", \"unknown\")\n",
    "M5_ROOT_DIR = Path(inputs.get(\"M5_ROOT_DIR\", \"\")).resolve() if inputs.get(\"M5_ROOT_DIR\") else Path(\"\")\n",
    "RATES_5M_DIR = (PROJECT_ROOT / \"bulk_data\" / \"rates_5m\").resolve()\n",
    "USE_RATES_5M = (M5_SOURCE_TYPE == \"flat_rates_5m\")\n",
    "PAD_DAY_INDEX = Path(inputs.get(\"PAD_DAY_INDEX\", \"\")).resolve()\n",
    "\n",
    "print(\">>> Celda 05 :: ER & PD (M5)\")\n",
    "print(f\"[Celda 05] RUN_ID           = {RUN_ID}\")\n",
    "print(f\"[Celda 05] Universo estad√≠stico para ER/PD/vol = '{UNIVERSE_KEY_DEFAULT}'\")\n",
    "print(f\"[Celda 05] n_symbols = {len(syms_for_stats)} | muestra = {syms_for_stats[:10]}\")\n",
    "print(f\"[Celda 05] M5_SOURCE_TYPE   = {M5_SOURCE_TYPE}\")\n",
    "print(f\"[Celda 05] M5_ROOT_DIR      = {str(M5_ROOT_DIR)}\")\n",
    "print(f\"[Celda 05] PAD_DAY_INDEX    = {str(PAD_DAY_INDEX)}\")\n",
    "print(f\"[Celda 05] policy.IS_OOS_split_q = {IS_OOS_split_q}\")\n",
    "if USE_RATES_5M:\n",
    "    print(f\"[Celda 05] Usando LEGACY bulk_data/rates_5m ‚Üí {str(RATES_5M_DIR)}\")\n",
    "else:\n",
    "    print(\"[Celda 05] NO se usar√° bulk_data/rates_5m (solo PAD_DAY_INDEX / m5_clean/m5_raw).\")\n",
    "\n",
    "# -------------------------------\n",
    "# Utilidades de normalizaci√≥n tiempo\n",
    "# -------------------------------\n",
    "def detect_epoch_unit(series: pl.Series) -> str:\n",
    "    try:\n",
    "        s = series.drop_nulls().cast(pl.Float64)\n",
    "        if s.len() == 0:\n",
    "            return \"s\"\n",
    "        med = float(s.quantile(0.5, interpolation=\"nearest\"))\n",
    "        return \"ms\" if med > 1e11 else \"s\"\n",
    "    except Exception:\n",
    "        return \"s\"\n",
    "\n",
    "def _to_datetime_series(col: pl.Series) -> Tuple[Optional[pl.Series], Optional[str]]:\n",
    "    if col is None or col.len() == 0:\n",
    "        return None, None\n",
    "    dtype = col.dtype\n",
    "\n",
    "    if str(dtype).startswith(\"Datetime\"):\n",
    "        try:\n",
    "            return col.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\"), None\n",
    "        except Exception:\n",
    "            return None, None\n",
    "\n",
    "    if dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Float64, pl.Float32):\n",
    "        unit = detect_epoch_unit(col)\n",
    "        for u in (unit, \"ms\", \"s\"):\n",
    "            try:\n",
    "                s = (\n",
    "                    pl.from_epoch(col.cast(pl.Float64), time_unit=u)\n",
    "                    .cast(pl.Datetime(\"us\"))\n",
    "                    .dt.replace_time_zone(\"UTC\")\n",
    "                )\n",
    "                return s, u\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None, None\n",
    "\n",
    "    if dtype == pl.Utf8:\n",
    "        try:\n",
    "            s = (\n",
    "                col.str.strptime(pl.Datetime, strict=False, time_unit=\"us\")\n",
    "                .dt.replace_time_zone(\"UTC\")\n",
    "            )\n",
    "            if s.null_count() < s.len():\n",
    "                return s.cast(pl.Datetime(\"us\")), None\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            s_num = col.str.replace_all(r\"[^\\d\\.]\", \"\").cast(pl.Float64)\n",
    "            unit = detect_epoch_unit(s_num)\n",
    "            s = (\n",
    "                pl.from_epoch(s_num, time_unit=unit)\n",
    "                .cast(pl.Datetime(\"us\"))\n",
    "                .dt.replace_time_zone(\"UTC\")\n",
    "            )\n",
    "            return s, unit\n",
    "        except Exception:\n",
    "            return None, None\n",
    "\n",
    "    try:\n",
    "        return col.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\"), None\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def _find_col_name(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# Lectura M5 desde LEGACY rates_5m\n",
    "# -------------------------------\n",
    "def _read_symbol_from_rates5m(sym: str) -> Optional[pl.DataFrame]:\n",
    "    if not USE_RATES_5M:\n",
    "        return None\n",
    "    if not RATES_5M_DIR.exists() or not RATES_5M_DIR.is_dir():\n",
    "        return None\n",
    "\n",
    "    files = list(RATES_5M_DIR.glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    frames: List[pl.DataFrame] = []\n",
    "    epoch_unit_detected_local: Optional[str] = None\n",
    "\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = pl.read_parquet(str(fp))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        cols = df.columns\n",
    "        sym_col = _find_col_name(cols, [\"symbol\", \"sym\"])\n",
    "        time_col = _find_col_name(cols, [\"time\", \"timestamp\", \"timestamp_utc\", \"datetime\"])\n",
    "        close_col = _find_col_name(cols, [\"close\", \"Close\", \"C\"])\n",
    "\n",
    "        if time_col is None or close_col is None:\n",
    "            continue\n",
    "\n",
    "        if sym_col is not None:\n",
    "            df = df.filter(\n",
    "                pl.col(sym_col).cast(pl.Utf8).str.to_uppercase() == sym.upper()\n",
    "            )\n",
    "            if df.is_empty():\n",
    "                continue\n",
    "        else:\n",
    "            fname = fp.stem.upper()\n",
    "            if sym.upper() not in fname:\n",
    "                continue\n",
    "\n",
    "        tser, unit = _to_datetime_series(df.get_column(time_col))\n",
    "        if tser is None or tser.len() == 0:\n",
    "            continue\n",
    "        if unit is not None and epoch_unit_detected_local is None:\n",
    "            epoch_unit_detected_local = unit\n",
    "\n",
    "        mini = pl.DataFrame(\n",
    "            {\"time_utc\": tser, \"close\": df.get_column(close_col).cast(pl.Float64)}\n",
    "        ).drop_nulls(subset=[\"time_utc\", \"close\"]).sort(\"time_utc\")\n",
    "\n",
    "        if not mini.is_empty():\n",
    "            frames.append(mini)\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "\n",
    "    out = (\n",
    "        pl.concat(frames, how=\"vertical_relaxed\")\n",
    "        .drop_nulls(subset=[\"time_utc\", \"close\"])\n",
    "        .sort(\"time_utc\")\n",
    "    )\n",
    "\n",
    "    GLOBAL_STATE.setdefault(\"time\", {})\n",
    "    if epoch_unit_detected_local is not None:\n",
    "        GLOBAL_STATE[\"time\"][\"detected_epoch_unit\"] = epoch_unit_detected_local\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Index cacheado: PAD day_index_m5 (can√≥nico)\n",
    "# -------------------------------\n",
    "sym_to_paths: Dict[str, List[Path]] = {}\n",
    "\n",
    "if PAD_DAY_INDEX.exists():\n",
    "    try:\n",
    "        di = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "        di = di.rename({c: c.lower() for c in di.columns})\n",
    "\n",
    "        if {\"symbol\", \"path\"}.issubset(set(di.columns)):\n",
    "            di = di.with_columns(\n",
    "                pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    "            )\n",
    "\n",
    "            # Reducir a universo stats para performance\n",
    "            uni_set = {str(s).upper().strip() for s in syms_for_stats}\n",
    "            di = di.filter(pl.col(\"symbol\").is_in(list(uni_set)))\n",
    "\n",
    "            ps = di.select([\"symbol\", \"path\"]) if not di.is_empty() else pl.DataFrame({\"symbol\": [], \"path\": []})\n",
    "\n",
    "            for r in ps.iter_rows(named=True):\n",
    "                sym = r.get(\"symbol\")\n",
    "                p_raw = r.get(\"path\")\n",
    "                if not sym or not p_raw:\n",
    "                    continue\n",
    "                try:\n",
    "                    sym_to_paths.setdefault(sym, []).append(Path(str(p_raw)))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"[Celda 05][WARN] PAD_DAY_INDEX no contiene columnas requeridas {symbol, path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 05][WARN] No se pudo cachear PAD_DAY_INDEX | reason={e}\")\n",
    "else:\n",
    "    print(\"[Celda 05][WARN] PAD_DAY_INDEX no existe; se intentar√° solo legacy si aplica.\")\n",
    "\n",
    "def _read_symbol_from_m5clean(sym: str) -> Optional[pl.DataFrame]:\n",
    "    paths_for_sym = sym_to_paths.get(sym.upper(), [])\n",
    "    if not paths_for_sym:\n",
    "        return None\n",
    "\n",
    "    frames: List[pl.DataFrame] = []\n",
    "    for pp in paths_for_sym:\n",
    "        try:\n",
    "            if not pp.exists() or not pp.is_file():\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        got_one = False\n",
    "        for time_cand in (\"timestamp_utc\", \"time\", \"datetime\", \"timestamp\"):\n",
    "            for close_cand in (\"close\", \"Close\", \"C\"):\n",
    "                try:\n",
    "                    df = pl.read_parquet(str(pp), columns=[time_cand, close_cand])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if df.is_empty():\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    tser, _ = _to_datetime_series(df.get_column(time_cand))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                if tser is None or tser.len() == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    close_s = df.get_column(close_cand).cast(pl.Float64)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                mini = (\n",
    "                    pl.DataFrame({\"time_utc\": tser, \"close\": close_s})\n",
    "                    .drop_nulls(subset=[\"time_utc\", \"close\"])\n",
    "                    .sort(\"time_utc\")\n",
    "                )\n",
    "\n",
    "                if not mini.is_empty():\n",
    "                    frames.append(mini)\n",
    "                    got_one = True\n",
    "                    break\n",
    "\n",
    "            if got_one:\n",
    "                break\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "\n",
    "    out = (\n",
    "        pl.concat(frames, how=\"vertical_relaxed\")\n",
    "        .drop_nulls(subset=[\"time_utc\", \"close\"])\n",
    "        .sort(\"time_utc\")\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# C√°lculo de ER (Kaufman) por s√≠mbolo\n",
    "# -------------------------------\n",
    "def compute_er_for_symbol(df: pl.DataFrame, window: int) -> pl.DataFrame:\n",
    "    if df is None or df.is_empty():\n",
    "        return pl.DataFrame({\"time_utc\": [], \"ER\": [], \"window\": []}).with_columns(\n",
    "            pl.col(\"window\").cast(pl.Int32)\n",
    "        )\n",
    "\n",
    "    df = df.drop_nulls(subset=[\"time_utc\", \"close\"]).sort(\"time_utc\")\n",
    "    df = df.unique(subset=[\"time_utc\"], keep=\"last\").sort(\"time_utc\")\n",
    "\n",
    "    # Chequeo suave de cadencia M5\n",
    "    dt = (\n",
    "        df.with_columns((pl.col(\"time_utc\").diff().dt.total_seconds()).alias(\"dt_s\"))\n",
    "        .get_column(\"dt_s\")\n",
    "        .drop_nulls()\n",
    "    )\n",
    "    if dt.len() > 0:\n",
    "        valid_frac = float(((dt >= 295) & (dt <= 305)).sum()) / dt.len()\n",
    "        if valid_frac < 0.95:\n",
    "            print(\n",
    "                f\"[Celda 05][WARN] cadencia M5 irregular (valid_frac={valid_frac:.3f}); \"\n",
    "                \"se contin√∫a para no romper el pipeline.\"\n",
    "            )\n",
    "\n",
    "    close = pl.col(\"close\")\n",
    "    abs_diff = (close - close.shift(1)).abs()\n",
    "    denom = abs_diff.rolling_sum(window_size=window - 1, min_samples=window - 1)\n",
    "    numer = (close - close.shift(window - 1)).abs()\n",
    "\n",
    "    ER_expr = pl.when(denom.is_not_null() & (denom > 0.0)).then(\n",
    "        (numer / denom).clip(0.0, 1.0)\n",
    "    ).otherwise(0.0)\n",
    "\n",
    "    out = df.with_columns(\n",
    "        [\n",
    "            ER_expr.alias(\"ER\").cast(pl.Float32),\n",
    "            pl.lit(window).alias(\"window\").cast(pl.Int32),\n",
    "        ]\n",
    "    ).select([\"time_utc\", \"ER\", \"window\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Flujo principal por s√≠mbolo\n",
    "# -------------------------------\n",
    "er_frames: List[pl.DataFrame] = []\n",
    "pd_frames: List[pl.DataFrame] = []\n",
    "\n",
    "total_syms = len(syms_for_stats)\n",
    "skipped_syms: List[str] = []\n",
    "processed_syms: List[str] = []\n",
    "\n",
    "vol_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, sym in enumerate(syms_for_stats, start=1):\n",
    "    sym = str(sym).upper().strip()\n",
    "\n",
    "    df_sym = _read_symbol_from_rates5m(sym)\n",
    "    if df_sym is None or df_sym.is_empty():\n",
    "        df_sym = _read_symbol_from_m5clean(sym)\n",
    "\n",
    "    if df_sym is None or df_sym.is_empty():\n",
    "        print(f\"[Celda 05][WARN] s√≠mbolo {sym} no encontrado / sin datos v√°lidos; se omite.\")\n",
    "        skipped_syms.append(sym)\n",
    "        if (i % 5 == 0) or (i == total_syms):\n",
    "            print(f\"[Celda 05] progreso: {i}/{total_syms} (omitidos={len(skipped_syms)})\")\n",
    "        continue\n",
    "\n",
    "    # Volatilidad base: retornos log 5M\n",
    "    df_ret = (\n",
    "        df_sym.sort(\"time_utc\")\n",
    "        .drop_nulls(subset=[\"time_utc\", \"close\"])\n",
    "        .with_columns(((pl.col(\"close\") / pl.col(\"close\").shift(1)).log()).alias(\"ret\"))\n",
    "        .drop_nulls(subset=[\"ret\"])\n",
    "    )\n",
    "\n",
    "    if not df_ret.is_empty():\n",
    "        vol_long_term = float(df_ret[\"ret\"].std())\n",
    "        vol_p90_abs_ret = float(df_ret[\"ret\"].abs().quantile(0.90, interpolation=\"nearest\"))\n",
    "        vol_rows.append(\n",
    "            {\"symbol\": sym, \"vol_long_term\": vol_long_term, \"vol_p90_abs_ret\": vol_p90_abs_ret, \"n_ret\": int(df_ret.height)}\n",
    "        )\n",
    "    else:\n",
    "        vol_rows.append({\"symbol\": sym, \"vol_long_term\": None, \"vol_p90_abs_ret\": None, \"n_ret\": 0})\n",
    "\n",
    "    # ER\n",
    "    er_df = compute_er_for_symbol(df_sym, ER_window)\n",
    "    if er_df.is_empty():\n",
    "        print(f\"[Celda 05][WARN] s√≠mbolo {sym} sin ER calculable. Se omite.\")\n",
    "        skipped_syms.append(sym)\n",
    "        if (i % 5 == 0) or (i == total_syms):\n",
    "            print(f\"[Celda 05] progreso: {i}/{total_syms} (omitidos={len(skipped_syms)})\")\n",
    "        continue\n",
    "\n",
    "    # PD = 1 - ER\n",
    "    pd_df = er_df.with_columns(\n",
    "        (1.0 - pl.col(\"ER\").cast(pl.Float64)).cast(pl.Float32).alias(\"PD\")\n",
    "    ).select([\"time_utc\", \"PD\", \"window\"])\n",
    "\n",
    "    # A√±adir s√≠mbolo + alias expl√≠citos (sin romper compatibilidad)\n",
    "    er_df = (\n",
    "        er_df.with_columns([\n",
    "            pl.lit(sym).alias(\"symbol\"),\n",
    "            pl.col(\"ER\").alias(\"ER_kaufman\"),\n",
    "        ])\n",
    "        .select([\"symbol\", \"time_utc\", \"ER\", \"ER_kaufman\", \"window\"])\n",
    "    )\n",
    "\n",
    "    pd_df = (\n",
    "        pd_df.with_columns([\n",
    "            pl.lit(sym).alias(\"symbol\"),\n",
    "            pl.col(\"PD\").alias(\"PD_kaufman\"),\n",
    "        ])\n",
    "        .select([\"symbol\", \"time_utc\", \"PD\", \"PD_kaufman\", \"window\"])\n",
    "    )\n",
    "\n",
    "    er_frames.append(er_df)\n",
    "    pd_frames.append(pd_df)\n",
    "    processed_syms.append(sym)\n",
    "\n",
    "    if (i % 5 == 0) or (i == total_syms):\n",
    "        print(\n",
    "            f\"[Celda 05] progreso: {i}/{total_syms} \"\n",
    "            f\"(procesados={len(processed_syms)}, omitidos={len(skipped_syms)})\"\n",
    "        )\n",
    "\n",
    "# -------------------------------\n",
    "# Unir y persistir ER / PD\n",
    "# -------------------------------\n",
    "if len(er_frames) == 0 or len(pd_frames) == 0:\n",
    "    raise RuntimeError(\"No se pudo generar ER/PD para ning√∫n s√≠mbolo del universo estad√≠stico.\")\n",
    "\n",
    "er_all = pl.concat(er_frames, how=\"vertical_relaxed\")\n",
    "pd_all = pl.concat(pd_frames, how=\"vertical_relaxed\")\n",
    "\n",
    "er_path = OUT_METRICS_DIR / \"er_series.parquet\"\n",
    "pd_path = OUT_METRICS_DIR / \"pd_series.parquet\"\n",
    "\n",
    "er_all.write_parquet(str(er_path))\n",
    "pd_all.write_parquet(str(pd_path))\n",
    "\n",
    "rows_er, cols_er = er_all.height, len(er_all.columns)\n",
    "rows_pd, cols_pd = pd_all.height, len(pd_all.columns)\n",
    "if rows_er == 0 or er_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Error al escribir {str(er_path)}\")\n",
    "if rows_pd == 0 or pd_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Error al escribir {str(pd_path)}\")\n",
    "\n",
    "er_time_min, er_time_max = er_all.select(\n",
    "    [pl.col(\"time_utc\").min().alias(\"min\"), pl.col(\"time_utc\").max().alias(\"max\")]\n",
    ").row(0)\n",
    "n_syms_er = er_all.select(pl.col(\"symbol\").n_unique().alias(\"n\")).item()\n",
    "\n",
    "print(f\"[Celda 05] OUTPUT ‚Üí {str(er_path)} (OK, rows={rows_er}, cols={cols_er})\")\n",
    "print(f\"[Celda 05] OUTPUT ‚Üí {str(pd_path)} (OK, rows={rows_pd}, cols={cols_pd})\")\n",
    "print(f\"[Celda 05] n_symbols (universo stats) = {len(syms_for_stats)}\")\n",
    "print(\n",
    "    f\"[Celda 05] ER: symbols={n_syms_er}, date_range=[{er_time_min} ‚Üí {er_time_max}] \"\n",
    "    f\"(procesados={len(processed_syms)}, omitidos={len(skipped_syms)})\"\n",
    ")\n",
    "if skipped_syms:\n",
    "    print(f\"[Celda 05] S√≠mbolos omitidos (m√°x 10): {skipped_syms[:10]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Resumen de volatilidad por s√≠mbolo\n",
    "# -------------------------------\n",
    "vol_summary_path = None\n",
    "if vol_rows:\n",
    "    vol_df = pl.DataFrame(vol_rows).filter(pl.col(\"n_ret\") > 0)\n",
    "\n",
    "    if not vol_df.is_empty():\n",
    "        q33, q66 = vol_df.select(\n",
    "            pl.col(\"vol_long_term\").quantile(0.33).alias(\"q33\"),\n",
    "            pl.col(\"vol_long_term\").quantile(0.66).alias(\"q66\"),\n",
    "        ).row(0)\n",
    "\n",
    "        vol_df = vol_df.with_columns(\n",
    "            pl.when(pl.col(\"vol_long_term\").is_null())\n",
    "            .then(pl.lit(\"DESCONOCIDA\"))\n",
    "            .when(pl.col(\"vol_long_term\") <= q33)\n",
    "            .then(pl.lit(\"BAJA\"))\n",
    "            .when(pl.col(\"vol_long_term\") <= q66)\n",
    "            .then(pl.lit(\"MEDIA\"))\n",
    "            .otherwise(pl.lit(\"ALTA\"))\n",
    "            .alias(\"vol_regime_flag\")\n",
    "        )\n",
    "\n",
    "        vol_summary_path = OUT_METRICS_DIR / \"regime_volatility_summary.parquet\"\n",
    "        vol_df.write_parquet(str(vol_summary_path))\n",
    "\n",
    "        print(\n",
    "            f\"[Celda 05] OUTPUT ‚Üí {str(vol_summary_path)} \"\n",
    "            f\"(OK, rows={vol_df.height}, cols={len(vol_df.columns)})\"\n",
    "        )\n",
    "\n",
    "        vol_range = vol_df.select(\n",
    "            pl.col(\"vol_long_term\").min().alias(\"vol_min\"),\n",
    "            pl.col(\"vol_long_term\").max().alias(\"vol_max\"),\n",
    "        ).row(0)\n",
    "        print(\n",
    "            \"[Celda 05] Rango vol_long_term (s√≠mbolos): \"\n",
    "            f\"[{vol_range[0]:.6f} ‚Üí {vol_range[1]:.6f}]\"\n",
    "        )\n",
    "\n",
    "        top_hi = vol_df.sort(\"vol_long_term\", descending=True).head(3)\n",
    "        top_lo = vol_df.sort(\"vol_long_term\", descending=False).head(3)\n",
    "\n",
    "        print(\"[Celda 05] Top-3 m√°s vol√°tiles (vol_long_term):\")\n",
    "        for r in top_hi.iter_rows(named=True):\n",
    "            print(\n",
    "                f\"  symbol={r['symbol']} \"\n",
    "                f\"vol_long_term={r['vol_long_term']:.6f} \"\n",
    "                f\"vol_p90_abs_ret={r['vol_p90_abs_ret']:.6f} \"\n",
    "                f\"flag={r['vol_regime_flag']}\"\n",
    "            )\n",
    "\n",
    "        print(\"[Celda 05] Top-3 menos vol√°tiles (vol_long_term):\")\n",
    "        for r in top_lo.iter_rows(named=True):\n",
    "            print(\n",
    "                f\"  symbol={r['symbol']} \"\n",
    "                f\"vol_long_term={r['vol_long_term']:.6f} \"\n",
    "                f\"vol_p90_abs_ret={r['vol_p90_abs_ret']:.6f} \"\n",
    "                f\"flag={r['vol_regime_flag']}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"[Celda 05] No hay suficientes retornos para construir resumen de volatilidad.\")\n",
    "else:\n",
    "    print(\"[Celda 05] vol_rows vac√≠o: no se construye resumen de volatilidad.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Percentiles P40/P60 por s√≠mbolo (ER / PD) ‚Äî FULL y SOLO-IS (anti-leak)\n",
    "# -------------------------------\n",
    "# FULL (legacy diagn√≥stico)\n",
    "er_pct_full = (\n",
    "    er_all.group_by(\"symbol\")\n",
    "    .agg([\n",
    "        pl.col(\"ER\").quantile(0.40, interpolation=\"nearest\").alias(\"ER_P40\"),\n",
    "        pl.col(\"ER\").quantile(0.60, interpolation=\"nearest\").alias(\"ER_P60\"),\n",
    "        pl.col(\"window\").max().alias(\"window\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "pd_pct_full = (\n",
    "    pd_all.group_by(\"symbol\")\n",
    "    .agg([\n",
    "        pl.col(\"PD\").quantile(0.40, interpolation=\"nearest\").alias(\"PD_P40\"),\n",
    "        pl.col(\"PD\").quantile(0.60, interpolation=\"nearest\").alias(\"PD_P60\"),\n",
    "        pl.col(\"window\").max().alias(\"window\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# SOLO-IS (mismo split que Celda 08IS)\n",
    "er_ts = (\n",
    "    er_all.select([\"symbol\", \"time_utc\", \"ER\"])\n",
    "    .drop_nulls(subset=[\"symbol\", \"time_utc\", \"ER\"])\n",
    "    .with_columns(pl.col(\"time_utc\").cast(pl.Int64).alias(\"ts_int\"))\n",
    ")\n",
    "\n",
    "ts_cut = (\n",
    "    er_ts.group_by([\"symbol\"])\n",
    "         .agg(\n",
    "             pl.col(\"ts_int\")\n",
    "               .quantile(IS_OOS_split_q, interpolation=\"nearest\")\n",
    "               .alias(\"ts_cut_int\")\n",
    "         )\n",
    ")\n",
    "\n",
    "er_folds = (\n",
    "    er_ts.join(ts_cut, on=[\"symbol\"], how=\"left\")\n",
    "         .with_columns(\n",
    "             pl.when(pl.col(\"ts_cut_int\").is_not_null() & (pl.col(\"ts_int\") <= pl.col(\"ts_cut_int\")))\n",
    "               .then(pl.lit(\"IS\"))\n",
    "               .otherwise(pl.lit(\"OOS\"))\n",
    "               .alias(\"segment\")\n",
    "         )\n",
    ")\n",
    "\n",
    "er_is = er_folds.filter(pl.col(\"segment\") == \"IS\")\n",
    "pd_ts = (\n",
    "    pd_all.select([\"symbol\", \"time_utc\", \"PD\"])\n",
    "    .drop_nulls(subset=[\"symbol\", \"time_utc\", \"PD\"])\n",
    "    .with_columns(pl.col(\"time_utc\").cast(pl.Int64).alias(\"ts_int\"))\n",
    ")\n",
    "\n",
    "pd_folds = (\n",
    "    pd_ts.join(ts_cut, on=[\"symbol\"], how=\"left\")\n",
    "         .with_columns(\n",
    "             pl.when(pl.col(\"ts_cut_int\").is_not_null() & (pl.col(\"ts_int\") <= pl.col(\"ts_cut_int\")))\n",
    "               .then(pl.lit(\"IS\"))\n",
    "               .otherwise(pl.lit(\"OOS\"))\n",
    "               .alias(\"segment\")\n",
    "         )\n",
    ")\n",
    "\n",
    "pd_is = pd_folds.filter(pl.col(\"segment\") == \"IS\")\n",
    "\n",
    "er_pct_is = (\n",
    "    er_is.group_by(\"symbol\")\n",
    "         .agg([\n",
    "             pl.col(\"ER\").quantile(0.40, interpolation=\"nearest\").alias(\"ER_IS_P40\"),\n",
    "             pl.col(\"ER\").quantile(0.60, interpolation=\"nearest\").alias(\"ER_IS_P60\"),\n",
    "             pl.len().alias(\"n_is_er\"),\n",
    "         ])\n",
    ")\n",
    "\n",
    "pd_pct_is = (\n",
    "    pd_is.group_by(\"symbol\")\n",
    "         .agg([\n",
    "             pl.col(\"PD\").quantile(0.40, interpolation=\"nearest\").alias(\"PD_IS_P40\"),\n",
    "             pl.col(\"PD\").quantile(0.60, interpolation=\"nearest\").alias(\"PD_IS_P60\"),\n",
    "             pl.len().alias(\"n_is_pd\"),\n",
    "         ])\n",
    ")\n",
    "\n",
    "# Rangos informativos\n",
    "if not er_pct_full.is_empty():\n",
    "    er_range = er_pct_full.select(\n",
    "        pl.col(\"ER_P40\").min().alias(\"ER_P40_min\"),\n",
    "        pl.col(\"ER_P40\").max().alias(\"ER_P40_max\"),\n",
    "        pl.col(\"ER_P60\").min().alias(\"ER_P60_min\"),\n",
    "        pl.col(\"ER_P60\").max().alias(\"ER_P60_max\"),\n",
    "    ).row(0)\n",
    "    print(\n",
    "        \"[Celda 05] Rango ER_P40/ER_P60 (FULL, legacy): \"\n",
    "        f\"ER_P40‚àà[{er_range[0]:.4f}, {er_range[1]:.4f}], \"\n",
    "        f\"ER_P60‚àà[{er_range[2]:.4f}, {er_range[3]:.4f}]\"\n",
    "    )\n",
    "\n",
    "if not pd_pct_full.is_empty():\n",
    "    pd_range = pd_pct_full.select(\n",
    "        pl.col(\"PD_P40\").min().alias(\"PD_P40_min\"),\n",
    "        pl.col(\"PD_P40\").max().alias(\"PD_P40_max\"),\n",
    "        pl.col(\"PD_P60\").min().alias(\"PD_P60_min\"),\n",
    "        pl.col(\"PD_P60\").max().alias(\"PD_P60_max\"),\n",
    "    ).row(0)\n",
    "    print(\n",
    "        \"[Celda 05] Rango PD_P40/PD_P60 (FULL, legacy): \"\n",
    "        f\"PD_P40‚àà[{pd_range[0]:.4f}, {pd_range[1]:.4f}], \"\n",
    "        f\"PD_P60‚àà[{pd_range[2]:.4f}, {pd_range[3]:.4f}]\"\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Construcci√≥n de JSON (FULL legacy + IS anti-leak + alias expl√≠citos)\n",
    "# -------------------------------\n",
    "# Map FULL (legacy)\n",
    "er_map_full = {\n",
    "    r[0]: {\"ER\": {\"P40\": float(r[1]) if r[1] is not None else None,\n",
    "                 \"P60\": float(r[2]) if r[2] is not None else None}}\n",
    "    for r in er_pct_full.select([\"symbol\", \"ER_P40\", \"ER_P60\"]).iter_rows()\n",
    "}\n",
    "pd_map_full = {\n",
    "    r[0]: {\"PD\": {\"P40\": float(r[1]) if r[1] is not None else None,\n",
    "                 \"P60\": float(r[2]) if r[2] is not None else None}}\n",
    "    for r in pd_pct_full.select([\"symbol\", \"PD_P40\", \"PD_P60\"]).iter_rows()\n",
    "}\n",
    "\n",
    "# Map IS\n",
    "er_map_is = {\n",
    "    r[0]: {\"ER_IS\": {\"P40\": float(r[1]) if r[1] is not None else None,\n",
    "                    \"P60\": float(r[2]) if r[2] is not None else None},\n",
    "           \"_n_is_er\": int(r[3]) if r[3] is not None else 0}\n",
    "    for r in er_pct_is.select([\"symbol\", \"ER_IS_P40\", \"ER_IS_P60\", \"n_is_er\"]).iter_rows()\n",
    "}\n",
    "pd_map_is = {\n",
    "    r[0]: {\"PD_IS\": {\"P40\": float(r[1]) if r[1] is not None else None,\n",
    "                    \"P60\": float(r[2]) if r[2] is not None else None},\n",
    "           \"_n_is_pd\": int(r[3]) if r[3] is not None else 0}\n",
    "    for r in pd_pct_is.select([\"symbol\", \"PD_IS_P40\", \"PD_IS_P60\", \"n_is_pd\"]).iter_rows()\n",
    "}\n",
    "\n",
    "all_syms_pct = set(er_map_full.keys()) | set(pd_map_full.keys()) | set(er_map_is.keys()) | set(pd_map_is.keys())\n",
    "er_pd_summary: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for s in sorted(all_syms_pct):\n",
    "    er_pd_summary.setdefault(s, {})\n",
    "    if s in er_map_full:\n",
    "        er_pd_summary[s].update(er_map_full[s])\n",
    "        # alias expl√≠citos FULL\n",
    "        er_pd_summary[s][\"ER_FULL\"] = dict(er_map_full[s][\"ER\"])\n",
    "    if s in pd_map_full:\n",
    "        er_pd_summary[s].update(pd_map_full[s])\n",
    "        er_pd_summary[s][\"PD_FULL\"] = dict(pd_map_full[s][\"PD\"])\n",
    "    if s in er_map_is:\n",
    "        er_pd_summary[s].update(er_map_is[s])\n",
    "    if s in pd_map_is:\n",
    "        er_pd_summary[s].update(pd_map_is[s])\n",
    "\n",
    "summary_obj = {\n",
    "    \"meta\": {\n",
    "        \"definition\": \"PD is defined as PD_t = 1.0 - ER_t\",\n",
    "        \"er_window\": ER_window,\n",
    "        \"pd_window\": PD_window,\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"universe_key\": UNIVERSE_KEY_DEFAULT,\n",
    "        \"policy\": {\n",
    "            \"IS_OOS_split_q\": float(IS_OOS_split_q),\n",
    "            \"split_rule\": \"IS if ts_int <= ts_cut_int; ts_cut_int = quantile(ts_int, q) per symbol; interpolation=nearest\",\n",
    "        },\n",
    "        \"pctiles\": {\n",
    "            \"P40\": 0.40,\n",
    "            \"P60\": 0.60,\n",
    "            \"interpolation\": \"nearest\",\n",
    "        },\n",
    "    },\n",
    "    \"symbols\": er_pd_summary,\n",
    "}\n",
    "\n",
    "summary_path = OUT_DIAG_DIR / \"er_pd_percentiles_summary.json\"\n",
    "summary_txt = json.dumps(summary_obj, ensure_ascii=False, indent=2, sort_keys=True)\n",
    "summary_path.write_text(summary_txt, encoding=\"utf-8\")\n",
    "\n",
    "if summary_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Error al escribir {str(summary_path)}\")\n",
    "\n",
    "print(f\"[Celda 05] OUTPUT ‚Üí {str(summary_path)} (OK, bytes={summary_path.stat().st_size})\")\n",
    "\n",
    "# Top lists (FULL legacy, solo para diagn√≥stico)\n",
    "if not er_pct_full.is_empty():\n",
    "    er_top10 = er_pct_full.sort(by=\"ER_P60\", descending=True).select([\"symbol\", \"ER_P60\"]).head(10)\n",
    "    print(\"[Celda 05] Top 10 s√≠mbolos por ER_P60 (FULL legacy):\")\n",
    "    for i, r in enumerate(er_top10.iter_rows(named=True), start=1):\n",
    "        print(f\"  rank={i} symbol={r['symbol']} ER_P60={r['ER_P60']:.4f}\")\n",
    "\n",
    "if not pd_pct_full.is_empty():\n",
    "    pd_top10 = pd_pct_full.sort(by=\"PD_P60\", descending=True).select([\"symbol\", \"PD_P60\"]).head(10)\n",
    "    print(\"[Celda 05] Top 10 s√≠mbolos por PD_P60 (FULL legacy):\")\n",
    "    for i, r in enumerate(pd_top10.iter_rows(named=True), start=1):\n",
    "        print(f\"  rank={i} symbol={r['symbol']} PD_P60={r['PD_P60']:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Contadores para report_stats\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c05\"] = {\n",
    "    \"universe_key\": UNIVERSE_KEY_DEFAULT,\n",
    "    \"n_universe_syms\": int(len(syms_for_stats)),\n",
    "    \"n_syms_with_er\": int(n_syms_er),\n",
    "    \"n_processed_syms\": int(len(processed_syms)),\n",
    "    \"n_skipped_syms\": int(len(skipped_syms)),\n",
    "    \"policy_IS_OOS_split_q\": float(IS_OOS_split_q),\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Actualizar GLOBAL_STATE con paths\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"er_series_path\"] = str(er_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"pd_series_path\"] = str(pd_path)\n",
    "if vol_summary_path is not None:\n",
    "    GLOBAL_STATE[\"metrics\"][\"regime_volatility_summary_path\"] = str(vol_summary_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"diagnostics\", {})\n",
    "GLOBAL_STATE[\"diagnostics\"][\"er_pd_percentiles_summary_path\"] = str(summary_path)\n",
    "\n",
    "# Epoch unit global informativo\n",
    "GLOBAL_STATE.setdefault(\"time\", {})\n",
    "if GLOBAL_STATE[\"time\"].get(\"detected_epoch_unit\") is None:\n",
    "    GLOBAL_STATE[\"time\"][\"detected_epoch_unit\"] = \"s\"\n",
    "if GLOBAL_STATE[\"time\"][\"detected_epoch_unit\"] in (\"s\", \"ms\"):\n",
    "    print(f\"[Celda 05] time epoch unit = {GLOBAL_STATE['time']['detected_epoch_unit']}\")\n",
    "\n",
    "def _to_jsonable(obj: Any) -> Any:\n",
    "    try:\n",
    "        import pandas as pd  # opcional\n",
    "    except Exception:\n",
    "        pd = None\n",
    "\n",
    "    if obj is None or isinstance(obj, (str, int, float, bool)):\n",
    "        return obj\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): _to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [_to_jsonable(x) for x in list(obj)]\n",
    "    if isinstance(obj, pl.DataFrame):\n",
    "        return {\"_type\": \"polars.DataFrame\", \"shape\": [obj.height, len(obj.columns)], \"columns\": obj.columns}\n",
    "    if hasattr(pl, \"LazyFrame\") and isinstance(obj, pl.LazyFrame):\n",
    "        return {\"_type\": \"polars.LazyFrame\"}\n",
    "    if pd is not None and \"pandas\" in str(type(obj)):\n",
    "        try:\n",
    "            shape = list(obj.shape)  # type: ignore[attr-defined]\n",
    "            cols = list(obj.columns)  # type: ignore[attr-defined]\n",
    "        except Exception:\n",
    "            shape, cols = None, None\n",
    "        return {\"_type\": \"pandas.DataFrame\", \"shape\": shape, \"columns\": cols}\n",
    "    return str(obj)\n",
    "\n",
    "snapshot_path = OUT_DIAG_DIR / \"global_state_snapshot_c05.json\"\n",
    "snapshot_txt = json.dumps(_to_jsonable(GLOBAL_STATE), ensure_ascii=False, indent=2, sort_keys=True)\n",
    "snapshot_path.write_text(snapshot_txt, encoding=\"utf-8\")\n",
    "if snapshot_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(\"snapshot size=0\")\n",
    "\n",
    "print(\">>> Celda 05 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2dd0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 06 :: Detector de R√©gimen (TREND/RANGE/NOISE)\n",
      "üìÅ INPUT ‚Üí ER series   : C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\er_series.parquet (exists=true)\n",
      "üìÅ INPUT ‚Üí PD series   : C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\pd_series.parquet (exists=true)\n",
      "üìÅ INPUT ‚Üí Pcts resumen: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\er_pd_percentiles_summary.json (exists=true)\n",
      "üìÅ INPUT ‚Üí config.json : C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json (exists=true)\n",
      "[Celda 06] Par√°metros ‚Üí hysteresis.pctile_delta=0.02 | ER.window=34 | PD.window=34\n",
      "[Celda 06] ER ‚Üí rows=24522466, symbols=84, date_range=[2021-11-19 00:00:00+00:00 ‚Üí 2025-12-02 23:50:00+00:00]\n",
      "[Celda 06] PD ‚Üí rows=24522466, symbols=84, date_range=[2021-11-19 00:00:00+00:00 ‚Üí 2025-12-02 23:50:00+00:00]\n",
      "[Celda 06] S√≠mbolos comunes ER‚à©PD = 84\n",
      "[Celda 06] Percentiles anti-leak: usando ER_IS/PD_IS si est√°n disponibles | split_q_meta=0.7\n",
      "[Celda 06] Volatilidad base unida desde regime_volatility_summary.parquet (rows=84, cols=5)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_thresholds.parquet (OK, rows=84, cols=12)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet (OK, rows=24522466, cols=10)\n",
      "[Celda 06] regime_labels incluye columnas m√≠nimas: symbol, time_utc, regime, session_label, er_value, pd_value\n",
      "[Celda 06] regime_thresholds incluye pct_source_ER/pct_source_PD para auditor√≠a (anti-leak).\n",
      "[Celda 06] Distribuci√≥n global de r√©gimen:\n",
      "  RANGE  ‚Üí barras=9,736,895 (39.71%)\n",
      "  TREND  ‚Üí barras=9,692,347 (39.52%)\n",
      "  NOISE  ‚Üí barras=5,093,224 (20.77%)\n",
      "Top-5 s√≠mbolos por % de barras en TREND:\n",
      "  symbol=WMT :: %TREND=40.97% | %RANGE=38.30% | %NOISE=20.72%\n",
      "  symbol=XAUEUR :: %TREND=40.58% | %RANGE=38.81% | %NOISE=20.61%\n",
      "  symbol=BTCUSD :: %TREND=40.54% | %RANGE=38.77% | %NOISE=20.69%\n",
      "  symbol=ETHUSD :: %TREND=40.41% | %RANGE=38.88% | %NOISE=20.71%\n",
      "  symbol=GBPCHF :: %TREND=40.40% | %RANGE=38.91% | %NOISE=20.69%\n",
      "Top-5 s√≠mbolos por % de barras en RANGE:\n",
      "  symbol=USDILS :: %TREND=32.84% | %RANGE=46.68% | %NOISE=20.48%\n",
      "  symbol=EURCZK :: %TREND=34.78% | %RANGE=44.13% | %NOISE=21.09%\n",
      "  symbol=EURHUF :: %TREND=35.48% | %RANGE=43.49% | %NOISE=21.03%\n",
      "  symbol=EURPLN :: %TREND=36.91% | %RANGE=41.91% | %NOISE=21.18%\n",
      "  symbol=EURNOK :: %TREND=37.50% | %RANGE=41.77% | %NOISE=20.73%\n",
      ">>> Celda 06 :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 06 ‚Äî Detector de R√©gimen (TREND / RANGE / NOISE) con hist√©resis + sesi√≥n + volatilidad\n",
    "# -----------------------------------------------------------------------------\n",
    "# Rol de esta celda:\n",
    "#   - Toma las series ER/PD (M5) ya calculadas en Celda 05.\n",
    "#   - Usa percentiles por s√≠mbolo + hist√©resis (bandas P40/P60 ¬± delta) para\n",
    "#     etiquetar cada barra como TREND, RANGE o NOISE.\n",
    "#   - NO FILTRA NADA: solo genera etiquetas de r√©gimen.\n",
    "#\n",
    "# Anti-leak (CR√çTICO):\n",
    "#   - Para thresholds se usan ER_IS/PD_IS desde diagnostics/er_pd_percentiles_summary.json\n",
    "#     (calculados SOLO con IS en Celda 05 usando policy.IS_OOS_split_q, igual que Celda 08IS).\n",
    "#   - Se agregan columnas de auditor√≠a:\n",
    "#         * pct_source_ER  (esperado: \"ER_IS\")\n",
    "#         * pct_source_PD  (esperado: \"PD_IS\")\n",
    "#\n",
    "# Sem√°ntica:\n",
    "#   - Se mantienen ER/PD como m√©tricas base (Kaufman) y labels contienen:\n",
    "#         * er_value = ER_t (Kaufman)\n",
    "#         * pd_value = PD_t (=1-ER_t)\n",
    "#\n",
    "# Outputs:\n",
    "#   - metrics/regime_thresholds.parquet\n",
    "#   - metrics/regime_labels.parquet\n",
    "#   - diagnostics/global_state_snapshot_c06.json\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones y paths base\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas previas.\")\n",
    "\n",
    "if \"paths\" not in GLOBAL_STATE:\n",
    "    raise RuntimeError(\"GLOBAL_STATE['paths'] no existe.\")\n",
    "for rk in (\"metrics\", \"diagnostics\"):\n",
    "    if rk not in GLOBAL_STATE[\"paths\"]:\n",
    "        raise RuntimeError(f\"Falta ruta '{rk}' en GLOBAL_STATE['paths'].\")\n",
    "\n",
    "OUT_METRICS_DIR = Path(GLOBAL_STATE[\"paths\"][\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(GLOBAL_STATE[\"paths\"][\"diagnostics\"]).resolve()\n",
    "\n",
    "er_path = OUT_METRICS_DIR / \"er_series.parquet\"\n",
    "pd_path = OUT_METRICS_DIR / \"pd_series.parquet\"\n",
    "summary_path = OUT_DIAG_DIR / \"er_pd_percentiles_summary.json\"\n",
    "config_path = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "print(\">>> Celda 06 :: Detector de R√©gimen (TREND/RANGE/NOISE)\")\n",
    "print(f\"üìÅ INPUT ‚Üí ER series   : {str(er_path)} (exists={str(er_path.exists()).lower()})\")\n",
    "print(f\"üìÅ INPUT ‚Üí PD series   : {str(pd_path)} (exists={str(pd_path.exists()).lower()})\")\n",
    "print(f\"üìÅ INPUT ‚Üí Pcts resumen: {str(summary_path)} (exists={str(summary_path.exists()).lower()})\")\n",
    "print(f\"üìÅ INPUT ‚Üí config.json : {str(config_path)} (exists={str(config_path.exists()).lower()})\")\n",
    "\n",
    "for need in (er_path, pd_path, summary_path, config_path):\n",
    "    if not need.exists():\n",
    "        raise RuntimeError(f\"Missing required input: {str(need)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar configuraci√≥n y percentiles\n",
    "# -------------------------------\n",
    "try:\n",
    "    cfg = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo leer config.json | reason={e}\")\n",
    "\n",
    "# Hist√©resis: delta aplicado a la banda [P40, P60]\n",
    "pctile_delta = float(cfg.get(\"hysteresis\", {}).get(\"pctile_delta\", 0.0))\n",
    "if not (pctile_delta > 0.0):\n",
    "    raise RuntimeError(\"Invalid hysteresis.pctile_delta (debe ser >0)\")\n",
    "\n",
    "# Ventanas (para info)\n",
    "ER_win_cfg = int(cfg.get(\"ER\", {}).get(\"window\", -1))\n",
    "PD_win_cfg = int(cfg.get(\"PD\", {}).get(\"window\", -1))\n",
    "\n",
    "print(f\"[Celda 06] Par√°metros ‚Üí hysteresis.pctile_delta={pctile_delta} | ER.window={ER_win_cfg} | PD.window={PD_win_cfg}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar series ER/PD y validar s√≠mbolos\n",
    "# -------------------------------\n",
    "try:\n",
    "    er_df = pl.read_parquet(str(er_path), columns=[\"symbol\", \"time_utc\", \"ER\"])\n",
    "    pd_df = pl.read_parquet(str(pd_path), columns=[\"symbol\", \"time_utc\", \"PD\"])\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudieron leer ER/PD series | reason={e}\")\n",
    "\n",
    "if er_df.is_empty() or pd_df.is_empty():\n",
    "    raise RuntimeError(\"Empty ER/PD inputs\")\n",
    "\n",
    "# Normalizar claves y tipos\n",
    "er_df = er_df.with_columns(\n",
    "    pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    ").sort([\"symbol\", \"time_utc\"])\n",
    "pd_df = pd_df.with_columns(\n",
    "    pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    ").sort([\"symbol\", \"time_utc\"])\n",
    "\n",
    "syms_er = set(er_df.get_column(\"symbol\").unique().to_list())\n",
    "syms_pd = set(pd_df.get_column(\"symbol\").unique().to_list())\n",
    "syms_series = sorted(syms_er & syms_pd)\n",
    "if not syms_series:\n",
    "    raise RuntimeError(\"No hay intersecci√≥n de s√≠mbolos entre ER y PD\")\n",
    "\n",
    "# Prints de shape/rango\n",
    "tmin_er = er_df.get_column(\"time_utc\").min()\n",
    "tmax_er = er_df.get_column(\"time_utc\").max()\n",
    "tmin_pd = pd_df.get_column(\"time_utc\").min()\n",
    "tmax_pd = pd_df.get_column(\"time_utc\").max()\n",
    "print(f\"[Celda 06] ER ‚Üí rows={er_df.height}, symbols={len(syms_er)}, date_range=[{tmin_er} ‚Üí {tmax_er}]\")\n",
    "print(f\"[Celda 06] PD ‚Üí rows={pd_df.height}, symbols={len(syms_pd)}, date_range=[{tmin_pd} ‚Üí {tmax_pd}]\")\n",
    "print(f\"[Celda 06] S√≠mbolos comunes ER‚à©PD = {len(syms_series)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar resumen de percentiles (FULL legacy + IS anti-leak)\n",
    "# -------------------------------\n",
    "try:\n",
    "    summary = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo leer er_pd_percentiles_summary.json | reason={e}\")\n",
    "\n",
    "sym_meta: Dict[str, Any] = summary.get(\"symbols\", {}) or {}\n",
    "policy_meta = (summary.get(\"meta\", {}) or {}).get(\"policy\", {}) or {}\n",
    "split_q_meta = policy_meta.get(\"IS_OOS_split_q\", None)\n",
    "\n",
    "print(f\"[Celda 06] Percentiles anti-leak: usando ER_IS/PD_IS si est√°n disponibles | split_q_meta={split_q_meta}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: extraer percentiles por s√≠mbolo con preferencia IS\n",
    "# -------------------------------\n",
    "def _get_pcts(meta_s: dict, key_preferred: str, key_fallback: str) -> Tuple[Optional[float], Optional[float], str]:\n",
    "    \"\"\"\n",
    "    Devuelve (P40, P60, source_key) para un bloque de percentiles.\n",
    "    Prioriza key_preferred (p.ej. 'ER_IS'); si no, usa key_fallback (p.ej. 'ER').\n",
    "    \"\"\"\n",
    "    if isinstance(meta_s.get(key_preferred, None), dict):\n",
    "        d = meta_s.get(key_preferred, {})\n",
    "        if (\"P40\" in d) and (\"P60\" in d) and (d[\"P40\"] is not None) and (d[\"P60\"] is not None):\n",
    "            return float(d[\"P40\"]), float(d[\"P60\"]), key_preferred\n",
    "    if isinstance(meta_s.get(key_fallback, None), dict):\n",
    "        d = meta_s.get(key_fallback, {})\n",
    "        if (\"P40\" in d) and (\"P60\" in d) and (d[\"P40\"] is not None) and (d[\"P60\"] is not None):\n",
    "            return float(d[\"P40\"]), float(d[\"P60\"]), key_fallback\n",
    "    return None, None, \"MISSING\"\n",
    "\n",
    "# -------------------------------\n",
    "# Construir thresholds por s√≠mbolo (usando IS para evitar leak)\n",
    "# -------------------------------\n",
    "EPS = 1e-6\n",
    "rows_thr: List[Dict[str, Any]] = []\n",
    "skipped_no_pct: List[str] = []\n",
    "\n",
    "for s in syms_series:\n",
    "    meta_s = sym_meta.get(s)\n",
    "    if not meta_s:\n",
    "        skipped_no_pct.append(s)\n",
    "        continue\n",
    "\n",
    "    ER_P40, ER_P60, src_er = _get_pcts(meta_s, \"ER_IS\", \"ER\")  # preferimos IS\n",
    "    PD_P40, PD_P60, src_pd = _get_pcts(meta_s, \"PD_IS\", \"PD\")  # preferimos IS\n",
    "\n",
    "    if ER_P40 is None or ER_P60 is None or PD_P40 is None or PD_P60 is None:\n",
    "        skipped_no_pct.append(s)\n",
    "        continue\n",
    "\n",
    "    dER = pctile_delta * max(ER_P60 - ER_P40, EPS)\n",
    "    dPD = pctile_delta * max(PD_P60 - PD_P40, EPS)\n",
    "\n",
    "    ER_hi = ER_P60 + dER\n",
    "    ER_lo = ER_P40 - dER\n",
    "    PD_hi = PD_P60 - dPD  # umbral alto para RANGE\n",
    "    PD_lo = PD_P40 + dPD  # umbral bajo para TREND\n",
    "\n",
    "    rows_thr.append({\n",
    "        \"symbol\": s,\n",
    "        \"ER_P40\": ER_P40,\n",
    "        \"ER_P60\": ER_P60,\n",
    "        \"ER_hi\": ER_hi,\n",
    "        \"ER_lo\": ER_lo,\n",
    "        \"PD_P40\": PD_P40,\n",
    "        \"PD_P60\": PD_P60,\n",
    "        \"PD_hi\": PD_hi,\n",
    "        \"PD_lo\": PD_lo,\n",
    "        \"pctile_delta\": pctile_delta,\n",
    "        # Auditor√≠a (anti-leak)\n",
    "        \"pct_source_ER\": src_er,\n",
    "        \"pct_source_PD\": src_pd,\n",
    "    })\n",
    "\n",
    "if not rows_thr:\n",
    "    raise RuntimeError(\"No se pudieron construir thresholds (percentiles faltantes para todos los s√≠mbolos).\")\n",
    "\n",
    "thr_df = pl.DataFrame(rows_thr)\n",
    "\n",
    "if skipped_no_pct:\n",
    "    print(f\"[Celda 06][WARN] {len(skipped_no_pct)} s√≠mbolos sin percentiles ER/PD en resumen. Ejemplo: {skipped_no_pct[:5]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Funci√≥n: aplicar hist√©resis (bandas por s√≠mbolo)\n",
    "# -------------------------------\n",
    "def apply_regime_hysteresis(\n",
    "    er_df: pl.DataFrame,\n",
    "    pd_df: pl.DataFrame,\n",
    "    thresholds: Dict[str, Dict[str, float]],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Une ER y PD por (symbol, time_utc), aplica umbrales por s√≠mbolo y devuelve labels:\n",
    "      - symbol       : str\n",
    "      - time_utc     : datetime[UTC]\n",
    "      - regime       : {'TREND','RANGE','NOISE'}\n",
    "      - er_value     : ER_t de la barra (Kaufman)\n",
    "      - pd_value     : PD_t (=1-ER_t)\n",
    "    \"\"\"\n",
    "    if not thresholds:\n",
    "        return pl.DataFrame({\"symbol\": [], \"time_utc\": [], \"regime\": [], \"er_value\": [], \"pd_value\": []})\n",
    "\n",
    "    thr_rows = []\n",
    "    for s, t in thresholds.items():\n",
    "        tr = dict(t)\n",
    "        tr[\"symbol\"] = s\n",
    "        thr_rows.append(tr)\n",
    "\n",
    "    thr_local = (\n",
    "        pl.DataFrame(thr_rows)\n",
    "        .select([\"symbol\", \"ER_hi\", \"ER_lo\", \"PD_hi\", \"PD_lo\"])\n",
    "        .with_columns(pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"))\n",
    "    )\n",
    "\n",
    "    joined = (\n",
    "        er_df.select([\"symbol\", \"time_utc\", \"ER\"])\n",
    "        .join(pd_df.select([\"symbol\", \"time_utc\", \"PD\"]), on=[\"symbol\", \"time_utc\"], how=\"inner\")\n",
    "        .join(thr_local, on=\"symbol\", how=\"inner\")\n",
    "    )\n",
    "\n",
    "    if joined.is_empty():\n",
    "        return pl.DataFrame({\"symbol\": [], \"time_utc\": [], \"regime\": [], \"er_value\": [], \"pd_value\": []})\n",
    "\n",
    "    labels = (\n",
    "        joined.with_columns(\n",
    "            pl.when((pl.col(\"ER\") >= pl.col(\"ER_hi\")) & (pl.col(\"PD\") <= pl.col(\"PD_lo\")))\n",
    "              .then(pl.lit(\"TREND\"))\n",
    "              .when((pl.col(\"ER\") <= pl.col(\"ER_lo\")) & (pl.col(\"PD\") >= pl.col(\"PD_hi\")))\n",
    "              .then(pl.lit(\"RANGE\"))\n",
    "              .otherwise(pl.lit(\"NOISE\"))\n",
    "              .alias(\"regime\")\n",
    "        )\n",
    "        .select(\n",
    "            [\n",
    "                \"symbol\",\n",
    "                \"time_utc\",\n",
    "                \"regime\",\n",
    "                \"ER\",\n",
    "                \"PD\",\n",
    "            ]\n",
    "        )\n",
    "        .rename(\n",
    "            {\n",
    "                \"ER\": \"er_value\",\n",
    "                \"PD\": \"pd_value\",\n",
    "            }\n",
    "        )\n",
    "        .sort([\"symbol\", \"time_utc\"])\n",
    "    )\n",
    "    return labels\n",
    "\n",
    "# Construir dict thresholds requerido por la funci√≥n\n",
    "thr_dict: Dict[str, Dict[str, float]] = {\n",
    "    r[\"symbol\"]: {\n",
    "        \"ER_hi\": r[\"ER_hi\"],\n",
    "        \"ER_lo\": r[\"ER_lo\"],\n",
    "        \"PD_hi\": r[\"PD_hi\"],\n",
    "        \"PD_lo\": r[\"PD_lo\"],\n",
    "    }\n",
    "    for r in rows_thr\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Aplicar hist√©resis y enriquecer labels\n",
    "# -------------------------------\n",
    "labels_df = apply_regime_hysteresis(er_df, pd_df, thr_dict)\n",
    "\n",
    "if labels_df.is_empty():\n",
    "    raise RuntimeError(\"regime_labels vac√≠o tras clasificaci√≥n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# A√±adir etiqueta de sesi√≥n (ASIA / LONDON / NY / OTHER)\n",
    "# ---------------------------------------------------------\n",
    "labels_df = labels_df.with_columns(\n",
    "    pl.col(\"time_utc\").dt.hour().alias(\"hour_utc\")\n",
    ")\n",
    "\n",
    "labels_df = labels_df.with_columns(\n",
    "    pl.when((pl.col(\"hour_utc\") >= 0) & (pl.col(\"hour_utc\") < 7))\n",
    "    .then(pl.lit(\"ASIA\"))\n",
    "    .when((pl.col(\"hour_utc\") >= 7) & (pl.col(\"hour_utc\") < 13))\n",
    "    .then(pl.lit(\"LONDON\"))\n",
    "    .when((pl.col(\"hour_utc\") >= 13) & (pl.col(\"hour_utc\") < 20))\n",
    "    .then(pl.lit(\"NY\"))\n",
    "    .otherwise(pl.lit(\"OTHER\"))\n",
    "    .alias(\"session_label\")\n",
    ").drop(\"hour_utc\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Unir volatilidad base por s√≠mbolo (vol_long_term, vol_p90_abs_ret, vol_regime_flag, n_ret)\n",
    "# ---------------------------------------------------------\n",
    "vol_summary_path = OUT_METRICS_DIR / \"regime_volatility_summary.parquet\"\n",
    "if vol_summary_path.exists():\n",
    "    try:\n",
    "        vol_df = pl.read_parquet(str(vol_summary_path))\n",
    "        vol_df = vol_df.rename({c: c.lower() for c in vol_df.columns})\n",
    "        if \"symbol\" in vol_df.columns:\n",
    "            vol_df = vol_df.with_columns(\n",
    "                pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    "            )\n",
    "            labels_df = labels_df.join(vol_df, on=\"symbol\", how=\"left\")\n",
    "            print(\n",
    "                f\"[Celda 06] Volatilidad base unida desde {vol_summary_path.name} \"\n",
    "                f\"(rows={vol_df.height}, cols={len(vol_df.columns)})\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"[Celda 06][WARN] regime_volatility_summary.parquet no tiene columna 'symbol'; \"\n",
    "                \"no se unen columnas de volatilidad.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 06][WARN] Error uniendo volatilidad base | reason={e}\")\n",
    "else:\n",
    "    print(\n",
    "        \"[Celda 06][WARN] Archivo de resumen de volatilidad no encontrado; \"\n",
    "        \"se omiten columnas vol_long_term/vol_p90_abs_ret/vol_regime_flag/n_ret.\"\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Persistencia en disco (umbrales + labels enriquecidos)\n",
    "# ---------------------------------------------------------\n",
    "thr_out = OUT_METRICS_DIR / \"regime_thresholds.parquet\"\n",
    "lbl_out = OUT_METRICS_DIR / \"regime_labels.parquet\"\n",
    "\n",
    "thr_df.write_parquet(str(thr_out))\n",
    "labels_df.write_parquet(str(lbl_out))\n",
    "\n",
    "print(\n",
    "    f\"üíæ OUTPUT ‚Üí {str(thr_out)} (OK, rows={thr_df.height}, cols={len(thr_df.columns)})\"\n",
    ")\n",
    "print(\n",
    "    f\"üíæ OUTPUT ‚Üí {str(lbl_out)} (OK, rows={labels_df.height}, cols={len(labels_df.columns)})\"\n",
    ")\n",
    "print(\n",
    "    \"[Celda 06] regime_labels incluye columnas m√≠nimas: \"\n",
    "    \"symbol, time_utc, regime, session_label, er_value, pd_value\"\n",
    ")\n",
    "print(\n",
    "    \"[Celda 06] regime_thresholds incluye pct_source_ER/pct_source_PD para auditor√≠a (anti-leak).\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Resumen global de r√©gimen\n",
    "# -------------------------------\n",
    "global_counts = (\n",
    "    labels_df\n",
    "    .group_by(\"regime\")\n",
    "    .len()\n",
    "    .rename({\"len\": \"count\"})\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "total_barras = int(global_counts[\"count\"].sum())\n",
    "\n",
    "print(\"[Celda 06] Distribuci√≥n global de r√©gimen:\")\n",
    "for row in global_counts.iter_rows(named=True):\n",
    "    reg = row[\"regime\"]\n",
    "    cnt = int(row[\"count\"])\n",
    "    pct = cnt / total_barras if total_barras > 0 else 0.0\n",
    "    print(f\"  {reg:6s} ‚Üí barras={cnt:,} ({pct:.2%})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Distribuci√≥n por s√≠mbolo (top 5 por %TREND y %RANGE)\n",
    "# -------------------------------\n",
    "counts = (\n",
    "    labels_df\n",
    "    .group_by([\"symbol\", \"regime\"])\n",
    "    .len()\n",
    "    .rename({\"len\": \"count\"})\n",
    ")\n",
    "totals = (\n",
    "    labels_df\n",
    "    .group_by(\"symbol\")\n",
    "    .len()\n",
    "    .rename({\"len\": \"total\"})\n",
    ")\n",
    "\n",
    "dist = (\n",
    "    counts.join(totals, on=\"symbol\", how=\"inner\")\n",
    "          .with_columns((pl.col(\"count\") / pl.col(\"total\")).alias(\"pct\"))\n",
    "          .select([\"symbol\", \"regime\", \"pct\", \"total\"])\n",
    ")\n",
    "\n",
    "dist_pvt_base = dist.pivot(values=\"pct\", index=\"symbol\", on=\"regime\").fill_null(0.0)\n",
    "for _col in (\"TREND\", \"RANGE\", \"NOISE\"):\n",
    "    if _col not in dist_pvt_base.columns:\n",
    "        dist_pvt_base = dist_pvt_base.with_columns(pl.lit(0.0).alias(_col))\n",
    "\n",
    "dist_pvt = (\n",
    "    dist_pvt_base\n",
    "        .join(totals, on=\"symbol\", how=\"inner\")\n",
    "        .select(\n",
    "            [\n",
    "                \"symbol\",\n",
    "                pl.col(\"TREND\").fill_null(0.0).alias(\"TREND\"),\n",
    "                pl.col(\"RANGE\").fill_null(0.0).alias(\"RANGE\"),\n",
    "                pl.col(\"NOISE\").fill_null(0.0).alias(\"NOISE\"),\n",
    "                \"total\",\n",
    "            ]\n",
    "        )\n",
    ")\n",
    "\n",
    "top_trend = dist_pvt.sort(\"TREND\", descending=True).head(5)\n",
    "print(\"Top-5 s√≠mbolos por % de barras en TREND:\")\n",
    "for row in top_trend.iter_rows(named=True):\n",
    "    print(f\"  symbol={row['symbol']} :: %TREND={row['TREND']:.2%} | %RANGE={row['RANGE']:.2%} | %NOISE={row['NOISE']:.2%}\")\n",
    "\n",
    "top_range = dist_pvt.sort(\"RANGE\", descending=True).head(5)\n",
    "print(\"Top-5 s√≠mbolos por % de barras en RANGE:\")\n",
    "for row in top_range.iter_rows(named=True):\n",
    "    print(f\"  symbol={row['symbol']} :: %TREND={row['TREND']:.2%} | %RANGE={row['RANGE']:.2%} | %NOISE={row['NOISE']:.2%}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Actualizar GLOBAL_STATE y snapshot\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"regime_labels_path\"] = str(lbl_out)\n",
    "GLOBAL_STATE[\"metrics\"][\"regime_thresholds_path\"] = str(thr_out)\n",
    "\n",
    "def _to_jsonable(obj: Any) -> Any:\n",
    "    try:\n",
    "        import pandas as pd  # opcional\n",
    "    except Exception:\n",
    "        pd = None\n",
    "    from pathlib import Path as _Path\n",
    "\n",
    "    if obj is None or isinstance(obj, (str, int, float, bool)):\n",
    "        return obj\n",
    "    if isinstance(obj, _Path):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): _to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [_to_jsonable(x) for x in list(obj)]\n",
    "    if isinstance(obj, pl.DataFrame):\n",
    "        return {\"_type\": \"polars.DataFrame\", \"shape\": [obj.height, len(obj.columns)], \"columns\": obj.columns}\n",
    "    if hasattr(pl, \"LazyFrame\") and isinstance(obj, pl.LazyFrame):\n",
    "        return {\"_type\": \"polars.LazyFrame\"}\n",
    "    if pd is not None and \"pandas\" in str(type(obj)):\n",
    "        try:\n",
    "            shape = list(obj.shape)  # type: ignore[attr-defined]\n",
    "            cols = list(obj.columns)  # type: ignore[attr-defined]\n",
    "        except Exception:\n",
    "            shape, cols = None, None\n",
    "        return {\"_type\": \"pandas.DataFrame\", \"shape\": shape, \"columns\": cols}\n",
    "    return str(obj)\n",
    "\n",
    "snapshot_path = OUT_DIAG_DIR / \"global_state_snapshot_c06.json\"\n",
    "try:\n",
    "    snapshot_txt = json.dumps(_to_jsonable(GLOBAL_STATE), ensure_ascii=False, indent=2, sort_keys=True)\n",
    "    snapshot_path.write_text(snapshot_txt, encoding=\"utf-8\")\n",
    "    if snapshot_path.stat().st_size <= 0:\n",
    "        raise RuntimeError(\"snapshot size=0\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo escribir snapshot GLOBAL_STATE en {str(snapshot_path)} | reason={e}\")\n",
    "\n",
    "print(\">>> Celda 06 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f331ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07 :: Viabilidad econ√≥mica (TR/NDQ) | H=6\n",
      "[Celda 07] RUN_ID        = 20251218_190810\n",
      "[Celda 07] Universo estad√≠stico para viabilidad TR/NDQ = 'universe_gold'\n",
      "[Celda 07] n_symbols     = 84 | muestra = ['AAPL', 'AAVUSD', 'ADAUSD', 'ALGUSD', 'ALVG', 'AMZN', 'AUDCAD', 'AUDCHF', 'AUDJPY', 'AUDNZD']\n",
      "[Celda 07] rates_5m_dir  = C:\\Quant\\MT5_Data_Extraction\\bulk_data\\rates_5m\n",
      "[Celda 07] PAD_DAY_INDEX = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\day_index_m5.parquet\n",
      "[Celda 07] policy.min_TR_after_cost (diag) = 0.25\n",
      "[Celda 07] INPUT ‚Üí regime_labels = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "[Celda 07] INPUT ‚Üí params        = C:\\Quant\\MT5_Data_Extraction\\data\\ea_params.parquet\n",
      "[Celda 07] regime_labels rows (TREND/RANGE & universo stats) = 19429242\n",
      "[Celda 07] Usando GLOBAL_STATE['tables']['params'] (Celda 02) como fuente de params.\n",
      "[Celda 07] params rows (universo stats) = 84\n",
      "[Celda 07][WARN] s√≠mbolo EURUSD sin costos completos; spread=None, commission=0.2, cost_raw=None\n",
      "[Celda 07] S√≠mbolos con costos v√°lidos (universo stats) = 83\n",
      "[Celda 07] progreso: 5/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 10/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 15/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 20/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 25/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 30/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 35/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 40/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 45/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 50/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 55/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 60/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 65/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 70/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 75/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 80/83 s√≠mbolos procesados\n",
      "[Celda 07] progreso: 83/83 s√≠mbolos procesados\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\trend_events.parquet (OK, rows=9119905)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\range_events.parquet (OK, rows=9163602)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\economic_viability.parquet (OK, rows=83, cols=13)\n",
      "[Celda 07] Nota: viab_flag_tr es SOLO diagn√≥stico (no filtra s√≠mbolos en el pipeline).\n",
      "[Celda 07][Costes] n_syms_with_costs = 83 de universo=84\n",
      "[Celda 07][TR_min diag] s√≠mbolos con viab_flag_tr=True = 7\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\economic_viability_meta.json (OK, bytes=1926)\n",
      "Resumen global de viabilidad econ√≥mica:\n",
      "  % s√≠mbolos con TR_trend ‚â• 0.25 = 8.43%\n",
      "  % s√≠mbolos con TR_range ‚â• 0.25 = 8.43%\n",
      "  % s√≠mbolos con TR_ci_low_trend ‚â• 0.25 = 8.43%\n",
      "  % s√≠mbolos con TR_ci_low_range ‚â• 0.25 = 7.23%\n",
      "  NDQ_trend_mean=0.4415 | NDQ_range_mean=0.4872\n",
      "[Celda 07] S√≠mbolos con viab_flag_tr=True:\n",
      "  BTCUSD | TR_trend=0.9543179616637802 | TR_range=0.9541640412485208 | CI_low_trend=0.9531964867221683 | CI_low_range=0.9530143362824866\n",
      "  XAUAUD | TR_trend=0.7464049183355761 | TR_range=0.7447025042707084 | CI_low_trend=0.7438676780047231 | CI_low_range=0.7421178397148499\n",
      "  ETHUSD | TR_trend=0.6435700448575827 | TR_range=0.5897702989407251 | CI_low_trend=0.6410183030658418 | CI_low_range=0.587099581857842\n",
      "  XAUUSD | TR_trend=0.6181595330053083 | TR_range=0.5871302192946493 | CI_low_trend=0.6153279017165443 | CI_low_range=0.5842144518216716\n",
      "  XAUEUR | TR_trend=0.430320612828019 | TR_range=0.38559609634399133 | CI_low_trend=0.4274500038490953 | CI_low_range=0.38271093065700557\n",
      "  BNBUSD | TR_trend=0.4181715540992459 | TR_range=0.3645283353721533 | CI_low_trend=0.4157786605607336 | CI_low_range=0.3621567009584009\n",
      "  LVMH | TR_trend=0.2849596886294134 | TR_range=0.2528179116712778 | CI_low_trend=0.28053291933152186 | CI_low_range=0.24853075758100696\n",
      ">>> Celda 07 :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 07 ‚Äî Viabilidad econ√≥mica (TR/NDQ) ‚Äî SOLO M5\n",
    "# -----------------------------------------------------------------------------\n",
    "# OBJETIVO:\n",
    "#   Evaluar si, incluso con buen r√©gimen (ER/PD), cada s√≠mbolo tiene movimiento\n",
    "#   neto suficiente para cubrir costes de trading:\n",
    "#\n",
    "#   - TR_trend / TR_range ‚âà tasa de eventos que alcanzan movimiento m√≠nimo after-cost.\n",
    "#   - NDQ_trend / NDQ_range ‚âà calidad: fracci√≥n del rango H recorrida a favor\n",
    "#     (continuaci√≥n en TREND, reversi√≥n en RANGE).\n",
    "#\n",
    "#   *** Esta celda eval√∫a \"potencial econ√≥mico\" bruto;\n",
    "#       el edge se confirmar√° con PD/ER en Celdas 08‚Äì09. ***\n",
    "#\n",
    "# PARCHES INSTITUCIONALES (OBLIGATORIOS):\n",
    "#   1) ret/direction se calculan sobre el OHLC COMPLETO, luego se filtra por r√©gimen.\n",
    "#   2) Los forward-extremes (t+1..t+H) y rng_H se calculan sobre OHLC COMPLETO\n",
    "#      para que H represente H barras reales (no H barras de subset).\n",
    "#   3) Convenci√≥n √∫nica de referencia: entry/reference = close(t) para TREND y RANGE.\n",
    "#\n",
    "# OUTPUTS:\n",
    "#   - events/trend_events.parquet\n",
    "#   - events/range_events.parquet\n",
    "#   - metrics/economic_viability.parquet\n",
    "#   - metrics/economic_viability_meta.json\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Se requiere 'polars' para la Celda 07 | reason={e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Validaciones de estado previo\n",
    "# -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas previas.\")\n",
    "\n",
    "required_keys = (\"project_root\", \"run_id\", \"paths\", \"symbols\", \"inputs\", \"config\")\n",
    "missing = [k for k in required_keys if k not in GLOBAL_STATE]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"GLOBAL_STATE incompleto; faltan claves: {missing}\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "RUN_ID = GLOBAL_STATE[\"run_id\"]\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "inputs = GLOBAL_STATE[\"inputs\"]\n",
    "symbols_state = GLOBAL_STATE[\"symbols\"]\n",
    "config = GLOBAL_STATE[\"config\"]\n",
    "\n",
    "for k in (\"metrics\", \"events\", \"diagnostics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"Falta GLOBAL_STATE['paths']['{k}']\")\n",
    "\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_EVENTS_DIR = Path(paths[\"events\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "for d in (OUT_METRICS_DIR, OUT_EVENTS_DIR, OUT_DIAG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Selector de universo estad√≠stico\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"config\", {})\n",
    "UNIVERSE_KEY_DEFAULT = GLOBAL_STATE[\"config\"].get(\"UNIVERSE_KEY_DEFAULT\", \"universe_gold\")\n",
    "GLOBAL_STATE[\"config\"][\"UNIVERSE_KEY_DEFAULT\"] = UNIVERSE_KEY_DEFAULT\n",
    "\n",
    "symbols_dict = GLOBAL_STATE.get(\"symbols\", {})\n",
    "syms_for_stats = symbols_dict.get(UNIVERSE_KEY_DEFAULT, [])\n",
    "\n",
    "if not syms_for_stats:\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 07] Universo '{UNIVERSE_KEY_DEFAULT}' est√° vac√≠o. \"\n",
    "        f\"Revisa Celda 03/02/05 o ajusta UNIVERSE_KEY_DEFAULT.\"\n",
    "    )\n",
    "\n",
    "# Par√°metros de la celda\n",
    "H = 6  # barras M5 (horizonte fijo)\n",
    "\n",
    "policy = config.get(\"policy\", {}) if isinstance(config, dict) else {}\n",
    "\n",
    "# Patch: alinear con config real\n",
    "min_TR = float(\n",
    "    policy.get(\n",
    "        \"min_TR_after_cost\",\n",
    "        policy.get(\"min_tr_after_cost\", 0.50)\n",
    "    )\n",
    ")  # UMBRAL DIAGN√ìSTICO (no gate duro)\n",
    "\n",
    "# Rutas insumo\n",
    "DE_RATES_M5_DIR = Path(inputs.get(\"DE_RATES_M5_DIR\", PROJECT_ROOT / \"bulk_data\" / \"rates_5m\")).resolve()\n",
    "PAD_DAY_INDEX = Path(inputs.get(\"PAD_DAY_INDEX\", PROJECT_ROOT / \"data\" / \"metadata\" / \"day_index_m5.parquet\")).resolve()\n",
    "regime_labels_path = (OUT_METRICS_DIR / \"regime_labels.parquet\").resolve()\n",
    "\n",
    "print(f\">>> Celda 07 :: Viabilidad econ√≥mica (TR/NDQ) | H={H}\")\n",
    "print(f\"[Celda 07] RUN_ID        = {RUN_ID}\")\n",
    "print(f\"[Celda 07] Universo estad√≠stico para viabilidad TR/NDQ = '{UNIVERSE_KEY_DEFAULT}'\")\n",
    "print(f\"[Celda 07] n_symbols     = {len(syms_for_stats)} | muestra = {syms_for_stats[:10]}\")\n",
    "print(f\"[Celda 07] rates_5m_dir  = {str(DE_RATES_M5_DIR)}\")\n",
    "print(f\"[Celda 07] PAD_DAY_INDEX = {str(PAD_DAY_INDEX)}\")\n",
    "print(f\"[Celda 07] policy.min_TR_after_cost (diag) = {min_TR}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Resoluci√≥n robusta de params.*\n",
    "# -------------------------------\n",
    "def _candidate_params_paths() -> List[Path]:\n",
    "    cands: List[Path] = []\n",
    "    cands.append((OUT_DIAG_DIR / \"params_autogen.parquet\").resolve())\n",
    "\n",
    "    user_sel = inputs.get(\"PARAMS_SELECTED_PATH\")\n",
    "    if user_sel:\n",
    "        p = Path(user_sel).resolve()\n",
    "        if p.is_file():\n",
    "            cands.append(p)\n",
    "        elif p.is_dir():\n",
    "            for rel in [\n",
    "                \"params_autogen.parquet\",\n",
    "                \"diagnostics/params_autogen.parquet\",\n",
    "                \"data/ea_params.parquet\",\n",
    "                \"data/ea_params.csv\",\n",
    "                \"ea_params.parquet\",\n",
    "                \"ea_params.csv\",\n",
    "            ]:\n",
    "                cands.append((p / rel).resolve())\n",
    "\n",
    "    cands.append((PROJECT_ROOT / \"data\" / \"ea_params.parquet\").resolve())\n",
    "    cands.append((PROJECT_ROOT / \"data\" / \"ea_params.csv\").resolve())\n",
    "    return cands\n",
    "\n",
    "def _resolve_params_path() -> Optional[Path]:\n",
    "    for cand in _candidate_params_paths():\n",
    "        if cand.exists() and cand.is_file():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "params_path = _resolve_params_path()\n",
    "if not regime_labels_path.exists():\n",
    "    raise RuntimeError(f\"Missing required input: {str(regime_labels_path)}\")\n",
    "if params_path is None:\n",
    "    tried = [str(p) for p in _candidate_params_paths()]\n",
    "    raise RuntimeError(f\"Missing required input: params (no se encontr√≥ archivo v√°lido). Tried: {tried}\")\n",
    "\n",
    "print(f\"[Celda 07] INPUT ‚Üí regime_labels = {str(regime_labels_path)}\")\n",
    "print(f\"[Celda 07] INPUT ‚Üí params        = {str(params_path)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Regime labels\n",
    "# -------------------------------\n",
    "try:\n",
    "    labels_df = pl.read_parquet(\n",
    "        str(regime_labels_path),\n",
    "        columns=[\"symbol\", \"time_utc\", \"regime\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo leer regime_labels: {str(regime_labels_path)} | reason={e}\")\n",
    "\n",
    "labels_df = (\n",
    "    labels_df\n",
    "    .with_columns(pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"))\n",
    "    .filter(pl.col(\"symbol\").is_in(syms_for_stats) & pl.col(\"regime\").is_in([\"TREND\", \"RANGE\"]))\n",
    ")\n",
    "\n",
    "print(f\"[Celda 07] regime_labels rows (TREND/RANGE & universo stats) = {labels_df.height}\")\n",
    "if labels_df.is_empty():\n",
    "    raise RuntimeError(\"regime_labels sin filas para s√≠mbolos del universo estad√≠stico (TREND/RANGE).\")\n",
    "\n",
    "# -------------------------------\n",
    "# Params reader\n",
    "# -------------------------------\n",
    "def _read_params(path: Path) -> pl.DataFrame:\n",
    "    try_order = []\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".parquet\":\n",
    "        try_order = [\"parquet\", \"csv\"]\n",
    "    elif suf == \".csv\":\n",
    "        try_order = [\"csv\", \"parquet\"]\n",
    "    else:\n",
    "        try_order = [\"parquet\", \"csv\"]\n",
    "\n",
    "    last_err = None\n",
    "    for mode in try_order:\n",
    "        try:\n",
    "            df = pl.read_parquet(str(path)) if mode == \"parquet\" else pl.read_csv(str(path))\n",
    "            lower_map = {c: c.lower() for c in df.columns}\n",
    "            df = df.rename(lower_map)\n",
    "\n",
    "            if \"symbol\" not in df.columns or \"commission\" not in df.columns:\n",
    "                raise RuntimeError(f\"Params missing required columns 'symbol'/'commission' en {str(path)}\")\n",
    "\n",
    "            if \"spread_est\" not in df.columns:\n",
    "                df = df.with_columns(pl.lit(None).alias(\"spread_est\"))\n",
    "            if \"spread_rule\" not in df.columns:\n",
    "                df = df.with_columns(pl.lit(None).alias(\"spread_rule\"))\n",
    "\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "                pl.col(\"commission\").cast(pl.Float64),\n",
    "            ])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"No se pudo leer params con parquet/csv: {str(path)} | last_reason={last_err}\")\n",
    "\n",
    "# Patch: reutilizar params de Celda 02 si existen\n",
    "params_df_from_state = None\n",
    "tables_state = GLOBAL_STATE.get(\"tables\")\n",
    "if isinstance(tables_state, dict):\n",
    "    maybe_params = tables_state.get(\"params\")\n",
    "    if isinstance(maybe_params, pl.DataFrame) and not maybe_params.is_empty():\n",
    "        params_df_from_state = maybe_params\n",
    "\n",
    "if params_df_from_state is not None:\n",
    "    print(\"[Celda 07] Usando GLOBAL_STATE['tables']['params'] (Celda 02) como fuente de params.\")\n",
    "    params_df = params_df_from_state.rename({c: c.lower() for c in params_df_from_state.columns})\n",
    "    if \"symbol\" not in params_df.columns or \"commission\" not in params_df.columns:\n",
    "        raise RuntimeError(\"GLOBAL_STATE['tables']['params'] no tiene columnas requeridas 'symbol'/'commission'.\")\n",
    "    if \"spread_est\" not in params_df.columns:\n",
    "        params_df = params_df.with_columns(pl.lit(None).alias(\"spread_est\"))\n",
    "    if \"spread_rule\" not in params_df.columns:\n",
    "        params_df = params_df.with_columns(pl.lit(None).alias(\"spread_rule\"))\n",
    "    params_df = params_df.with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"commission\").cast(pl.Float64),\n",
    "    ])\n",
    "else:\n",
    "    try:\n",
    "        params_df = _read_params(params_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No se pudo leer params: {str(params_path)} | reason={e}\")\n",
    "\n",
    "params_df = params_df.filter(pl.col(\"symbol\").is_in(syms_for_stats))\n",
    "print(f\"[Celda 07] params rows (universo stats) = {params_df.height}\")\n",
    "\n",
    "# -------------------------------\n",
    "# OHLC helpers (M5)\n",
    "# -------------------------------\n",
    "def _find_col_name(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _to_dt_utc(series: pl.Series) -> Optional[pl.Series]:\n",
    "    try:\n",
    "        if str(series.dtype).startswith(\"Datetime\"):\n",
    "            return series.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "        if series.dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Float64, pl.Float32):\n",
    "            s = series.cast(pl.Float64)\n",
    "            med = float(s.drop_nulls().quantile(0.5, interpolation=\"nearest\")) if s.len() else 0.0\n",
    "            unit = \"ms\" if med > 1e11 else \"s\"\n",
    "            return pl.from_epoch(s, time_unit=unit).cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "        if series.dtype == pl.Utf8:\n",
    "            return series.str.strptime(pl.Datetime, strict=False, time_unit=\"us\").dt.replace_time_zone(\"UTC\")\n",
    "        return series.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _read_symbol_from_rates5m(sym: str) -> Optional[pl.DataFrame]:\n",
    "    files = list(DE_RATES_M5_DIR.glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    frames: List[pl.DataFrame] = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = pl.read_parquet(str(fp))\n",
    "        except Exception:\n",
    "            continue\n",
    "        cols = df.columns\n",
    "        sym_col = _find_col_name(cols, [\"symbol\", \"sym\"])\n",
    "        time_col = _find_col_name(cols, [\"time\", \"timestamp\", \"timestamp_utc\", \"datetime\"])\n",
    "        open_col = _find_col_name(cols, [\"open\", \"o\"])\n",
    "        high_col = _find_col_name(cols, [\"high\", \"h\"])\n",
    "        low_col  = _find_col_name(cols, [\"low\", \"l\"])\n",
    "        close_col= _find_col_name(cols, [\"close\", \"c\"])\n",
    "        if time_col is None or close_col is None or high_col is None or low_col is None:\n",
    "            continue\n",
    "        if sym_col is not None:\n",
    "            dff = df.filter(pl.col(sym_col).cast(pl.Utf8).str.to_uppercase() == sym)\n",
    "            if dff.is_empty():\n",
    "                continue\n",
    "        else:\n",
    "            fname = fp.stem.upper()\n",
    "            if sym not in fname:\n",
    "                continue\n",
    "            dff = df\n",
    "        tser = _to_dt_utc(dff.get_column(time_col))\n",
    "        if tser is None:\n",
    "            continue\n",
    "        mini_cols = {\n",
    "            \"time_utc\": tser,\n",
    "            \"open\": dff.get_column(open_col).cast(pl.Float64) if open_col else dff.get_column(close_col).cast(pl.Float64),\n",
    "            \"high\": dff.get_column(high_col).cast(pl.Float64),\n",
    "            \"low\":  dff.get_column(low_col).cast(pl.Float64),\n",
    "            \"close\":dff.get_column(close_col).cast(pl.Float64),\n",
    "        }\n",
    "        mini = pl.DataFrame(mini_cols).drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"]).sort(\"time_utc\")\n",
    "        if not mini.is_empty():\n",
    "            frames.append(mini)\n",
    "    if not frames:\n",
    "        return None\n",
    "    return pl.concat(frames, how=\"vertical_relaxed\").sort(\"time_utc\")\n",
    "\n",
    "def _read_symbol_from_m5clean(sym: str) -> Optional[pl.DataFrame]:\n",
    "    if not PAD_DAY_INDEX.exists():\n",
    "        return None\n",
    "    try:\n",
    "        di = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "    except Exception:\n",
    "        return None\n",
    "    di = di.rename({c: c.lower() for c in di.columns})\n",
    "    if not {\"symbol\", \"path\"}.issubset(set(di.columns)):\n",
    "        return None\n",
    "    di = di.with_columns(pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"))\n",
    "    di = di.filter(pl.col(\"symbol\") == sym)\n",
    "    paths_for_sym = di.get_column(\"path\").to_list() if not di.is_empty() else []\n",
    "    if not paths_for_sym:\n",
    "        return None\n",
    "    frames: List[pl.DataFrame] = []\n",
    "    for p in paths_for_sym:\n",
    "        pp = Path(str(p))\n",
    "        if not pp.exists():\n",
    "            continue\n",
    "        try:\n",
    "            df = pl.read_parquet(str(pp))\n",
    "        except Exception:\n",
    "            continue\n",
    "        cols = df.columns\n",
    "        time_col = _find_col_name(cols, [\"timestamp_utc\", \"time\", \"datetime\", \"timestamp\"])\n",
    "        open_col = _find_col_name(cols, [\"open\",\"o\",\"Open\"])\n",
    "        high_col = _find_col_name(cols, [\"high\",\"h\",\"High\"])\n",
    "        low_col  = _find_col_name(cols, [\"low\",\"l\",\"Low\"])\n",
    "        close_col= _find_col_name(cols, [\"close\",\"c\",\"Close\"])\n",
    "        if time_col is None or high_col is None or low_col is None or close_col is None:\n",
    "            continue\n",
    "        tser = _to_dt_utc(df.get_column(time_col))\n",
    "        if tser is None:\n",
    "            continue\n",
    "        mini = pl.DataFrame({\n",
    "            \"time_utc\": tser,\n",
    "            \"open\": df.get_column(open_col).cast(pl.Float64) if open_col else df.get_column(close_col).cast(pl.Float64),\n",
    "            \"high\": df.get_column(high_col).cast(pl.Float64),\n",
    "            \"low\":  df.get_column(low_col).cast(pl.Float64),\n",
    "            \"close\":df.get_column(close_col).cast(pl.Float64),\n",
    "        }).drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"]).sort(\"time_utc\")\n",
    "        if not mini.is_empty():\n",
    "            frames.append(mini)\n",
    "    if not frames:\n",
    "        return None\n",
    "    return pl.concat(frames, how=\"vertical_relaxed\").sort(\"time_utc\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preparar costos por s√≠mbolo\n",
    "# -------------------------------\n",
    "numeric_dtypes = {pl.Float64, pl.Float32, pl.Int64, pl.Int32, pl.UInt64, pl.UInt32}\n",
    "schema = params_df.schema\n",
    "spread_src_col: Optional[str] = None\n",
    "\n",
    "if \"spread_est\" in schema and schema[\"spread_est\"] in numeric_dtypes:\n",
    "    spread_src_col = \"spread_est\"\n",
    "elif \"spread_rule\" in schema and schema[\"spread_rule\"] in numeric_dtypes:\n",
    "    spread_src_col = \"spread_rule\"\n",
    "\n",
    "if spread_src_col is None:\n",
    "    raise RuntimeError(\n",
    "        \"Params: ninguna de 'spread_est'/'spread_rule' es num√©rica; \"\n",
    "        \"revisa ea_params.parquet / params_autogen (spread_est deber√≠a ser f64).\"\n",
    "    )\n",
    "\n",
    "params_df = params_df.with_columns([\n",
    "    pl.col(spread_src_col).cast(pl.Float64).alias(\"spread_component\"),\n",
    "    pl.col(\"commission\").cast(pl.Float64).alias(\"commission_component\"),\n",
    "])\n",
    "\n",
    "params_df = params_df.with_columns(\n",
    "    (pl.col(\"spread_component\") + pl.col(\"commission_component\")).alias(\"cost_raw\")\n",
    ")\n",
    "\n",
    "zero_or_neg_costs = params_df.filter(\n",
    "    pl.col(\"spread_component\").is_not_null()\n",
    "    & pl.col(\"commission_component\").is_not_null()\n",
    "    & (pl.col(\"cost_raw\") <= 0.0)\n",
    ")\n",
    "\n",
    "if zero_or_neg_costs.height > 0:\n",
    "    print(\"[Celda 07][WARN] s√≠mbolos con cost_after <= 0.0 detectados (se incluyen igualmente en viabilidad TR/NDQ):\")\n",
    "    for r in zero_or_neg_costs.select(\n",
    "        [\"symbol\", \"spread_component\", \"commission_component\", \"cost_raw\"]\n",
    "    ).iter_rows(named=True):\n",
    "        print(\n",
    "            f\"  - {r['symbol']}: spread={r['spread_component']}, \"\n",
    "            f\"commission={r['commission_component']}, cost_raw={r['cost_raw']}\"\n",
    "        )\n",
    "\n",
    "params_df = params_df.with_columns(\n",
    "    (\n",
    "        pl.col(\"spread_component\").is_not_null()\n",
    "        & pl.col(\"commission_component\").is_not_null()\n",
    "    ).alias(\"has_cost\")\n",
    ")\n",
    "\n",
    "invalid_costs = params_df.filter(~pl.col(\"has_cost\"))\n",
    "if invalid_costs.height > 0:\n",
    "    for r in invalid_costs.select(\n",
    "        [\"symbol\", \"spread_component\", \"commission_component\", \"cost_raw\"]\n",
    "    ).iter_rows(named=True):\n",
    "        print(\n",
    "            f\"[Celda 07][WARN] s√≠mbolo {r['symbol']} sin costos completos; \"\n",
    "            f\"spread={r['spread_component']}, commission={r['commission_component']}, cost_raw={r['cost_raw']}\"\n",
    "        )\n",
    "\n",
    "valid_costs = params_df.filter(pl.col(\"has_cost\"))\n",
    "\n",
    "cost_map = {\n",
    "    r[0]: float(r[3])\n",
    "    for r in valid_costs.select(\n",
    "        [\"symbol\", \"spread_component\", \"commission_component\", \"cost_raw\"]\n",
    "    ).iter_rows()\n",
    "}\n",
    "eligible_cost_syms = sorted([s for s in syms_for_stats if s in cost_map])\n",
    "\n",
    "print(f\"[Celda 07] S√≠mbolos con costos v√°lidos (universo stats) = {len(eligible_cost_syms)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluadores de eventos (INSTITUCIONAL)\n",
    "# -------------------------------\n",
    "EVENT_SCHEMA = {\n",
    "    \"symbol\": pl.Utf8,\n",
    "    \"time_utc\": pl.Datetime(time_unit=\"us\", time_zone=\"UTC\"),\n",
    "    \"direction\": pl.Int8,\n",
    "    \"cost_after\": pl.Float64,\n",
    "    \"horizon\": pl.Int32,\n",
    "    \"success\": pl.Int8,\n",
    "    \"ndq_event\": pl.Float64,\n",
    "    \"rng_H\": pl.Float64,\n",
    "    \"family\": pl.Utf8,\n",
    "}\n",
    "\n",
    "def _relu(expr: pl.Expr) -> pl.Expr:\n",
    "    return pl.when(expr > 0.0).then(expr).otherwise(0.0)\n",
    "\n",
    "def _prep_base_ohlc_with_forward_metrics(ohlc: pl.DataFrame, H: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    PREP institucional:\n",
    "      - Ordena/normaliza OHLC.\n",
    "      - Calcula ret_full & direction en la SERIE COMPLETA.\n",
    "      - Calcula max_high_fwd/min_low_fwd en t+1..t+H en la SERIE COMPLETA.\n",
    "      - Calcula rng_H en t..t+H en la SERIE COMPLETA.\n",
    "      - Excluye las √∫ltimas H barras (sin horizonte completo).\n",
    "    \"\"\"\n",
    "    if ohlc.is_empty():\n",
    "        return pl.DataFrame(schema={\n",
    "            \"time_utc\": pl.Datetime(time_unit=\"us\", time_zone=\"UTC\"),\n",
    "            \"open\": pl.Float64, \"high\": pl.Float64, \"low\": pl.Float64, \"close\": pl.Float64,\n",
    "            \"ret_full\": pl.Float64, \"direction\": pl.Int8,\n",
    "            \"max_high_fwd\": pl.Float64, \"min_low_fwd\": pl.Float64, \"rng_H\": pl.Float64\n",
    "        })\n",
    "\n",
    "    base = (\n",
    "        ohlc\n",
    "        .select([\"time_utc\", \"open\", \"high\", \"low\", \"close\"])\n",
    "        .drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "        .sort(\"time_utc\")\n",
    "        .with_columns([\n",
    "            (pl.col(\"close\") - pl.col(\"close\").shift(1)).alias(\"ret_full\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"ret_full\") > 0).then(1)\n",
    "              .when(pl.col(\"ret_full\") < 0).then(-1)\n",
    "              .otherwise(0)\n",
    "              .alias(\"direction\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Forward extremes en t+1..t+H\n",
    "    highs_fwd = [pl.col(\"high\").shift(-k) for k in range(1, H + 1)]\n",
    "    lows_fwd  = [pl.col(\"low\").shift(-k) for k in range(1, H + 1)]\n",
    "    max_high_fwd = pl.max_horizontal(*highs_fwd)\n",
    "    min_low_fwd  = pl.min_horizontal(*lows_fwd)\n",
    "\n",
    "    # Rango en t..t+H\n",
    "    highs_H = [pl.col(\"high\")] + [pl.col(\"high\").shift(-k) for k in range(1, H + 1)]\n",
    "    lows_H  = [pl.col(\"low\")]  + [pl.col(\"low\").shift(-k) for k in range(1, H + 1)]\n",
    "    max_high_H = pl.max_horizontal(*highs_H)\n",
    "    min_low_H  = pl.min_horizontal(*lows_H)\n",
    "\n",
    "    base = base.with_columns([\n",
    "        max_high_fwd.alias(\"max_high_fwd\"),\n",
    "        min_low_fwd.alias(\"min_low_fwd\"),\n",
    "        (max_high_H - min_low_H + 1e-9).alias(\"rng_H\"),\n",
    "        # Validez de horizonte completo (excluye √∫ltimas H barras)\n",
    "        pl.col(\"high\").shift(-H).is_not_null().alias(\"_has_horizon\"),\n",
    "    ]).filter(pl.col(\"_has_horizon\") == True).drop(\"_has_horizon\")\n",
    "\n",
    "    return base\n",
    "\n",
    "def evaluate_trend_events(\n",
    "    ohlc: pl.DataFrame, labels_sym: pl.DataFrame, cost: float, H: int\n",
    ") -> pl.DataFrame:\n",
    "    if ohlc.is_empty() or labels_sym.is_empty():\n",
    "        return pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "\n",
    "    base = _prep_base_ohlc_with_forward_metrics(ohlc, H)\n",
    "    if base.is_empty():\n",
    "        return pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "\n",
    "    df = (\n",
    "        base.join(labels_sym.filter(pl.col(\"regime\") == \"TREND\"), on=\"time_utc\", how=\"inner\")\n",
    "        .with_columns([\n",
    "            pl.lit(cost).alias(\"cost_after\"),\n",
    "            pl.lit(H).cast(pl.Int32).alias(\"horizon\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ENTRY/REF unificado = close(t)\n",
    "    success_pos = ((pl.col(\"max_high_fwd\") - pl.col(\"close\")) >= pl.col(\"cost_after\"))\n",
    "    success_neg = ((pl.col(\"close\") - pl.col(\"min_low_fwd\")) >= pl.col(\"cost_after\"))\n",
    "\n",
    "    move_pos = _relu(pl.col(\"max_high_fwd\") - pl.col(\"close\"))\n",
    "    move_neg = _relu(pl.col(\"close\") - pl.col(\"min_low_fwd\"))\n",
    "\n",
    "    ndq = (\n",
    "        pl.when(pl.col(\"direction\") == 1).then(move_pos / pl.col(\"rng_H\"))\n",
    "          .when(pl.col(\"direction\") == -1).then(move_neg / pl.col(\"rng_H\"))\n",
    "          .otherwise(None)\n",
    "    )\n",
    "\n",
    "    success = (\n",
    "        pl.when(pl.col(\"direction\") == 1).then(success_pos)\n",
    "          .when(pl.col(\"direction\") == -1).then(success_neg)\n",
    "          .otherwise(False)\n",
    "    )\n",
    "\n",
    "    sym_val = labels_sym.get_column(\"symbol\")[0] if labels_sym.height > 0 else None\n",
    "\n",
    "    out = (\n",
    "        df\n",
    "        .filter(\n",
    "            (pl.col(\"direction\") != 0)\n",
    "            & pl.col(\"max_high_fwd\").is_not_null()\n",
    "            & pl.col(\"min_low_fwd\").is_not_null()\n",
    "            & pl.col(\"rng_H\").is_not_null()\n",
    "        )\n",
    "        .select([\n",
    "            pl.lit(sym_val).cast(pl.Utf8).alias(\"symbol\"),\n",
    "            \"time_utc\",\n",
    "            pl.col(\"direction\").cast(pl.Int8),\n",
    "            pl.col(\"cost_after\").cast(pl.Float64),\n",
    "            pl.col(\"horizon\"),\n",
    "            success.cast(pl.Int8).alias(\"success\"),\n",
    "            ndq.cast(pl.Float64).alias(\"ndq_event\"),\n",
    "            pl.col(\"rng_H\").cast(pl.Float64),\n",
    "        ])\n",
    "        .with_columns(pl.lit(\"TREND\").alias(\"family\"))\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def evaluate_range_events(\n",
    "    ohlc: pl.DataFrame, labels_sym: pl.DataFrame, cost: float, H: int\n",
    ") -> pl.DataFrame:\n",
    "    if ohlc.is_empty() or labels_sym.is_empty():\n",
    "        return pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "\n",
    "    base = _prep_base_ohlc_with_forward_metrics(ohlc, H)\n",
    "    if base.is_empty():\n",
    "        return pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "\n",
    "    df = (\n",
    "        base.join(labels_sym.filter(pl.col(\"regime\") == \"RANGE\"), on=\"time_utc\", how=\"inner\")\n",
    "        .with_columns([\n",
    "            pl.lit(cost).alias(\"cost_after\"),\n",
    "            pl.lit(H).cast(pl.Int32).alias(\"horizon\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # ENTRY/REF unificado = close(t)\n",
    "    # Reversi√≥n:\n",
    "    # dir=+1 (sube)  => √©xito si cae >= cost desde close(t) dentro de t+1..t+H\n",
    "    # dir=-1 (baja)  => √©xito si sube >= cost desde close(t) dentro de t+1..t+H\n",
    "    success_pos = (pl.col(\"min_low_fwd\") <= (pl.col(\"close\") - pl.col(\"cost_after\")))\n",
    "    success_neg = (pl.col(\"max_high_fwd\") >= (pl.col(\"close\") + pl.col(\"cost_after\")))\n",
    "\n",
    "    move_pos = _relu(pl.col(\"close\") - pl.col(\"min_low_fwd\"))\n",
    "    move_neg = _relu(pl.col(\"max_high_fwd\") - pl.col(\"close\"))\n",
    "\n",
    "    ndq = (\n",
    "        pl.when(pl.col(\"direction\") == 1).then(move_pos / pl.col(\"rng_H\"))\n",
    "          .when(pl.col(\"direction\") == -1).then(move_neg / pl.col(\"rng_H\"))\n",
    "          .otherwise(None)\n",
    "    )\n",
    "\n",
    "    success = (\n",
    "        pl.when(pl.col(\"direction\") == 1).then(success_pos)\n",
    "          .when(pl.col(\"direction\") == -1).then(success_neg)\n",
    "          .otherwise(False)\n",
    "    )\n",
    "\n",
    "    sym_val = labels_sym.get_column(\"symbol\")[0] if labels_sym.height > 0 else None\n",
    "\n",
    "    out = (\n",
    "        df\n",
    "        .filter(\n",
    "            (pl.col(\"direction\") != 0)\n",
    "            & pl.col(\"max_high_fwd\").is_not_null()\n",
    "            & pl.col(\"min_low_fwd\").is_not_null()\n",
    "            & pl.col(\"rng_H\").is_not_null()\n",
    "        )\n",
    "        .select([\n",
    "            pl.lit(sym_val).cast(pl.Utf8).alias(\"symbol\"),\n",
    "            \"time_utc\",\n",
    "            pl.col(\"direction\").cast(pl.Int8),\n",
    "            pl.col(\"cost_after\").cast(pl.Float64),\n",
    "            pl.col(\"horizon\"),\n",
    "            success.cast(pl.Int8).alias(\"success\"),\n",
    "            ndq.cast(pl.Float64).alias(\"ndq_event\"),\n",
    "            pl.col(\"rng_H\").cast(pl.Float64),\n",
    "        ])\n",
    "        .with_columns(pl.lit(\"RANGE\").alias(\"family\"))\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Procesamiento por s√≠mbolo\n",
    "# -------------------------------\n",
    "trend_events_frames: List[pl.DataFrame] = []\n",
    "range_events_frames: List[pl.DataFrame] = []\n",
    "\n",
    "total_syms = len(eligible_cost_syms)\n",
    "for i, sym in enumerate(eligible_cost_syms, start=1):\n",
    "    lab_sym = labels_df.filter(pl.col(\"symbol\") == sym)\n",
    "    if lab_sym.is_empty():\n",
    "        continue\n",
    "\n",
    "    ohlc = _read_symbol_from_rates5m(sym)\n",
    "    if ohlc is None or ohlc.is_empty():\n",
    "        ohlc = _read_symbol_from_m5clean(sym)\n",
    "    if ohlc is None or ohlc.is_empty():\n",
    "        print(f\"[Celda 07][WARN] s√≠mbolo {sym} sin OHLC disponible; se omite.\")\n",
    "        continue\n",
    "\n",
    "    ohlc = (\n",
    "        ohlc\n",
    "        .select([\"time_utc\", \"open\", \"high\", \"low\", \"close\"])\n",
    "        .drop_nulls(subset=[\"time_utc\",\"high\",\"low\",\"close\"])\n",
    "        .sort(\"time_utc\")\n",
    "        .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "    cost_after = float(cost_map[sym])\n",
    "\n",
    "    trend_ev = evaluate_trend_events(ohlc, lab_sym, cost_after, H)\n",
    "    range_ev = evaluate_range_events(ohlc, lab_sym, cost_after, H)\n",
    "\n",
    "    if not trend_ev.is_empty():\n",
    "        trend_events_frames.append(trend_ev.with_columns(pl.lit(sym).alias(\"symbol\")))\n",
    "    if not range_ev.is_empty():\n",
    "        range_events_frames.append(range_ev.with_columns(pl.lit(sym).alias(\"symbol\")))\n",
    "\n",
    "    if (i % 5 == 0) or (i == total_syms):\n",
    "        print(f\"[Celda 07] progreso: {i}/{total_syms} s√≠mbolos procesados\")\n",
    "\n",
    "trend_events = pl.concat(trend_events_frames, how=\"vertical_relaxed\") if trend_events_frames else pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "range_events = pl.concat(range_events_frames, how=\"vertical_relaxed\") if range_events_frames else pl.DataFrame(schema=EVENT_SCHEMA)\n",
    "\n",
    "trend_path = OUT_EVENTS_DIR / \"trend_events.parquet\"\n",
    "range_path = OUT_EVENTS_DIR / \"range_events.parquet\"\n",
    "trend_events.write_parquet(str(trend_path))\n",
    "range_events.write_parquet(str(range_path))\n",
    "\n",
    "if trend_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Failed writing {str(trend_path)}\")\n",
    "if range_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Failed writing {str(range_path)}\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(trend_path)} (OK, rows={trend_events.height})\")\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(range_path)} (OK, rows={range_events.height})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Agregaci√≥n por s√≠mbolo: TR, NDQ y TR_ci_low (Wilson 95%)\n",
    "# -------------------------------\n",
    "def _agg_family(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"TR\": pl.Float64,\n",
    "            \"NDQ\": pl.Float64,\n",
    "            \"candidates\": pl.Int64,\n",
    "            \"success\": pl.Int64,\n",
    "            \"TR_ci_low\": pl.Float64,\n",
    "        })\n",
    "\n",
    "    grp = df.group_by(\"symbol\")\n",
    "    out = grp.agg([\n",
    "        (pl.col(\"success\").sum().cast(pl.Float64) / (pl.len().cast(pl.Float64))).alias(\"TR\"),\n",
    "        pl.col(\"ndq_event\").mean().alias(\"NDQ\"),\n",
    "        pl.len().alias(\"candidates\"),\n",
    "        pl.col(\"success\").sum().alias(\"success\"),\n",
    "    ])\n",
    "\n",
    "    # Intervalo de Wilson 95%\n",
    "    z = 1.96\n",
    "    z2 = z * z\n",
    "    n = pl.col(\"candidates\").cast(pl.Float64)\n",
    "    p = pl.col(\"TR\")\n",
    "\n",
    "    inner = (p * (1.0 - p) + z2 / (4.0 * n)) / n\n",
    "    sqrt_inner = (pl.when(inner >= 0.0).then(inner).otherwise(0.0)) ** 0.5\n",
    "\n",
    "    tr_ci_low_expr = (p + z2 / (2.0 * n) - z * sqrt_inner) / (1.0 + z2 / n)\n",
    "\n",
    "    out = out.with_columns(\n",
    "        pl.when(n > 0.0).then(tr_ci_low_expr).otherwise(None).alias(\"TR_ci_low\")\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(\"TR_ci_low\") < 0.0).then(0.0)\n",
    "          .when(pl.col(\"TR_ci_low\") > 1.0).then(1.0)\n",
    "          .otherwise(pl.col(\"TR_ci_low\"))\n",
    "          .alias(\"TR_ci_low\")\n",
    "    )\n",
    "\n",
    "    out = out.with_columns(pl.col(\"symbol\").cast(pl.Utf8))\n",
    "    return out\n",
    "\n",
    "trend_agg = _agg_family(trend_events).rename({\n",
    "    \"TR\": \"TR_trend\",\n",
    "    \"NDQ\": \"NDQ_trend\",\n",
    "    \"candidates\": \"trend_candidates\",\n",
    "    \"success\": \"trend_success\",\n",
    "    \"TR_ci_low\": \"TR_ci_low_trend\",\n",
    "}).with_columns(pl.col(\"symbol\").cast(pl.Utf8))\n",
    "\n",
    "range_agg = _agg_family(range_events).rename({\n",
    "    \"TR\": \"TR_range\",\n",
    "    \"NDQ\": \"NDQ_range\",\n",
    "    \"candidates\": \"range_candidates\",\n",
    "    \"success\": \"range_success\",\n",
    "    \"TR_ci_low\": \"TR_ci_low_range\",\n",
    "}).with_columns(pl.col(\"symbol\").cast(pl.Utf8))\n",
    "\n",
    "base_syms_df = pl.DataFrame({\"symbol\": eligible_cost_syms}).with_columns(\n",
    "    pl.col(\"symbol\").cast(pl.Utf8)\n",
    ")\n",
    "\n",
    "ev_df = (\n",
    "    base_syms_df\n",
    "    .join(trend_agg, on=\"symbol\", how=\"left\")\n",
    "    .join(range_agg, on=\"symbol\", how=\"left\")\n",
    "    .with_columns(\n",
    "        pl.lit(min_TR).cast(pl.Float64).alias(\"TR_after_cost_min\"),\n",
    "        (\n",
    "            (pl.col(\"TR_trend\").is_not_null() & (pl.col(\"TR_trend\") >= min_TR)) |\n",
    "            (pl.col(\"TR_range\").is_not_null() & (pl.col(\"TR_range\") >= min_TR))\n",
    "        ).alias(\"viab_flag_tr\")\n",
    "    )\n",
    ")\n",
    "\n",
    "econ_path = OUT_METRICS_DIR / \"economic_viability.parquet\"\n",
    "ev_df.write_parquet(str(econ_path))\n",
    "if econ_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Failed writing {str(econ_path)}\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(econ_path)} (OK, rows={ev_df.height}, cols={len(ev_df.columns)})\")\n",
    "print(\"[Celda 07] Nota: viab_flag_tr es SOLO diagn√≥stico (no filtra s√≠mbolos en el pipeline).\")\n",
    "\n",
    "# -------------------------------\n",
    "# Contadores diagn√≥sticos para report_stats\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "\n",
    "n_syms_with_costs = len(eligible_cost_syms)\n",
    "n_after_TR_min = int(ev_df.filter(pl.col(\"viab_flag_tr\") == True).height) if not ev_df.is_empty() else 0\n",
    "\n",
    "GLOBAL_STATE[\"report_stats\"][\"c07\"] = {\n",
    "    \"universe_key\": UNIVERSE_KEY_DEFAULT,\n",
    "    \"universe_size\": int(len(syms_for_stats)),\n",
    "    \"n_syms_with_costs\": int(n_syms_with_costs),\n",
    "    \"TR_threshold\": float(min_TR),\n",
    "    \"n_syms_after_TR_min\": int(n_after_TR_min),\n",
    "}\n",
    "\n",
    "print(f\"[Celda 07][Costes] n_syms_with_costs = {n_syms_with_costs} de universo={len(syms_for_stats)}\")\n",
    "print(f\"[Celda 07][TR_min diag] s√≠mbolos con viab_flag_tr=True = {n_after_TR_min}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Meta JSON\n",
    "# -------------------------------\n",
    "meta_obj = {\n",
    "    \"H\": H,\n",
    "    \"universe_key\": UNIVERSE_KEY_DEFAULT,\n",
    "    \"definitions\": {\n",
    "        \"cost_after\": \"spread_component (spread_est or spread_rule) + commission_component\",\n",
    "        \"direction\": \"sign(close_t - close_{t-1}) calculado sobre OHLC COMPLETO (antes de filtrar por r√©gimen)\",\n",
    "        \"trend_event\": \"label=TREND & direction=sign(close_t - close_{t-1}) != 0\",\n",
    "        \"trend_success\": \"dir=+1: max_high_{t+1..t+H} - close_t >= cost; dir=-1: close_t - min_low_{t+1..t+H} >= cost\",\n",
    "        \"range_event\": \"label=RANGE & direction=sign(close_t - close_{t-1}) != 0\",\n",
    "        \"range_success\": \"dir=+1: min_low_{t+1..t+H} <= close_t - cost; dir=-1: max_high_{t+1..t+H} >= close_t + cost\",\n",
    "        \"NDQ_trend_event\": \"move_continuation/rng_H (ref=close_t)\",\n",
    "        \"NDQ_range_event\": \"move_reversion/rng_H (ref=close_t)\",\n",
    "        \"rng_H\": \"max(high_{t..t+H}) - min(low_{t..t+H}) (calculado sobre OHLC COMPLETO)\",\n",
    "        \"TR_ci_low_trend\": \"l√≠mite inferior (Wilson 95%) de TR_trend\",\n",
    "        \"TR_ci_low_range\": \"l√≠mite inferior (Wilson 95%) de TR_range\",\n",
    "        \"viab_flag_tr\": \"diagn√≥stico: (TR_trend >= TR_after_cost_min) OR (TR_range >= TR_after_cost_min); NO gate.\",\n",
    "        \"note\": \"No es backtesting; solo conteo local de eventos y magnitudes after-cost sobre horizonte fijo H (barras M5 reales). \"\n",
    "                \"Esta celda eval√∫a 'potencial econ√≥mico' bruto; el edge se confirmar√° con PD/ER en Celdas 08‚Äì09.\"\n",
    "    },\n",
    "    \"ci_method\": {\n",
    "        \"TR_ci_low\": \"Wilson score interval, 95% confidence, z=1.96\"\n",
    "    },\n",
    "    \"created_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"inputs\": {\n",
    "        \"regime_labels\": str(regime_labels_path),\n",
    "        \"params\": str(params_path),\n",
    "        \"rates_5m_dir\": str(DE_RATES_M5_DIR),\n",
    "        \"pad_day_index\": str(PAD_DAY_INDEX) if PAD_DAY_INDEX.exists() else None\n",
    "    }\n",
    "}\n",
    "meta_path = OUT_METRICS_DIR / \"economic_viability_meta.json\"\n",
    "meta_txt = json.dumps(meta_obj, ensure_ascii=False, indent=2, sort_keys=True)\n",
    "meta_path.write_text(meta_txt, encoding=\"utf-8\")\n",
    "if meta_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(f\"Failed writing {str(meta_path)}\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(meta_path)} (OK, bytes={meta_path.stat().st_size})\")\n",
    "\n",
    "# -------------------------------\n",
    "# Res√∫menes\n",
    "# -------------------------------\n",
    "def _pct_ge(series: pl.Series, thr: float) -> float:\n",
    "    if series is None or series.len() == 0:\n",
    "        return 0.0\n",
    "    valid = series.drop_nulls()\n",
    "    if valid.len() == 0:\n",
    "        return 0.0\n",
    "    return 100.0 * float((valid >= thr).sum()) / float(valid.len())\n",
    "\n",
    "pct_trend = _pct_ge(\n",
    "    ev_df.get_column(\"TR_trend\") if \"TR_trend\" in ev_df.columns else pl.Series(\"x\", [], dtype=pl.Float64),\n",
    "    min_TR\n",
    ")\n",
    "pct_range = _pct_ge(\n",
    "    ev_df.get_column(\"TR_range\") if \"TR_range\" in ev_df.columns else pl.Series(\"x\", [], dtype=pl.Float64),\n",
    "    min_TR\n",
    ")\n",
    "\n",
    "pct_trend_ci = _pct_ge(\n",
    "    ev_df.get_column(\"TR_ci_low_trend\") if \"TR_ci_low_trend\" in ev_df.columns else pl.Series(\"x\", [], dtype=pl.Float64),\n",
    "    min_TR\n",
    ")\n",
    "pct_range_ci = _pct_ge(\n",
    "    ev_df.get_column(\"TR_ci_low_range\") if \"TR_ci_low_range\" in ev_df.columns else pl.Series(\"x\", [], dtype=pl.Float64),\n",
    "    min_TR\n",
    ")\n",
    "\n",
    "ndq_trend_mean = float(trend_events.get_column(\"ndq_event\").mean()) if not trend_events.is_empty() else float(\"nan\")\n",
    "ndq_range_mean = float(range_events.get_column(\"ndq_event\").mean()) if not range_events.is_empty() else float(\"nan\")\n",
    "\n",
    "print(\"Resumen global de viabilidad econ√≥mica:\")\n",
    "print(f\"  % s√≠mbolos con TR_trend ‚â• {min_TR} = {pct_trend:.2f}%\")\n",
    "print(f\"  % s√≠mbolos con TR_range ‚â• {min_TR} = {pct_range:.2f}%\")\n",
    "print(f\"  % s√≠mbolos con TR_ci_low_trend ‚â• {min_TR} = {pct_trend_ci:.2f}%\")\n",
    "print(f\"  % s√≠mbolos con TR_ci_low_range ‚â• {min_TR} = {pct_range_ci:.2f}%\")\n",
    "print(f\"  NDQ_trend_mean={ndq_trend_mean:.4f} | NDQ_range_mean={ndq_range_mean:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Actualizar GLOBAL_STATE y snapshot\n",
    "# -------------------------------\n",
    "GLOBAL_STATE.setdefault(\"events\", {})\n",
    "GLOBAL_STATE[\"events\"][\"trend_events_path\"] = str(trend_path)\n",
    "GLOBAL_STATE[\"events\"][\"range_events_path\"] = str(range_path)\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"economic_viability_path\"] = str(econ_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"economic_viability_meta_path\"] = str(meta_path)\n",
    "\n",
    "snapshot_path = OUT_DIAG_DIR / \"global_state_snapshot_c07.json\"\n",
    "try:\n",
    "    snap_dict = {\n",
    "        \"project_root\": GLOBAL_STATE.get(\"project_root\"),\n",
    "        \"run_id\": GLOBAL_STATE.get(\"run_id\"),\n",
    "        \"paths\": {\n",
    "            \"events\": GLOBAL_STATE[\"events\"],\n",
    "            \"metrics\": {\n",
    "                \"economic_viability_path\": GLOBAL_STATE[\"metrics\"][\"economic_viability_path\"],\n",
    "                \"economic_viability_meta_path\": GLOBAL_STATE[\"metrics\"][\"economic_viability_meta_path\"],\n",
    "            },\n",
    "            \"diagnostics\": str(OUT_DIAG_DIR),\n",
    "        },\n",
    "        \"params_used\": str(params_path),\n",
    "        \"H\": H,\n",
    "        \"min_TR_threshold\": min_TR,\n",
    "        \"universe_key\": UNIVERSE_KEY_DEFAULT,\n",
    "        \"universe_size\": len(syms_for_stats),\n",
    "        \"n_syms_with_costs\": n_syms_with_costs,\n",
    "        \"n_syms_after_TR_min\": n_after_TR_min,\n",
    "        \"ci_method\": \"Wilson score interval, 95% confidence, z=1.96\",\n",
    "        \"institutional_patches\": [\n",
    "            \"direction computed on full OHLC before regime filtering\",\n",
    "            \"forward extremes and rng_H computed on full OHLC to enforce fixed H bars\",\n",
    "            \"entry/reference unified to close(t) for TREND and RANGE\",\n",
    "        ],\n",
    "    }\n",
    "    snapshot_path.write_text(json.dumps(snap_dict, ensure_ascii=False, indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo escribir snapshot GLOBAL_STATE en {str(snapshot_path)} | reason={e}\")\n",
    "\n",
    "print(\"[Celda 07] S√≠mbolos con viab_flag_tr=True:\")\n",
    "if not ev_df.is_empty():\n",
    "    flagged = ev_df.filter(pl.col(\"viab_flag_tr\") == True).select([\n",
    "        \"symbol\", \"TR_trend\", \"TR_range\", \"TR_ci_low_trend\", \"TR_ci_low_range\"\n",
    "    ]).sort([\"TR_trend\", \"TR_range\"], descending=True)\n",
    "    for r in flagged.iter_rows(named=True):\n",
    "        print(\n",
    "            f\"  {r['symbol']} | TR_trend={r['TR_trend']} | TR_range={r['TR_range']} | \"\n",
    "            f\"CI_low_trend={r['TR_ci_low_trend']} | CI_low_range={r['TR_ci_low_range']}\"\n",
    "        )\n",
    "\n",
    "print(\">>> Celda 07 :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0ce791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07B :: M√©tricas de estructura (diagn√≥stico + score)\n",
      "[Celda 07B][CONFIG] enabled=True | min_events=100 | small_move_thr=0.5 | score_strict=0.3 | score_loose=0.6\n",
      "[Celda 07B][DIAG] events_total=18,283,507 | null_cost=0 | cost<=0=0 | null_rng_H=0 | null_ndq=0\n",
      "[Celda 07B][DIAG] cost_p50=4.15044 | rng_p50=0.016 | cost/rng‚âà259\n",
      "[Celda 07B][DIAG] ret_units_abs null tras guardrails suaves: 7,951,436 / 18,283,507 (43.49%)\n",
      "-------------------------------------------------------------------------------\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\structure_summary.parquet (rows=166, symbols=83)\n",
      "[Celda 07B] sample_flag :: {'OK': 166}\n",
      "[Celda 07B] qa_flag     :: {'BAD': 97, 'GOOD': 21, 'WARN': 48}\n",
      ">>> Celda 07B :: OK\n"
     ]
    }
   ],
   "source": [
    "# ======================= Celda 07B ‚Äî M√©tricas de estructura (diagn√≥stico + score) =======================\n",
    "# OBJETIVO:\n",
    "#   A√±adir una capa de diagn√≥stico + scoring de estructura por s√≠mbolo y familia (TREND/RANGE),\n",
    "#   bas√°ndonos en los eventos generados en la Celda 07.\n",
    "#\n",
    "#   Esta versi√≥n est√° \"blindada\":\n",
    "#     - Lee config de forma robusta.\n",
    "#     - Protege unidades con guardrails suaves (no rompe pipeline).\n",
    "#     - Prints diagn√≥sticos m√≠nimos (resumen corto y √∫til).\n",
    "#     - NO cambia el dise√±o del pipeline ni nombres de outputs/keys.\n",
    "#\n",
    "# INPUTS:\n",
    "#   - events/trend_events.parquet\n",
    "#   - events/range_events.parquet\n",
    "#\n",
    "# OUTPUT:\n",
    "#   - metrics/structure_summary.parquet\n",
    "#\n",
    "# GLOBAL_STATE:\n",
    "#   - Lee:\n",
    "#       GLOBAL_STATE[\"paths\"][\"events\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"metrics\"]\n",
    "#       GLOBAL_STATE[\"config\"] / GLOBAL_STATE[\"config_sections\"][\"structure\"]\n",
    "#   - Escribe:\n",
    "#       GLOBAL_STATE[\"metrics\"][\"structure_summary_path\"]\n",
    "# ===========================================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 07B :: M√©tricas de estructura (diagn√≥stico + score)\")\n",
    "\n",
    "# ------------------------------- Guardas b√°sicas --------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 07B][ERROR] GLOBAL_STATE no existe. Ejecuta la Celda 00 primero.\")\n",
    "\n",
    "paths: Dict[str, Any] = GLOBAL_STATE.get(\"paths\", {})\n",
    "if not isinstance(paths, dict):\n",
    "    raise RuntimeError(\"[Celda 07B][ERROR] GLOBAL_STATE['paths'] no es un dict. Revisa Celda 00.\")\n",
    "\n",
    "for k in (\"events\", \"metrics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 07B][ERROR] Falta GLOBAL_STATE['paths']['{k}']. Revisa Celda 00/07.\")\n",
    "\n",
    "OUT_EVENTS_DIR = Path(paths[\"events\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------- Config de estructura ---------------------------\n",
    "CFG          = GLOBAL_STATE.get(\"config\", {}) or {}\n",
    "CFG_SECTIONS = GLOBAL_STATE.get(\"config_sections\", {}) or {}\n",
    "\n",
    "if isinstance(CFG_SECTIONS, dict) and \"structure\" in CFG_SECTIONS:\n",
    "    structure_cfg = CFG_SECTIONS.get(\"structure\", {}) or {}\n",
    "else:\n",
    "    structure_cfg = CFG.get(\"structure\", {}) or {}\n",
    "\n",
    "if not isinstance(structure_cfg, dict):\n",
    "    structure_cfg = {}\n",
    "\n",
    "STRUCT_ENABLED        = bool(structure_cfg.get(\"enabled\", False))\n",
    "MIN_EVENTS_PER_SYMBOL = int(structure_cfg.get(\"min_events_per_symbol\", 100))\n",
    "SMALL_MOVE_THRESHOLD  = float(structure_cfg.get(\"small_move_threshold\", 0.5))\n",
    "\n",
    "score_cfg = structure_cfg.get(\"score\", {}) or {}\n",
    "if not isinstance(score_cfg, dict):\n",
    "    score_cfg = {}\n",
    "\n",
    "# Defaults de respaldo (la calibraci√≥n real vive en config.json)\n",
    "SM_MAX_STRICT = float(score_cfg.get(\"small_move_max_strict\", 0.4))\n",
    "SM_MAX_LOOSE  = float(score_cfg.get(\"small_move_max_loose\", 0.8))\n",
    "\n",
    "SK_ABS_STRICT = float(score_cfg.get(\"skew_abs_max_strict\", 1.0))\n",
    "SK_ABS_LOOSE  = float(score_cfg.get(\"skew_abs_max_loose\", 2.5))\n",
    "\n",
    "KURT_STRICT   = float(score_cfg.get(\"kurt_max_strict\", 4.0))\n",
    "KURT_LOOSE    = float(score_cfg.get(\"kurt_max_loose\", 8.0))\n",
    "\n",
    "W_SMALL_MOVE  = float(score_cfg.get(\"w_small_move\", 0.5))\n",
    "W_SKEW        = float(score_cfg.get(\"w_skew\", 0.25))\n",
    "W_KURT        = float(score_cfg.get(\"w_kurt\", 0.25))\n",
    "\n",
    "SCORE_STRICT_CFG = float(score_cfg.get(\"score_strict\", 0.3))\n",
    "SCORE_LOOSE_CFG  = float(score_cfg.get(\"score_loose\", 0.6))\n",
    "SCORE_STRICT, SCORE_LOOSE = sorted([SCORE_STRICT_CFG, SCORE_LOOSE_CFG])\n",
    "\n",
    "# Guardrails suaves de unidades (opcionales)\n",
    "guard_cfg = structure_cfg.get(\"guardrails\", {}) or {}\n",
    "if not isinstance(guard_cfg, dict):\n",
    "    guard_cfg = {}\n",
    "\n",
    "COST_MIN_EPS           = float(guard_cfg.get(\"cost_min_eps\", 1e-9))\n",
    "NDQ_MIN                = float(guard_cfg.get(\"ndq_min\", 0.0))\n",
    "NDQ_MAX                = float(guard_cfg.get(\"ndq_max\", 1.0))\n",
    "MAX_COST_TO_RANGE_MULT = float(guard_cfg.get(\"max_cost_to_range_mult\", 100.0))\n",
    "MAX_RET_UNITS_ABS      = float(guard_cfg.get(\"max_ret_units_abs\", 1000.0))\n",
    "\n",
    "print(\n",
    "    f\"[Celda 07B][CONFIG] enabled={STRUCT_ENABLED} | min_events={MIN_EVENTS_PER_SYMBOL} | \"\n",
    "    f\"small_move_thr={SMALL_MOVE_THRESHOLD} | score_strict={SCORE_STRICT} | score_loose={SCORE_LOOSE}\"\n",
    ")\n",
    "\n",
    "# Si est√° desactivado ‚Üí no rompemos pipeline ni GLOBAL_STATE\n",
    "if not STRUCT_ENABLED:\n",
    "    print(\"[Celda 07B] structure.enabled=False ‚Üí se omite c√°lculo.\")\n",
    "    print(\">>> Celda 07B :: SKIPPED\")\n",
    "else:\n",
    "    # ------------------------------- Rutas de eventos ---------------------------\n",
    "    trend_events_path      = OUT_EVENTS_DIR / \"trend_events.parquet\"\n",
    "    range_events_path      = OUT_EVENTS_DIR / \"range_events.parquet\"\n",
    "    structure_summary_path = OUT_METRICS_DIR / \"structure_summary.parquet\"\n",
    "\n",
    "    if (not trend_events_path.exists()) and (not range_events_path.exists()):\n",
    "        print(\"[Celda 07B] No existen archivos de eventos TREND/RANGE ‚Üí se omite.\")\n",
    "        print(\">>> Celda 07B :: SKIPPED\")\n",
    "    else:\n",
    "        # ------------------------------- Lectura segura de eventos --------------\n",
    "        def _empty_events_df() -> pl.DataFrame:\n",
    "            return pl.DataFrame(\n",
    "                schema={\n",
    "                    \"symbol\":     pl.Utf8,\n",
    "                    \"direction\":  pl.Int8,\n",
    "                    \"cost_after\": pl.Float64,\n",
    "                    \"ndq_event\":  pl.Float64,\n",
    "                    \"rng_H\":      pl.Float64,\n",
    "                    \"family\":     pl.Utf8,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        def _safe_read_events(path: Path, label: str) -> pl.DataFrame:\n",
    "            if not path.exists():\n",
    "                return _empty_events_df()\n",
    "            try:\n",
    "                df = pl.read_parquet(\n",
    "                    str(path),\n",
    "                    columns=[\"symbol\", \"direction\", \"cost_after\", \"ndq_event\", \"rng_H\", \"family\"],\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"[Celda 07B][ERROR] No se pudo leer {label}: {path} | reason={e}\")\n",
    "\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\"),\n",
    "                pl.col(\"direction\").cast(pl.Int8),\n",
    "                pl.col(\"cost_after\").cast(pl.Float64),\n",
    "                pl.col(\"ndq_event\").cast(pl.Float64),\n",
    "                pl.col(\"rng_H\").cast(pl.Float64),\n",
    "                pl.col(\"family\").cast(pl.Utf8),\n",
    "            ])\n",
    "            return df\n",
    "\n",
    "        trend_events = _safe_read_events(trend_events_path, \"trend_events.parquet\")\n",
    "        range_events = _safe_read_events(range_events_path, \"range_events.parquet\")\n",
    "\n",
    "        if trend_events.is_empty() and range_events.is_empty():\n",
    "            print(\"[Celda 07B] Eventos vac√≠os ‚Üí se omite.\")\n",
    "            print(\">>> Celda 07B :: SKIPPED\")\n",
    "        else:\n",
    "            # ------------------------------- Unificar eventos ---------------------\n",
    "            events = pl.concat([trend_events, range_events], how=\"vertical_relaxed\").rechunk()\n",
    "\n",
    "            # ------------------------------- Diagn√≥stico m√≠nimo -------------------\n",
    "            # Conteos b√°sicos de nulos / valores no v√°lidos sin imprimir listados largos\n",
    "            diag_counts = events.select([\n",
    "                pl.len().alias(\"events_total\"),\n",
    "                pl.col(\"cost_after\").is_null().sum().alias(\"null_cost\"),\n",
    "                ((pl.col(\"cost_after\").is_not_null()) & (pl.col(\"cost_after\") <= 0.0)).sum().alias(\"cost_le_0\"),\n",
    "                pl.col(\"rng_H\").is_null().sum().alias(\"null_rng_H\"),\n",
    "                pl.col(\"ndq_event\").is_null().sum().alias(\"null_ndq\"),\n",
    "            ]).to_dicts()[0]\n",
    "\n",
    "            print(\n",
    "                \"[Celda 07B][DIAG] \"\n",
    "                f\"events_total={int(diag_counts['events_total']):,} | \"\n",
    "                f\"null_cost={int(diag_counts['null_cost']):,} | cost<=0={int(diag_counts['cost_le_0']):,} | \"\n",
    "                f\"null_rng_H={int(diag_counts['null_rng_H']):,} | null_ndq={int(diag_counts['null_ndq']):,}\"\n",
    "            )\n",
    "\n",
    "            # Magnitudes robustas (medianas)\n",
    "            # Nota: esto ayuda a detectar desalineaciones t√≠picas de unidades sin bloquear nada.\n",
    "            mag = events.select([\n",
    "                pl.col(\"cost_after\").drop_nulls().quantile(0.50).alias(\"cost_p50\"),\n",
    "                pl.col(\"rng_H\").drop_nulls().quantile(0.50).alias(\"rng_p50\"),\n",
    "            ]).to_dicts()[0]\n",
    "\n",
    "            cost_p50 = float(mag.get(\"cost_p50\")) if mag.get(\"cost_p50\") is not None else float(\"nan\")\n",
    "            rng_p50  = float(mag.get(\"rng_p50\")) if mag.get(\"rng_p50\") is not None else float(\"nan\")\n",
    "\n",
    "            if (not (cost_p50 != cost_p50)) and (not (rng_p50 != rng_p50)) and rng_p50 > 0:\n",
    "                ratio_cost_rng = cost_p50 / rng_p50\n",
    "                print(f\"[Celda 07B][DIAG] cost_p50={cost_p50:.6g} | rng_p50={rng_p50:.6g} | cost/rng‚âà{ratio_cost_rng:.3g}\")\n",
    "            else:\n",
    "                print(f\"[Celda 07B][DIAG] cost_p50={cost_p50} | rng_p50={rng_p50}\")\n",
    "\n",
    "            # ------------------------------- Guardrails suaves --------------------\n",
    "            # 1) NDQ debe estar en rango razonable (no forzamos clamp; solo anulamos outliers)\n",
    "            ndq_clean = (\n",
    "                pl.when(pl.col(\"ndq_event\").is_null())\n",
    "                  .then(None)\n",
    "                  .when((pl.col(\"ndq_event\") >= NDQ_MIN) & (pl.col(\"ndq_event\") <= NDQ_MAX))\n",
    "                  .then(pl.col(\"ndq_event\"))\n",
    "                  .otherwise(None)\n",
    "            )\n",
    "\n",
    "            # 2) move_price = ndq_clean * rng_H\n",
    "            events = events.with_columns([\n",
    "                ndq_clean.alias(\"_ndq_clean\"),\n",
    "                (ndq_clean * pl.col(\"rng_H\")).alias(\"_move_price\"),\n",
    "            ])\n",
    "\n",
    "            # 3) ret_units_abs con defensas suaves:\n",
    "            #    - cost_after debe ser > COST_MIN_EPS\n",
    "            #    - rng_H debe ser > 0\n",
    "            #    - cost_after no debe ser absurdamente mayor que el rango del evento\n",
    "            ret_units_abs_expr = (\n",
    "                pl.when(\n",
    "                    (pl.col(\"cost_after\").is_not_null()) &\n",
    "                    (pl.col(\"cost_after\") > COST_MIN_EPS) &\n",
    "                    (pl.col(\"rng_H\").is_not_null()) &\n",
    "                    (pl.col(\"rng_H\") > 0.0) &\n",
    "                    (pl.col(\"_move_price\").is_not_null()) &\n",
    "                    (pl.col(\"cost_after\") <= (MAX_COST_TO_RANGE_MULT * pl.col(\"rng_H\") + COST_MIN_EPS))\n",
    "                )\n",
    "                .then(pl.col(\"_move_price\") / pl.col(\"cost_after\"))\n",
    "                .otherwise(None)\n",
    "            )\n",
    "\n",
    "            events = events.with_columns([\n",
    "                ret_units_abs_expr.alias(\"ret_units_abs\"),\n",
    "            ])\n",
    "\n",
    "            # 4) Cap suave de outliers extremos de ret_units_abs (no error; solo null)\n",
    "            events = events.with_columns([\n",
    "                pl.when(pl.col(\"ret_units_abs\").is_not_null() & (pl.col(\"ret_units_abs\") > MAX_RET_UNITS_ABS))\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(\"ret_units_abs\"))\n",
    "                  .alias(\"ret_units_abs\")\n",
    "            ])\n",
    "\n",
    "            # 5) ret_units_signed\n",
    "            events = events.with_columns([\n",
    "                (pl.col(\"direction\").cast(pl.Float64) * pl.col(\"ret_units_abs\")).alias(\"ret_units_signed\")\n",
    "            ])\n",
    "\n",
    "            # Mini-print de impacto guardrails\n",
    "            # (contador de nulos en ret_units_abs tras defensas)\n",
    "            null_ret_units = events.select([\n",
    "                pl.col(\"ret_units_abs\").is_null().sum().alias(\"null_ret_units\"),\n",
    "                pl.len().alias(\"events_total\"),\n",
    "            ]).to_dicts()[0]\n",
    "            n_null_ru = int(null_ret_units[\"null_ret_units\"])\n",
    "            n_tot_ru  = int(null_ret_units[\"events_total\"])\n",
    "            pct_null  = (100.0 * n_null_ru / n_tot_ru) if n_tot_ru > 0 else 0.0\n",
    "            print(f\"[Celda 07B][DIAG] ret_units_abs null tras guardrails suaves: {n_null_ru:,} / {n_tot_ru:,} ({pct_null:.2f}%)\")\n",
    "\n",
    "            # Limpieza de columnas auxiliares para que no \"ensucien\" el output final\n",
    "            # (se remueven despu√©s del summary)\n",
    "            # ------------------------------- Agregaci√≥n por s√≠mbolo/family --------\n",
    "            structure_summary = (\n",
    "                events\n",
    "                .group_by([\"symbol\", \"family\"])\n",
    "                .agg([\n",
    "                    pl.len().alias(\"n_events\"),\n",
    "\n",
    "                    pl.col(\"ret_units_signed\").mean().alias(\"ret_units_mean\"),\n",
    "                    pl.col(\"ret_units_signed\").std(ddof=1).alias(\"ret_units_std\"),\n",
    "                    pl.col(\"ret_units_signed\").skew().alias(\"ret_units_skew\"),\n",
    "                    pl.col(\"ret_units_signed\").kurtosis().alias(\"ret_units_kurt\"),\n",
    "                    pl.col(\"ret_units_signed\").quantile(0.50).alias(\"ret_units_p50\"),\n",
    "                    pl.col(\"ret_units_signed\").quantile(0.90).alias(\"ret_units_p90\"),\n",
    "\n",
    "                    (pl.col(\"ret_units_abs\") < SMALL_MOVE_THRESHOLD)\n",
    "                        .cast(pl.Float64)\n",
    "                        .mean()\n",
    "                        .alias(\"small_move_ratio\"),\n",
    "                ])\n",
    "                .with_columns([\n",
    "                    pl.when(pl.col(\"n_events\") >= MIN_EVENTS_PER_SYMBOL)\n",
    "                      .then(pl.lit(\"OK\"))\n",
    "                      .otherwise(pl.lit(\"LOW_SAMPLE\"))\n",
    "                      .alias(\"structure_sample_flag\")\n",
    "                ])\n",
    "                .sort([\"symbol\", \"family\"])\n",
    "            )\n",
    "\n",
    "            # ------------------------------- Score de estructura -----------------\n",
    "            # Asegurar tipos float m√≠nimos\n",
    "            structure_summary = structure_summary.with_columns([\n",
    "                pl.col(\"small_move_ratio\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"ret_units_skew\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"ret_units_kurt\").cast(pl.Float64, strict=False),\n",
    "            ])\n",
    "\n",
    "            # Sub-score small_move_ratio\n",
    "            sm_strict, sm_loose = sorted([SM_MAX_STRICT, SM_MAX_LOOSE])\n",
    "            denom_sm = (sm_loose - sm_strict) if (sm_loose - sm_strict) != 0.0 else 1.0\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"small_move_ratio\").is_null())\n",
    "                  .then(None)\n",
    "                  .when(pl.col(\"small_move_ratio\") <= sm_strict)\n",
    "                  .then(pl.lit(1.0))\n",
    "                  .when(pl.col(\"small_move_ratio\") >= sm_loose)\n",
    "                  .then(pl.lit(0.0))\n",
    "                  .otherwise(1.0 - ((pl.col(\"small_move_ratio\") - sm_strict) / denom_sm))\n",
    "                  .alias(\"score_small_move\")\n",
    "            )\n",
    "\n",
    "            # Sub-score |skew|\n",
    "            sk_strict, sk_loose = sorted([SK_ABS_STRICT, SK_ABS_LOOSE])\n",
    "            denom_sk = (sk_loose - sk_strict) if (sk_loose - sk_strict) != 0.0 else 1.0\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"ret_units_skew\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(\"ret_units_skew\").abs())\n",
    "                  .alias(\"ret_units_skew_abs\")\n",
    "            )\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"ret_units_skew_abs\").is_null())\n",
    "                  .then(None)\n",
    "                  .when(pl.col(\"ret_units_skew_abs\") <= sk_strict)\n",
    "                  .then(pl.lit(1.0))\n",
    "                  .when(pl.col(\"ret_units_skew_abs\") >= sk_loose)\n",
    "                  .then(pl.lit(0.0))\n",
    "                  .otherwise(1.0 - ((pl.col(\"ret_units_skew_abs\") - sk_strict) / denom_sk))\n",
    "                  .alias(\"score_skew\")\n",
    "            )\n",
    "\n",
    "            # Sub-score kurtosis\n",
    "            kt_strict, kt_loose = sorted([KURT_STRICT, KURT_LOOSE])\n",
    "            denom_kt = (kt_loose - kt_strict) if (kt_loose - kt_strict) != 0.0 else 1.0\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"ret_units_kurt\").is_null())\n",
    "                  .then(None)\n",
    "                  .otherwise(pl.col(\"ret_units_kurt\"))\n",
    "                  .alias(\"ret_units_kurt_val\")\n",
    "            )\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"ret_units_kurt_val\").is_null())\n",
    "                  .then(None)\n",
    "                  .when(pl.col(\"ret_units_kurt_val\") <= kt_strict)\n",
    "                  .then(pl.lit(1.0))\n",
    "                  .when(pl.col(\"ret_units_kurt_val\") >= kt_loose)\n",
    "                  .then(pl.lit(0.0))\n",
    "                  .otherwise(1.0 - ((pl.col(\"ret_units_kurt_val\") - kt_strict) / denom_kt))\n",
    "                  .alias(\"score_kurt\")\n",
    "            )\n",
    "\n",
    "            # Media ponderada ignorando nulls\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.lit(W_SMALL_MOVE).alias(\"_w_sm\"),\n",
    "                pl.lit(W_SKEW).alias(\"_w_sk\"),\n",
    "                pl.lit(W_KURT).alias(\"_w_kt\"),\n",
    "            )\n",
    "\n",
    "            num_expr = (\n",
    "                (pl.col(\"score_small_move\") * pl.col(\"_w_sm\")).fill_null(0.0)\n",
    "                + (pl.col(\"score_skew\") * pl.col(\"_w_sk\")).fill_null(0.0)\n",
    "                + (pl.col(\"score_kurt\") * pl.col(\"_w_kt\")).fill_null(0.0)\n",
    "            )\n",
    "\n",
    "            den_expr = (\n",
    "                pl.when(pl.col(\"score_small_move\").is_not_null()).then(pl.col(\"_w_sm\")).otherwise(0.0)\n",
    "                + pl.when(pl.col(\"score_skew\").is_not_null()).then(pl.col(\"_w_sk\")).otherwise(0.0)\n",
    "                + pl.when(pl.col(\"score_kurt\").is_not_null()).then(pl.col(\"_w_kt\")).otherwise(0.0)\n",
    "            )\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                den_expr.alias(\"_w_den\")\n",
    "            )\n",
    "\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"_w_den\") > 0.0)\n",
    "                  .then(num_expr / pl.col(\"_w_den\"))\n",
    "                  .otherwise(None)\n",
    "                  .alias(\"structure_score\")\n",
    "            )\n",
    "\n",
    "            structure_summary = structure_summary.drop([\"_w_sm\", \"_w_sk\", \"_w_kt\", \"_w_den\"])\n",
    "\n",
    "            # Flags QA (NO gate)\n",
    "            structure_summary = structure_summary.with_columns(\n",
    "                pl.when(pl.col(\"n_events\") < MIN_EVENTS_PER_SYMBOL)\n",
    "                  .then(pl.lit(\"BAD\"))\n",
    "                  .when(pl.col(\"structure_score\").is_null())\n",
    "                  .then(pl.lit(\"WARN\"))\n",
    "                  .when(pl.col(\"structure_score\") >= SCORE_LOOSE)\n",
    "                  .then(pl.lit(\"GOOD\"))\n",
    "                  .when(pl.col(\"structure_score\") >= SCORE_STRICT)\n",
    "                  .then(pl.lit(\"WARN\"))\n",
    "                  .otherwise(pl.lit(\"BAD\"))\n",
    "                  .alias(\"structure_flag_qa\")\n",
    "            )\n",
    "\n",
    "            # Limpieza de columnas auxiliares internas si quedaron en memoria (no est√°n en summary)\n",
    "            # ------------------------------- Persistencia ------------------------\n",
    "            structure_summary.write_parquet(str(structure_summary_path), compression=\"zstd\")\n",
    "\n",
    "            GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "            GLOBAL_STATE[\"metrics\"][\"structure_summary_path\"] = str(structure_summary_path)\n",
    "\n",
    "            # ------------------------------- Resumen corto ------------------------\n",
    "            n_rows = structure_summary.height\n",
    "            n_syms = structure_summary.get_column(\"symbol\").n_unique() if n_rows > 0 else 0\n",
    "\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "            print(f\"üíæ OUTPUT ‚Üí {structure_summary_path} (rows={n_rows}, symbols={n_syms})\")\n",
    "\n",
    "            if n_rows > 0:\n",
    "                by_sample_flag = (\n",
    "                    structure_summary\n",
    "                    .group_by(\"structure_sample_flag\")\n",
    "                    .agg(pl.len().alias(\"n_rows\"))\n",
    "                    .sort(\"structure_sample_flag\")\n",
    "                )\n",
    "                by_struct_flag = (\n",
    "                    structure_summary\n",
    "                    .group_by(\"structure_flag_qa\")\n",
    "                    .agg(pl.len().alias(\"n_rows\"))\n",
    "                    .sort(\"structure_flag_qa\")\n",
    "                )\n",
    "\n",
    "                # Prints m√≠nimos\n",
    "                sf_map = {r[\"structure_sample_flag\"]: int(r[\"n_rows\"]) for r in by_sample_flag.iter_rows(named=True)}\n",
    "                qf_map = {r[\"structure_flag_qa\"]: int(r[\"n_rows\"]) for r in by_struct_flag.iter_rows(named=True)}\n",
    "\n",
    "                print(f\"[Celda 07B] sample_flag :: {sf_map}\")\n",
    "                print(f\"[Celda 07B] qa_flag     :: {qf_map}\")\n",
    "\n",
    "            print(\">>> Celda 07B :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e17c2c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 07C :: Microestructura b√°sica (volumen & volatilidad por s√≠mbolo)\n",
      "[Celda 07C][CONFIG] enabled=True | min_bars=10000 | liq_strict=0.3 | liq_loose=0.6 | illq_strict=0.2 | illq_loose=0.5 | cv_strict=1.0 | cv_loose=2.0\n",
      "[Celda 07C] RUN_ID=20251218_190810 | universe='universe_gold' | n_symbols=84\n",
      "-------------------------------------------------------------------------------\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\microstructure_summary.parquet (rows=84, symbols=84)\n",
      "[Celda 07C] skipped_no_data=0 | low_sample=0 | no_volume=0\n",
      "[Celda 07C] liquidity_flag :: {'BAD': 4, 'GOOD': 54, 'WARN': 26}\n",
      "[Celda 07C] vol_flag      :: {'HIGH_VOL': 21, 'LOW_VOL': 21, 'MID_VOL': 42}\n",
      "[Celda 07C] NOTA: scores/flags son diagn√≥sticos suaves; NO gates.\n",
      ">>> Celda 07C :: OK\n"
     ]
    }
   ],
   "source": [
    "# ========================= Celda 07C ‚Äî Microestructura: volumen & volatilidad =========================\n",
    "# OBJETIVO:\n",
    "#   A√±adir una capa ligera de an√°lisis de microestructura por s√≠mbolo (solo M5) para complementar:\n",
    "#     - Viabilidad econ√≥mica (Celda 07, TR/NDQ).\n",
    "#     - Estructura de eventos (Celda 07B, skew/kurt/small_moves).\n",
    "#\n",
    "#   Esta celda NO filtra s√≠mbolos ni aplica gates duros. Solo construye m√©tricas y scores\n",
    "#   que se mezclar√°n m√°s adelante en el score global (Celdas 10B‚Äì10C).\n",
    "#\n",
    "# OUTPUT:\n",
    "#   - metrics/microstructure_summary.parquet\n",
    "# ================================================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Se requiere 'polars' para la Celda 07C | reason={e}\")\n",
    "\n",
    "print(\">>> Celda 07C :: Microestructura b√°sica (volumen & volatilidad por s√≠mbolo)\")\n",
    "\n",
    "# -------------------------------- Guardas b√°sicas de GLOBAL_STATE -------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 07C][ERROR] GLOBAL_STATE no existe. Ejecuta Celdas 00‚Äì04 primero.\")\n",
    "\n",
    "paths: Dict[str, Any] = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "inputs: Dict[str, Any] = GLOBAL_STATE.get(\"inputs\", {}) or {}\n",
    "config: Dict[str, Any] = GLOBAL_STATE.get(\"config\", {}) or {}\n",
    "config_sections: Dict[str, Any] = GLOBAL_STATE.get(\"config_sections\", {}) or {}\n",
    "\n",
    "for key in (\"metrics\", \"diagnostics\"):\n",
    "    if key not in paths:\n",
    "        raise RuntimeError(f\"[Celda 07C][ERROR] Falta GLOBAL_STATE['paths']['{key}']. Revisa Celda 00.\")\n",
    "\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE.get(\"project_root\", \".\")).resolve()\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\", \"UNKNOWN_RUN\")\n",
    "\n",
    "# -------------------------------- Config microstructure -----------------------------------------\n",
    "if isinstance(config_sections, dict) and \"microstructure\" in config_sections:\n",
    "    micro_cfg = config_sections.get(\"microstructure\", {}) or {}\n",
    "else:\n",
    "    micro_cfg = config.get(\"microstructure\", {}) or {}\n",
    "if not isinstance(micro_cfg, dict):\n",
    "    micro_cfg = {}\n",
    "\n",
    "MICRO_ENABLED = bool(micro_cfg.get(\"enabled\", True))\n",
    "MIN_BARS_PER_SYMBOL = int(micro_cfg.get(\"min_bars_per_symbol\", 10_000))\n",
    "\n",
    "vol_cfg = micro_cfg.get(\"volume\", {}) or {}\n",
    "if not isinstance(vol_cfg, dict):\n",
    "    vol_cfg = {}\n",
    "\n",
    "ILLQ_MAX_STRICT = float(vol_cfg.get(\"illiquid_bar_max_strict\", 0.20))\n",
    "ILLQ_MAX_LOOSE  = float(vol_cfg.get(\"illiquid_bar_max_loose\", 0.50))\n",
    "CV_MAX_STRICT   = float(vol_cfg.get(\"cv_max_strict\", 1.0))\n",
    "CV_MAX_LOOSE    = float(vol_cfg.get(\"cv_max_loose\", 2.0))\n",
    "\n",
    "W_LEVEL   = float(vol_cfg.get(\"w_level\", 0.6))\n",
    "W_ILLQ    = float(vol_cfg.get(\"w_illiquid\", 0.25))\n",
    "W_CV      = float(vol_cfg.get(\"w_cv\", 0.15))\n",
    "SCORE_STRICT_CFG = float(vol_cfg.get(\"score_strict\", 0.3))\n",
    "SCORE_LOOSE_CFG  = float(vol_cfg.get(\"score_loose\", 0.6))\n",
    "SCORE_STRICT, SCORE_LOOSE = sorted([SCORE_STRICT_CFG, SCORE_LOOSE_CFG])\n",
    "\n",
    "vol_vol_cfg = micro_cfg.get(\"vol\", {}) or {}\n",
    "if not isinstance(vol_vol_cfg, dict):\n",
    "    vol_vol_cfg = {}\n",
    "\n",
    "ATR_WINDOW   = int(vol_vol_cfg.get(\"atr_window\", 14))  # no cambia c√°lculo core; queda como par√°metro futuro\n",
    "VOL_Q_LO     = float(vol_vol_cfg.get(\"score_q_lo\", 0.10))\n",
    "VOL_Q_LO_MID = float(vol_vol_cfg.get(\"score_q_lo_mid\", 0.25))\n",
    "VOL_Q_HI_MID = float(vol_vol_cfg.get(\"score_q_hi_mid\", 0.75))\n",
    "VOL_Q_HI     = float(vol_vol_cfg.get(\"score_q_hi\", 0.90))\n",
    "\n",
    "# Prints m√≠nimos\n",
    "print(\n",
    "    f\"[Celda 07C][CONFIG] enabled={MICRO_ENABLED} | min_bars={MIN_BARS_PER_SYMBOL} | \"\n",
    "    f\"liq_strict={SCORE_STRICT} | liq_loose={SCORE_LOOSE} | \"\n",
    "    f\"illq_strict={ILLQ_MAX_STRICT} | illq_loose={ILLQ_MAX_LOOSE} | \"\n",
    "    f\"cv_strict={CV_MAX_STRICT} | cv_loose={CV_MAX_LOOSE}\"\n",
    ")\n",
    "\n",
    "if not MICRO_ENABLED:\n",
    "    print(\">>> Celda 07C :: SKIPPED (microstructure.enabled=False)\")\n",
    "else:\n",
    "    # -------------------------------- Universo y PAD_DAY_INDEX ----------------------------------\n",
    "    GLOBAL_STATE.setdefault(\"config\", {})\n",
    "    UNIVERSE_KEY_DEFAULT = GLOBAL_STATE[\"config\"].get(\"UNIVERSE_KEY_DEFAULT\", \"universe_gold\")\n",
    "    GLOBAL_STATE[\"config\"][\"UNIVERSE_KEY_DEFAULT\"] = UNIVERSE_KEY_DEFAULT\n",
    "\n",
    "    symbols_dict = GLOBAL_STATE.get(\"symbols\", {}) or {}\n",
    "    syms_for_stats: List[str] = symbols_dict.get(UNIVERSE_KEY_DEFAULT, []) or []\n",
    "    syms_for_stats = [str(s).upper().strip() for s in syms_for_stats]\n",
    "\n",
    "    if not syms_for_stats:\n",
    "        raise RuntimeError(\n",
    "            f\"[Celda 07C] Universo '{UNIVERSE_KEY_DEFAULT}' est√° vac√≠o. \"\n",
    "            \"Revisa Celdas 03‚Äì05 o ajusta UNIVERSE_KEY_DEFAULT.\"\n",
    "        )\n",
    "\n",
    "    PAD_DAY_INDEX = Path(\n",
    "        inputs.get(\"PAD_DAY_INDEX\", PROJECT_ROOT / \"data\" / \"metadata\" / \"day_index_m5.parquet\")\n",
    "    ).resolve()\n",
    "\n",
    "    if not PAD_DAY_INDEX.exists():\n",
    "        raise RuntimeError(f\"[Celda 07C][ERROR] PAD_DAY_INDEX no existe: {str(PAD_DAY_INDEX)}\")\n",
    "\n",
    "    print(\n",
    "        f\"[Celda 07C] RUN_ID={RUN_ID} | universe='{UNIVERSE_KEY_DEFAULT}' | \"\n",
    "        f\"n_symbols={len(syms_for_stats)}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------- Volatility summary (opcional) -----------------------------\n",
    "    vol_summary_path = OUT_METRICS_DIR / \"regime_volatility_summary.parquet\"\n",
    "    vol_df: Optional[pl.DataFrame] = None\n",
    "    if vol_summary_path.exists():\n",
    "        try:\n",
    "            vol_df = pl.read_parquet(str(vol_summary_path))\n",
    "            vol_df = vol_df.rename({c: c.lower() for c in vol_df.columns})\n",
    "            if \"symbol\" in vol_df.columns:\n",
    "                vol_df = vol_df.with_columns(\n",
    "                    pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    "                )\n",
    "            else:\n",
    "                vol_df = None\n",
    "        except Exception:\n",
    "            vol_df = None\n",
    "\n",
    "    # -------------------------------- Helpers OHLCV ---------------------------------------------\n",
    "    def _find_col_name(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "        cols_lower = {c.lower(): c for c in cols}\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in cols_lower:\n",
    "                return cols_lower[cand.lower()]\n",
    "        return None\n",
    "\n",
    "    def _to_dt_utc(series: pl.Series) -> Optional[pl.Series]:\n",
    "        try:\n",
    "            if str(series.dtype).startswith(\"Datetime\"):\n",
    "                return series.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "            if series.dtype in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32, pl.Float64, pl.Float32):\n",
    "                s = series.cast(pl.Float64)\n",
    "                if s.len() == 0:\n",
    "                    return None\n",
    "                nn = s.drop_nulls()\n",
    "                if nn.len() == 0:\n",
    "                    return None\n",
    "                med = float(nn.quantile(0.5, interpolation=\"nearest\"))\n",
    "                unit = \"ms\" if med > 1e11 else \"s\"\n",
    "                return pl.from_epoch(s, time_unit=unit).cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "            if series.dtype == pl.Utf8:\n",
    "                return series.str.strptime(pl.Datetime, strict=False, time_unit=\"us\").dt.replace_time_zone(\"UTC\")\n",
    "            return series.cast(pl.Datetime(\"us\")).dt.replace_time_zone(\"UTC\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _choose_volume_series(df: pl.DataFrame) -> Optional[pl.Series]:\n",
    "        \"\"\"\n",
    "        Selecci√≥n robusta de volumen:\n",
    "        1) tick_volume\n",
    "        2) volume\n",
    "        3) real_volume\n",
    "        4) alias\n",
    "        Preferimos columna con datos no nulos y no todos 0.\n",
    "        \"\"\"\n",
    "        if df.height == 0:\n",
    "            return None\n",
    "\n",
    "        cols_lower = {c.lower(): c for c in df.columns}\n",
    "        priority = [\"tick_volume\", \"volume\", \"real_volume\", \"tickvol\", \"vol\"]\n",
    "\n",
    "        # Preferir con se√±al real\n",
    "        for cand in priority:\n",
    "            if cand in cols_lower:\n",
    "                col = cols_lower[cand]\n",
    "                try:\n",
    "                    s = df.get_column(col).cast(pl.Float64)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                nn = s.drop_nulls()\n",
    "                if nn.len() == 0:\n",
    "                    continue\n",
    "                max_abs = float(nn.abs().max())\n",
    "                if max_abs > 0.0:\n",
    "                    return s\n",
    "\n",
    "        # Aceptar aunque sea degenerado\n",
    "        for cand in priority:\n",
    "            if cand in cols_lower:\n",
    "                col = cols_lower[cand]\n",
    "                try:\n",
    "                    s = df.get_column(col).cast(pl.Float64)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if s.drop_nulls().len() > 0:\n",
    "                    return s\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _read_symbol_ohlcv(sym: str) -> Optional[pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Lee OHLCV M5 v√≠a PAD_DAY_INDEX.\n",
    "        Devuelve: time_utc, open, high, low, close, volume (si se puede).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            di = pl.read_parquet(str(PAD_DAY_INDEX))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        di = di.rename({c: c.lower() for c in di.columns})\n",
    "        if not {\"symbol\", \"path\"}.issubset(set(di.columns)):\n",
    "            return None\n",
    "\n",
    "        di = di.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    "        )\n",
    "        di_sym = di.filter(pl.col(\"symbol\") == sym)\n",
    "        paths_for_sym = di_sym.get_column(\"path\").to_list() if not di_sym.is_empty() else []\n",
    "        if not paths_for_sym:\n",
    "            return None\n",
    "\n",
    "        frames: List[pl.DataFrame] = []\n",
    "        for p in paths_for_sym:\n",
    "            pp = Path(str(p))\n",
    "            if not pp.exists() or not pp.is_file():\n",
    "                continue\n",
    "            try:\n",
    "                df = pl.read_parquet(str(pp))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            cols = df.columns\n",
    "            time_col = _find_col_name(cols, [\"timestamp_utc\", \"time\", \"datetime\", \"timestamp\"])\n",
    "            open_col = _find_col_name(cols, [\"open\", \"Open\", \"o\", \"O\"])\n",
    "            high_col = _find_col_name(cols, [\"high\", \"High\", \"h\", \"H\"])\n",
    "            low_col  = _find_col_name(cols, [\"low\", \"Low\", \"l\", \"L\"])\n",
    "            close_col= _find_col_name(cols, [\"close\", \"Close\", \"c\", \"C\"])\n",
    "\n",
    "            if time_col is None or high_col is None or low_col is None or close_col is None:\n",
    "                continue\n",
    "\n",
    "            tser = _to_dt_utc(df.get_column(time_col))\n",
    "            if tser is None:\n",
    "                continue\n",
    "\n",
    "            vol_series = _choose_volume_series(df)\n",
    "            if vol_series is None:\n",
    "                volume = pl.Series([None] * df.height, dtype=pl.Float64)\n",
    "            else:\n",
    "                volume = vol_series\n",
    "\n",
    "            mini = (\n",
    "                pl.DataFrame({\n",
    "                    \"time_utc\": tser,\n",
    "                    \"open\": df.get_column(open_col).cast(pl.Float64) if open_col else df.get_column(close_col).cast(pl.Float64),\n",
    "                    \"high\": df.get_column(high_col).cast(pl.Float64),\n",
    "                    \"low\":  df.get_column(low_col).cast(pl.Float64),\n",
    "                    \"close\":df.get_column(close_col).cast(pl.Float64),\n",
    "                    \"volume\": volume,\n",
    "                })\n",
    "                .drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"])\n",
    "                .sort(\"time_utc\")\n",
    "            )\n",
    "\n",
    "            if not mini.is_empty():\n",
    "                frames.append(mini)\n",
    "\n",
    "        if not frames:\n",
    "            return None\n",
    "\n",
    "        out = (\n",
    "            pl.concat(frames, how=\"vertical_relaxed\")\n",
    "            .drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"])\n",
    "            .sort(\"time_utc\")\n",
    "            .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    # -------------------------------- Bucle por s√≠mbolo ------------------------------------------\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    skipped_no_data = 0\n",
    "    low_sample = 0\n",
    "    no_volume = 0\n",
    "\n",
    "    for sym in syms_for_stats:\n",
    "        sym = str(sym).upper().strip()\n",
    "        ohlcv = _read_symbol_ohlcv(sym)\n",
    "\n",
    "        if ohlcv is None or ohlcv.is_empty():\n",
    "            skipped_no_data += 1\n",
    "            continue\n",
    "\n",
    "        ohlcv = (\n",
    "            ohlcv\n",
    "            .select([\"time_utc\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "            .sort(\"time_utc\")\n",
    "            .drop_nulls(subset=[\"time_utc\", \"high\", \"low\", \"close\"])\n",
    "            .unique(subset=[\"time_utc\"], keep=\"last\")\n",
    "        )\n",
    "\n",
    "        n_bars = int(ohlcv.height)\n",
    "        if n_bars < MIN_BARS_PER_SYMBOL:\n",
    "            low_sample += 1\n",
    "\n",
    "        metrics_sym: Dict[str, Any] = {\n",
    "            \"symbol\": sym,\n",
    "            \"n_bars\": n_bars,\n",
    "            \"volume_mean_5m\": None,\n",
    "            \"volume_median_5m\": None,\n",
    "            \"volume_p10_5m\": None,\n",
    "            \"volume_p90_5m\": None,\n",
    "            \"illiquid_bar_ratio\": None,\n",
    "            \"cv_volume\": None,\n",
    "            \"realized_vol_daily_mean\": None,\n",
    "            \"realized_vol_daily_median\": None,\n",
    "            \"atr_mean_5m\": None,\n",
    "            \"close_median_5m\": None,\n",
    "            \"atr_rel_mean_5m\": None,\n",
    "            \"abs_ret_p50_5m\": None,\n",
    "            \"abs_ret_p90_5m\": None,\n",
    "        }\n",
    "\n",
    "        # ------------------- Volumen / liquidez -------------------\n",
    "        vol_series = ohlcv.get_column(\"volume\") if \"volume\" in ohlcv.columns else None\n",
    "        has_volume_col = vol_series is not None and vol_series.drop_nulls().len() > 0\n",
    "\n",
    "        if has_volume_col:\n",
    "            vol_stats = ohlcv.select(\n",
    "                pl.col(\"volume\").mean().alias(\"vol_mean\"),\n",
    "                pl.col(\"volume\").median().alias(\"vol_median\"),\n",
    "                pl.col(\"volume\").quantile(0.10).alias(\"vol_p10\"),\n",
    "                pl.col(\"volume\").quantile(0.90).alias(\"vol_p90\"),\n",
    "                pl.col(\"volume\").std(ddof=1).alias(\"vol_std\"),\n",
    "            ).row(0)\n",
    "\n",
    "            vol_mean = float(vol_stats[0]) if vol_stats[0] is not None else None\n",
    "            vol_median = float(vol_stats[1]) if vol_stats[1] is not None else None\n",
    "            vol_p10 = float(vol_stats[2]) if vol_stats[2] is not None else None\n",
    "            vol_p90 = float(vol_stats[3]) if vol_stats[3] is not None else None\n",
    "            vol_std = float(vol_stats[4]) if vol_stats[4] is not None else None\n",
    "\n",
    "            illiquid_ratio = None\n",
    "            if vol_p10 is not None:\n",
    "                illiquid_ratio = float(\n",
    "                    ohlcv.select(\n",
    "                        (pl.col(\"volume\") <= vol_p10)\n",
    "                        .cast(pl.Float64)\n",
    "                        .mean()\n",
    "                        .alias(\"illq_ratio\")\n",
    "                    ).item()\n",
    "                )\n",
    "\n",
    "            cv_volume = None\n",
    "            if vol_mean is not None and vol_mean > 0.0 and vol_std is not None:\n",
    "                cv_volume = float(vol_std / vol_mean)\n",
    "\n",
    "            metrics_sym.update(\n",
    "                {\n",
    "                    \"volume_mean_5m\": vol_mean,\n",
    "                    \"volume_median_5m\": vol_median,\n",
    "                    \"volume_p10_5m\": vol_p10,\n",
    "                    \"volume_p90_5m\": vol_p90,\n",
    "                    \"illiquid_bar_ratio\": illiquid_ratio,\n",
    "                    \"cv_volume\": cv_volume,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            no_volume += 1\n",
    "\n",
    "        # ------------------- Volatilidad ---------------------------\n",
    "        # close median (para ATR relativo, defensa suave de unidades)\n",
    "        try:\n",
    "            close_med = ohlcv.select(pl.col(\"close\").median().alias(\"cmed\")).item()\n",
    "            metrics_sym[\"close_median_5m\"] = float(close_med) if close_med is not None else None\n",
    "        except Exception:\n",
    "            metrics_sym[\"close_median_5m\"] = None\n",
    "\n",
    "        # log returns\n",
    "        ret_df = (\n",
    "            ohlcv\n",
    "            .select(\n",
    "                [\n",
    "                    \"time_utc\",\n",
    "                    ((pl.col(\"close\") / pl.col(\"close\").shift(1)).log()).alias(\"ret\"),\n",
    "                ]\n",
    "            )\n",
    "            .drop_nulls(subset=[\"time_utc\", \"ret\"])\n",
    "            .sort(\"time_utc\")\n",
    "        )\n",
    "\n",
    "        if ret_df.height > 0:\n",
    "            daily_vol = (\n",
    "                ret_df\n",
    "                .with_columns(pl.col(\"time_utc\").dt.date().alias(\"date\"))\n",
    "                .group_by(\"date\")\n",
    "                .agg(\n",
    "                    pl.col(\"ret\").pow(2).sum().sqrt().alias(\"realized_vol_daily\")\n",
    "                )\n",
    "            )\n",
    "            if daily_vol.height > 0:\n",
    "                metrics_sym[\"realized_vol_daily_mean\"] = float(daily_vol[\"realized_vol_daily\"].mean())\n",
    "                metrics_sym[\"realized_vol_daily_median\"] = float(daily_vol[\"realized_vol_daily\"].median())\n",
    "\n",
    "            abs_stats = ret_df.select(\n",
    "                pl.col(\"ret\").abs().quantile(0.50).alias(\"abs_p50\"),\n",
    "                pl.col(\"ret\").abs().quantile(0.90).alias(\"abs_p90\"),\n",
    "            ).row(0)\n",
    "            metrics_sym[\"abs_ret_p50_5m\"] = float(abs_stats[0]) if abs_stats[0] is not None else None\n",
    "            metrics_sym[\"abs_ret_p90_5m\"] = float(abs_stats[1]) if abs_stats[1] is not None else None\n",
    "\n",
    "        # True Range y ATR simple medio (no cambia dise√±o)\n",
    "        if n_bars > 1:\n",
    "            tr_df = (\n",
    "                ohlcv\n",
    "                .with_columns(pl.col(\"close\").shift(1).alias(\"close_prev\"))\n",
    "                .drop_nulls(subset=[\"close_prev\"])\n",
    "                .with_columns(\n",
    "                    [\n",
    "                        (pl.col(\"high\") - pl.col(\"low\")).alias(\"tr1\"),\n",
    "                        (pl.col(\"high\") - pl.col(\"close_prev\")).abs().alias(\"tr2\"),\n",
    "                        (pl.col(\"low\") - pl.col(\"close_prev\")).abs().alias(\"tr3\"),\n",
    "                    ]\n",
    "                )\n",
    "                .with_columns(pl.max_horizontal(\"tr1\", \"tr2\", \"tr3\").alias(\"true_range\"))\n",
    "            )\n",
    "            if tr_df.height > 0:\n",
    "                atr_mean = float(tr_df[\"true_range\"].mean())\n",
    "                metrics_sym[\"atr_mean_5m\"] = atr_mean\n",
    "\n",
    "                cmed = metrics_sym.get(\"close_median_5m\")\n",
    "                if isinstance(cmed, (int, float)) and cmed and cmed > 0.0:\n",
    "                    metrics_sym[\"atr_rel_mean_5m\"] = float(atr_mean / cmed)\n",
    "\n",
    "        rows.append(metrics_sym)\n",
    "\n",
    "    # -------------------------------- Construir micro_df -----------------------------------------\n",
    "    if not rows:\n",
    "        print(\">>> Celda 07C :: SKIPPED (sin datos de microestructura)\")\n",
    "    else:\n",
    "        micro_df = pl.DataFrame(rows).with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase().alias(\"symbol\")\n",
    "        )\n",
    "\n",
    "        # Join opcional con summary de volatilidad\n",
    "        if vol_df is not None and not vol_df.is_empty():\n",
    "            micro_df = micro_df.join(vol_df, on=\"symbol\", how=\"left\")\n",
    "\n",
    "        # ------------------- Proxies -------------------\n",
    "        # Volumen: defensa suave de unidades mediante log1p\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.col(\"volume_mean_5m\").cast(pl.Float64).alias(\"volume_proxy_raw\")\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col(\"volume_proxy_raw\").is_not_null() & (pl.col(\"volume_proxy_raw\") > 0.0))\n",
    "              .then((pl.col(\"volume_proxy_raw\") + 1.0).log())\n",
    "              .otherwise(pl.col(\"volume_proxy_raw\"))  # 0 o null se dejan igual\n",
    "              .alias(\"volume_proxy\")\n",
    "        )\n",
    "\n",
    "        # Volatilidad: preferir ATR relativo si existe\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.when(pl.col(\"atr_rel_mean_5m\").is_not_null())\n",
    "              .then(pl.col(\"atr_rel_mean_5m\"))\n",
    "              .when(pl.col(\"realized_vol_daily_median\").is_not_null())\n",
    "              .then(pl.col(\"realized_vol_daily_median\"))\n",
    "              .when(pl.col(\"vol_long_term\").is_not_null())\n",
    "              .then(pl.col(\"vol_long_term\"))\n",
    "              .when(pl.col(\"atr_mean_5m\").is_not_null())\n",
    "              .then(pl.col(\"atr_mean_5m\"))\n",
    "              .otherwise(None)\n",
    "              .cast(pl.Float64)\n",
    "              .alias(\"vol_proxy\")\n",
    "        )\n",
    "\n",
    "        # ------------------- Scores de liquidez -------------------\n",
    "        vol_non_null = micro_df.get_column(\"volume_proxy\").drop_nulls()\n",
    "        if vol_non_null.len() > 0:\n",
    "            q20 = float(vol_non_null.quantile(0.20))\n",
    "            q80 = float(vol_non_null.quantile(0.80))\n",
    "            if not (q80 > q20):\n",
    "                q20, q80 = float(vol_non_null.min()), float(vol_non_null.max())\n",
    "        else:\n",
    "            q20, q80 = 0.0, 1.0\n",
    "\n",
    "        denom_level = (q80 - q20) if (q80 - q20) != 0.0 else 1.0\n",
    "\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.when(pl.col(\"volume_proxy\").is_null())\n",
    "              .then(None)\n",
    "              .when(pl.col(\"volume_proxy\") <= q20)\n",
    "              .then(pl.lit(0.0))\n",
    "              .when(pl.col(\"volume_proxy\") >= q80)\n",
    "              .then(pl.lit(1.0))\n",
    "              .otherwise((pl.col(\"volume_proxy\") - q20) / denom_level)\n",
    "              .alias(\"score_vol_level\")\n",
    "        )\n",
    "\n",
    "        # score_illiquid\n",
    "        illq_strict, illq_loose = sorted([ILLQ_MAX_STRICT, ILLQ_MAX_LOOSE])\n",
    "        denom_illq = (illq_loose - illq_strict) if (illq_loose - illq_strict) != 0.0 else 1.0\n",
    "\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.col(\"illiquid_bar_ratio\").cast(pl.Float64, strict=False)\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col(\"illiquid_bar_ratio\").is_null())\n",
    "              .then(None)\n",
    "              .when(pl.col(\"illiquid_bar_ratio\") <= illq_strict)\n",
    "              .then(pl.lit(1.0))\n",
    "              .when(pl.col(\"illiquid_bar_ratio\") >= illq_loose)\n",
    "              .then(pl.lit(0.0))\n",
    "              .otherwise(1.0 - ((pl.col(\"illiquid_bar_ratio\") - illq_strict) / denom_illq))\n",
    "              .alias(\"score_illiquid\")\n",
    "        )\n",
    "\n",
    "        # score_cv_volume\n",
    "        cv_strict, cv_loose = sorted([CV_MAX_STRICT, CV_MAX_LOOSE])\n",
    "        denom_cv = (cv_loose - cv_strict) if (cv_loose - cv_strict) != 0.0 else 1.0\n",
    "\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.col(\"cv_volume\").cast(pl.Float64, strict=False)\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col(\"cv_volume\").is_null())\n",
    "              .then(None)\n",
    "              .when(pl.col(\"cv_volume\") <= cv_strict)\n",
    "              .then(pl.lit(1.0))\n",
    "              .when(pl.col(\"cv_volume\") >= cv_loose)\n",
    "              .then(pl.lit(0.0))\n",
    "              .otherwise(1.0 - ((pl.col(\"cv_volume\") - cv_strict) / denom_cv))\n",
    "              .alias(\"score_cv_volume\")\n",
    "        )\n",
    "\n",
    "        # Media ponderada ignorando null\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.lit(W_LEVEL).alias(\"_w_lvl\"),\n",
    "            pl.lit(W_ILLQ).alias(\"_w_illq\"),\n",
    "            pl.lit(W_CV).alias(\"_w_cv\"),\n",
    "        )\n",
    "\n",
    "        num_liq = (\n",
    "            (pl.col(\"score_vol_level\") * pl.col(\"_w_lvl\")).fill_null(0.0)\n",
    "            + (pl.col(\"score_illiquid\") * pl.col(\"_w_illq\")).fill_null(0.0)\n",
    "            + (pl.col(\"score_cv_volume\") * pl.col(\"_w_cv\")).fill_null(0.0)\n",
    "        )\n",
    "\n",
    "        den_liq = (\n",
    "            pl.when(pl.col(\"score_vol_level\").is_not_null()).then(pl.col(\"_w_lvl\")).otherwise(0.0)\n",
    "            + pl.when(pl.col(\"score_illiquid\").is_not_null()).then(pl.col(\"_w_illq\")).otherwise(0.0)\n",
    "            + pl.when(pl.col(\"score_cv_volume\").is_not_null()).then(pl.col(\"_w_cv\")).otherwise(0.0)\n",
    "        )\n",
    "\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.when(den_liq > 0.0)\n",
    "              .then(num_liq / den_liq)\n",
    "              .otherwise(None)\n",
    "              .alias(\"liquidity_score\")\n",
    "        ).drop([\"_w_lvl\", \"_w_illq\", \"_w_cv\"])\n",
    "\n",
    "        # ------------------- Scores de volatilidad -------------------\n",
    "        vol_non_null2 = micro_df.get_column(\"vol_proxy\").drop_nulls()\n",
    "        if vol_non_null2.len() > 0:\n",
    "            q_lo = float(vol_non_null2.quantile(VOL_Q_LO))\n",
    "            q_lo_mid = float(vol_non_null2.quantile(VOL_Q_LO_MID))\n",
    "            q_hi_mid = float(vol_non_null2.quantile(VOL_Q_HI_MID))\n",
    "            q_hi = float(vol_non_null2.quantile(VOL_Q_HI))\n",
    "        else:\n",
    "            q_lo, q_lo_mid, q_hi_mid, q_hi = 0.0, 0.25, 0.75, 1.0\n",
    "\n",
    "        q_lo, q_lo_mid = sorted([q_lo, q_lo_mid])\n",
    "        q_hi_mid, q_hi = sorted([q_hi_mid, q_hi])\n",
    "        denom_lo = (q_lo_mid - q_lo) if (q_lo_mid - q_lo) != 0.0 else 1.0\n",
    "        denom_hi = (q_hi - q_hi_mid) if (q_hi - q_hi_mid) != 0.0 else 1.0\n",
    "\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.when(pl.col(\"vol_proxy\").is_null())\n",
    "              .then(None)\n",
    "              .when(pl.col(\"vol_proxy\") <= q_lo)\n",
    "              .then(pl.lit(0.0))\n",
    "              .when(pl.col(\"vol_proxy\") < q_lo_mid)\n",
    "              .then((pl.col(\"vol_proxy\") - q_lo) / denom_lo)\n",
    "              .when(pl.col(\"vol_proxy\") <= q_hi_mid)\n",
    "              .then(pl.lit(1.0))\n",
    "              .when(pl.col(\"vol_proxy\") < q_hi)\n",
    "              .then((q_hi - pl.col(\"vol_proxy\")) / denom_hi)\n",
    "              .otherwise(pl.lit(0.0))\n",
    "              .alias(\"vol_regime_score\")\n",
    "        )\n",
    "\n",
    "        # ------------------- Flags (informativos) -------------------\n",
    "        micro_df = micro_df.with_columns(\n",
    "            pl.when(pl.col(\"volume_proxy\").is_null())\n",
    "              .then(pl.lit(\"NO_VOLUME\"))\n",
    "              .when(pl.col(\"n_bars\") < MIN_BARS_PER_SYMBOL)\n",
    "              .then(pl.lit(\"LOW_SAMPLE\"))\n",
    "              .when(pl.col(\"liquidity_score\").is_null())\n",
    "              .then(pl.lit(\"WARN\"))\n",
    "              .when(pl.col(\"liquidity_score\") >= SCORE_LOOSE)\n",
    "              .then(pl.lit(\"GOOD\"))\n",
    "              .when(pl.col(\"liquidity_score\") >= SCORE_STRICT)\n",
    "              .then(pl.lit(\"WARN\"))\n",
    "              .otherwise(pl.lit(\"BAD\"))\n",
    "              .alias(\"liquidity_flag\")\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col(\"liquidity_flag\").is_in([\"BAD\", \"NO_VOLUME\"]))\n",
    "              .then(pl.lit(True))\n",
    "              .otherwise(pl.lit(False))\n",
    "              .alias(\"illiquid_flag\")\n",
    "        ).with_columns(\n",
    "            pl.when(pl.col(\"vol_proxy\").is_null())\n",
    "              .then(pl.lit(\"NO_VOL\"))\n",
    "              .when(pl.col(\"vol_proxy\") < q_lo_mid)\n",
    "              .then(pl.lit(\"LOW_VOL\"))\n",
    "              .when(pl.col(\"vol_proxy\") <= q_hi_mid)\n",
    "              .then(pl.lit(\"MID_VOL\"))\n",
    "              .otherwise(pl.lit(\"HIGH_VOL\"))\n",
    "              .alias(\"vol_flag\")\n",
    "        )\n",
    "\n",
    "        # ------------------- Persistencia -------------------\n",
    "        micro_path = OUT_METRICS_DIR / \"microstructure_summary.parquet\"\n",
    "        micro_df.write_parquet(str(micro_path), compression=\"zstd\")\n",
    "\n",
    "        GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "        GLOBAL_STATE[\"metrics\"][\"microstructure_summary_path\"] = str(micro_path)\n",
    "\n",
    "        # ------------------- Prints diagn√≥sticos m√≠nimos -------------------\n",
    "        n_rows = micro_df.height\n",
    "        n_syms = micro_df.get_column(\"symbol\").n_unique() if n_rows else 0\n",
    "\n",
    "        liq_counts = (\n",
    "            micro_df.group_by(\"liquidity_flag\")\n",
    "            .agg(pl.len().alias(\"n\"))\n",
    "            .sort(\"liquidity_flag\")\n",
    "        )\n",
    "        vol_counts = (\n",
    "            micro_df.group_by(\"vol_flag\")\n",
    "            .agg(pl.len().alias(\"n\"))\n",
    "            .sort(\"vol_flag\")\n",
    "        )\n",
    "\n",
    "        # Diccionarios compactos\n",
    "        liq_dict = {r[\"liquidity_flag\"]: int(r[\"n\"]) for r in liq_counts.iter_rows(named=True)}\n",
    "        vol_dict = {r[\"vol_flag\"]: int(r[\"n\"]) for r in vol_counts.iter_rows(named=True)}\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"üíæ OUTPUT ‚Üí {str(micro_path)} (rows={n_rows}, symbols={n_syms})\")\n",
    "        print(f\"[Celda 07C] skipped_no_data={skipped_no_data} | low_sample={low_sample} | no_volume={no_volume}\")\n",
    "        print(f\"[Celda 07C] liquidity_flag :: {liq_dict}\")\n",
    "        print(f\"[Celda 07C] vol_flag      :: {vol_dict}\")\n",
    "        print(\"[Celda 07C] NOTA: scores/flags son diagn√≥sticos suaves; NO gates.\")\n",
    "        print(\">>> Celda 07C :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6fa759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================================================================================\n",
      "CELDA 08 v3.1 SAFE PROXY (GLOBAL_STATE CFG FIX) - INICIO\n",
      "============================================================================================================================================\n",
      "Entorno:\n",
      "   - Python: 3.11.9\n",
      "   - Polars: 1.35.1\n",
      "   - CWD: c:\\Quant\\MT5_Data_Extraction\n",
      "   - Archivos en CWD (top 30): ['AUDITOR_CODE.ipynb', 'ER_FILTER_5M_V4.ipynb', 'ER_STRATEGY_LAB', 'MT5_DE_5M_V1.ipynb', 'artifacts', 'bulk_data', 'config', 'data', 'diagnostics_global', 'filtered_symbols.parquet', 'outputs', 'pipeline.log', 'processed_data', 'respaldo', 'venv1']\n",
      "\n",
      "Resoluci√≥n de estado global:\n",
      "   - GLOBAL_STATE disponible? True\n",
      "CONFIG resuelto desde GLOBAL_STATE['config'].\n",
      "\n",
      "Par√°metros efectivos de Celda 08 v3.1:\n",
      "   - H = 6\n",
      "   - SUSPECT_MEDIAN_TH = 0.9\n",
      "   - SUSPECT_FRAC_TH   = 0.8\n",
      "   - SUSPECT_RATIO_GE  = 0.6\n",
      "   - MIN_OPP_PER_MONTH = 20\n",
      "   - STRICT_CELL_08    = False\n",
      "\n",
      "Resoluci√≥n de paths:\n",
      "Usando rutas del pipeline desde GLOBAL_STATE.\n",
      "   - RUN_ID          = 20251218_190810\n",
      "   - OUT_EVENTS_DIR  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\n",
      "   - OUT_METRICS_DIR = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\n",
      "\n",
      "Paths efectivos:\n",
      "   - trend_events_path = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\trend_events.parquet\n",
      "   - range_events_path = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\range_events.parquet\n",
      "   - labels_path       = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "   - out_path          = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\frequency_opportunity_table.parquet\n",
      "\n",
      "Existencia de archivos clave:\n",
      "   - trend_events exists? True\n",
      "   - range_events exists? True\n",
      "   - labels exists?       True\n",
      "\n",
      "Intentando cargar trend_events ...\n",
      "   - path: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\trend_events.parquet\n",
      "trend_events cargado OK.\n",
      "   - rows: 9,119,905\n",
      "   - cols: 9\n",
      "   - schema: Schema([('symbol', String), ('time_utc', Datetime(time_unit='us', time_zone='UTC')), ('direction', Int8), ('cost_after', Float64), ('horizon', Int32), ('success', Int8), ('ndq_event', Float64), ('rng_H', Float64), ('family', String)])\n",
      "   - head(3):\n",
      "shape: (3, 9)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ time_utc            ‚îÜ direction ‚îÜ cost_after ‚îÜ ‚Ä¶ ‚îÜ success ‚îÜ ndq_event ‚îÜ rng_H ‚îÜ family ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---                 ‚îÜ ---       ‚îÜ ---        ‚îÜ   ‚îÜ ---     ‚îÜ ---       ‚îÜ ---   ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, UTC]   ‚îÜ i8        ‚îÜ f64        ‚îÜ   ‚îÜ i8      ‚îÜ f64       ‚îÜ f64   ‚îÜ str    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:15:00 ‚îÜ -1        ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.387755  ‚îÜ 0.49  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:20:00 ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.666667  ‚îÜ 0.6   ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:25:00 ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.65625   ‚îÜ 0.64  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "Intentando cargar range_events ...\n",
      "   - path: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\range_events.parquet\n",
      "range_events cargado OK.\n",
      "   - rows: 9,163,602\n",
      "   - cols: 9\n",
      "   - schema: Schema([('symbol', String), ('time_utc', Datetime(time_unit='us', time_zone='UTC')), ('direction', Int8), ('cost_after', Float64), ('horizon', Int32), ('success', Int8), ('ndq_event', Float64), ('rng_H', Float64), ('family', String)])\n",
      "   - head(3):\n",
      "shape: (3, 9)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ time_utc            ‚îÜ direction ‚îÜ cost_after ‚îÜ ‚Ä¶ ‚îÜ success ‚îÜ ndq_event ‚îÜ rng_H ‚îÜ family ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---                 ‚îÜ ---       ‚îÜ ---        ‚îÜ   ‚îÜ ---     ‚îÜ ---       ‚îÜ ---   ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, UTC]   ‚îÜ i8        ‚îÜ f64        ‚îÜ   ‚îÜ i8      ‚îÜ f64       ‚îÜ f64   ‚îÜ str    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 16:35:00 ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.450777  ‚îÜ 1.93  ‚îÜ RANGE  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 16:40:00 ‚îÜ -1        ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.682635  ‚îÜ 1.67  ‚îÜ RANGE  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 16:45:00 ‚îÜ -1        ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0       ‚îÜ 0.742515  ‚îÜ 1.67  ‚îÜ RANGE  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                 ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ         ‚îÜ           ‚îÜ       ‚îÜ        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "Intentando cargar regime_labels ...\n",
      "   - path: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "regime_labels cargado OK.\n",
      "   - rows: 24,522,466\n",
      "   - cols: 10\n",
      "   - schema: Schema([('symbol', String), ('time_utc', Datetime(time_unit='us', time_zone='UTC')), ('regime', String), ('er_value', Float32), ('pd_value', Float32), ('session_label', String), ('vol_long_term', Float64), ('vol_p90_abs_ret', Float64), ('n_ret', Int64), ('vol_regime_flag', String)])\n",
      "   - head(3):\n",
      "shape: (3, 10)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ time_utc     ‚îÜ regime ‚îÜ er_value ‚îÜ ‚Ä¶ ‚îÜ vol_long_ter ‚îÜ vol_p90_abs ‚îÜ n_ret ‚îÜ vol_regime_ ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---          ‚îÜ ---    ‚îÜ ---      ‚îÜ   ‚îÜ m            ‚îÜ _ret        ‚îÜ ---   ‚îÜ flag        ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, ‚îÜ str    ‚îÜ f32      ‚îÜ   ‚îÜ ---          ‚îÜ ---         ‚îÜ i64   ‚îÜ ---         ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC]         ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ f64          ‚îÜ f64         ‚îÜ       ‚îÜ str         ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:30:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:35:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:40:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "Validaci√≥n de columnas m√≠nimas esperadas:\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Unificando eventos TREND + RANGE\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "events unificados: rows=18,283,507, cols=10\n",
      "schema: Schema([('symbol', String), ('time_utc', Datetime(time_unit='us', time_zone='UTC')), ('direction', Int8), ('cost_after', Float64), ('horizon', Int32), ('success', Int8), ('ndq_event', Float64), ('rng_H', Float64), ('family', String), ('regime', String)])\n",
      "head(5):\n",
      "shape: (5, 10)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ time_utc             ‚îÜ direction ‚îÜ cost_after ‚îÜ ‚Ä¶ ‚îÜ ndq_event ‚îÜ rng_H ‚îÜ family ‚îÜ regime ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---                  ‚îÜ ---       ‚îÜ ---        ‚îÜ   ‚îÜ ---       ‚îÜ ---   ‚îÜ ---    ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, UTC]    ‚îÜ i8        ‚îÜ f64        ‚îÜ   ‚îÜ f64       ‚îÜ f64   ‚îÜ str    ‚îÜ str    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:15:00  ‚îÜ -1        ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0.387755  ‚îÜ 0.49  ‚îÜ TREND  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                  ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ           ‚îÜ       ‚îÜ        ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:20:00  ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0.666667  ‚îÜ 0.6   ‚îÜ TREND  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                  ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ           ‚îÜ       ‚îÜ        ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:25:00  ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0.65625   ‚îÜ 0.64  ‚îÜ TREND  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                  ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ           ‚îÜ       ‚îÜ        ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:30:00  ‚îÜ -1        ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0.055556  ‚îÜ 0.72  ‚îÜ TREND  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                  ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ           ‚îÜ       ‚îÜ        ‚îÜ        ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19 19:35:00  ‚îÜ 1         ‚îÜ 4.38521    ‚îÜ ‚Ä¶ ‚îÜ 0.72619   ‚îÜ 0.84  ‚îÜ TREND  ‚îÜ TREND  ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC                  ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ           ‚îÜ       ‚îÜ        ‚îÜ        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Conteo mensual de eventos\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "events_monthly generado.\n",
      "rows=8,283\n",
      "head(10):\n",
      "shape: (10, 4)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ regime ‚îÜ month                   ‚îÜ n_events ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---                     ‚îÜ ---      ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ datetime[Œºs, UTC]       ‚îÜ u32      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ USDNOK ‚îÜ RANGE  ‚îÜ 2025-03-01 00:00:00 UTC ‚îÜ 2608     ‚îÇ\n",
      "‚îÇ AMZN   ‚îÜ TREND  ‚îÜ 2025-02-01 00:00:00 UTC ‚îÜ 594      ‚îÇ\n",
      "‚îÇ GBPAUD ‚îÜ TREND  ‚îÜ 2024-05-01 00:00:00 UTC ‚îÜ 2448     ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ RANGE  ‚îÜ 2023-09-01 00:00:00 UTC ‚îÜ 2120     ‚îÇ\n",
      "‚îÇ USDPLN ‚îÜ TREND  ‚îÜ 2024-01-01 00:00:00 UTC ‚îÜ 2597     ‚îÇ\n",
      "‚îÇ AUDJPY ‚îÜ TREND  ‚îÜ 2025-04-01 00:00:00 UTC ‚îÜ 2455     ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ RANGE  ‚îÜ 2025-06-01 00:00:00 UTC ‚îÜ 2303     ‚îÇ\n",
      "‚îÇ XLMUSD ‚îÜ TREND  ‚îÜ 2021-12-01 00:00:00 UTC ‚îÜ 3382     ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ TREND  ‚îÜ 2021-12-01 00:00:00 UTC ‚îÜ 2306     ‚îÇ\n",
      "‚îÇ NZDCHF ‚îÜ RANGE  ‚îÜ 2023-06-01 00:00:00 UTC ‚îÜ 2372     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "events_monthly_total generado.\n",
      "rows=4,148\n",
      "head(10):\n",
      "shape: (10, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ month                   ‚îÜ n_events_total ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---                     ‚îÜ ---            ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, UTC]       ‚îÜ u32            ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ USDMXN ‚îÜ 2025-02-01 00:00:00 UTC ‚îÜ 4485           ‚îÇ\n",
      "‚îÇ MANUSD ‚îÜ 2022-03-01 00:00:00 UTC ‚îÜ 589            ‚îÇ\n",
      "‚îÇ XAGAUD ‚îÜ 2024-06-01 00:00:00 UTC ‚îÜ 4280           ‚îÇ\n",
      "‚îÇ AVAUSD ‚îÜ 2024-03-01 00:00:00 UTC ‚îÜ 6652           ‚îÇ\n",
      "‚îÇ GRTUSD ‚îÜ 2022-12-01 00:00:00 UTC ‚îÜ 6571           ‚îÇ\n",
      "‚îÇ GBPJPY ‚îÜ 2023-05-01 00:00:00 UTC ‚îÜ 5136           ‚îÇ\n",
      "‚îÇ LVMH   ‚îÜ 2024-03-01 00:00:00 UTC ‚îÜ 1480           ‚îÇ\n",
      "‚îÇ GBPNZD ‚îÜ 2024-03-01 00:00:00 UTC ‚îÜ 4660           ‚îÇ\n",
      "‚îÇ AUDJPY ‚îÜ 2023-06-01 00:00:00 UTC ‚îÜ 4898           ‚îÇ\n",
      "‚îÇ USDCHF ‚îÜ 2022-03-01 00:00:00 UTC ‚îÜ 5081           ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Conteo mensual de barras operables desde labels\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "labels_operable filtrado: rows=19,429,242\n",
      "head(5):\n",
      "shape: (5, 10)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ time_utc     ‚îÜ regime ‚îÜ er_value ‚îÜ ‚Ä¶ ‚îÜ vol_long_ter ‚îÜ vol_p90_abs ‚îÜ n_ret ‚îÜ vol_regime_ ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---          ‚îÜ ---    ‚îÜ ---      ‚îÜ   ‚îÜ m            ‚îÜ _ret        ‚îÜ ---   ‚îÜ flag        ‚îÇ\n",
      "‚îÇ str    ‚îÜ datetime[Œºs, ‚îÜ str    ‚îÜ f32      ‚îÜ   ‚îÜ ---          ‚îÜ ---         ‚îÜ i64   ‚îÜ ---         ‚îÇ\n",
      "‚îÇ        ‚îÜ UTC]         ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ f64          ‚îÜ f64         ‚îÜ       ‚îÜ str         ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:30:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:35:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:40:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:45:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ 2021-11-19   ‚îÜ RANGE  ‚îÜ 0.0      ‚îÜ ‚Ä¶ ‚îÜ 0.002022     ‚îÜ 0.002419    ‚îÜ 78092 ‚îÜ MEDIA       ‚îÇ\n",
      "‚îÇ        ‚îÜ 16:50:00 UTC ‚îÜ        ‚îÜ          ‚îÜ   ‚îÜ              ‚îÜ             ‚îÜ       ‚îÜ             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "operable_monthly generado.\n",
      "rows=8,383\n",
      "head(10):\n",
      "shape: (10, 4)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ regime ‚îÜ month                   ‚îÜ n_operable_bars ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---                     ‚îÜ ---             ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ datetime[Œºs, UTC]       ‚îÜ u32             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ MSFT   ‚îÜ RANGE  ‚îÜ 2023-11-01 00:00:00 UTC ‚îÜ 610             ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ RANGE  ‚îÜ 2022-01-01 00:00:00 UTC ‚îÜ 2597            ‚îÇ\n",
      "‚îÇ GOOG   ‚îÜ TREND  ‚îÜ 2023-11-01 00:00:00 UTC ‚îÜ 607             ‚îÇ\n",
      "‚îÇ USDZAR ‚îÜ RANGE  ‚îÜ 2023-09-01 00:00:00 UTC ‚îÜ 2427            ‚îÇ\n",
      "‚îÇ BCHUSD ‚îÜ TREND  ‚îÜ 2021-11-01 00:00:00 UTC ‚îÜ 1476            ‚îÇ\n",
      "‚îÇ EURNOK ‚îÜ TREND  ‚îÜ 2024-04-01 00:00:00 UTC ‚îÜ 2553            ‚îÇ\n",
      "‚îÇ EURCZK ‚îÜ TREND  ‚îÜ 2025-07-01 00:00:00 UTC ‚îÜ 1145            ‚îÇ\n",
      "‚îÇ IBE    ‚îÜ TREND  ‚îÜ 2022-08-01 00:00:00 UTC ‚îÜ 682             ‚îÇ\n",
      "‚îÇ GBPJPY ‚îÜ RANGE  ‚îÜ 2023-10-01 00:00:00 UTC ‚îÜ 2157            ‚îÇ\n",
      "‚îÇ UNIUSD ‚îÜ RANGE  ‚îÜ 2024-05-01 00:00:00 UTC ‚îÜ 3458            ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "operable_monthly_total generado.\n",
      "rows=4,198\n",
      "head(10):\n",
      "shape: (10, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol  ‚îÜ month                   ‚îÜ n_operable_bars_total ‚îÇ\n",
      "‚îÇ ---     ‚îÜ ---                     ‚îÜ ---                   ‚îÇ\n",
      "‚îÇ str     ‚îÜ datetime[Œºs, UTC]       ‚îÜ u32                   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD  ‚îÜ 2023-03-01 00:00:00 UTC ‚îÜ 6834                  ‚îÇ\n",
      "‚îÇ NZDCHF  ‚îÜ 2023-04-01 00:00:00 UTC ‚îÜ 4607                  ‚îÇ\n",
      "‚îÇ DOGEUSD ‚îÜ 2025-11-01 00:00:00 UTC ‚îÜ 6645                  ‚îÇ\n",
      "‚îÇ ETHUSD  ‚îÜ 2022-11-01 00:00:00 UTC ‚îÜ 6724                  ‚îÇ\n",
      "‚îÇ GBPCHF  ‚îÜ 2022-04-01 00:00:00 UTC ‚îÜ 4733                  ‚îÇ\n",
      "‚îÇ ALGUSD  ‚îÜ 2024-11-01 00:00:00 UTC ‚îÜ 6754                  ‚îÇ\n",
      "‚îÇ EURGBP  ‚îÜ 2023-11-01 00:00:00 UTC ‚îÜ 4918                  ‚îÇ\n",
      "‚îÇ NZDJPY  ‚îÜ 2023-05-01 00:00:00 UTC ‚îÜ 5213                  ‚îÇ\n",
      "‚îÇ NZDUSD  ‚îÜ 2024-05-01 00:00:00 UTC ‚îÜ 5162                  ‚îÇ\n",
      "‚îÇ GALUSD  ‚îÜ 2021-11-01 00:00:00 UTC ‚îÜ 253                   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Diagn√≥stico: ¬øeventos ‚âà barras operables?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "ratio_by_month generado.\n",
      "rows=4,198\n",
      "head(12):\n",
      "shape: (12, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol  ‚îÜ month                 ‚îÜ n_events_total ‚îÜ n_operable_bars_total ‚îÜ event_to_operable_bar ‚îÇ\n",
      "‚îÇ ---     ‚îÜ ---                   ‚îÜ ---            ‚îÜ ---                   ‚îÜ _ratio                ‚îÇ\n",
      "‚îÇ str     ‚îÜ datetime[Œºs, UTC]     ‚îÜ u32            ‚îÜ u32                   ‚îÜ ---                   ‚îÇ\n",
      "‚îÇ         ‚îÜ                       ‚îÜ                ‚îÜ                       ‚îÜ f64                   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD  ‚îÜ 2023-03-01 00:00:00   ‚îÜ 6796           ‚îÜ 6834                  ‚îÜ 0.99444               ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ NZDCHF  ‚îÜ 2023-04-01 00:00:00   ‚îÜ 4436           ‚îÜ 4607                  ‚îÜ 0.962883              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ DOGEUSD ‚îÜ 2025-11-01 00:00:00   ‚îÜ 6563           ‚îÜ 6645                  ‚îÜ 0.98766               ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ ETHUSD  ‚îÜ 2022-11-01 00:00:00   ‚îÜ 6698           ‚îÜ 6724                  ‚îÜ 0.996133              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ GBPCHF  ‚îÜ 2022-04-01 00:00:00   ‚îÜ 4603           ‚îÜ 4733                  ‚îÜ 0.972533              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ ‚Ä¶       ‚îÜ ‚Ä¶                     ‚îÜ ‚Ä¶              ‚îÜ ‚Ä¶                     ‚îÜ ‚Ä¶                     ‚îÇ\n",
      "‚îÇ NZDJPY  ‚îÜ 2023-05-01 00:00:00   ‚îÜ 5135           ‚îÜ 5213                  ‚îÜ 0.985037              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ NZDUSD  ‚îÜ 2024-05-01 00:00:00   ‚îÜ 4955           ‚îÜ 5162                  ‚îÜ 0.959899              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ GALUSD  ‚îÜ 2021-11-01 00:00:00   ‚îÜ 252            ‚îÜ 253                   ‚îÜ 0.996047              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ XRPUSD  ‚îÜ 2025-09-01 00:00:00   ‚îÜ 6298           ‚îÜ 6376                  ‚îÜ 0.987767              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îÇ AAVUSD  ‚îÜ 2023-04-01 00:00:00   ‚îÜ 6306           ‚îÜ 6588                  ‚îÜ 0.957195              ‚îÇ\n",
      "‚îÇ         ‚îÜ UTC                   ‚îÜ                ‚îÜ                       ‚îÜ                       ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "ratio_summary generado.\n",
      "rows=84\n",
      "head(20):\n",
      "shape: (20, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ event_to_operable_ba ‚îÜ event_to_operable_ba ‚îÜ months_ratio_observ ‚îÜ events_definition_s ‚îÇ\n",
      "‚îÇ ---    ‚îÜ r_ratio_me‚Ä¶          ‚îÜ r_ratio_fr‚Ä¶          ‚îÜ ed                  ‚îÜ uspect              ‚îÇ\n",
      "‚îÇ str    ‚îÜ ---                  ‚îÜ ---                  ‚îÜ ---                 ‚îÜ ---                 ‚îÇ\n",
      "‚îÇ        ‚îÜ f64                  ‚îÜ f64                  ‚îÜ u32                 ‚îÜ bool                ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ EURGBP ‚îÜ 0.957881             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ TSLA   ‚îÜ 0.992162             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ NZDCHF ‚îÜ 0.965433             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.990729             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ USDPLN ‚îÜ 0.968017             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶                    ‚îÜ ‚Ä¶                    ‚îÜ ‚Ä¶                   ‚îÜ ‚Ä¶                   ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ 0.998625             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ DOTUSD ‚îÜ 0.959037             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ 0.972212             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ WMT    ‚îÜ 0.915361             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îÇ XAGAUD ‚îÜ 0.979003             ‚îÜ 1.0                  ‚îÜ 50                  ‚îÜ true                ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "M√©tricas base: n_per_month_* y coverage_p\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Construyendo summary final con SAFE PROXY\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "summary construido.\n",
      "rows=84, cols=14\n",
      "schema:\n",
      "Schema([('symbol', String), ('n_per_month_total', Float64), ('n_per_month_tr', Float64), ('n_per_month_rg', Float64), ('coverage_p', Float64), ('operable_bars_per_month_est', Float64), ('event_to_operable_bar_ratio_median', Float64), ('event_to_operable_bar_ratio_frac_ge_threshold', Float64), ('months_ratio_observed', UInt32), ('events_definition_suspect', Boolean), ('n_per_month_total_safe', Float64), ('n_per_month_total_safe_H', Float64), ('has_basic_opportunity', Boolean), ('has_basic_opportunity_safe', Boolean)])\n",
      "head(20):\n",
      "shape: (20, 14)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ n_per_mont ‚îÜ n_per_mont ‚îÜ n_per_mon ‚îÜ ‚Ä¶ ‚îÜ n_per_mon ‚îÜ n_per_mon ‚îÜ has_basic ‚îÜ has_basic ‚îÇ\n",
      "‚îÇ ---    ‚îÜ h_total    ‚îÜ h_tr       ‚îÜ th_rg     ‚îÜ   ‚îÜ th_total_ ‚îÜ th_total_ ‚îÜ _opportun ‚îÜ _opportun ‚îÇ\n",
      "‚îÇ str    ‚îÜ ---        ‚îÜ ---        ‚îÜ ---       ‚îÜ   ‚îÜ safe      ‚îÜ safe_H    ‚îÜ ity       ‚îÜ ity_safe  ‚îÇ\n",
      "‚îÇ        ‚îÜ f64        ‚îÜ f64        ‚îÜ f64       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ f64       ‚îÜ f64       ‚îÜ bool      ‚îÜ bool      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ ALGUSD ‚îÜ 5727.92    ‚îÜ 2898.4     ‚îÜ 2829.52   ‚îÜ ‚Ä¶ ‚îÜ 6466.42   ‚îÜ 1077.7366 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 67        ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ GBPAUD ‚îÜ 4699.12    ‚îÜ 2373.0     ‚îÜ 2326.12   ‚îÜ ‚Ä¶ ‚îÜ 4751.06   ‚îÜ 791.84333 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 3         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ GOOG   ‚îÜ 1202.84    ‚îÜ 608.8      ‚îÜ 594.04    ‚îÜ ‚Ä¶ ‚îÜ 1234.72   ‚îÜ 205.78666 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 7         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ AMZN   ‚îÜ 1211.02    ‚îÜ 611.86     ‚îÜ 599.16    ‚îÜ ‚Ä¶ ‚îÜ 1236.9    ‚îÜ 206.15    ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ NZDJPY ‚îÜ 4673.64    ‚îÜ 2355.88    ‚îÜ 2317.76   ‚îÜ ‚Ä¶ ‚îÜ 4753.9    ‚îÜ 792.31666 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 7         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶         ‚îÇ\n",
      "‚îÇ EURPLN ‚îÜ 4183.12    ‚îÜ 1915.86    ‚îÜ 2267.26   ‚îÜ ‚Ä¶ ‚îÜ 4183.12   ‚îÜ 4183.12   ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ USDCAD ‚îÜ 4611.84    ‚îÜ 2325.44    ‚îÜ 2286.4    ‚îÜ ‚Ä¶ ‚îÜ 4738.6    ‚îÜ 789.76666 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 7         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ 4611.06    ‚îÜ 2330.88    ‚îÜ 2280.18   ‚îÜ ‚Ä¶ ‚îÜ 4744.62   ‚îÜ 790.77    ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ AUDJPY ‚îÜ 4672.88    ‚îÜ 2352.3     ‚îÜ 2320.58   ‚îÜ ‚Ä¶ ‚îÜ 4745.6    ‚îÜ 790.93333 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 3         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îÇ EURCHF ‚îÜ 4574.36    ‚îÜ 2295.98    ‚îÜ 2278.38   ‚îÜ ‚Ä¶ ‚îÜ 4744.46   ‚îÜ 790.74333 ‚îÜ true      ‚îÜ true      ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ 3         ‚îÜ           ‚îÜ           ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Auditor√≠a interna integrada\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "S√≠mbolos sospechosos: 74 / 84\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Guardando output Celda 08\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Output escrito correctamente en: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\frequency_opportunity_table.parquet\n",
      "rows=84, cols=14\n",
      "GLOBAL_STATE actualizado: metrics + tables.\n",
      "\n",
      "============================================================================================================================================\n",
      "CELDA 08 v3.1 SAFE PROXY - FIN\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Celda 08 ‚Äî Frecuencia & Oportunidad (n/mes) + Cobertura (coverage_p) ‚Äî v3.1 SAFE PROXY (GLOBAL_STATE CFG FIX)\n",
    "# =========================\n",
    "# OBJETIVO:\n",
    "# 1) Mantener m√©tricas originales de frecuencia (compatibilidad).\n",
    "# 2) Detectar cuando \"eventos ‚âà barras operables\" (definici√≥n bar-level de Celda 07).\n",
    "# 3) Crear columnas SAFE basadas en labels/operable bars para no inflar \"oportunidad\".\n",
    "# 4) Auditor√≠a interna con prints excesivos para eliminar Celda 08B.\n",
    "#\n",
    "# NOTA CR√çTICA:\n",
    "# - Esta versi√≥n prioriza rutas del PIPELINE REAL (GLOBAL_STATE[\"paths\"]).\n",
    "# - Prioriza tambi√©n CFG del RUN: GLOBAL_STATE[\"config\"] si CONFIG no existe.\n",
    "#\n",
    "# WARNING conceptual:\n",
    "# - coverage_p aqu√≠ es COMPATIBLE pero puede ser tendencioso porque\n",
    "#   events_monthly_total solo contiene meses donde hubo eventos.\n",
    "#   Si la usas para decisiones \"reales\", te puede enga√±ar.\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"CELDA 08 v3.1 SAFE PROXY (GLOBAL_STATE CFG FIX) - INICIO\")\n",
    "print(\"=\"*140)\n",
    "\n",
    "# =========================\n",
    "# 0) Entorno visible\n",
    "# =========================\n",
    "print(\"Entorno:\")\n",
    "print(f\"   - Python: {sys.version.split()[0]}\")\n",
    "print(f\"   - Polars: {pl.__version__}\")\n",
    "print(f\"   - CWD: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    print(\"   - Archivos en CWD (top 30):\", sorted(os.listdir(os.getcwd()))[:30])\n",
    "except Exception as e:\n",
    "    print(\"   - No se pudo listar CWD:\", repr(e))\n",
    "\n",
    "# =========================\n",
    "# 1) Resolver GLOBAL_STATE temprano\n",
    "# =========================\n",
    "use_global_state = (\"GLOBAL_STATE\" in globals() and isinstance(GLOBAL_STATE, dict))\n",
    "print(\"\\nResoluci√≥n de estado global:\")\n",
    "print(f\"   - GLOBAL_STATE disponible? {use_global_state}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Resolver CONFIG y par√°metros\n",
    "# =========================\n",
    "cfg = None\n",
    "\n",
    "# Prioridad 1: CONFIG expl√≠cito (compatibilidad)\n",
    "try:\n",
    "    cfg = CONFIG\n",
    "    print(\"\\nCONFIG detectado en el entorno (variable CONFIG).\")\n",
    "except NameError:\n",
    "    cfg = None\n",
    "\n",
    "# Prioridad 2: Config del RUN (pipeline real)\n",
    "if cfg is None and use_global_state:\n",
    "    gs_cfg = GLOBAL_STATE.get(\"config\", None)\n",
    "    if isinstance(gs_cfg, dict) and gs_cfg:\n",
    "        cfg = gs_cfg\n",
    "        print(\"CONFIG resuelto desde GLOBAL_STATE['config'].\")\n",
    "\n",
    "# Fallback final\n",
    "if cfg is None:\n",
    "    cfg = {}\n",
    "    print(\"\\nCONFIG no encontrado. Usando defaults internos de Celda 08.\")\n",
    "\n",
    "H = (\n",
    "    cfg.get(\"H\", None)\n",
    "    or cfg.get(\"H_BARS\", None)\n",
    "    or cfg.get(\"H_M5\", None)\n",
    "    or 6\n",
    ")\n",
    "\n",
    "SUSPECT_MEDIAN_TH = cfg.get(\"SUSPECT_MEDIAN_TH\", 0.90)\n",
    "SUSPECT_FRAC_TH   = cfg.get(\"SUSPECT_FRAC_TH\", 0.80)\n",
    "SUSPECT_RATIO_GE  = cfg.get(\"SUSPECT_RATIO_GE\", 0.60)\n",
    "\n",
    "MIN_OPP_PER_MONTH = cfg.get(\"MIN_OPP_PER_MONTH\", 20)\n",
    "STRICT_08 = cfg.get(\"STRICT_CELL_08\", False)\n",
    "\n",
    "print(\"\\nPar√°metros efectivos de Celda 08 v3.1:\")\n",
    "print(f\"   - H = {H}\")\n",
    "print(f\"   - SUSPECT_MEDIAN_TH = {SUSPECT_MEDIAN_TH}\")\n",
    "print(f\"   - SUSPECT_FRAC_TH   = {SUSPECT_FRAC_TH}\")\n",
    "print(f\"   - SUSPECT_RATIO_GE  = {SUSPECT_RATIO_GE}\")\n",
    "print(f\"   - MIN_OPP_PER_MONTH = {MIN_OPP_PER_MONTH}\")\n",
    "print(f\"   - STRICT_CELL_08    = {STRICT_08}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Resolver paths: GLOBAL_STATE primero\n",
    "# =========================\n",
    "print(\"\\nResoluci√≥n de paths:\")\n",
    "\n",
    "ART_DIR = None\n",
    "OUT_EVENTS_DIR = None\n",
    "OUT_METRICS_DIR = None\n",
    "RUN_ID = None\n",
    "\n",
    "if use_global_state:\n",
    "    try:\n",
    "        RUN_ID = GLOBAL_STATE.get(\"run_id\", None) or GLOBAL_STATE.get(\"RUN_ID\", None)\n",
    "        paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "        OUT_EVENTS_DIR  = Path(paths[\"events\"]).resolve()\n",
    "        OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "\n",
    "        OUT_EVENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"Usando rutas del pipeline desde GLOBAL_STATE.\")\n",
    "        print(f\"   - RUN_ID          = {RUN_ID}\")\n",
    "        print(f\"   - OUT_EVENTS_DIR  = {OUT_EVENTS_DIR}\")\n",
    "        print(f\"   - OUT_METRICS_DIR = {OUT_METRICS_DIR}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"GLOBAL_STATE existe pero no pude resolver paths['events'/'metrics']:\", repr(e))\n",
    "        print(\"   ‚Üí Fallback a modo standalone con artifacts/.\")\n",
    "        use_global_state = False\n",
    "\n",
    "if not use_global_state:\n",
    "    ART_DIR = Path(cfg.get(\"ARTIFACTS_DIR\", \"artifacts\")).resolve()\n",
    "    ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Modo standalone.\")\n",
    "    print(f\"   - ART_DIR = {ART_DIR}\")\n",
    "\n",
    "# Paths efectivos de entrada/salida\n",
    "if use_global_state:\n",
    "    trend_events_path = Path(cfg.get(\"TREND_EVENTS_PATH\", OUT_EVENTS_DIR / \"trend_events.parquet\"))\n",
    "    range_events_path = Path(cfg.get(\"RANGE_EVENTS_PATH\", OUT_EVENTS_DIR / \"range_events.parquet\"))\n",
    "    labels_path       = Path(cfg.get(\"REGIME_LABELS_PATH\", OUT_METRICS_DIR / \"regime_labels.parquet\"))\n",
    "    out_path          = Path(cfg.get(\"FREQ_OPP_TABLE_PATH\", OUT_METRICS_DIR / \"frequency_opportunity_table.parquet\"))\n",
    "else:\n",
    "    trend_events_path = Path(cfg.get(\"TREND_EVENTS_PATH\", ART_DIR / \"trend_events.parquet\"))\n",
    "    range_events_path = Path(cfg.get(\"RANGE_EVENTS_PATH\", ART_DIR / \"range_events.parquet\"))\n",
    "    labels_path       = Path(cfg.get(\"REGIME_LABELS_PATH\", ART_DIR / \"regime_labels.parquet\"))\n",
    "    out_path          = Path(cfg.get(\"FREQ_OPP_TABLE_PATH\", ART_DIR / \"frequency_opportunity_table.parquet\"))\n",
    "\n",
    "print(\"\\nPaths efectivos:\")\n",
    "print(f\"   - trend_events_path = {trend_events_path}\")\n",
    "print(f\"   - range_events_path = {range_events_path}\")\n",
    "print(f\"   - labels_path       = {labels_path}\")\n",
    "print(f\"   - out_path          = {out_path}\")\n",
    "\n",
    "print(\"\\nExistencia de archivos clave:\")\n",
    "print(f\"   - trend_events exists? {trend_events_path.exists()}\")\n",
    "print(f\"   - range_events exists? {range_events_path.exists()}\")\n",
    "print(f\"   - labels exists?       {labels_path.exists()}\")\n",
    "\n",
    "# =========================\n",
    "# 4) Loader seguro\n",
    "# =========================\n",
    "def load_parquet_safe(p: Path, name: str):\n",
    "    print(f\"\\nIntentando cargar {name} ...\")\n",
    "    print(f\"   - path: {p}\")\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(p)\n",
    "        print(f\"{name} cargado OK.\")\n",
    "        print(f\"   - rows: {df.height:,}\")\n",
    "        print(f\"   - cols: {df.width}\")\n",
    "        print(f\"   - schema: {df.schema}\")\n",
    "        print(f\"   - head(3):\\n{df.head(3)}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"{name} NO encontrado.\")\n",
    "        print(\"   ‚Üí Se usar√° DataFrame vac√≠o (degrada SAFE si faltan labels).\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "trend_events = load_parquet_safe(trend_events_path, \"trend_events\")\n",
    "range_events = load_parquet_safe(range_events_path, \"range_events\")\n",
    "labels_df    = load_parquet_safe(labels_path, \"regime_labels\")\n",
    "\n",
    "# =========================\n",
    "# 5) Validaci√≥n m√≠nima de columnas\n",
    "# =========================\n",
    "def assert_cols(df, cols, df_name):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msg = f\"{df_name}: faltan columnas esperadas {missing}. Columnas actuales: {df.columns}\"\n",
    "        print(msg)\n",
    "        if STRICT_08:\n",
    "            raise ValueError(msg)\n",
    "\n",
    "print(\"\\nValidaci√≥n de columnas m√≠nimas esperadas:\")\n",
    "assert_cols(trend_events, [\"symbol\", \"time_utc\"], \"trend_events\")\n",
    "assert_cols(range_events, [\"symbol\", \"time_utc\"], \"range_events\")\n",
    "assert_cols(labels_df,    [\"symbol\", \"time_utc\", \"regime\"], \"regime_labels\")\n",
    "\n",
    "# =========================\n",
    "# 5B) Helpers anti-duplicates / schemas vac√≠os\n",
    "# =========================\n",
    "def empty_operable_monthly_total():\n",
    "    return pl.DataFrame(schema={\n",
    "        \"symbol\": pl.Utf8,\n",
    "        \"month\": pl.Datetime(time_unit=\"us\", time_zone=\"UTC\"),\n",
    "        \"n_operable_bars_total\": pl.UInt32,\n",
    "    })\n",
    "\n",
    "def full_join_symbol_month(left: pl.DataFrame, right: pl.DataFrame):\n",
    "    out = left.join(right, on=[\"symbol\", \"month\"], how=\"full\", coalesce=True)\n",
    "    for c in [\"symbol_right\", \"month_right\"]:\n",
    "        if c in out.columns:\n",
    "            out = out.drop(c)\n",
    "    return out\n",
    "\n",
    "def full_join_symbol(left: pl.DataFrame, right: pl.DataFrame):\n",
    "    out = left.join(right, on=\"symbol\", how=\"full\", coalesce=True)\n",
    "    if \"symbol_right\" in out.columns:\n",
    "        try:\n",
    "            out = (\n",
    "                out.with_columns(\n",
    "                    pl.coalesce([pl.col(\"symbol\"), pl.col(\"symbol_right\")]).alias(\"symbol\")\n",
    "                )\n",
    "                .drop(\"symbol_right\")\n",
    "            )\n",
    "        except Exception:\n",
    "            out = out.drop(\"symbol_right\")\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 6) Si no hay eventos ‚Üí output vac√≠o compatible\n",
    "# =========================\n",
    "no_events = (trend_events.height == 0 and range_events.height == 0)\n",
    "\n",
    "if no_events:\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"No hay trend_events ni range_events. Celda 08 no puede calcular frecuencia.\")\n",
    "    print(\"   ‚Üí Revisa salida de Celda 07 o corrige paths del pipeline.\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    empty_out = pl.DataFrame({\n",
    "        \"symbol\": pl.Series([], dtype=pl.Utf8),\n",
    "        \"n_per_month_tr\": pl.Series([], dtype=pl.Float64),\n",
    "        \"n_per_month_rg\": pl.Series([], dtype=pl.Float64),\n",
    "        \"n_per_month_total\": pl.Series([], dtype=pl.Float64),\n",
    "        \"coverage_p\": pl.Series([], dtype=pl.Float64),\n",
    "        \"events_definition_suspect\": pl.Series([], dtype=pl.Boolean),\n",
    "        \"operable_bars_per_month_est\": pl.Series([], dtype=pl.Float64),\n",
    "        \"n_per_month_total_safe\": pl.Series([], dtype=pl.Float64),\n",
    "        \"n_per_month_total_safe_H\": pl.Series([], dtype=pl.Float64),\n",
    "        \"has_basic_opportunity\": pl.Series([], dtype=pl.Boolean),\n",
    "        \"has_basic_opportunity_safe\": pl.Series([], dtype=pl.Boolean),\n",
    "        \"event_to_operable_bar_ratio_median\": pl.Series([], dtype=pl.Float64),\n",
    "        \"event_to_operable_bar_ratio_frac_ge_threshold\": pl.Series([], dtype=pl.Float64),\n",
    "        \"months_ratio_observed\": pl.Series([], dtype=pl.UInt32),\n",
    "    })\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    empty_out.write_parquet(out_path)\n",
    "\n",
    "    print(f\"\\nOutput vac√≠o escrito en: {out_path}\")\n",
    "    print(\"CELDA 08 v3.1 - FIN (modo degradado)\")\n",
    "    print(\"=\"*140)\n",
    "\n",
    "else:\n",
    "    # =========================\n",
    "    # 7) Unificar eventos + agregar r√©gimen\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Unificando eventos TREND + RANGE\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    if trend_events.height > 0:\n",
    "        trend_events = trend_events.with_columns(pl.lit(\"TREND\").alias(\"regime\"))\n",
    "    if range_events.height > 0:\n",
    "        range_events = range_events.with_columns(pl.lit(\"RANGE\").alias(\"regime\"))\n",
    "\n",
    "    events = pl.concat(\n",
    "        [df for df in [trend_events, range_events] if df.height > 0],\n",
    "        how=\"vertical_relaxed\"\n",
    "    )\n",
    "\n",
    "    print(f\"events unificados: rows={events.height:,}, cols={events.width}\")\n",
    "    print(f\"schema: {events.schema}\")\n",
    "    print(f\"head(5):\\n{events.head(5)}\")\n",
    "\n",
    "    # =========================\n",
    "    # 8) Conteo mensual de eventos\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Conteo mensual de eventos\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    events_monthly = (\n",
    "        events\n",
    "        .with_columns(pl.col(\"time_utc\").dt.truncate(\"1mo\").alias(\"month\"))\n",
    "        .group_by([\"symbol\", \"regime\", \"month\"])\n",
    "        .agg(pl.len().alias(\"n_events\"))\n",
    "    )\n",
    "\n",
    "    print(\"events_monthly generado.\")\n",
    "    print(f\"rows={events_monthly.height:,}\")\n",
    "    print(f\"head(10):\\n{events_monthly.head(10)}\")\n",
    "\n",
    "    events_monthly_total = (\n",
    "        events_monthly\n",
    "        .group_by([\"symbol\", \"month\"])\n",
    "        .agg(pl.sum(\"n_events\").alias(\"n_events_total\"))\n",
    "    )\n",
    "\n",
    "    print(\"events_monthly_total generado.\")\n",
    "    print(f\"rows={events_monthly_total.height:,}\")\n",
    "    print(f\"head(10):\\n{events_monthly_total.head(10)}\")\n",
    "\n",
    "    # =========================\n",
    "    # 9) Conteo mensual de operable bars (labels)\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Conteo mensual de barras operables desde labels\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    operable_monthly_total = empty_operable_monthly_total()\n",
    "\n",
    "    if labels_df.height > 0 and \"regime\" in labels_df.columns:\n",
    "        labels_operable = labels_df.filter(pl.col(\"regime\").is_in([\"TREND\", \"RANGE\"]))\n",
    "        print(f\"labels_operable filtrado: rows={labels_operable.height:,}\")\n",
    "        print(f\"head(5):\\n{labels_operable.head(5)}\")\n",
    "\n",
    "        operable_monthly = (\n",
    "            labels_operable\n",
    "            .with_columns(pl.col(\"time_utc\").dt.truncate(\"1mo\").alias(\"month\"))\n",
    "            .group_by([\"symbol\", \"regime\", \"month\"])\n",
    "            .agg(pl.len().alias(\"n_operable_bars\"))\n",
    "        )\n",
    "\n",
    "        print(\"operable_monthly generado.\")\n",
    "        print(f\"rows={operable_monthly.height:,}\")\n",
    "        print(f\"head(10):\\n{operable_monthly.head(10)}\")\n",
    "\n",
    "        operable_monthly_total = (\n",
    "            operable_monthly\n",
    "            .group_by([\"symbol\", \"month\"])\n",
    "            .agg(pl.sum(\"n_operable_bars\").alias(\"n_operable_bars_total\"))\n",
    "        )\n",
    "\n",
    "        print(\"operable_monthly_total generado.\")\n",
    "        print(f\"rows={operable_monthly_total.height:,}\")\n",
    "        print(f\"head(10):\\n{operable_monthly_total.head(10)}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No hay labels v√°lidos para estimar operable bars.\")\n",
    "        print(\"   ‚Üí SAFE se degradar√° y caer√° a eventos.\")\n",
    "\n",
    "    # =========================\n",
    "    # 10) Ratio eventos / barras operables + sospecha\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Diagn√≥stico: ¬øeventos ‚âà barras operables?\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    ratio_by_month = (\n",
    "        full_join_symbol_month(events_monthly_total, operable_monthly_total)\n",
    "        .with_columns([\n",
    "            pl.col(\"n_events_total\").fill_null(0),\n",
    "            pl.col(\"n_operable_bars_total\").fill_null(0),\n",
    "        ])\n",
    "        .with_columns(\n",
    "            pl.when(pl.col(\"n_operable_bars_total\") > 0)\n",
    "              .then(pl.col(\"n_events_total\") / pl.col(\"n_operable_bars_total\"))\n",
    "              .otherwise(None)\n",
    "              .alias(\"event_to_operable_bar_ratio\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if \"symbol\" in ratio_by_month.columns:\n",
    "        ratio_by_month = ratio_by_month.filter(pl.col(\"symbol\").is_not_null())\n",
    "\n",
    "    print(\"ratio_by_month generado.\")\n",
    "    print(f\"rows={ratio_by_month.height:,}\")\n",
    "    print(f\"head(12):\\n{ratio_by_month.head(12)}\")\n",
    "\n",
    "    ratio_summary = (\n",
    "        ratio_by_month\n",
    "        .group_by(\"symbol\")\n",
    "        .agg([\n",
    "            pl.median(\"event_to_operable_bar_ratio\").alias(\"event_to_operable_bar_ratio_median\"),\n",
    "            (pl.col(\"event_to_operable_bar_ratio\") >= SUSPECT_RATIO_GE).mean()\n",
    "                .alias(\"event_to_operable_bar_ratio_frac_ge_threshold\"),\n",
    "            pl.count(\"event_to_operable_bar_ratio\").alias(\"months_ratio_observed\"),\n",
    "        ])\n",
    "        .with_columns(\n",
    "            (\n",
    "                (pl.col(\"event_to_operable_bar_ratio_median\") >= SUSPECT_MEDIAN_TH) &\n",
    "                (pl.col(\"event_to_operable_bar_ratio_frac_ge_threshold\") >= SUSPECT_FRAC_TH)\n",
    "            ).alias(\"events_definition_suspect\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"ratio_summary generado.\")\n",
    "    print(f\"rows={ratio_summary.height:,}\")\n",
    "    print(f\"head(20):\\n{ratio_summary.head(20)}\")\n",
    "\n",
    "    # =========================\n",
    "    # 11) M√©tricas base n/mes + coverage\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"M√©tricas base: n_per_month_* y coverage_p\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    n_tr = (\n",
    "        events_monthly\n",
    "        .filter(pl.col(\"regime\") == \"TREND\")\n",
    "        .group_by(\"symbol\")\n",
    "        .agg(pl.mean(\"n_events\").alias(\"n_per_month_tr\"))\n",
    "    )\n",
    "\n",
    "    n_rg = (\n",
    "        events_monthly\n",
    "        .filter(pl.col(\"regime\") == \"RANGE\")\n",
    "        .group_by(\"symbol\")\n",
    "        .agg(pl.mean(\"n_events\").alias(\"n_per_month_rg\"))\n",
    "    )\n",
    "\n",
    "    n_total = (\n",
    "        events_monthly_total\n",
    "        .group_by(\"symbol\")\n",
    "        .agg(pl.mean(\"n_events_total\").alias(\"n_per_month_total\"))\n",
    "    )\n",
    "\n",
    "    coverage = (\n",
    "        events_monthly_total\n",
    "        .group_by(\"symbol\")\n",
    "        .agg((pl.col(\"n_events_total\") > 0).mean().alias(\"coverage_p\"))\n",
    "    )\n",
    "\n",
    "    operable_month_est = pl.DataFrame()\n",
    "    if operable_monthly_total.height > 0 and \"n_operable_bars_total\" in operable_monthly_total.columns:\n",
    "        operable_month_est = (\n",
    "            operable_monthly_total\n",
    "            .group_by(\"symbol\")\n",
    "            .agg(pl.mean(\"n_operable_bars_total\").alias(\"operable_bars_per_month_est\"))\n",
    "        )\n",
    "\n",
    "    # =========================\n",
    "    # 12) Summary final con SAFE\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Construyendo summary final con SAFE PROXY\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    n_total_s  = n_total.select([\"symbol\", \"n_per_month_total\"]) if n_total.height > 0 else pl.DataFrame(schema={\"symbol\": pl.Utf8, \"n_per_month_total\": pl.Float64})\n",
    "    n_tr_s     = n_tr.select([\"symbol\", \"n_per_month_tr\"]) if n_tr.height > 0 else pl.DataFrame(schema={\"symbol\": pl.Utf8, \"n_per_month_tr\": pl.Float64})\n",
    "    n_rg_s     = n_rg.select([\"symbol\", \"n_per_month_rg\"]) if n_rg.height > 0 else pl.DataFrame(schema={\"symbol\": pl.Utf8, \"n_per_month_rg\": pl.Float64})\n",
    "    coverage_s = coverage.select([\"symbol\", \"coverage_p\"]) if coverage.height > 0 else pl.DataFrame(schema={\"symbol\": pl.Utf8, \"coverage_p\": pl.Float64})\n",
    "\n",
    "    operable_month_est_s = (\n",
    "        operable_month_est.select([\"symbol\", \"operable_bars_per_month_est\"])\n",
    "        if operable_month_est.height > 0\n",
    "        else pl.DataFrame(schema={\"symbol\": pl.Utf8, \"operable_bars_per_month_est\": pl.Float64})\n",
    "    )\n",
    "\n",
    "    ratio_summary_s = (\n",
    "        ratio_summary.select([\n",
    "            \"symbol\",\n",
    "            \"event_to_operable_bar_ratio_median\",\n",
    "            \"event_to_operable_bar_ratio_frac_ge_threshold\",\n",
    "            \"months_ratio_observed\",\n",
    "            \"events_definition_suspect\",\n",
    "        ])\n",
    "        if ratio_summary.height > 0\n",
    "        else pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"event_to_operable_bar_ratio_median\": pl.Float64,\n",
    "            \"event_to_operable_bar_ratio_frac_ge_threshold\": pl.Float64,\n",
    "            \"months_ratio_observed\": pl.UInt32,\n",
    "            \"events_definition_suspect\": pl.Boolean,\n",
    "        })\n",
    "    )\n",
    "\n",
    "    summary = full_join_symbol(n_total_s, n_tr_s)\n",
    "    summary = full_join_symbol(summary, n_rg_s)\n",
    "    summary = full_join_symbol(summary, coverage_s)\n",
    "    summary = full_join_symbol(summary, operable_month_est_s)\n",
    "\n",
    "    summary = summary.join(ratio_summary_s, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    summary = summary.with_columns([\n",
    "        pl.col(\"n_per_month_tr\").fill_null(0.0),\n",
    "        pl.col(\"n_per_month_rg\").fill_null(0.0),\n",
    "        pl.col(\"n_per_month_total\").fill_null(0.0),\n",
    "        pl.col(\"coverage_p\").fill_null(0.0),\n",
    "        pl.col(\"events_definition_suspect\").fill_null(False),\n",
    "    ])\n",
    "\n",
    "    summary = summary.with_columns([\n",
    "        pl.when(pl.col(\"events_definition_suspect\") & pl.col(\"operable_bars_per_month_est\").is_not_null())\n",
    "          .then(pl.col(\"operable_bars_per_month_est\"))\n",
    "          .otherwise(pl.col(\"n_per_month_total\"))\n",
    "          .alias(\"n_per_month_total_safe\"),\n",
    "\n",
    "        pl.when(pl.col(\"events_definition_suspect\") & pl.col(\"operable_bars_per_month_est\").is_not_null())\n",
    "          .then(pl.col(\"operable_bars_per_month_est\") / float(H))\n",
    "          .otherwise(pl.col(\"n_per_month_total\"))\n",
    "          .alias(\"n_per_month_total_safe_H\"),\n",
    "    ])\n",
    "\n",
    "    summary = summary.with_columns([\n",
    "        (pl.col(\"n_per_month_total\") >= MIN_OPP_PER_MONTH).alias(\"has_basic_opportunity\"),\n",
    "        (pl.col(\"n_per_month_total_safe\") >= MIN_OPP_PER_MONTH).alias(\"has_basic_opportunity_safe\"),\n",
    "    ])\n",
    "\n",
    "    print(\"summary construido.\")\n",
    "    print(f\"rows={summary.height:,}, cols={summary.width}\")\n",
    "    print(f\"schema:\\n{summary.schema}\")\n",
    "    print(f\"head(20):\\n{summary.head(20)}\")\n",
    "\n",
    "    # =========================\n",
    "    # 13) Auditor√≠a integrada\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Auditor√≠a interna integrada\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    n_sus = summary.filter(pl.col(\"events_definition_suspect\") == True).height\n",
    "    print(f\"S√≠mbolos sospechosos: {n_sus} / {summary.height}\")\n",
    "\n",
    "    # =========================\n",
    "    # 14) Guardar output + GLOBAL_STATE\n",
    "    # =========================\n",
    "    print(\"\\n\" + \"-\"*120)\n",
    "    print(\"Guardando output Celda 08\")\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    summary.write_parquet(out_path)\n",
    "\n",
    "    print(f\"Output escrito correctamente en: {out_path}\")\n",
    "    print(f\"rows={summary.height:,}, cols={summary.width}\")\n",
    "\n",
    "    if use_global_state:\n",
    "        GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "        GLOBAL_STATE[\"metrics\"][\"frequency_opportunity_table_path\"] = str(out_path)\n",
    "\n",
    "        GLOBAL_STATE.setdefault(\"tables\", {})\n",
    "        GLOBAL_STATE[\"tables\"][\"frequency_opportunity_5m\"] = summary\n",
    "\n",
    "        print(\"GLOBAL_STATE actualizado: metrics + tables.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*140)\n",
    "    print(\"CELDA 08 v3.1 SAFE PROXY - FIN\")\n",
    "    print(\"=\"*140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f145702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 08STAB v2.3.1 INSTITUTIONAL :: Estabilidad base (ER/PD + m√©tricas separadas + bars/day observado sanitizado)\n",
      "[Celda 08STAB] RUN_ID      = 20251218_190810\n",
      "[Celda 08STAB] events_dir  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\n",
      "[Celda 08STAB] stability  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\n",
      "[Celda 08STAB] config.json = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json (exists=true)\n",
      "[Celda 08STAB] bars/day fallback=288.0\n",
      "[Celda 08STAB] MIN_EVENTS_IS=2000 | MIN_MONTHS_IS=6\n",
      "[Celda 08STAB] strict_event_definition=false\n",
      "[Celda 08STAB] base agg rows = 166 | cols = 7\n",
      "[Celda 08STAB] INPUT ‚Üí PAD_DAY_INDEX = C:\\Quant\\MT5_Data_Extraction\\data\\metadata\\day_index_m5.parquet\n",
      "[Celda 08STAB] bars/day used (sanitized) :: median=209.60 | min=202.20 | max=294.20\n",
      "[Celda 08STAB] INPUT ‚Üí regime_labels = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "[Celda 08STAB] opportunity_rate_bar_IS :: median=0.493436 | frac>=thr(0.60)=0.0%\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (rows=166, cols=31)\n",
      ">>> Celda 08STAB v2.3.1 INSTITUTIONAL :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 08STAB v2.3.1 INSTITUTIONAL (Observed bars/day + sanitize) =====================\n",
    "# Patch sobre v2.3:\n",
    "#   - Sanitiza bars/day observado:\n",
    "#       si bars_per_day_observed > 500 -> intenta /5 (t√≠pico de day_index tipo M1)\n",
    "#       acepta solo si queda en rango [40, 400]; si no, NULL -> fallback\n",
    "#   - Mantiene auditor√≠a: agrega *_raw para ver valores antes del ajuste\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 08STAB v2.3.1 INSTITUTIONAL :: Estabilidad base (ER/PD + m√©tricas separadas + bars/day observado sanitizado)\")\n",
    "\n",
    "# ========================= Validaciones =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe.\")\n",
    "\n",
    "for k in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE incompleto; falta clave: '{k}'\")\n",
    "\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "for k in (\"events\", \"stability\", \"diagnostics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"Falta GLOBAL_STATE['paths']['{k}']\")\n",
    "\n",
    "RUN_ID       = GLOBAL_STATE[\"run_id\"]\n",
    "OUT_EVENTS   = Path(paths[\"events\"]).resolve()\n",
    "OUT_STAB_DIR = Path(paths[\"stability\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "\n",
    "OUT_STAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stab_table_path = OUT_STAB_DIR / \"stability_table.parquet\"\n",
    "config_path     = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "print(f\"[Celda 08STAB] RUN_ID      = {RUN_ID}\")\n",
    "print(f\"[Celda 08STAB] events_dir  = {OUT_EVENTS}\")\n",
    "print(f\"[Celda 08STAB] stability  = {OUT_STAB_DIR}\")\n",
    "print(f\"[Celda 08STAB] config.json = {config_path} (exists={'true' if config_path.exists() else 'false'})\")\n",
    "\n",
    "# ========================= Config =========================\n",
    "def _load_config(p: Path) -> dict:\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        return obj if isinstance(obj, dict) else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "CONFIG = GLOBAL_STATE.get(\"config\")\n",
    "if not isinstance(CONFIG, dict) or not CONFIG:\n",
    "    CONFIG = _load_config(config_path)\n",
    "    GLOBAL_STATE[\"config\"] = CONFIG if isinstance(CONFIG, dict) else {}\n",
    "\n",
    "stats_cfg = (GLOBAL_STATE.get(\"config\", {}) or {}).get(\"stats\", {}) or {}\n",
    "opp_cfg   = (GLOBAL_STATE.get(\"config\", {}) or {}).get(\"opportunity\", {}) or {}\n",
    "\n",
    "def _cfg_float(section: dict, key: str, default: float) -> float:\n",
    "    try:\n",
    "        return float(section.get(key, default))\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _cfg_bool(section: dict, key: str, default: bool) -> bool:\n",
    "    try:\n",
    "        v = section.get(key, default)\n",
    "        if isinstance(v, bool): return v\n",
    "        if isinstance(v, str):  return v.strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "        return bool(v)\n",
    "    except Exception:\n",
    "        return bool(default)\n",
    "\n",
    "BARS_PER_DAY_FALLBACK = _cfg_float(stats_cfg, \"bars_per_day_fallback\", 288.0)\n",
    "MIN_EVENTS_IS         = int(_cfg_float(stats_cfg, \"min_events_IS\", 2000.0))\n",
    "MIN_MONTHS_IS         = int(_cfg_float(stats_cfg, \"min_months_IS\", 6.0))\n",
    "\n",
    "STRICT_EVENT_DEFINITION = _cfg_bool(opp_cfg, \"strict_event_definition\", False)\n",
    "RATIO_SUSPECT_THR       = _cfg_float(opp_cfg, \"event_bar_ratio_suspect_thr\", 0.60)\n",
    "RATIO_SUSPECT_PCT       = _cfg_float(opp_cfg, \"event_bar_ratio_suspect_pct\", 0.70)\n",
    "\n",
    "print(f\"[Celda 08STAB] bars/day fallback={BARS_PER_DAY_FALLBACK}\")\n",
    "print(f\"[Celda 08STAB] MIN_EVENTS_IS={MIN_EVENTS_IS} | MIN_MONTHS_IS={MIN_MONTHS_IS}\")\n",
    "print(f\"[Celda 08STAB] strict_event_definition={'true' if STRICT_EVENT_DEFINITION else 'false'}\")\n",
    "\n",
    "# ========================= Helpers =========================\n",
    "def _exists(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _norm_symbol_expr(col: str = \"symbol\") -> pl.Expr:\n",
    "    return pl.col(col).cast(pl.Utf8, strict=False).str.to_uppercase()\n",
    "\n",
    "def _scan_events(path: Path, family_name: str) -> pl.LazyFrame:\n",
    "    if not _exists(path):\n",
    "        print(f\"[Celda 08STAB] WARN: no existe {path.name}; family={family_name} sin eventos.\")\n",
    "        return pl.LazyFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"time_utc\": pl.Datetime(\"us\", \"UTC\"),\n",
    "            \"success\": pl.Int8,\n",
    "            \"ndq_event\": pl.Float64,\n",
    "            \"family\": pl.Utf8,\n",
    "        })\n",
    "\n",
    "    lf = pl.scan_parquet(str(path))\n",
    "\n",
    "    # Evitar PerformanceWarning: schema names sin resolver completo\n",
    "    try:\n",
    "        cols = set(lf.collect_schema().names())\n",
    "    except Exception:\n",
    "        cols = set(lf.columns)\n",
    "\n",
    "    if \"family\" not in cols:\n",
    "        lf = lf.with_columns(pl.lit(family_name).alias(\"family\"))\n",
    "\n",
    "    if \"success\" not in cols:\n",
    "        lf = lf.with_columns(pl.lit(None).cast(pl.Int8).alias(\"success\"))\n",
    "\n",
    "    if \"ndq_event\" not in cols:\n",
    "        lf = lf.with_columns(pl.lit(None).cast(pl.Float64).alias(\"ndq_event\"))\n",
    "\n",
    "    if \"time_utc\" not in cols:\n",
    "        lf = lf.with_columns(pl.lit(None).cast(pl.Datetime(\"us\", \"UTC\")).alias(\"time_utc\"))\n",
    "    else:\n",
    "        lf = lf.with_columns(\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\", None), strict=False).dt.replace_time_zone(\"UTC\")\n",
    "        )\n",
    "\n",
    "    if \"segment\" in cols:\n",
    "        lf = lf.with_columns(pl.col(\"segment\").cast(pl.Utf8, strict=False).str.to_uppercase())\n",
    "    else:\n",
    "        lf = lf.with_columns(pl.lit(\"IS\").alias(\"segment\"))\n",
    "\n",
    "    lf = lf.with_columns([\n",
    "        _norm_symbol_expr(\"symbol\").alias(\"symbol\"),\n",
    "        pl.col(\"family\").cast(pl.Utf8, strict=False).alias(\"preset\"),\n",
    "        pl.col(\"success\").cast(pl.Float64, strict=False).alias(\"success_f\"),\n",
    "        pl.col(\"ndq_event\").cast(pl.Float64, strict=False).alias(\"ndq_event_f\"),\n",
    "    ])\n",
    "\n",
    "    return lf.select([\"symbol\", \"time_utc\", \"preset\", \"success_f\", \"ndq_event_f\", \"segment\"])\n",
    "\n",
    "def _resolve_pad_day_index_path() -> Optional[Path]:\n",
    "    try:\n",
    "        if \"PAD_DAY_INDEX\" in globals():\n",
    "            p = Path(str(globals()[\"PAD_DAY_INDEX\"])).expanduser()\n",
    "            if _exists(p):\n",
    "                return p.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for key in (\"PAD_DAY_INDEX\", \"pad_day_index\", \"day_index_path\"):\n",
    "        try:\n",
    "            if key in GLOBAL_STATE:\n",
    "                p = Path(str(GLOBAL_STATE[key])).expanduser()\n",
    "                if _exists(p):\n",
    "                    return p.resolve()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        pr = Path(GLOBAL_STATE[\"project_root\"])\n",
    "        cand = pr / \"data\" / \"metadata\" / \"day_index_m5.parquet\"\n",
    "        if _exists(cand):\n",
    "            return cand.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if \"metadata\" in paths:\n",
    "            cand = Path(paths[\"metadata\"]).resolve() / \"day_index_m5.parquet\"\n",
    "            if _exists(cand):\n",
    "                return cand.resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "def _bars_per_day_lookup(day_index_path: Path, symbols: List[str]) -> pl.DataFrame:\n",
    "    lf = pl.scan_parquet(str(day_index_path))\n",
    "\n",
    "    try:\n",
    "        cols = set(lf.collect_schema().names())\n",
    "    except Exception:\n",
    "        cols = set(lf.columns)\n",
    "\n",
    "    if \"symbol\" not in cols:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"bars_per_day_observed\": pl.Float64,\n",
    "            \"bars_per_day_p10\": pl.Float64,\n",
    "            \"bars_per_day_p90\": pl.Float64,\n",
    "            \"n_days_observed_for_bars\": pl.Int64,\n",
    "        })\n",
    "\n",
    "    lf = lf.with_columns(_norm_symbol_expr(\"symbol\").alias(\"symbol\")).filter(pl.col(\"symbol\").is_in(symbols))\n",
    "\n",
    "    day_col = None\n",
    "    if \"day_utc\" in cols: day_col = \"day_utc\"\n",
    "    elif \"date_utc\" in cols: day_col = \"date_utc\"\n",
    "    elif \"date\" in cols: day_col = \"date\"\n",
    "    elif \"time_utc\" in cols:\n",
    "        lf = lf.with_columns(\n",
    "            pl.col(\"time_utc\").cast(pl.Datetime(\"us\", None), strict=False).dt.replace_time_zone(\"UTC\").dt.truncate(\"1d\").alias(\"day_utc\")\n",
    "        )\n",
    "        day_col = \"day_utc\"\n",
    "\n",
    "    if day_col is None:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"bars_per_day_observed\": pl.Float64,\n",
    "            \"bars_per_day_p10\": pl.Float64,\n",
    "            \"bars_per_day_p90\": pl.Float64,\n",
    "            \"n_days_observed_for_bars\": pl.Int64,\n",
    "        })\n",
    "\n",
    "    lf = lf.with_columns(\n",
    "        pl.col(day_col).cast(pl.Datetime(\"us\", None), strict=False).dt.date().alias(\"day_date\")\n",
    "        if day_col in (\"day_utc\", \"date_utc\", \"time_utc\")\n",
    "        else pl.col(day_col).cast(pl.Date, strict=False).alias(\"day_date\")\n",
    "    )\n",
    "\n",
    "    candidates = [\"bars_in_day\",\"n_bars\",\"bars\",\"bar_count\",\"bars_count\",\"count_bars\",\"n_bars_day\",\"bars_per_day\",\"bars_day\"]\n",
    "    count_col = next((c for c in candidates if c in cols), None)\n",
    "\n",
    "    if count_col is not None:\n",
    "        lf_day = lf.select([\n",
    "            pl.col(\"symbol\"),\n",
    "            pl.col(\"day_date\"),\n",
    "            pl.col(count_col).cast(pl.Float64, strict=False).alias(\"bars_in_day\")\n",
    "        ]).filter(pl.col(\"bars_in_day\").is_not_null() & (pl.col(\"bars_in_day\") > 0))\n",
    "    else:\n",
    "        lf_day = (\n",
    "            lf.select([pl.col(\"symbol\"), pl.col(\"day_date\")])\n",
    "              .group_by([\"symbol\", \"day_date\"])\n",
    "              .agg(pl.len().cast(pl.Float64).alias(\"bars_in_day\"))\n",
    "        )\n",
    "\n",
    "    lf_sym = (\n",
    "        lf_day.group_by(\"symbol\")\n",
    "              .agg([\n",
    "                  pl.col(\"bars_in_day\").median().alias(\"bars_per_day_observed\"),\n",
    "                  pl.col(\"bars_in_day\").quantile(0.10, interpolation=\"nearest\").alias(\"bars_per_day_p10\"),\n",
    "                  pl.col(\"bars_in_day\").quantile(0.90, interpolation=\"nearest\").alias(\"bars_per_day_p90\"),\n",
    "                  pl.len().alias(\"n_days_observed_for_bars\"),\n",
    "              ])\n",
    "    )\n",
    "\n",
    "    return lf_sym.collect(engine=\"streaming\")\n",
    "\n",
    "# ========================= Lectura de eventos TREND/RANGE =========================\n",
    "trend_path = OUT_EVENTS / \"trend_events.parquet\"\n",
    "range_path = OUT_EVENTS / \"range_events.parquet\"\n",
    "\n",
    "lf_trend = _scan_events(trend_path, \"TREND\")\n",
    "lf_range = _scan_events(range_path, \"RANGE\")\n",
    "lf_all = pl.concat([lf_trend, lf_range], how=\"vertical\")\n",
    "\n",
    "lf_all = lf_all.filter(\n",
    "    pl.col(\"symbol\").is_not_null()\n",
    "    & pl.col(\"preset\").is_in([\"TREND\", \"RANGE\"])\n",
    "    & pl.col(\"time_utc\").is_not_null()\n",
    ")\n",
    "\n",
    "# Si existe segment y trae IS, filtramos IS\n",
    "try:\n",
    "    seg_vals = lf_all.select(pl.col(\"segment\")).unique().collect(engine=\"streaming\").get_column(\"segment\").to_list()\n",
    "    if any(str(x).upper() == \"IS\" for x in seg_vals):\n",
    "        lf_all = lf_all.filter(pl.col(\"segment\") == \"IS\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "lf_all = lf_all.with_columns([\n",
    "    pl.col(\"time_utc\").dt.truncate(\"1d\").alias(\"day_utc\"),\n",
    "    pl.col(\"time_utc\").dt.truncate(\"1mo\").alias(\"month_utc\"),\n",
    "])\n",
    "\n",
    "lf_agg = (\n",
    "    lf_all.group_by([\"symbol\", \"preset\"])\n",
    "         .agg([\n",
    "             pl.len().alias(\"n_events_IS\"),\n",
    "             pl.col(\"day_utc\").n_unique().alias(\"days_IS\"),\n",
    "             pl.col(\"month_utc\").n_unique().alias(\"months_IS\"),\n",
    "             pl.col(\"ndq_event_f\").mean().alias(\"ER_IS\"),\n",
    "             pl.col(\"success_f\").mean().alias(\"PD_IS\"),\n",
    "         ])\n",
    "         .with_columns([\n",
    "             pl.when(pl.col(\"days_IS\") <= 0).then(1).otherwise(pl.col(\"days_IS\")).alias(\"days_IS\"),\n",
    "             pl.when(pl.col(\"months_IS\") <= 0).then(1).otherwise(pl.col(\"months_IS\")).alias(\"months_IS\"),\n",
    "         ])\n",
    ")\n",
    "\n",
    "stab_base = lf_agg.collect(engine=\"streaming\")\n",
    "if stab_base.is_empty():\n",
    "    raise RuntimeError(\"[Celda 08STAB] No se pudieron construir agregados desde eventos (tabla vac√≠a).\")\n",
    "\n",
    "print(f\"[Celda 08STAB] base agg rows = {stab_base.height} | cols = {len(stab_base.columns)}\")\n",
    "\n",
    "# ========================= bars/day observado (PAD_DAY_INDEX) =========================\n",
    "syms = stab_base.get_column(\"symbol\").unique().to_list()\n",
    "\n",
    "pad_day_index_path = _resolve_pad_day_index_path()\n",
    "if pad_day_index_path is None:\n",
    "    print(\"[Celda 08STAB][WARN] No se encontr√≥ PAD_DAY_INDEX/day_index_m5.parquet. Se usar√° fallback bars/day.\")\n",
    "    bars_lookup = pl.DataFrame(schema={\n",
    "        \"symbol\": pl.Utf8,\n",
    "        \"bars_per_day_observed\": pl.Float64,\n",
    "        \"bars_per_day_p10\": pl.Float64,\n",
    "        \"bars_per_day_p90\": pl.Float64,\n",
    "        \"n_days_observed_for_bars\": pl.Int64,\n",
    "    })\n",
    "else:\n",
    "    print(f\"[Celda 08STAB] INPUT ‚Üí PAD_DAY_INDEX = {pad_day_index_path}\")\n",
    "    bars_lookup = _bars_per_day_lookup(pad_day_index_path, syms)\n",
    "\n",
    "stab_base = stab_base.join(bars_lookup, on=\"symbol\", how=\"left\")\n",
    "\n",
    "# Guardar raw\n",
    "stab_base = stab_base.with_columns([\n",
    "    pl.col(\"bars_per_day_observed\").alias(\"bars_per_day_observed_raw\"),\n",
    "    pl.col(\"bars_per_day_p10\").alias(\"bars_per_day_p10_raw\"),\n",
    "    pl.col(\"bars_per_day_p90\").alias(\"bars_per_day_p90_raw\"),\n",
    "])\n",
    "\n",
    "# Sanitizar (si >500, intentar /5 y aceptar solo en [40,400])\n",
    "def _sanitize_expr(x: pl.Expr) -> pl.Expr:\n",
    "    adj = pl.when(x.is_not_null() & (x > 500.0)).then(x / 5.0).otherwise(x)\n",
    "    return pl.when(adj.is_not_null() & (adj >= 40.0) & (adj <= 400.0)).then(adj).otherwise(pl.lit(None))\n",
    "\n",
    "stab_base = stab_base.with_columns([\n",
    "    _sanitize_expr(pl.col(\"bars_per_day_observed_raw\").cast(pl.Float64, strict=False)).alias(\"bars_per_day_observed\"),\n",
    "    _sanitize_expr(pl.col(\"bars_per_day_p10_raw\").cast(pl.Float64, strict=False)).alias(\"bars_per_day_p10\"),\n",
    "    _sanitize_expr(pl.col(\"bars_per_day_p90_raw\").cast(pl.Float64, strict=False)).alias(\"bars_per_day_p90\"),\n",
    "])\n",
    "\n",
    "# bars_per_day_used (auditado) + bars_IS_est\n",
    "stab_base = stab_base.with_columns(\n",
    "    pl.when(pl.col(\"bars_per_day_observed\").is_not_null() & (pl.col(\"bars_per_day_observed\") > 0.0))\n",
    "      .then(pl.col(\"bars_per_day_observed\"))\n",
    "      .otherwise(pl.lit(float(BARS_PER_DAY_FALLBACK)))\n",
    "      .alias(\"bars_per_day_used\")\n",
    ").with_columns(\n",
    "    (pl.col(\"days_IS\").cast(pl.Float64) * pl.col(\"bars_per_day_used\").cast(pl.Float64)).alias(\"bars_IS_est\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    used_stats = stab_base.select([\n",
    "        pl.col(\"bars_per_day_used\").median().alias(\"bars/day median used\"),\n",
    "        pl.col(\"bars_per_day_used\").min().alias(\"bars/day min used\"),\n",
    "        pl.col(\"bars_per_day_used\").max().alias(\"bars/day max used\"),\n",
    "    ]).row(0, named=True)\n",
    "    print(f\"[Celda 08STAB] bars/day used (sanitized) :: median={used_stats['bars/day median used']:.2f} | \"\n",
    "          f\"min={used_stats['bars/day min used']:.2f} | max={used_stats['bars/day max used']:.2f}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ========================= M√©tricas de oportunidad =========================\n",
    "stab_base = stab_base.with_columns([\n",
    "    pl.when(pl.col(\"bars_IS_est\") > 0.0)\n",
    "      .then(pl.col(\"n_events_IS\").cast(pl.Float64) / pl.col(\"bars_IS_est\"))\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"opportunity_rate_bar_IS\"),\n",
    "\n",
    "    pl.when(pl.col(\"days_IS\") > 0)\n",
    "      .then(pl.col(\"n_events_IS\").cast(pl.Float64) / (pl.col(\"days_IS\").cast(pl.Float64) / 30.0))\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"opportunity_rate_month_IS\"),\n",
    "])\n",
    "\n",
    "stab_base = stab_base.with_columns(pl.col(\"opportunity_rate_bar_IS\").alias(\"p_event_IS\"))\n",
    "stab_base = stab_base.with_columns(pl.col(\"PD_IS\").alias(\"success_rate_after_cost_IS\"))\n",
    "\n",
    "# ========================= regime_share desde regime_labels =========================\n",
    "regime_labels_path: Optional[Path] = None\n",
    "if \"metrics\" in paths:\n",
    "    cand = (Path(paths[\"metrics\"]).resolve() / \"regime_labels.parquet\")\n",
    "    if _exists(cand):\n",
    "        regime_labels_path = cand\n",
    "if regime_labels_path is None:\n",
    "    cand = OUT_STAB_DIR.parent / \"metrics\" / \"regime_labels.parquet\"\n",
    "    if _exists(cand):\n",
    "        regime_labels_path = cand\n",
    "\n",
    "if regime_labels_path is None:\n",
    "    print(\"[Celda 08STAB][WARN] No se encontr√≥ regime_labels.parquet; regime_share quedar√° NULL.\")\n",
    "    stab_base = stab_base.with_columns([\n",
    "        pl.lit(None).cast(pl.Float64).alias(\"regime_share\"),\n",
    "        pl.lit(None).cast(pl.Int64).alias(\"bars_total\"),\n",
    "        pl.lit(None).cast(pl.Int64).alias(\"regime_bars\"),\n",
    "        pl.lit(None).cast(pl.Float64).alias(\"opportunity_share_in_regime\"),\n",
    "    ])\n",
    "else:\n",
    "    print(f\"[Celda 08STAB] INPUT ‚Üí regime_labels = {regime_labels_path}\")\n",
    "\n",
    "    lf_lab = pl.scan_parquet(str(regime_labels_path)).select([\"symbol\", \"regime\"])\n",
    "    lf_lab = lf_lab.with_columns(_norm_symbol_expr(\"symbol\").alias(\"symbol\"))\n",
    "    lf_lab = lf_lab.filter(pl.col(\"symbol\").is_in(syms))\n",
    "\n",
    "    df_total = lf_lab.group_by(\"symbol\").agg(pl.len().alias(\"bars_total\")).collect(engine=\"streaming\")\n",
    "    df_reg = (\n",
    "        lf_lab.filter(pl.col(\"regime\").is_in([\"TREND\", \"RANGE\"]))\n",
    "              .group_by([\"symbol\", \"regime\"])\n",
    "              .agg(pl.len().alias(\"bars_regime\"))\n",
    "              .collect(engine=\"streaming\")\n",
    "    )\n",
    "\n",
    "    if df_total.is_empty():\n",
    "        stab_base = stab_base.with_columns([\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"regime_share\"),\n",
    "            pl.lit(None).cast(pl.Int64).alias(\"bars_total\"),\n",
    "            pl.lit(None).cast(pl.Int64).alias(\"regime_bars\"),\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"opportunity_share_in_regime\"),\n",
    "        ])\n",
    "    else:\n",
    "        if df_reg.is_empty():\n",
    "            df_piv = pl.DataFrame({\"symbol\": df_total[\"symbol\"], \"bars_TREND\": [0]*df_total.height, \"bars_RANGE\": [0]*df_total.height})\n",
    "        else:\n",
    "            df_piv = (\n",
    "                df_reg.pivot(values=\"bars_regime\", index=\"symbol\", on=\"regime\", aggregate_function=\"first\")\n",
    "                      .fill_null(0)\n",
    "            )\n",
    "            if \"TREND\" in df_piv.columns:\n",
    "                df_piv = df_piv.rename({\"TREND\": \"bars_TREND\"})\n",
    "            else:\n",
    "                df_piv = df_piv.with_columns(pl.lit(0).alias(\"bars_TREND\"))\n",
    "            if \"RANGE\" in df_piv.columns:\n",
    "                df_piv = df_piv.rename({\"RANGE\": \"bars_RANGE\"})\n",
    "            else:\n",
    "                df_piv = df_piv.with_columns(pl.lit(0).alias(\"bars_RANGE\"))\n",
    "\n",
    "        lab_stats = df_total.join(df_piv, on=\"symbol\", how=\"left\").fill_null(0)\n",
    "\n",
    "        stab_base = (\n",
    "            stab_base.join(lab_stats, on=\"symbol\", how=\"left\")\n",
    "                     .with_columns([\n",
    "                         pl.when(pl.col(\"preset\") == \"TREND\").then(pl.col(\"bars_TREND\"))\n",
    "                          .when(pl.col(\"preset\") == \"RANGE\").then(pl.col(\"bars_RANGE\"))\n",
    "                          .otherwise(None)\n",
    "                          .cast(pl.Int64)\n",
    "                          .alias(\"regime_bars\"),\n",
    "                     ])\n",
    "                     .with_columns([\n",
    "                         pl.when((pl.col(\"bars_total\") > 0) & pl.col(\"regime_bars\").is_not_null())\n",
    "                           .then(pl.col(\"regime_bars\").cast(pl.Float64) / pl.col(\"bars_total\").cast(pl.Float64))\n",
    "                           .otherwise(None)\n",
    "                           .alias(\"regime_share\"),\n",
    "\n",
    "                         pl.when((pl.col(\"regime_bars\") > 0) & pl.col(\"n_events_IS\").is_not_null())\n",
    "                           .then(pl.col(\"n_events_IS\").cast(pl.Float64) / pl.col(\"regime_bars\").cast(pl.Float64))\n",
    "                           .otherwise(None)\n",
    "                           .alias(\"opportunity_share_in_regime\"),\n",
    "                     ])\n",
    "        )\n",
    "\n",
    "# ========================= Flags + sanity =========================\n",
    "stab_base = stab_base.with_columns([\n",
    "    (pl.col(\"months_IS\") < MIN_MONTHS_IS).alias(\"short_history_flag\"),\n",
    "    ((pl.col(\"n_events_IS\") < MIN_EVENTS_IS) | (pl.col(\"months_IS\") < MIN_MONTHS_IS)).alias(\"low_sample_flag\"),\n",
    "]).with_columns((~pl.col(\"low_sample_flag\")).alias(\"passed_data_gate\"))\n",
    "\n",
    "stab_base = stab_base.with_columns(pl.col(\"opportunity_rate_bar_IS\").alias(\"event_to_bar_ratio_stab\"))\n",
    "\n",
    "valid_ratio = stab_base.filter(pl.col(\"event_to_bar_ratio_stab\").is_not_null())\n",
    "ratio_median = float(valid_ratio[\"event_to_bar_ratio_stab\"].median()) if valid_ratio.height else float(\"nan\")\n",
    "ratio_frac_sus = (\n",
    "    valid_ratio.filter(pl.col(\"event_to_bar_ratio_stab\") >= RATIO_SUSPECT_THR).height / valid_ratio.height\n",
    ") if valid_ratio.height else 0.0\n",
    "\n",
    "events_definition_suspect = bool(\n",
    "    (valid_ratio.height > 0) and\n",
    "    (ratio_median >= RATIO_SUSPECT_THR) and\n",
    "    (ratio_frac_sus >= RATIO_SUSPECT_PCT)\n",
    ")\n",
    "\n",
    "stab_base = stab_base.with_columns(pl.lit(events_definition_suspect).alias(\"events_definition_suspect_stab\"))\n",
    "\n",
    "print(\"[Celda 08STAB] opportunity_rate_bar_IS :: \"\n",
    "      f\"median={ratio_median:.6f} | frac>=thr({RATIO_SUSPECT_THR:.2f})={ratio_frac_sus*100:.1f}%\")\n",
    "\n",
    "if events_definition_suspect and STRICT_EVENT_DEFINITION:\n",
    "    raise RuntimeError(\"[Celda 08STAB] strict_event_definition=True ‚Üí abortando.\")\n",
    "\n",
    "# ========================= Escritura (hard overwrite) =========================\n",
    "try:\n",
    "    if stab_table_path.exists():\n",
    "        stab_table_path.unlink()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "stab_base.write_parquet(str(stab_table_path), compression=\"zstd\")\n",
    "if (not stab_table_path.exists()) or stab_table_path.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 08STAB] Error al escribir stability_table.parquet\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {stab_table_path} (rows={stab_base.height}, cols={len(stab_base.columns)})\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"stability_table_path\"] = str(stab_table_path)\n",
    "if pad_day_index_path is not None:\n",
    "    GLOBAL_STATE[\"metrics\"][\"pad_day_index_path\"] = str(pad_day_index_path)\n",
    "\n",
    "print(\">>> Celda 08STAB v2.3.1 INSTITUTIONAL :: OK\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95431ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Celda 08IS v2.3.1 PRO :: Split IS/OOS y m√©tricas por segmento (CONSISTENTE)\n",
      "[Celda 08IS] RUN_ID = 20251218_190810\n",
      "[Celda 08IS] ER_CLIP = 3.0\n",
      "[Celda 08IS] BARS_PER_DAY_FALLBACK = 288.0\n",
      "[Celda 08IS] IS_OOS_split_q = 0.7\n",
      "[Celda 08IS] policy: ER_OOS_min=None, PD_OOS_min=None, p_event_OOS_min=None, min_events_OOS=1\n",
      "[Celda 08IS] has_policy = False\n",
      "üìÅ INPUT ‚Üí stability_table = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (rows=166, cols=31)\n",
      "[Celda 08IS] Presets en stability_table = ['RANGE', 'TREND']\n",
      "[Celda 08IS] INFO: eliminando columnas IS/OOS antiguas: ['n_events_IS', 'ER_IS', 'PD_IS', 'p_event_IS', 'days_IS']\n",
      "üìÅ INPUT ‚Üí trend_events = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\trend_events.parquet (rows=9119905)\n",
      "üìÅ INPUT ‚Üí range_events = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\range_events.parquet (rows=9163602)\n",
      "[Celda 08IS] Presets en eventos = ['RANGE', 'TREND']\n",
      "[Celda 08IS] ev_folds rows=18,283,507 | head:\n",
      "shape: (5, 9)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ ts          ‚îÜ ret       ‚îÜ ‚Ä¶ ‚îÜ ts_cut_int ‚îÜ ts_cut_ISOO ‚îÜ split_q_ISO ‚îÜ segment ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---         ‚îÜ ---       ‚îÜ   ‚îÜ ---        ‚îÜ S           ‚îÜ OS          ‚îÜ ---     ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ datetime[Œºs ‚îÜ f64       ‚îÜ   ‚îÜ f64        ‚îÜ ---         ‚îÜ ---         ‚îÜ str     ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ ]           ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ datetime[Œºs ‚îÜ f64         ‚îÜ         ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ             ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ ]           ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ AAPL   ‚îÜ TREND  ‚îÜ 2021-11-19  ‚îÜ -0.037246 ‚îÜ ‚Ä¶ ‚îÜ 1.7248e15  ‚îÜ 2024-08-27  ‚îÜ 0.7         ‚îÜ IS      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ 19:15:00    ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ 17:40:00    ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ TREND  ‚îÜ 2021-11-19  ‚îÜ 0.045608  ‚îÜ ‚Ä¶ ‚îÜ 1.7248e15  ‚îÜ 2024-08-27  ‚îÜ 0.7         ‚îÜ IS      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ 19:20:00    ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ 17:40:00    ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ TREND  ‚îÜ 2021-11-19  ‚îÜ 0.048648  ‚îÜ ‚Ä¶ ‚îÜ 1.7248e15  ‚îÜ 2024-08-27  ‚îÜ 0.7         ‚îÜ IS      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ 19:25:00    ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ 17:40:00    ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ TREND  ‚îÜ 2021-11-19  ‚îÜ -0.054729 ‚îÜ ‚Ä¶ ‚îÜ 1.7248e15  ‚îÜ 2024-08-27  ‚îÜ 0.7         ‚îÜ IS      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ 19:30:00    ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ 17:40:00    ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ TREND  ‚îÜ 2021-11-19  ‚îÜ 0.063851  ‚îÜ ‚Ä¶ ‚îÜ 1.7248e15  ‚îÜ 2024-08-27  ‚îÜ 0.7         ‚îÜ IS      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ 19:35:00    ‚îÜ           ‚îÜ   ‚îÜ            ‚îÜ 17:40:00    ‚îÜ             ‚îÜ         ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 08IS] seg_wide rows=166, cols=16 | head:\n",
      "shape: (5, 16)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ n_events_I ‚îÜ ER_IS      ‚îÜ ‚Ä¶ ‚îÜ p_event_IS ‚îÜ p_event_OO ‚îÜ ts_cut_ISO ‚îÜ split_q_I ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ S          ‚îÜ ---        ‚îÜ   ‚îÜ _month     ‚îÜ S_month    ‚îÜ OS         ‚îÜ SOOS      ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ ---        ‚îÜ f64        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ ---        ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ i64        ‚îÜ            ‚îÜ   ‚îÜ f64        ‚îÜ f64        ‚îÜ datetime[Œº ‚îÜ f64       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ s]         ‚îÜ           ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ CHFJPY ‚îÜ RANGE  ‚îÜ 80813      ‚îÜ 0.00056    ‚îÜ ‚Ä¶ ‚îÜ 3348.60497 ‚îÜ 3196.89230 ‚îÜ 2024-09-02 ‚îÜ 0.7       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ 2          ‚îÜ 8          ‚îÜ 15:35:00   ‚îÜ           ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ RANGE  ‚îÜ 77037      ‚îÜ -0.003463  ‚îÜ ‚Ä¶ ‚îÜ 3209.875   ‚îÜ 3066.40866 ‚îÜ 2024-09-03 ‚îÜ 0.7       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ 9          ‚îÜ 18:50:00   ‚îÜ           ‚îÇ\n",
      "‚îÇ AAPL   ‚îÜ RANGE  ‚îÜ 21391      ‚îÜ 0.00085    ‚îÜ ‚Ä¶ ‚îÜ 906.398305 ‚îÜ 916.7      ‚îÜ 2024-09-20 ‚îÜ 0.7       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 21:55:00   ‚îÜ           ‚îÇ\n",
      "‚îÇ VECUSD ‚îÜ TREND  ‚îÜ 70571      ‚îÜ -5.7488e-7 ‚îÜ ‚Ä¶ ‚îÜ 1876.88829 ‚îÜ 2926.83871 ‚îÜ 2025-01-22 ‚îÜ 0.7       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ 8          ‚îÜ            ‚îÜ 13:55:00   ‚îÜ           ‚îÇ\n",
      "‚îÇ EURCAD ‚îÜ TREND  ‚îÜ 82597      ‚îÜ 7.7557e-7  ‚îÜ ‚Ä¶ ‚îÜ 3344.00809 ‚îÜ 3447.85714 ‚îÜ 2024-09-25 ‚îÜ 0.7       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ 7          ‚îÜ 3          ‚îÜ 21:05:00   ‚îÜ           ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 08IS] flag_oos_ok summary:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ share_oos_ok ‚îÜ n_oos_fail ‚îÇ\n",
      "‚îÇ ---          ‚îÜ ---        ‚îÇ\n",
      "‚îÇ f64          ‚îÜ i64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1.0          ‚îÜ 0          ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 08IS] stability_table (after join) rows=166, cols=42\n",
      "üíæ UPDATE ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (rows=166, cols=42)\n",
      "üíæ folds  ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stab_folds.parquet (rows=18283507)\n",
      ">>> Celda 08IS v2.3.1 PRO :: OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Celda 08IS v2.3.1 PRO ‚Äî Split IS/OOS y m√©tricas por segmento (CONSISTENTE)\n",
    "# -----------------------------------------------------------------------------\n",
    "# FIXES / FEATURES:\n",
    "#   1) Evita TypeError float(None) en pol√≠tica OOS.\n",
    "#   2) Elimina pivot(columns=...) -> agregaci√≥n por segmento sin pivot.\n",
    "#   3) Mantiene alineaci√≥n robusta de preset.\n",
    "#   4) Idempotente: limpia columnas IS/OOS viejas y sufijos *_right.\n",
    "#   5) FIX Polars: reemplaza pl.mean(expr) / pl.sum(expr) por expr.mean()/sum().\n",
    "#   6) FIX CR√çTICO CONSISTENCIA PD:\n",
    "#        - ignora cualquier 'success' preexistente en archivos de eventos\n",
    "#        - recalcula success desde ret (>0)\n",
    "#   7) Consistencia estad√≠stica con 08STAB/08S PRO:\n",
    "#        - p_event_* como ratio eventos/barras (no \"eventos/mes\" gigante)\n",
    "#        - adem√°s conserva p_event_*_month como m√©trica auxiliar.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Optional, List\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\"\\n>>> Celda 08IS v2.3.1 PRO :: Split IS/OOS y m√©tricas por segmento (CONSISTENTE)\")\n",
    "\n",
    "# ===========================\n",
    "# 0) Validaciones GLOBAL_STATE y paths\n",
    "# ===========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta las celdas iniciales del pipeline.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "required_path_keys = (\"stability\", \"metrics\", \"events\", \"diagnostics\", \"config\")\n",
    "for k in required_path_keys:\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"Falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "OUT_STAB_DIR    = Path(paths[\"stability\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_EVENTS_DIR  = Path(paths[\"events\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "CONFIG_RUN_PATH = Path(paths[\"config\"]).resolve()\n",
    "\n",
    "OUT_STAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\", None)\n",
    "print(f\"[Celda 08IS] RUN_ID = {RUN_ID}\")\n",
    "\n",
    "stab_table_path   = OUT_STAB_DIR / \"stability_table.parquet\"\n",
    "stab_folds_path   = OUT_STAB_DIR / \"stab_folds.parquet\"\n",
    "trend_events_path = OUT_EVENTS_DIR / \"trend_events.parquet\"\n",
    "range_events_path = OUT_EVENTS_DIR / \"range_events.parquet\"\n",
    "\n",
    "def _exists(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if not _exists(stab_table_path):\n",
    "    raise RuntimeError(\n",
    "        f\"[Celda 08IS] stability_table.parquet no encontrado: {stab_table_path}\\n\"\n",
    "        \"Debe existir tras ejecutar 08STAB.\"\n",
    "    )\n",
    "\n",
    "# ===========================\n",
    "# 1) Config\n",
    "# ===========================\n",
    "def _load_config_from_global() -> dict:\n",
    "    cfg = GLOBAL_STATE.get(\"config\", {})\n",
    "    if isinstance(cfg, dict) and cfg:\n",
    "        return cfg\n",
    "\n",
    "    # Intento 1: paths[\"config\"]\n",
    "    if _exists(CONFIG_RUN_PATH) and CONFIG_RUN_PATH.is_file():\n",
    "        try:\n",
    "            cfg = json.loads(CONFIG_RUN_PATH.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(cfg, dict):\n",
    "                GLOBAL_STATE[\"config\"] = cfg\n",
    "                return cfg\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Intento 2: diagnostics/config.json\n",
    "    cfg_disk = OUT_DIAG_DIR / \"config.json\"\n",
    "    if _exists(cfg_disk):\n",
    "        try:\n",
    "            cfg = json.loads(cfg_disk.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(cfg, dict):\n",
    "                GLOBAL_STATE[\"config\"] = cfg\n",
    "                return cfg\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {}\n",
    "\n",
    "CONFIG = _load_config_from_global()\n",
    "stats_cfg  = (CONFIG.get(\"stats\", {}) or {}) if isinstance(CONFIG, dict) else {}\n",
    "policy_cfg = (CONFIG.get(\"policy\", {}) or {}) if isinstance(CONFIG, dict) else {}\n",
    "\n",
    "def _cfg_float(section: dict, key: str, default: float) -> float:\n",
    "    try:\n",
    "        v = section.get(key, default)\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _cfg_any(section: dict, key: str, default=None):\n",
    "    try:\n",
    "        return section.get(key, default)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# Debe coincidir con 08STAB\n",
    "ER_CLIP = _cfg_float(stats_cfg, \"ER_clip_units\", 3.0)\n",
    "BARS_PER_DAY_FALLBACK = _cfg_float(stats_cfg, \"bars_per_day_fallback\", 288.0)\n",
    "\n",
    "# Split Q\n",
    "IS_OOS_split_q = _cfg_float(policy_cfg, \"IS_OOS_split_q\", 0.70)\n",
    "\n",
    "# Pol√≠tica OOS opcional\n",
    "ER_OOS_min      = _cfg_any(policy_cfg, \"ER_OOS_min\", None)\n",
    "PD_OOS_min      = _cfg_any(policy_cfg, \"PD_OOS_min\", None)\n",
    "p_event_OOS_min = _cfg_any(policy_cfg, \"p_event_OOS_min\", None)\n",
    "min_events_OOS  = int(_cfg_float(policy_cfg, \"min_events_OOS\", 1.0) or 1)\n",
    "\n",
    "# Flags para pol√≠tica\n",
    "has_policy_thresholds = any(v is not None for v in [ER_OOS_min, PD_OOS_min, p_event_OOS_min])\n",
    "has_policy = bool(policy_cfg) and (has_policy_thresholds or (\"min_events_OOS\" in policy_cfg))\n",
    "\n",
    "# === FIX CR√çTICO: n√∫meros seguros para expresiones ===\n",
    "ER_OOS_min_num      = float(ER_OOS_min) if ER_OOS_min is not None else 0.0\n",
    "PD_OOS_min_num      = float(PD_OOS_min) if PD_OOS_min is not None else 0.0\n",
    "p_event_OOS_min_num = float(p_event_OOS_min) if p_event_OOS_min is not None else 0.0\n",
    "\n",
    "print(f\"[Celda 08IS] ER_CLIP = {ER_CLIP}\")\n",
    "print(f\"[Celda 08IS] BARS_PER_DAY_FALLBACK = {BARS_PER_DAY_FALLBACK}\")\n",
    "print(f\"[Celda 08IS] IS_OOS_split_q = {IS_OOS_split_q}\")\n",
    "print(\"[Celda 08IS] policy:\",\n",
    "      f\"ER_OOS_min={ER_OOS_min}, PD_OOS_min={PD_OOS_min}, \"\n",
    "      f\"p_event_OOS_min={p_event_OOS_min}, min_events_OOS={min_events_OOS}\")\n",
    "print(f\"[Celda 08IS] has_policy = {has_policy}\")\n",
    "\n",
    "# ===========================\n",
    "# 2) Helpers\n",
    "# ===========================\n",
    "def _norm_symbol_series(expr: pl.Expr) -> pl.Expr:\n",
    "    return expr.cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "\n",
    "def _detect_preset_col(df: pl.DataFrame) -> Optional[str]:\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for cand in [\"preset\", \"family\", \"regime\", \"strategy\", \"model\", \"model_id\", \"preset_name\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "        if cand.lower() in low:\n",
    "            return low[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _read_events_isoos(p: Path) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve columnas est√°ndar:\n",
    "      symbol, preset, ts (Datetime us), ret ([-1,1]), success (0/1)\n",
    "\n",
    "    Construye ret normalizado como 08STAB:\n",
    "      ret_units_abs    = rng_H / cost_after\n",
    "      ret_units_signed = direction * ret_units_abs\n",
    "      ret_clipped      = clip(-ER_CLIP, ER_CLIP)\n",
    "      ret              = ret_clipped / ER_CLIP\n",
    "\n",
    "    FIX CONSISTENCIA PD:\n",
    "      - Si archivo trae 'success', SE IGNORA.\n",
    "      - success se recalcula SIEMPRE como (ret > 0).\n",
    "    \"\"\"\n",
    "    if not _exists(p):\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8,\n",
    "            \"preset\": pl.Utf8,\n",
    "            \"ts\": pl.Datetime(\"us\"),\n",
    "            \"ret\": pl.Float64,\n",
    "            \"success\": pl.Int8,\n",
    "        })\n",
    "\n",
    "    df = pl.read_parquet(str(p))\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # symbol\n",
    "    sym_col = low.get(\"symbol\", None) or (\"symbol\" if \"symbol\" in df.columns else None)\n",
    "    if sym_col is None:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8, \"preset\": pl.Utf8, \"ts\": pl.Datetime(\"us\"),\n",
    "            \"ret\": pl.Float64, \"success\": pl.Int8\n",
    "        })\n",
    "\n",
    "    df = df.with_columns(_norm_symbol_series(pl.col(sym_col)).alias(\"symbol\"))\n",
    "    if sym_col != \"symbol\":\n",
    "        df = df.drop(sym_col)\n",
    "\n",
    "    # preset\n",
    "    preset_col = _detect_preset_col(df)\n",
    "    if preset_col is None:\n",
    "        df = df.with_columns(pl.lit(\"DEFAULT\").alias(\"preset\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col(preset_col).cast(pl.Utf8, strict=False).alias(\"preset\"))\n",
    "        if preset_col != \"preset\":\n",
    "            df = df.drop(preset_col)\n",
    "\n",
    "    # time\n",
    "    tcol = low.get(\"time_utc\", None) or (\"time_utc\" if \"time_utc\" in df.columns else None)\n",
    "    if tcol is None:\n",
    "        for cand in [\"exit_time\", \"timestamp_utc\", \"close_time\", \"time\", \"datetime\", \"timestamp\"]:\n",
    "            c = low.get(cand, None) or (cand if cand in df.columns else None)\n",
    "            if c is not None:\n",
    "                tcol = c\n",
    "                break\n",
    "\n",
    "    if tcol is None:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"symbol\": pl.Utf8, \"preset\": pl.Utf8, \"ts\": pl.Datetime(\"us\"),\n",
    "            \"ret\": pl.Float64, \"success\": pl.Int8\n",
    "        })\n",
    "\n",
    "    # normalizar ts a Datetime(us)\n",
    "    try:\n",
    "        ts_expr = pl.col(tcol).cast(pl.Datetime(\"us\", None), strict=False)\n",
    "        df = df.with_columns(ts_expr.alias(\"ts\"))\n",
    "    except Exception:\n",
    "        df = df.with_columns(pl.col(tcol).cast(pl.Datetime(\"us\"), strict=False).alias(\"ts\"))\n",
    "\n",
    "    if tcol != \"ts\":\n",
    "        df = df.drop(tcol)\n",
    "\n",
    "    # columnas para retorno\n",
    "    for col_name, dtype in ((\"direction\", pl.Float64), (\"cost_after\", pl.Float64), (\"rng_H\", pl.Float64)):\n",
    "        if col_name in df.columns:\n",
    "            df = df.with_columns(pl.col(col_name).cast(dtype, strict=False))\n",
    "        else:\n",
    "            df = df.with_columns(pl.lit(None).cast(dtype).alias(col_name))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when((pl.col(\"cost_after\") > 0.0) & pl.col(\"rng_H\").is_not_null())\n",
    "          .then(pl.col(\"rng_H\") / pl.col(\"cost_after\"))\n",
    "          .otherwise(0.0)\n",
    "          .alias(\"ret_units_abs\")\n",
    "    ).with_columns(\n",
    "        (pl.col(\"direction\") * pl.col(\"ret_units_abs\")).alias(\"ret_units_signed\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"ret_units_signed\").clip(-ER_CLIP, ER_CLIP).alias(\"ret_clipped\")\n",
    "    ).with_columns(\n",
    "        (pl.col(\"ret_clipped\") / ER_CLIP).alias(\"ret\")\n",
    "    )\n",
    "\n",
    "    # FIX CONSISTENCIA PD: ignorar 'success' preexistente\n",
    "    if \"success\" in df.columns:\n",
    "        df = df.drop(\"success\")\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"ret\") > 0.0).cast(pl.Int8).alias(\"success\")\n",
    "    )\n",
    "\n",
    "    return df.select([\"symbol\", \"preset\", \"ts\", \"ret\", \"success\"])\n",
    "\n",
    "def _full_join_symbol_preset(l: pl.DataFrame, r: pl.DataFrame) -> pl.DataFrame:\n",
    "    out = l.join(r, on=[\"symbol\", \"preset\"], how=\"full\", coalesce=True)\n",
    "    for c in [\"symbol_right\", \"preset_right\"]:\n",
    "        if c in out.columns:\n",
    "            out = out.drop(c)\n",
    "    return out\n",
    "\n",
    "# ===========================\n",
    "# 3) Cargar stability_table y limpiar columnas IS/OOS viejas\n",
    "# ===========================\n",
    "stab_prev = pl.read_parquet(str(stab_table_path))\n",
    "print(f\"üìÅ INPUT ‚Üí stability_table = {stab_table_path} (rows={stab_prev.height}, cols={len(stab_prev.columns)})\")\n",
    "\n",
    "# Normalizar symbol\n",
    "if \"symbol\" in stab_prev.columns:\n",
    "    stab_prev = stab_prev.with_columns(_norm_symbol_series(pl.col(\"symbol\")).alias(\"symbol\"))\n",
    "\n",
    "# Asegurar preset en stability_table\n",
    "if \"preset\" in stab_prev.columns:\n",
    "    stab_prev = stab_prev.with_columns(pl.col(\"preset\").cast(pl.Utf8, strict=False))\n",
    "elif \"family\" in stab_prev.columns:\n",
    "    stab_prev = stab_prev.with_columns(pl.col(\"family\").cast(pl.Utf8, strict=False).alias(\"preset\"))\n",
    "elif \"regime\" in stab_prev.columns:\n",
    "    stab_prev = stab_prev.with_columns(pl.col(\"regime\").cast(pl.Utf8, strict=False).alias(\"preset\"))\n",
    "else:\n",
    "    stab_prev = stab_prev.with_columns(pl.lit(\"DEFAULT\").alias(\"preset\"))\n",
    "\n",
    "presets_stab = (\n",
    "    stab_prev.select(\"preset\").unique().drop_nulls().get_column(\"preset\").to_list()\n",
    ")\n",
    "print(f\"[Celda 08IS] Presets en stability_table = {presets_stab}\")\n",
    "\n",
    "# Drop columnas IS/OOS antiguas + restos de joins\n",
    "cols_isoos_old = [\n",
    "    \"n_events_IS\", \"ER_IS\", \"PD_IS\", \"p_event_IS\", \"days_IS\",\n",
    "    \"n_events_OOS\", \"ER_OOS\", \"PD_OOS\", \"p_event_OOS\", \"days_OOS\",\n",
    "    \"p_event_IS_month\", \"p_event_OOS_month\",\n",
    "    \"ts_cut_ISOOS\", \"split_q_ISOOS\",\n",
    "    \"flag_oos_ok\", \"oos_fail_reason\",\n",
    "]\n",
    "to_drop_isoos = [c for c in cols_isoos_old if c in stab_prev.columns]\n",
    "if to_drop_isoos:\n",
    "    print(f\"[Celda 08IS] INFO: eliminando columnas IS/OOS antiguas: {to_drop_isoos}\")\n",
    "    stab_prev = stab_prev.drop(to_drop_isoos)\n",
    "\n",
    "suffix_right = [c for c in stab_prev.columns if c.endswith(\"_right\")]\n",
    "if suffix_right:\n",
    "    print(f\"[Celda 08IS] INFO: eliminando sufijos *_right antiguos: {suffix_right}\")\n",
    "    stab_prev = stab_prev.drop(suffix_right)\n",
    "\n",
    "# ===========================\n",
    "# 4) Cargar eventos trend + range\n",
    "# ===========================\n",
    "ev_trend = _read_events_isoos(trend_events_path)\n",
    "ev_range = _read_events_isoos(range_events_path)\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí trend_events = {trend_events_path} (rows={ev_trend.height})\")\n",
    "print(f\"üìÅ INPUT ‚Üí range_events = {range_events_path} (rows={ev_range.height})\")\n",
    "\n",
    "pieces: List[pl.DataFrame] = []\n",
    "if ev_trend.height > 0: pieces.append(ev_trend)\n",
    "if ev_range.height > 0: pieces.append(ev_range)\n",
    "\n",
    "if not pieces:\n",
    "    raise RuntimeError(\"[Celda 08IS] No hay eventos en trend/range para calcular split IS/OOS.\")\n",
    "\n",
    "ev_all = pl.concat(pieces, how=\"vertical\")\n",
    "\n",
    "presets_events = (\n",
    "    ev_all.select(\"preset\").unique().drop_nulls().get_column(\"preset\").to_list()\n",
    ")\n",
    "print(f\"[Celda 08IS] Presets en eventos = {sorted(set(presets_events))}\")\n",
    "\n",
    "# ===========================\n",
    "# 5) Regla de alineaci√≥n DEFAULT (si aplica)\n",
    "# ===========================\n",
    "only_default_stab = (len(presets_stab) == 1 and presets_stab[0] == \"DEFAULT\")\n",
    "events_have_non_default = any(p != \"DEFAULT\" for p in presets_events)\n",
    "\n",
    "if only_default_stab and events_have_non_default:\n",
    "    print(\"[Celda 08IS] ‚ö†Ô∏è Alineaci√≥n: stability_table solo DEFAULT y eventos con presets m√∫ltiples.\")\n",
    "    print(\"               ‚Üí colapsando eventos a preset='DEFAULT'.\")\n",
    "    ev_all = ev_all.with_columns(pl.lit(\"DEFAULT\").alias(\"preset\"))\n",
    "\n",
    "# ===========================\n",
    "# 6) Construir folds IS/OOS por cuant√≠l temporal\n",
    "# ===========================\n",
    "ev_all = ev_all.with_columns(\n",
    "    pl.col(\"ts\").cast(pl.Int64).alias(\"ts_int\"),\n",
    "    pl.col(\"ts\").dt.truncate(\"1d\").alias(\"day_utc\"),\n",
    ")\n",
    "\n",
    "ts_cut = (\n",
    "    ev_all.group_by([\"symbol\", \"preset\"])\n",
    "          .agg(\n",
    "              pl.col(\"ts_int\")\n",
    "                .quantile(IS_OOS_split_q, interpolation=\"nearest\")\n",
    "                .alias(\"ts_cut_int\")\n",
    "          )\n",
    ")\n",
    "\n",
    "ev_folds = ev_all.join(ts_cut, on=[\"symbol\", \"preset\"], how=\"left\")\n",
    "\n",
    "ev_folds = ev_folds.with_columns(\n",
    "    pl.col(\"ts_cut_int\").cast(pl.Datetime(\"us\")).alias(\"ts_cut_ISOOS\"),\n",
    "    pl.lit(float(IS_OOS_split_q)).alias(\"split_q_ISOOS\"),\n",
    "    pl.when(pl.col(\"ts_cut_int\").is_not_null() & (pl.col(\"ts_int\") <= pl.col(\"ts_cut_int\")))\n",
    "      .then(pl.lit(\"IS\"))\n",
    "      .otherwise(pl.lit(\"OOS\"))\n",
    "      .alias(\"segment\"),\n",
    ")\n",
    "\n",
    "print(f\"[Celda 08IS] ev_folds rows={ev_folds.height:,} | head:\")\n",
    "print(ev_folds.select(\n",
    "    [\"symbol\",\"preset\",\"ts\",\"ret\",\"success\",\"ts_cut_int\",\"ts_cut_ISOOS\",\"split_q_ISOOS\",\"segment\"]\n",
    ").head(5))\n",
    "\n",
    "# Guardamos folds para auditor√≠a\n",
    "ev_folds.select(\n",
    "    [\"symbol\",\"preset\",\"ts\",\"ret\",\"success\",\"segment\",\"ts_cut_ISOOS\",\"split_q_ISOOS\"]\n",
    ").write_parquet(str(stab_folds_path), compression=\"zstd\")\n",
    "\n",
    "# ===========================\n",
    "# 7) Agregados por segmento (sin pivot)\n",
    "# ===========================\n",
    "keys = [\"symbol\", \"preset\"]\n",
    "\n",
    "ev_is  = ev_folds.filter(pl.col(\"segment\") == \"IS\")\n",
    "ev_oos = ev_folds.filter(pl.col(\"segment\") == \"OOS\")\n",
    "\n",
    "agg_is = (\n",
    "    ev_is.group_by(keys)\n",
    "         .agg([\n",
    "             pl.len().alias(\"n_events_IS\"),\n",
    "             pl.col(\"ret\").mean().alias(\"ER_IS\"),\n",
    "             pl.col(\"success\").mean().alias(\"PD_IS\"),\n",
    "             pl.col(\"day_utc\").n_unique().alias(\"days_IS\"),\n",
    "         ])\n",
    ")\n",
    "\n",
    "agg_oos = (\n",
    "    ev_oos.group_by(keys)\n",
    "          .agg([\n",
    "              pl.len().alias(\"n_events_OOS\"),\n",
    "              pl.col(\"ret\").mean().alias(\"ER_OOS\"),\n",
    "              pl.col(\"success\").mean().alias(\"PD_OOS\"),\n",
    "              pl.col(\"day_utc\").n_unique().alias(\"days_OOS\"),\n",
    "          ])\n",
    ")\n",
    "\n",
    "seg_wide = _full_join_symbol_preset(agg_is, agg_oos)\n",
    "\n",
    "# Rellenos m√≠nimos seguros\n",
    "seg_wide = seg_wide.with_columns([\n",
    "    pl.col(\"n_events_IS\").fill_null(0).cast(pl.Int64),\n",
    "    pl.col(\"n_events_OOS\").fill_null(0).cast(pl.Int64),\n",
    "    pl.col(\"days_IS\").fill_null(0).cast(pl.Int64),\n",
    "    pl.col(\"days_OOS\").fill_null(0).cast(pl.Int64),\n",
    "])\n",
    "\n",
    "# ===========================\n",
    "# 7.1) p_event coherente con 08STAB/08S PRO\n",
    "#   - p_event_* = ratio eventos/barras aproximadas\n",
    "#   - bars ‚âà days * 288 (fallback)\n",
    "# ===========================\n",
    "seg_wide = seg_wide.with_columns([\n",
    "    pl.when(pl.col(\"days_IS\") > 0)\n",
    "      .then(\n",
    "          pl.col(\"n_events_IS\").cast(pl.Float64)\n",
    "          / (pl.col(\"days_IS\").cast(pl.Float64) * float(BARS_PER_DAY_FALLBACK))\n",
    "      )\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"p_event_IS\"),\n",
    "\n",
    "    pl.when(pl.col(\"days_OOS\") > 0)\n",
    "      .then(\n",
    "          pl.col(\"n_events_OOS\").cast(pl.Float64)\n",
    "          / (pl.col(\"days_OOS\").cast(pl.Float64) * float(BARS_PER_DAY_FALLBACK))\n",
    "      )\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"p_event_OOS\"),\n",
    "])\n",
    "\n",
    "# M√©trica auxiliar \"humana\": eventos/mes aproximados\n",
    "seg_wide = seg_wide.with_columns([\n",
    "    pl.when(pl.col(\"days_IS\") > 0)\n",
    "      .then(pl.col(\"n_events_IS\").cast(pl.Float64) / (pl.col(\"days_IS\").cast(pl.Float64) / 30.0))\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"p_event_IS_month\"),\n",
    "\n",
    "    pl.when(pl.col(\"days_OOS\") > 0)\n",
    "      .then(pl.col(\"n_events_OOS\").cast(pl.Float64) / (pl.col(\"days_OOS\").cast(pl.Float64) / 30.0))\n",
    "      .otherwise(0.0)\n",
    "      .alias(\"p_event_OOS_month\"),\n",
    "])\n",
    "\n",
    "# A√±adir columnas de corte a nivel (symbol,preset) para trazabilidad\n",
    "cuts_meta = (\n",
    "    ev_folds.select([\"symbol\",\"preset\",\"ts_cut_ISOOS\",\"split_q_ISOOS\"])\n",
    "            .unique()\n",
    ")\n",
    "seg_wide = seg_wide.join(cuts_meta, on=keys, how=\"left\")\n",
    "\n",
    "print(f\"[Celda 08IS] seg_wide rows={seg_wide.height}, cols={len(seg_wide.columns)} | head:\")\n",
    "print(seg_wide.head(5))\n",
    "\n",
    "# ===========================\n",
    "# 8) Pol√≠tica OOS (FIX None)\n",
    "# ===========================\n",
    "if has_policy:\n",
    "    seg_wide = seg_wide.with_columns([\n",
    "        pl.col(\"n_events_OOS\").fill_null(0).alias(\"__nOOS__\"),\n",
    "        pl.col(\"ER_OOS\").alias(\"__EROOS__\"),\n",
    "        pl.col(\"PD_OOS\").alias(\"__PDOOS__\"),\n",
    "        pl.col(\"p_event_OOS\").alias(\"__pOOS__\"),\n",
    "    ])\n",
    "\n",
    "    fail_reason_expr = (\n",
    "        pl.when(pl.col(\"__nOOS__\") <= 0)\n",
    "          .then(pl.lit(\"NO_EVENTS_OOS\"))\n",
    "        .when(pl.col(\"__nOOS__\") < int(min_events_OOS))\n",
    "          .then(pl.lit(\"LOW_EVENTS_OOS\"))\n",
    "        .when(\n",
    "            pl.lit(ER_OOS_min is not None)\n",
    "            & pl.col(\"__EROOS__\").is_not_null()\n",
    "            & (pl.col(\"__EROOS__\") < pl.lit(ER_OOS_min_num))\n",
    "        ).then(pl.lit(\"ER_OOS_BELOW_MIN\"))\n",
    "        .when(\n",
    "            pl.lit(PD_OOS_min is not None)\n",
    "            & pl.col(\"__PDOOS__\").is_not_null()\n",
    "            & (pl.col(\"__PDOOS__\") < pl.lit(PD_OOS_min_num))\n",
    "        ).then(pl.lit(\"PD_OOS_BELOW_MIN\"))\n",
    "        .when(\n",
    "            pl.lit(p_event_OOS_min is not None)\n",
    "            & pl.col(\"__pOOS__\").is_not_null()\n",
    "            & (pl.col(\"__pOOS__\") < pl.lit(p_event_OOS_min_num))\n",
    "        ).then(pl.lit(\"P_EVENT_OOS_BELOW_MIN\"))\n",
    "        .otherwise(pl.lit(None))\n",
    "    )\n",
    "\n",
    "    seg_wide = (\n",
    "        seg_wide.with_columns(\n",
    "            fail_reason_expr.alias(\"oos_fail_reason\")\n",
    "        ).with_columns(\n",
    "            (pl.col(\"oos_fail_reason\").is_null()).alias(\"flag_oos_ok\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seg_wide = seg_wide.drop([\"__nOOS__\",\"__EROOS__\",\"__PDOOS__\",\"__pOOS__\"])\n",
    "\n",
    "else:\n",
    "    seg_wide = seg_wide.with_columns([\n",
    "        (pl.col(\"n_events_OOS\") > 0).alias(\"flag_oos_ok\"),\n",
    "        pl.when(pl.col(\"n_events_OOS\") > 0)\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.lit(\"NO_EVENTS_OOS\"))\n",
    "          .alias(\"oos_fail_reason\"),\n",
    "    ])\n",
    "\n",
    "# ===========================\n",
    "# 8.1) Resumen flag_oos_ok (PATCH Polars)\n",
    "# ===========================\n",
    "print(\"[Celda 08IS] flag_oos_ok summary:\")\n",
    "print(\n",
    "    seg_wide.select([\n",
    "        pl.col(\"flag_oos_ok\")\n",
    "          .fill_null(False)\n",
    "          .cast(pl.Float64)\n",
    "          .mean()\n",
    "          .alias(\"share_oos_ok\"),\n",
    "\n",
    "        (pl.col(\"flag_oos_ok\")\n",
    "           .fill_null(False) == False)\n",
    "          .cast(pl.Int64)\n",
    "          .sum()\n",
    "          .alias(\"n_oos_fail\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 9) Join final contra stability_table\n",
    "# ===========================\n",
    "stab_new = stab_prev.join(\n",
    "    seg_wide.select([\n",
    "        \"symbol\",\"preset\",\n",
    "        \"n_events_IS\",\"ER_IS\",\"PD_IS\",\"p_event_IS\",\"days_IS\",\"p_event_IS_month\",\n",
    "        \"n_events_OOS\",\"ER_OOS\",\"PD_OOS\",\"p_event_OOS\",\"days_OOS\",\"p_event_OOS_month\",\n",
    "        \"ts_cut_ISOOS\",\"split_q_ISOOS\",\n",
    "        \"flag_oos_ok\",\"oos_fail_reason\",\n",
    "    ]),\n",
    "    on=[\"symbol\",\"preset\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"[Celda 08IS] stability_table (after join) rows={stab_new.height}, cols={len(stab_new.columns)}\")\n",
    "\n",
    "# ===========================\n",
    "# 10) Escritura\n",
    "# ===========================\n",
    "stab_new.write_parquet(str(stab_table_path), compression=\"zstd\")\n",
    "\n",
    "if (not _exists(stab_table_path)) or stab_table_path.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 08IS] Error al escribir stability_table.parquet\")\n",
    "\n",
    "print(f\"üíæ UPDATE ‚Üí {stab_table_path} (rows={stab_new.height}, cols={len(stab_new.columns)})\")\n",
    "print(f\"üíæ folds  ‚Üí {stab_folds_path} (rows={ev_folds.height})\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"stability_table_path\"] = str(stab_table_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"stab_folds_path\"] = str(stab_folds_path)\n",
    "\n",
    "print(\">>> Celda 08IS v2.3.1 PRO :: OK\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3faba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 08S v2.5.1 INSTITUTIONAL :: Significancia combinada (ER/PD/opportunity)\n",
      "[Celda 08S] Nota: p-values/t-values son heur√≠sticos (ranking suave), no inferencia IID estricta.\n",
      "[Celda 08S] INPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (label=stability_table) | shape=(rows=166, cols=42)\n",
      "[Celda 08S] p_event_IS FORZADO desde opportunity_rate_bar_IS (evita colisiones).\n",
      "[Celda 08S] p0_opportunity_null (mediana p_event_IS_clip) ‚âà 0.493436\n",
      "üíæ UPDATE ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet | shape=(rows=166, cols=49)\n",
      "[Celda 08S] run_significance_state=WEAK | weight_stat_component=0.40 | pd_inflation_flag=False\n",
      ">>> Celda 08S v2.5.1 INSTITUTIONAL :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 08S v2.5.1 INSTITUTIONAL (force p_event from opportunity_rate_bar) =====================\n",
    "# Patch sobre v2.5:\n",
    "#   - Si existe opportunity_rate_bar_IS, FORZAR p_event_IS = opportunity_rate_bar_IS (evita colisi√≥n sem√°ntica)\n",
    "# ========================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 08S v2.5.1 INSTITUTIONAL :: Significancia combinada (ER/PD/opportunity)\")\n",
    "print(\"[Celda 08S] Nota: p-values/t-values son heur√≠sticos (ranking suave), no inferencia IID estricta.\")\n",
    "\n",
    "# ========================= Validaciones =========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe.\")\n",
    "\n",
    "paths = GLOBAL_STATE.get(\"paths\", {})\n",
    "if \"stability\" not in paths or \"diagnostics\" not in paths:\n",
    "    raise RuntimeError(\"Falta GLOBAL_STATE['paths']['stability'/'diagnostics'].\")\n",
    "\n",
    "OUT_STAB_DIR = Path(paths[\"stability\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "\n",
    "stab_table_base_path = OUT_STAB_DIR / \"stability_table.parquet\"\n",
    "stab_table_adv_path  = OUT_STAB_DIR / \"stability_table_advanced.parquet\"\n",
    "config_path          = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "def _exists(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if _exists(stab_table_base_path):\n",
    "    stab_path = stab_table_base_path\n",
    "    stab_label = \"stability_table\"\n",
    "elif _exists(stab_table_adv_path):\n",
    "    stab_path = stab_table_adv_path\n",
    "    stab_label = \"stability_table_advanced\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Celda 08S] No se encontr√≥ stability_table.parquet ni stability_table_advanced.parquet.\")\n",
    "\n",
    "stab = pl.read_parquet(str(stab_path))\n",
    "print(f\"[Celda 08S] INPUT ‚Üí {stab_path} (label={stab_label}) | shape=(rows={stab.height}, cols={len(stab.columns)})\")\n",
    "\n",
    "stab = stab.with_columns(pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase())\n",
    "\n",
    "# Drop pvals/tvals previos\n",
    "drop_old_stats = [c for c in stab.columns if c.lower().startswith((\"pvals_\", \"tvals_\"))]\n",
    "if drop_old_stats:\n",
    "    stab = stab.drop(drop_old_stats)\n",
    "\n",
    "# ========================= Cargar config =========================\n",
    "def _load_config() -> dict:\n",
    "    cfg = GLOBAL_STATE.get(\"config\", {})\n",
    "    if isinstance(cfg, dict) and cfg:\n",
    "        return cfg\n",
    "    if _exists(config_path):\n",
    "        try:\n",
    "            cfg_disk = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(cfg_disk, dict):\n",
    "                GLOBAL_STATE[\"config\"] = cfg_disk\n",
    "                return cfg_disk\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "CONFIG = _load_config()\n",
    "stats_cfg = (CONFIG.get(\"stats\", {}) or {})\n",
    "\n",
    "def _cfg_float(section: dict, key: str, default: float) -> float:\n",
    "    try:\n",
    "        return float(section.get(key, default))\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def _cfg_bool(section: dict, key: str, default: bool) -> bool:\n",
    "    try:\n",
    "        v = section.get(key, default)\n",
    "        if isinstance(v, bool): return v\n",
    "        if isinstance(v, str):  return v.strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "        return bool(v)\n",
    "    except Exception:\n",
    "        return bool(default)\n",
    "\n",
    "alpha_strict   = _cfg_float(stats_cfg, \"alpha_strict\", 0.02)\n",
    "alpha_balanced = _cfg_float(stats_cfg, \"alpha_balanced\", 0.08)\n",
    "alpha_loose    = _cfg_float(stats_cfg, \"alpha_loose\", 0.25)\n",
    "\n",
    "n_eff_cap_ER = _cfg_float(stats_cfg, \"n_eff_cap_ER\", 250.0)\n",
    "n_eff_cap_PD = _cfg_float(stats_cfg, \"n_eff_cap_PD\", 250.0)\n",
    "\n",
    "n_eff_cap_p_event_bars = _cfg_float(stats_cfg, \"n_eff_cap_p_event_bars\", 72000.0)\n",
    "min_cap_p_event_bars   = _cfg_float(stats_cfg, \"min_cap_p_event_bars\", 5000.0)\n",
    "\n",
    "bars_per_day_fallback = _cfg_float(stats_cfg, \"bars_per_day_fallback\", 288.0)\n",
    "\n",
    "p0_PD_null = _cfg_float(stats_cfg, \"p0_PD_null\", 0.5)\n",
    "p0_pevent_null_cfg = stats_cfg.get(\"p0_p_event_null\", None)\n",
    "\n",
    "e_max_log10 = _cfg_float(stats_cfg, \"e_max_log10\", 6.0)\n",
    "\n",
    "w_ER_base = _cfg_float(stats_cfg, \"w_ER\", 0.4)\n",
    "w_PD_base = _cfg_float(stats_cfg, \"w_PD\", 0.4)\n",
    "w_PE_base = _cfg_float(stats_cfg, \"w_p_event\", 0.2)\n",
    "\n",
    "conservative_mode = _cfg_bool(stats_cfg, \"conservative_mode\", True)\n",
    "\n",
    "best_p_strict_conc_thr = _cfg_float(stats_cfg, \"best_p_strict_concentration_thr\", 0.9)\n",
    "p_event_best_share_thr = _cfg_float(stats_cfg, \"p_event_best_share_thr\", 0.95)\n",
    "auto_adjust_dom_thr    = _cfg_float(stats_cfg, \"auto_adjust_dominance_thr\", 0.98)\n",
    "\n",
    "auto_cap_reduction_factor = _cfg_float(stats_cfg, \"auto_cap_reduction_factor\", 0.5)\n",
    "auto_wpe_reduction_factor = _cfg_float(stats_cfg, \"auto_wpe_reduction_factor\", 0.5)\n",
    "\n",
    "loose_info_thr   = _cfg_float(stats_cfg, \"loose_info_thr\", 0.05)\n",
    "weak_strict_thr  = _cfg_float(stats_cfg, \"weak_strict_thr\", 0.10)\n",
    "weak_bal_thr     = _cfg_float(stats_cfg, \"weak_bal_thr\", 0.30)\n",
    "\n",
    "pd_guard_ge_thr          = _cfg_float(stats_cfg, \"pd_guard_ge_060_thr\", 0.60)\n",
    "pd_infl_share_thr        = _cfg_float(stats_cfg, \"pd_inflation_share_thr\", 0.30)\n",
    "pd_infl_median_thr       = _cfg_float(stats_cfg, \"pd_inflation_median_thr\", 0.58)\n",
    "\n",
    "dominance_requires_erpd_support = _cfg_bool(stats_cfg, \"dominance_requires_erpd_support\", True)\n",
    "erpd_support_loose_rate_thr     = _cfg_float(stats_cfg, \"erpd_support_loose_rate_thr\", 0.10)\n",
    "\n",
    "post_auto_penalty = _cfg_bool(stats_cfg, \"post_auto_penalty\", False)\n",
    "post_auto_penalty_lambda = _cfg_float(stats_cfg, \"post_auto_penalty_lambda\", 0.25)\n",
    "\n",
    "if conservative_mode:\n",
    "    w_PE_base = min(w_PE_base, 0.15)\n",
    "    auto_adjust_dom_thr = min(auto_adjust_dom_thr, 0.95)\n",
    "\n",
    "def _normal_2sided_p(zabs: float) -> float:\n",
    "    z = abs(float(zabs))\n",
    "    return float(math.erfc(z / math.sqrt(2.0)))\n",
    "\n",
    "# ========================= FORZAR p_event_IS desde opportunity_rate_bar_IS =========================\n",
    "if \"opportunity_rate_bar_IS\" in stab.columns:\n",
    "    stab = stab.with_columns(pl.col(\"opportunity_rate_bar_IS\").cast(pl.Float64, strict=False).alias(\"p_event_IS\"))\n",
    "    print(\"[Celda 08S] p_event_IS FORZADO desde opportunity_rate_bar_IS (evita colisiones).\")\n",
    "\n",
    "required_cols = [\"ER_IS\", \"PD_IS\", \"p_event_IS\", \"n_events_IS\", \"days_IS\"]\n",
    "missing_cols = [c for c in required_cols if c not in stab.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"[Celda 08S] faltan columnas requeridas: {missing_cols}. Ejecuta 08STAB primero.\")\n",
    "\n",
    "stab = stab.with_columns([\n",
    "    pl.col(\"ER_IS\").cast(pl.Float64, strict=False),\n",
    "    pl.col(\"PD_IS\").cast(pl.Float64, strict=False),\n",
    "    pl.col(\"p_event_IS\").cast(pl.Float64, strict=False),\n",
    "    pl.col(\"n_events_IS\").cast(pl.Float64, strict=False),\n",
    "    pl.col(\"days_IS\").cast(pl.Float64, strict=False),\n",
    "])\n",
    "\n",
    "stab = stab.with_columns(pl.col(\"p_event_IS\").fill_null(0.0).clip(0.0, 1.0).alias(\"p_event_IS_clip\"))\n",
    "\n",
    "# Baseline p0 opportunity\n",
    "if p0_pevent_null_cfg is None:\n",
    "    s_pe = stab.get_column(\"p_event_IS_clip\").drop_nulls()\n",
    "    p0_pevent_null = float(s_pe.median()) if s_pe.len() else 0.35\n",
    "    print(f\"[Celda 08S] p0_opportunity_null (mediana p_event_IS_clip) ‚âà {p0_pevent_null:.6f}\")\n",
    "else:\n",
    "    p0_pevent_null = float(p0_pevent_null_cfg)\n",
    "\n",
    "if not (0.0 < p0_PD_null < 1.0): p0_PD_null = 0.5\n",
    "if not (0.0 < p0_pevent_null < 1.0): p0_pevent_null = 0.35\n",
    "\n",
    "# ER\n",
    "stab = stab.with_columns(pl.min_horizontal(pl.col(\"n_events_IS\"), pl.lit(float(n_eff_cap_ER))).alias(\"__n_eff_ER__\"))\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"ER_IS\").is_not_null() & (pl.col(\"__n_eff_ER__\") > 1.0))\n",
    "      .then(pl.col(\"ER_IS\") * pl.col(\"__n_eff_ER__\").sqrt())\n",
    "      .otherwise(pl.lit(None)).alias(\"tvals_ER\")\n",
    ")\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"tvals_ER\").is_not_null())\n",
    "      .then(pl.col(\"tvals_ER\").abs().map_elements(_normal_2sided_p, return_dtype=pl.Float64))\n",
    "      .otherwise(pl.lit(1.0)).alias(\"pvals_combined_ER\")\n",
    ")\n",
    "\n",
    "# PD\n",
    "stab = stab.with_columns(pl.min_horizontal(pl.col(\"n_events_IS\"), pl.lit(float(n_eff_cap_PD))).alias(\"__n_eff_PD__\"))\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"PD_IS\").is_not_null() & (pl.col(\"__n_eff_PD__\") > 0.0))\n",
    "      .then((pl.col(\"PD_IS\") - float(p0_PD_null)) / ((float(p0_PD_null) * (1.0 - float(p0_PD_null)) / pl.col(\"__n_eff_PD__\")).sqrt()))\n",
    "      .otherwise(pl.lit(None)).alias(\"tvals_PD\")\n",
    ")\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"tvals_PD\").is_not_null())\n",
    "      .then(pl.col(\"tvals_PD\").abs().map_elements(_normal_2sided_p, return_dtype=pl.Float64))\n",
    "      .otherwise(pl.lit(1.0)).alias(\"pvals_combined_PD\")\n",
    ")\n",
    "\n",
    "# Opportunity n_eff por barras IS alineado\n",
    "if \"bars_IS_est\" in stab.columns:\n",
    "    stab = stab.with_columns(pl.col(\"bars_IS_est\").cast(pl.Float64, strict=False).alias(\"__bars_IS_est__\"))\n",
    "else:\n",
    "    if \"bars_per_day_used\" in stab.columns:\n",
    "        stab = stab.with_columns((pl.col(\"days_IS\") * pl.col(\"bars_per_day_used\").cast(pl.Float64, strict=False)).alias(\"__bars_IS_est__\"))\n",
    "    else:\n",
    "        stab = stab.with_columns((pl.col(\"days_IS\") * pl.lit(float(bars_per_day_fallback))).alias(\"__bars_IS_est__\"))\n",
    "\n",
    "cap_pe_base = float(n_eff_cap_p_event_bars)\n",
    "stab = stab.with_columns(pl.min_horizontal(pl.col(\"__bars_IS_est__\"), pl.lit(cap_pe_base)).alias(\"__bars_eff_opportunity__\"))\n",
    "\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"p_event_IS_clip\").is_not_null() & (pl.col(\"__bars_eff_opportunity__\") > 0.0))\n",
    "      .then((pl.col(\"p_event_IS_clip\") - float(p0_pevent_null)) / ((float(p0_pevent_null) * (1.0 - float(p0_pevent_null)) / pl.col(\"__bars_eff_opportunity__\")).sqrt()))\n",
    "      .otherwise(pl.lit(None)).alias(\"tvals_p_event\")\n",
    ")\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"tvals_p_event\").is_not_null())\n",
    "      .then(pl.col(\"tvals_p_event\").abs().map_elements(_normal_2sided_p, return_dtype=pl.Float64))\n",
    "      .otherwise(pl.lit(1.0)).alias(\"pvals_combined_p_event\")\n",
    ")\n",
    "\n",
    "# PD guardrail\n",
    "pd_s = stab.get_column(\"PD_IS\").drop_nulls()\n",
    "pd_median = float(pd_s.median()) if pd_s.len() else float(\"nan\")\n",
    "pd_share_ge = float(stab.select((pl.col(\"PD_IS\") >= pl.lit(pd_guard_ge_thr)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "pd_inflation_flag = bool((not math.isnan(pd_median) and pd_median >= pd_infl_median_thr) or (pd_share_ge >= pd_infl_share_thr))\n",
    "\n",
    "# Dominance pre-auto\n",
    "stab = stab.with_columns(pl.min_horizontal(\"pvals_combined_ER\",\"pvals_combined_PD\",\"pvals_combined_p_event\").alias(\"__best_p_all__\"))\n",
    "stab = stab.with_columns([\n",
    "    (pl.col(\"pvals_combined_ER\") == pl.col(\"__best_p_all__\")).alias(\"__best_is_ER__\"),\n",
    "    (pl.col(\"pvals_combined_PD\") == pl.col(\"__best_p_all__\")).alias(\"__best_is_PD__\"),\n",
    "    (pl.col(\"pvals_combined_p_event\") == pl.col(\"__best_p_all__\")).alias(\"__best_is_PE__\"),\n",
    "])\n",
    "dom_pre = stab.select([\n",
    "    pl.col(\"__best_is_ER__\").cast(pl.Float64).mean().alias(\"ER_best\"),\n",
    "    pl.col(\"__best_is_PD__\").cast(pl.Float64).mean().alias(\"PD_best\"),\n",
    "    pl.col(\"__best_is_PE__\").cast(pl.Float64).mean().alias(\"opp_best\"),\n",
    "]).row(0, named=True)\n",
    "\n",
    "best_rates_pre = stab.select([\n",
    "    (pl.col(\"__best_p_all__\") < pl.lit(alpha_strict)).cast(pl.Float64).mean().alias(\"best_strict\"),\n",
    "    (pl.col(\"__best_p_all__\") < pl.lit(alpha_balanced)).cast(pl.Float64).mean().alias(\"best_balanced\"),\n",
    "    (pl.col(\"__best_p_all__\") < pl.lit(alpha_loose)).cast(pl.Float64).mean().alias(\"best_loose\"),\n",
    "]).row(0, named=True)\n",
    "\n",
    "PE_best_pre = float(dom_pre[\"opp_best\"] or 0.0)\n",
    "best_strict_pre = float(best_rates_pre[\"best_strict\"] or 0.0)\n",
    "\n",
    "auto_adjust = bool((best_strict_pre >= best_p_strict_conc_thr) and (PE_best_pre >= p_event_best_share_thr))\n",
    "\n",
    "w_ER_final = float(w_ER_base)\n",
    "w_PD_final = float(w_PD_base)\n",
    "w_PE_final = float(w_PE_base)\n",
    "cap_pe_final = float(cap_pe_base)\n",
    "\n",
    "if auto_adjust:\n",
    "    cap_pe_final = max(cap_pe_base * float(auto_cap_reduction_factor), float(min_cap_p_event_bars))\n",
    "    w_PE_final = max(w_PE_base * float(auto_wpe_reduction_factor), 0.05)\n",
    "\n",
    "# Recalcular opportunity con cap final\n",
    "stab = stab.drop([c for c in [\"__bars_eff_opportunity__\",\"tvals_p_event\",\"pvals_combined_p_event\"] if c in stab.columns])\n",
    "stab = stab.with_columns(pl.min_horizontal(pl.col(\"__bars_IS_est__\"), pl.lit(float(cap_pe_final))).alias(\"__bars_eff_opportunity__\"))\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"p_event_IS_clip\").is_not_null() & (pl.col(\"__bars_eff_opportunity__\") > 0.0))\n",
    "      .then((pl.col(\"p_event_IS_clip\") - float(p0_pevent_null)) / ((float(p0_pevent_null) * (1.0 - float(p0_pevent_null)) / pl.col(\"__bars_eff_opportunity__\")).sqrt()))\n",
    "      .otherwise(pl.lit(None)).alias(\"tvals_p_event\")\n",
    ")\n",
    "stab = stab.with_columns(\n",
    "    pl.when(pl.col(\"tvals_p_event\").is_not_null())\n",
    "      .then(pl.col(\"tvals_p_event\").abs().map_elements(_normal_2sided_p, return_dtype=pl.Float64))\n",
    "      .otherwise(pl.lit(1.0)).alias(\"pvals_combined_p_event\")\n",
    ")\n",
    "\n",
    "# Evidence ER/PD loose\n",
    "er_loose_rate = float(stab.select((pl.col(\"pvals_combined_ER\") < pl.lit(alpha_loose)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "pd_loose_rate = float(stab.select((pl.col(\"pvals_combined_PD\") < pl.lit(alpha_loose)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "\n",
    "# Score bucket\n",
    "w_sum = w_ER_final + w_PD_final + w_PE_final\n",
    "if w_sum <= 0:\n",
    "    w_ER_final, w_PD_final, w_PE_final = 1.0, 1.0, 1.0\n",
    "    w_sum = 3.0\n",
    "\n",
    "stab = stab.with_columns([\n",
    "    pl.when(pl.col(\"pvals_combined_ER\") > 0.0).then((-pl.col(\"pvals_combined_ER\").log10()).clip(0.0, e_max_log10)).otherwise(0.0).alias(\"__e_ER__\"),\n",
    "    pl.when(pl.col(\"pvals_combined_PD\") > 0.0).then((-pl.col(\"pvals_combined_PD\").log10()).clip(0.0, e_max_log10)).otherwise(0.0).alias(\"__e_PD__\"),\n",
    "    pl.when(pl.col(\"pvals_combined_p_event\") > 0.0).then((-pl.col(\"pvals_combined_p_event\").log10()).clip(0.0, e_max_log10)).otherwise(0.0).alias(\"__e_opportunity__\"),\n",
    "])\n",
    "stab = stab.with_columns(\n",
    "    (((pl.lit(w_ER_final) * pl.col(\"__e_ER__\") + pl.lit(w_PD_final) * pl.col(\"__e_PD__\") + pl.lit(w_PE_final) * pl.col(\"__e_opportunity__\"))\n",
    "      / pl.lit(w_sum * e_max_log10)).clip(0.0, 1.0)).alias(\"stat_score_bucket\")\n",
    ")\n",
    "\n",
    "# Run state\n",
    "best_loose = float(stab.select((pl.min_horizontal(\"pvals_combined_ER\",\"pvals_combined_PD\",\"pvals_combined_p_event\") < pl.lit(alpha_loose)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "\n",
    "if best_loose < loose_info_thr:\n",
    "    run_significance_state = \"NO_INFO\"\n",
    "    weight_stat_component = 0.0\n",
    "else:\n",
    "    # regla base\n",
    "    best_strict = float(stab.select((pl.min_horizontal(\"pvals_combined_ER\",\"pvals_combined_PD\",\"pvals_combined_p_event\") < pl.lit(alpha_strict)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "    best_bal = float(stab.select((pl.min_horizontal(\"pvals_combined_ER\",\"pvals_combined_PD\",\"pvals_combined_p_event\") < pl.lit(alpha_balanced)).cast(pl.Float64).mean()).row(0)[0] or 0.0)\n",
    "    if (best_strict < weak_strict_thr) and (best_bal < weak_bal_thr):\n",
    "        run_significance_state = \"WEAK\"\n",
    "        weight_stat_component = 0.4\n",
    "    else:\n",
    "        run_significance_state = \"STRONG\"\n",
    "        weight_stat_component = 1.0\n",
    "\n",
    "# dominance extreme gate\n",
    "dom = stab.select((pl.col(\"__best_is_PE__\").cast(pl.Float64).mean()).alias(\"opp_best\")).row(0)[0] or 0.0\n",
    "dominance_extreme = bool(float(dom) >= auto_adjust_dom_thr)\n",
    "if dominance_requires_erpd_support and dominance_extreme and (max(er_loose_rate, pd_loose_rate) < erpd_support_loose_rate_thr):\n",
    "    run_significance_state = \"WEAK\" if best_loose >= loose_info_thr else \"NO_INFO\"\n",
    "    weight_stat_component = min(weight_stat_component, 0.4) if run_significance_state == \"WEAK\" else 0.0\n",
    "\n",
    "# Cleanup\n",
    "helper_cols = [c for c in [\n",
    "    \"__n_eff_ER__\",\"__n_eff_PD__\",\"__bars_IS_est__\",\"__bars_eff_opportunity__\",\n",
    "    \"__best_p_all__\",\"__best_is_ER__\",\"__best_is_PD__\",\"__best_is_PE__\",\n",
    "    \"__e_ER__\",\"__e_PD__\",\"__e_opportunity__\",\"p_event_IS_clip\"\n",
    "] if c in stab.columns]\n",
    "if helper_cols:\n",
    "    stab = stab.drop(helper_cols)\n",
    "\n",
    "stab.write_parquet(str(stab_path), compression=\"zstd\")\n",
    "print(f\"üíæ UPDATE ‚Üí {stab_path} | shape=(rows={stab.height}, cols={len(stab.columns)})\")\n",
    "print(f\"[Celda 08S] run_significance_state={run_significance_state} | weight_stat_component={weight_stat_component:.2f} | pd_inflation_flag={pd_inflation_flag}\")\n",
    "print(\">>> Celda 08S v2.5.1 INSTITUTIONAL :: OK\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c589e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 09 v3.3.1 PRO :: Estabilidad avanzada + Alpha Decay + p-values combinados (diagn√≥stico) + EstabScore_final\n",
      "[Celda 09] RUN_ID           = 20251218_190810\n",
      "üìÅ INPUT ‚Üí stab_folds       = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stab_folds.parquet (exists=true, rows=18283507, cols=8)\n",
      "üìÅ INPUT ‚Üí stability_table  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (rows=166, cols=49)\n",
      "üìÅ INPUT ‚Üí config.json      = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json (exists=true)\n",
      "üìÅ INPUT ‚Üí regime_thresholds= C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_thresholds.parquet (exists=true, rows=84, cols=12)\n",
      "[Celda 09] decay.penalty_lambda                   = 0.0\n",
      "[Celda 09] alpha_strict (decay claro)             = 0.05\n",
      "[Celda 09] alpha_balanced (referencia)           = 0.1\n",
      "[Celda 09] alpha_loose (p-values diag)           = 0.2\n",
      "[Celda 09] Pol√≠tica estabilidad LEGACY ‚Üí Estab_min=0.6, Estab_min_salvado=0.4, score_super_alto=0.7\n",
      "[Celda 09] Pol√≠tica estabilidad por preset ‚Üí min_estab_strict=0.7, min_estab_balanced=0.5, min_estab_loose=0.3\n",
      "[Celda 09] default min_TR_required (fallback cuant) = 0.25\n",
      "[Celda 09] proxy_target_folds (K objetivo)          = 6\n",
      "[Celda 09] join keys = ['symbol', 'preset']\n",
      "[Celda 09] folds.columns = ['symbol', 'preset', 'ts', 'ret', 'success', 'segment', 'ts_cut_ISOOS', 'split_q_ISOOS']\n",
      "[Celda 09] cols folds detectadas ‚Üí ER=None, PD=None, order_col=split_q_ISOOS\n",
      "[Celda 09] INFO: stab_folds evento-level detectado. Intentando PROXY#0 temporal realista por quantiles usando ts='ts' y K=6.\n",
      "[Celda 09][DEBUG] PROXY#0 TIMEQ ‚Üí rows=996, cols=9 | median_n_folds‚âà6 | pct_gt1‚âà1.000\n",
      "[Celda 09][DEBUG] Evaluando candidatos fold_id por KEY:\n",
      "[Celda 09][DEBUG] fold_id 'segment' ‚Üí keys=166 | pct_gt1=1.000 | med=2.00 | mean=2.00\n",
      "[Celda 09][DEBUG] fold_id 'split_q_ISOOS' ‚Üí keys=166 | pct_gt1=0.000 | med=1.00 | mean=1.00\n",
      "[Celda 09][DEBUG] fold_id 'ts_cut_ISOOS' ‚Üí keys=166 | pct_gt1=0.000 | med=1.00 | mean=1.00\n",
      "[Celda 09][DEBUG] fold_id ganador (plausible) ‚Üí segment\n",
      "[Celda 09] INFO: Construyendo PROXY#1 fold-level usando fold_id sem√°ntico 'segment'.\n",
      "[Celda 09][DEBUG] PROXY#1 ‚Üí rows=332, cols=5 | median_n_folds‚âà2 | pct_gt1‚âà1.000\n",
      "[Celda 09][DEBUG] PROXY#1 n_folds_proxy (head 10):\n",
      "shape: (10, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol  ‚îÜ preset ‚îÜ n_folds_proxy ‚îÇ\n",
      "‚îÇ ---     ‚îÜ ---    ‚îÜ ---           ‚îÇ\n",
      "‚îÇ str     ‚îÜ str    ‚îÜ u32           ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ NZDCAD  ‚îÜ RANGE  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ NERUSD  ‚îÜ TREND  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ BTCUSD  ‚îÜ RANGE  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ XAGAUD  ‚îÜ RANGE  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ DASHUSD ‚îÜ TREND  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ XAGUSD  ‚îÜ RANGE  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ EURPLN  ‚îÜ TREND  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ EURGBP  ‚îÜ RANGE  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ XAUEUR  ‚îÜ TREND  ‚îÜ 2             ‚îÇ\n",
      "‚îÇ USDNOK  ‚îÜ TREND  ‚îÜ 2             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 09][INFO] Proxy elegido autom√°ticamente ‚Üí PROXY0_TIMEQ\n",
      "[Celda 09] cols folds (PROXY FINAL) ‚Üí ER=ER_fold_proxy, PD=PD_fold_proxy, order_col=fold_order\n",
      "[Celda 09][DEBUG] n_folds_proxy FINAL ‚Üí n_unique=1, min=6, max=6, keys>1fold=100.00%\n",
      "[Celda 09] metrics_df construido (rows=166, cols=19)\n",
      "[Celda 09] HEAD(5) de m√©tricas:\n",
      "shape: (5, 19)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ n_folds_ER ‚îÜ ER_mean_fo ‚îÜ ‚Ä¶ ‚îÜ decay_pena ‚îÜ decay_pena ‚îÜ EstabScore ‚îÜ flag_stab ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---        ‚îÜ ld         ‚îÜ   ‚îÜ lty_PD     ‚îÜ lty        ‚îÜ ---        ‚îÜ _data_mis ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ i64        ‚îÜ ---        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ f64        ‚îÜ sing      ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ f64        ‚îÜ   ‚îÜ f64        ‚îÜ f64        ‚îÜ            ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ bool      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ USDZAR ‚îÜ TREND  ‚îÜ 6          ‚îÜ 0.000032   ‚îÜ ‚Ä¶ ‚îÜ 1.0        ‚îÜ 1.0        ‚îÜ 0.791803   ‚îÜ false     ‚îÇ\n",
      "‚îÇ DOTUSD ‚îÜ RANGE  ‚îÜ 6          ‚îÜ -0.000004  ‚îÜ ‚Ä¶ ‚îÜ 1.0        ‚îÜ 1.0        ‚îÜ 0.560686   ‚îÜ false     ‚îÇ\n",
      "‚îÇ XAGUSD ‚îÜ TREND  ‚îÜ 6          ‚îÜ 0.000065   ‚îÜ ‚Ä¶ ‚îÜ 1.0        ‚îÜ 1.0        ‚îÜ 0.689468   ‚îÜ false     ‚îÇ\n",
      "‚îÇ EURNOK ‚îÜ RANGE  ‚îÜ 6          ‚îÜ 0.000017   ‚îÜ ‚Ä¶ ‚îÜ 1.0        ‚îÜ 1.0        ‚îÜ 0.756139   ‚îÜ false     ‚îÇ\n",
      "‚îÇ WMT    ‚îÜ TREND  ‚îÜ 6          ‚îÜ 0.000164   ‚îÜ ‚Ä¶ ‚îÜ 1.0        ‚îÜ 1.0        ‚îÜ 0.695367   ‚îÜ false     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "[Celda 09][VERIFY] n_folds_ER: non_null=166/166, n_unique=1, min=6.0, med=6.0, max=6.0\n",
      "[Celda 09][VERIFY] n_folds_PD: non_null=166/166, n_unique=1, min=6.0, med=6.0, max=6.0\n",
      "[Celda 09][VERIFY] cv_ER_fold: non_null=166/166, n_unique=166, min=0.33488583605988875, med=1.9441093802712062, max=118.24199411429512\n",
      "[Celda 09][VERIFY] cv_PD_fold: non_null=166/166, n_unique=166, min=0.002089755483985511, med=0.009495432518419517, max=0.02973820731782221\n",
      "[Celda 09][VERIFY] decay_penalty (metrics_df): non_null=166/166, n_unique=1, min=1.0, med=1.0, max=1.0\n",
      "[Celda 09][VERIFY] EstabScore (metrics_df): non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] flag_stab_data_missing rate = 0.00%\n",
      "[Celda 09] INFO: No se encontraron betas de decay en stability_table. EstabScore_final = CV+decay.\n",
      "[Celda 09][DEBUG] regime_thresholds.columns = ['symbol', 'ER_P40', 'ER_P60', 'ER_hi', 'ER_lo', 'PD_P40', 'PD_P60', 'PD_hi', 'PD_lo', 'pctile_delta', 'pct_source_ER', 'pct_source_PD']\n",
      "[PATCH 09] regime_thresholds presente pero no contiene columna min_TR_required reconocible.\n",
      "[PATCH 09] min_TR_required aplicado con fallback=0.25\n",
      "[Celda 09][VERIFY] Resumen EstabScore (post-join):\n",
      "[Celda 09][VERIFY] EstabScore: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] EstabScore_base: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] EstabScore_final: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] decay_penalty (adv_join): non_null=166/166, n_unique=1, min=1.0, med=1.0, max=1.0\n",
      "[Celda 09][VERIFY] min_TR_required (adv_join): non_null=166/166, n_unique=1, min=0.25, med=0.25, max=0.25\n",
      "[Celda 09][VERIFY] passed_stability_gate True = 166/166 (100.00%)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet (OK, rows=166, cols=77)\n",
      "[Celda 09] sanity: rows_in=166 -> rows_out=166 (deber√≠a ser igual)\n",
      "[Celda 09][Diag] PD significativo (p<=Œ±_loose=0.2) = 0\n",
      "[Celda 09][Diag] PD significativo + passed_stability_gate=True = 0\n",
      "[Celda 09] p-values v√°lidos en pvals_combined_ER = 166/166\n",
      "[Celda 09] ER: p<=Œ±_loose ‚Üí 0/166 (0.00%)\n",
      "[Celda 09] p-values v√°lidos en pvals_combined_PD = 166/166\n",
      "[Celda 09] PD: p<=Œ±_loose ‚Üí 0/166 (0.00%)\n",
      "[Celda 09] p-values v√°lidos en pvals_combined_p_event = 166/166\n",
      "[Celda 09] p_event: p<=Œ±_loose ‚Üí 164/166 (98.80%)\n",
      "---- [Celda 09][VERIFY] Resumen global de salud ----\n",
      "[Celda 09][VERIFY] EstabScore: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] EstabScore_base: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] EstabScore_final: non_null=166/166, n_unique=166, min=0.5365351326708477, med=0.663325480686924, max=0.8725522223451045\n",
      "[Celda 09][VERIFY] decay_penalty: non_null=166/166, n_unique=1, min=1.0, med=1.0, max=1.0\n",
      "[Celda 09][VERIFY] min_TR_required: non_null=166/166, n_unique=1, min=0.25, med=0.25, max=0.25\n",
      ">>> Celda 09 v3.3.1 PRO :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 09 v3.3.1 PRO ‚Äî Estabilidad avanzada + Alpha Decay + p-values combinados (diagn√≥stico)\n",
    "#                     + EstabScore_final + PATCH min_TR_required robusto\n",
    "#                     + PROXY temporal realista por quantiles de tiempo (Idea B) [FIXED]\n",
    "#                     + SUPER-HOTFIX: proxy folds con K objetivo si evento-level\n",
    "#                     + Guardrail de granularidad (evita ts monstruoso)\n",
    "# --------------------------------------------------------------------------------\n",
    "# OBJETIVO:\n",
    "#   - Consolidar estabilidad avanzada POR CLAVE (symbol [+ preset si existe]):\n",
    "#       * CV_ER, CV_PD, m√≠nimos por fold\n",
    "#       * Alpha decay (corr r vs √≠ndice de fold) + decay_penalty\n",
    "#       * EstabScore_base y EstabScore_final\n",
    "#       * Flags de estabilidad:\n",
    "#           - keep_stab_base\n",
    "#           - passed_stability_gate\n",
    "#           - flag_stab_data_missing\n",
    "#           - stability_gate_reason\n",
    "#   - Mantener p-values combinados SOLO como diagn√≥stico:\n",
    "#       - pvals_combined_ER\n",
    "#       - pvals_combined_PD\n",
    "#       - pvals_combined_p_event\n",
    "#\n",
    "# FIXES CLAVE v3.3.1:\n",
    "#   1) Soporta granularidad por preset.\n",
    "#   2) Detecci√≥n robusta ER/PD en folds.\n",
    "#   3) Si stab_folds viene evento-level (ret/success):\n",
    "#       - ‚úÖ PROXY#0 TIMEQ realista por quantiles temporales POR KEY (K objetivo) [ARREGLADO]\n",
    "#       - intenta fold_id real plausible (segment/ts_cut/split_q/etc.)\n",
    "#       - si el proxy queda con pocos folds por key:\n",
    "#            ‚úÖ PROXY#2 sint√©tico temporal por KEY con K objetivo (default 6)\n",
    "#            ‚úÖ elige autom√°ticamente el proxy m√°s informativo\n",
    "#   4) Guardrail de granularidad:\n",
    "#       - evita que 'ts' gane por granularidad absurda.\n",
    "#   5) decay_penalty nunca queda NULL (neutral=1.0).\n",
    "#   6) min_TR_required robusto:\n",
    "#       - intenta leer de regime_thresholds\n",
    "#       - fallback inteligente a config/policy\n",
    "#       - persiste en stability_table_advanced\n",
    "#   7) Limpieza tech: pl.count() -> pl.len()\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import json, math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 09 v3.3.1 PRO :: Estabilidad avanzada + Alpha Decay + p-values combinados (diagn√≥stico) + EstabScore_final\")\n",
    "\n",
    "# ======================== Estado previo / Validaciones ========================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas previas.\")\n",
    "\n",
    "req_keys = (\"project_root\", \"run_id\", \"paths\")\n",
    "missing = [k for k in req_keys if k not in GLOBAL_STATE]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"GLOBAL_STATE incompleto; faltan claves: {missing}\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE[\"run_id\"]\n",
    "paths  = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "for k in (\"stability\", \"diagnostics\", \"metrics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"Falta GLOBAL_STATE['paths']['{k}']\")\n",
    "\n",
    "OUT_STAB_DIR    = Path(paths[\"stability\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "\n",
    "OUT_STAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stab_folds_path   = (OUT_STAB_DIR / \"stab_folds.parquet\").resolve()\n",
    "stab_table_path   = (OUT_STAB_DIR / \"stability_table.parquet\").resolve()\n",
    "adv_out_path      = (OUT_STAB_DIR / \"stability_table_advanced.parquet\").resolve()\n",
    "config_path       = (OUT_DIAG_DIR / \"config.json\").resolve()\n",
    "thr_path          = (OUT_METRICS_DIR / \"regime_thresholds.parquet\").resolve()\n",
    "\n",
    "def _probe_df(p: Path) -> Tuple[bool, int, int]:\n",
    "    if not p.exists():\n",
    "        return False, 0, 0\n",
    "    try:\n",
    "        df = pl.read_parquet(str(p))\n",
    "        return True, int(df.height), len(df.columns)\n",
    "    except Exception:\n",
    "        return False, 0, 0\n",
    "\n",
    "ok_folds, r_folds, c_folds = _probe_df(stab_folds_path)\n",
    "ok_tab,   r_tab,   c_tab   = _probe_df(stab_table_path)\n",
    "ok_cfg                    = config_path.exists()\n",
    "ok_thr,   r_thr,   c_thr   = _probe_df(thr_path)\n",
    "\n",
    "print(f\"[Celda 09] RUN_ID           = {RUN_ID}\")\n",
    "print(f\"üìÅ INPUT ‚Üí stab_folds       = {str(stab_folds_path)} (exists={'true' if ok_folds else 'false'}, rows={r_folds}, cols={c_folds})\")\n",
    "print(f\"üìÅ INPUT ‚Üí stability_table  = {str(stab_table_path)} (rows={r_tab}, cols={c_tab})\")\n",
    "print(f\"üìÅ INPUT ‚Üí config.json      = {str(config_path)} (exists={'true' if ok_cfg else 'false'})\")\n",
    "print(f\"üìÅ INPUT ‚Üí regime_thresholds= {str(thr_path)} (exists={'true' if ok_thr else 'false'}, rows={r_thr}, cols={c_thr})\")\n",
    "\n",
    "if not ok_tab:\n",
    "    raise RuntimeError(f\"Missing required input: {stab_table_path}\")\n",
    "if (not ok_cfg) and (not GLOBAL_STATE.get(\"config\")):\n",
    "    raise RuntimeError(\"No se encontr√≥ config.json y GLOBAL_STATE['config'] est√° vac√≠o. Revisa celdas iniciales.\")\n",
    "\n",
    "folds = pl.read_parquet(str(stab_folds_path)) if ok_folds else pl.DataFrame()\n",
    "stab  = pl.read_parquet(str(stab_table_path))\n",
    "thr   = pl.read_parquet(str(thr_path)) if ok_thr else pl.DataFrame()\n",
    "\n",
    "# ======================== Config (desde GLOBAL_STATE, fallback a archivo) ========================\n",
    "cfg_from_state = GLOBAL_STATE.get(\"config\", {})\n",
    "if isinstance(cfg_from_state, dict) and cfg_from_state:\n",
    "    CONFIG = cfg_from_state\n",
    "else:\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        CONFIG = json.load(fh)\n",
    "    if isinstance(CONFIG, dict):\n",
    "        GLOBAL_STATE[\"config\"] = CONFIG\n",
    "\n",
    "if not isinstance(CONFIG, dict):\n",
    "    CONFIG = {}\n",
    "\n",
    "decay_cfg     = CONFIG.get(\"decay\", {})      or {}\n",
    "stats_cfg     = CONFIG.get(\"stats\", {})      or {}\n",
    "policy_cfg    = CONFIG.get(\"policy\", {})     or {}\n",
    "stability_cfg = CONFIG.get(\"stability\", {})  or {}\n",
    "\n",
    "def _cfg_float(section: dict, key: str, default: float) -> float:\n",
    "    try:\n",
    "        val = section.get(key, default)\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "penalty_lambda = _cfg_float(decay_cfg, \"penalty_lambda\", 0.15)\n",
    "alpha_strict   = _cfg_float(stats_cfg, \"alpha_strict\", 0.02)\n",
    "alpha_balanced = _cfg_float(stats_cfg, \"alpha_balanced\", 0.08)\n",
    "alpha_loose    = _cfg_float(stats_cfg, \"alpha_loose\", 0.25)\n",
    "\n",
    "# clamp defensivo\n",
    "alpha_strict   = min(max(alpha_strict, 0.0001), 0.2)\n",
    "alpha_balanced = min(max(alpha_balanced, alpha_strict), 0.5)\n",
    "alpha_loose    = min(max(alpha_loose, alpha_balanced), 0.9)\n",
    "\n",
    "# LEGACY\n",
    "Estab_min         = _cfg_float(policy_cfg, \"Estab_min\", 0.60)\n",
    "Estab_min_salvado = _cfg_float(policy_cfg, \"Estab_min_salvado\", 0.40)\n",
    "score_super_alto  = _cfg_float(policy_cfg, \"score_super_alto\", 0.70)\n",
    "\n",
    "# Umbrales por preset\n",
    "min_estab_strict   = _cfg_float(stability_cfg, \"min_estab_strict\",   0.70)\n",
    "min_estab_balanced = _cfg_float(stability_cfg, \"min_estab_balanced\", 0.50)\n",
    "min_estab_loose    = _cfg_float(stability_cfg, \"min_estab_loose\",    0.30)\n",
    "\n",
    "# ‚úÖ Objetivo de folds sint√©ticos (robusto)\n",
    "proxy_target_folds = int(stability_cfg.get(\"proxy_target_folds\", 6) or 6)\n",
    "proxy_target_folds = max(3, min(proxy_target_folds, 12))\n",
    "\n",
    "# Default cuant para min_TR_required\n",
    "def _default_min_tr_required() -> float:\n",
    "    for sec in (stability_cfg, policy_cfg):\n",
    "        try:\n",
    "            if isinstance(sec, dict) and (\"min_TR_required\" in sec):\n",
    "                return float(sec[\"min_TR_required\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return float(policy_cfg.get(\"min_TR_after_cost\", 0.25))\n",
    "    except Exception:\n",
    "        return 0.25\n",
    "\n",
    "DEFAULT_MIN_TR_REQUIRED = _default_min_tr_required()\n",
    "\n",
    "print(f\"[Celda 09] decay.penalty_lambda                   = {penalty_lambda}\")\n",
    "print(f\"[Celda 09] alpha_strict (decay claro)             = {alpha_strict}\")\n",
    "print(f\"[Celda 09] alpha_balanced (referencia)           = {alpha_balanced}\")\n",
    "print(f\"[Celda 09] alpha_loose (p-values diag)           = {alpha_loose}\")\n",
    "print(\n",
    "    f\"[Celda 09] Pol√≠tica estabilidad LEGACY ‚Üí \"\n",
    "    f\"Estab_min={Estab_min}, Estab_min_salvado={Estab_min_salvado}, score_super_alto={score_super_alto}\"\n",
    ")\n",
    "print(\n",
    "    f\"[Celda 09] Pol√≠tica estabilidad por preset ‚Üí \"\n",
    "    f\"min_estab_strict={min_estab_strict}, min_estab_balanced={min_estab_balanced}, min_estab_loose={min_estab_loose}\"\n",
    ")\n",
    "print(f\"[Celda 09] default min_TR_required (fallback cuant) = {DEFAULT_MIN_TR_REQUIRED}\")\n",
    "print(f\"[Celda 09] proxy_target_folds (K objetivo)          = {proxy_target_folds}\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"stability_policy\", {})\n",
    "GLOBAL_STATE[\"stability_policy\"].update({\n",
    "    \"Estab_min\": Estab_min,\n",
    "    \"Estab_min_salvado\": Estab_min_salvado,\n",
    "    \"score_super_alto\": score_super_alto,\n",
    "    \"min_estab_strict\": min_estab_strict,\n",
    "    \"min_estab_balanced\": min_estab_balanced,\n",
    "    \"min_estab_loose\": min_estab_loose,\n",
    "    \"alpha_strict_decay\": alpha_strict,\n",
    "    \"alpha_loose\": alpha_loose,\n",
    "    \"alpha_balanced\": alpha_balanced,\n",
    "    \"min_TR_required_default\": DEFAULT_MIN_TR_REQUIRED,\n",
    "    \"proxy_target_folds\": proxy_target_folds,\n",
    "})\n",
    "\n",
    "# ======================== Helpers num√©ricos ========================\n",
    "P_MIN = 1e-300\n",
    "\n",
    "def _drop_nan(a: List[float]) -> List[float]:\n",
    "    out: List[float] = []\n",
    "    for v in a:\n",
    "        try:\n",
    "            x = float(v)\n",
    "            if math.isfinite(x):\n",
    "                out.append(x)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def _mean_std_cv(vals: List[float]) -> Tuple[Optional[float], Optional[float], Optional[float], Optional[float]]:\n",
    "    v = _drop_nan(vals)\n",
    "    n = len(v)\n",
    "    if n < 2:\n",
    "        return None, None, None, (min(v) if v else None)\n",
    "    m = sum(v) / n\n",
    "    var = sum((x - m) ** 2 for x in v) / (n - 1)\n",
    "    sd = math.sqrt(var) if var > 0 else 0.0\n",
    "    cv = None if m == 0.0 else sd / abs(m)\n",
    "    vmin = min(v) if v else None\n",
    "    return m, sd, cv, vmin\n",
    "\n",
    "def _corr_idx(vals: List[float]) -> Optional[float]:\n",
    "    v = _drop_nan(vals)\n",
    "    n = len(v)\n",
    "    if n < 2:\n",
    "        return None\n",
    "    xs = list(range(n))\n",
    "    mx = sum(xs) / n\n",
    "    my = sum(v) / n\n",
    "    num = sum((xi - mx) * (yi - my) for xi, yi in zip(xs, v))\n",
    "    denx = math.sqrt(sum((xi - mx) ** 2 for xi in xs))\n",
    "    deny = math.sqrt(sum((yi - my) ** 2 for yi in v))\n",
    "    if denx <= 0.0 or deny <= 0.0:\n",
    "        return None\n",
    "    r = num / (denx * deny)\n",
    "    if not math.isfinite(r):\n",
    "        return None\n",
    "    return float(max(-1.0, min(1.0, r)))\n",
    "\n",
    "def _decay_penalty_from_corr(r: Optional[float], lam: float) -> Optional[float]:\n",
    "    if r is None or not math.isfinite(r):\n",
    "        return None\n",
    "    raw = max(0.0, -float(r))  # solo penaliza tendencia negativa\n",
    "    lam = max(float(lam), 0.0)\n",
    "    if lam == 0.0:\n",
    "        return 1.0\n",
    "    return 1.0 / (1.0 + lam * raw)\n",
    "\n",
    "def _first_present(df: pl.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    if df is None or df.is_empty():\n",
    "        return None\n",
    "    cols = set(df.columns)\n",
    "    low  = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c in cols:\n",
    "            return c\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n",
    "\n",
    "def _find_by_substring(df: pl.DataFrame, needle: str) -> Optional[str]:\n",
    "    if df is None or df.is_empty():\n",
    "        return None\n",
    "    needle = needle.lower()\n",
    "    hits = [c for c in df.columns if needle in c.lower()]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda x: len(x))\n",
    "        return hits[0]\n",
    "    return None\n",
    "\n",
    "def _series_by_key(df: pl.DataFrame, key: Dict[str, Any], col: str, order_col: Optional[str]) -> List[float]:\n",
    "    if df.is_empty() or col not in df.columns:\n",
    "        return []\n",
    "    cond = (pl.col(\"symbol\") == key[\"symbol\"])\n",
    "    if \"preset\" in key and \"preset\" in df.columns:\n",
    "        cond = cond & (pl.col(\"preset\") == key[\"preset\"])\n",
    "    q = df.filter(cond)\n",
    "    if order_col and order_col in q.columns:\n",
    "        q = q.sort(order_col)\n",
    "    return q[col].to_list()\n",
    "\n",
    "# ======================== Normalizaci√≥n de columnas clave ========================\n",
    "if \"symbol\" not in stab.columns:\n",
    "    raise RuntimeError(\"stability_table.parquet no contiene columna 'symbol'\")\n",
    "\n",
    "stab = stab.with_columns(\n",
    "    pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    ")\n",
    "if \"preset\" in stab.columns:\n",
    "    stab = stab.with_columns(\n",
    "        pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"preset\")\n",
    "    )\n",
    "\n",
    "if \"symbol\" in folds.columns:\n",
    "    folds = folds.with_columns(\n",
    "        pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    "    )\n",
    "if \"preset\" in folds.columns:\n",
    "    folds = folds.with_columns(\n",
    "        pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"preset\")\n",
    "    )\n",
    "\n",
    "if not thr.is_empty() and \"symbol\" in thr.columns:\n",
    "    thr = thr.with_columns(\n",
    "        pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    "    )\n",
    "\n",
    "# ======================== Definir keys de uni√≥n ========================\n",
    "key_cols = [\"symbol\"]\n",
    "if \"preset\" in stab.columns and (not folds.is_empty()) and (\"preset\" in folds.columns):\n",
    "    key_cols.append(\"preset\")\n",
    "\n",
    "print(f\"[Celda 09] join keys = {key_cols}\")\n",
    "\n",
    "adv = stab\n",
    "artefact_cols = [c for c in adv.columns if c.endswith(\"_right\") or c.endswith(\"_left\") or c.endswith(\"_new\")]\n",
    "if artefact_cols:\n",
    "    adv = adv.drop(artefact_cols)\n",
    "\n",
    "# ======================== Asegurar p-values en tabla base ========================\n",
    "PVAL_ER_CANDS = [\"pvals_combined_ER\", \"p_ER_combined\", \"p_ER\", \"pval_ER\", \"p_value_ER\", \"p_value_er\"]\n",
    "PVAL_PD_CANDS = [\"pvals_combined_PD\", \"p_PD_combined\", \"p_PD\", \"pval_PD\", \"p_value_PD\", \"p_value_pd\"]\n",
    "PVAL_EV_CANDS = [\"pvals_combined_p_event\", \"p_event_combined\", \"p_event\", \"pval_event\", \"p_value_event\"]\n",
    "\n",
    "def _ensure_pvals(df: pl.DataFrame, cands: List[str], target: str) -> pl.DataFrame:\n",
    "    src = _first_present(df, cands)\n",
    "    if src is None:\n",
    "        if target not in df.columns:\n",
    "            df = df.with_columns(pl.lit(None).alias(target))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col(src).cast(pl.Float64, strict=False).alias(target))\n",
    "    return df\n",
    "\n",
    "adv = _ensure_pvals(adv, PVAL_ER_CANDS, \"pvals_combined_ER\")\n",
    "adv = _ensure_pvals(adv, PVAL_PD_CANDS, \"pvals_combined_PD\")\n",
    "adv = _ensure_pvals(adv, PVAL_EV_CANDS, \"pvals_combined_p_event\")\n",
    "\n",
    "# ======================== Detecci√≥n robusta de columnas ER/PD/orden en folds ========================\n",
    "print(f\"[Celda 09] folds.columns = {folds.columns if not folds.is_empty() else '[]'}\")\n",
    "\n",
    "ER_CANDS = [\"ER_fold\", \"ER_value_fold\", \"ER_mean_fold\", \"ER_by_fold\", \"ER_fold_ret\", \"ER\", \"er\"]\n",
    "PD_CANDS = [\"PD_fold\", \"PD_value_fold\", \"PD_mean_fold\", \"PD_by_fold\", \"PD\", \"pd\"]\n",
    "\n",
    "# evento-level ts candidates (m√°s completos)\n",
    "EVENT_TS_CANDS = [\n",
    "    \"ts\",\n",
    "    \"timestamp_utc\", \"timestamp_gye\",\n",
    "    \"timestamp\",\n",
    "    \"time_utc\",\n",
    "    \"datetime_utc\",\n",
    "    \"datetime\",\n",
    "    \"time\",\n",
    "]\n",
    "\n",
    "IDX_CANDS = [\"fold_idx\", \"fold_id\", \"fold\", \"k\", \"idx\", \"index\", \"split_q_ISOOS\", \"ts_cut_ISOOS\", \"segment\"]\n",
    "\n",
    "col_ER  = _first_present(folds, ER_CANDS) if not folds.is_empty() else None\n",
    "col_PD  = _first_present(folds, PD_CANDS) if not folds.is_empty() else None\n",
    "col_TS  = _first_present(folds, EVENT_TS_CANDS) if not folds.is_empty() else None\n",
    "col_IDX = _first_present(folds, IDX_CANDS) if not folds.is_empty() else None\n",
    "\n",
    "if col_ER is None and not folds.is_empty():\n",
    "    col_ER = _find_by_substring(folds, \"er\")\n",
    "if col_PD is None and not folds.is_empty():\n",
    "    col_PD = _find_by_substring(folds, \"pd\")\n",
    "\n",
    "order_col = col_IDX or col_TS\n",
    "\n",
    "print(f\"[Celda 09] cols folds detectadas ‚Üí ER={col_ER}, PD={col_PD}, order_col={order_col}\")\n",
    "\n",
    "# ======================== Selecci√≥n fold_id plausible (guardrail) ========================\n",
    "def _choose_best_fold_id_per_key(\n",
    "    df: pl.DataFrame,\n",
    "    candidates: List[str],\n",
    "    key_cols: List[str],\n",
    "    plausible_min: int = 2,\n",
    "    plausible_max: int = 50,\n",
    ") -> Optional[str]:\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "\n",
    "    existing = [c for c in candidates if c in df.columns]\n",
    "    if not existing:\n",
    "        return None\n",
    "\n",
    "    print(\"[Celda 09][DEBUG] Evaluando candidatos fold_id por KEY:\")\n",
    "\n",
    "    base_cols = [c for c in (key_cols + existing) if c in df.columns]\n",
    "    df_small = df.select(base_cols)\n",
    "\n",
    "    stats = []\n",
    "    for cand in existing:\n",
    "        try:\n",
    "            tmp = (\n",
    "                df_small\n",
    "                .select(key_cols + [cand])\n",
    "                .drop_nulls(subset=[cand])\n",
    "                .unique()\n",
    "                .group_by(key_cols)\n",
    "                .agg(pl.col(cand).n_unique().alias(\"__n__\"))\n",
    "            )\n",
    "\n",
    "            if tmp.is_empty():\n",
    "                print(f\"[Celda 09][DEBUG] fold_id '{cand}' ‚Üí tmp vac√≠o\")\n",
    "                continue\n",
    "\n",
    "            n_list_raw = tmp[\"__n__\"].to_list()\n",
    "            n_list = [int(x) for x in n_list_raw if x is not None]\n",
    "            if not n_list:\n",
    "                print(f\"[Celda 09][DEBUG] fold_id '{cand}' ‚Üí sin n_list\")\n",
    "                continue\n",
    "\n",
    "            gt1 = sum(1 for x in n_list if x > 1)\n",
    "            pct_gt1 = float(gt1) / float(len(n_list))\n",
    "\n",
    "            n_sorted = sorted(n_list)\n",
    "            med = float(n_sorted[len(n_sorted)//2]) if n_sorted else 0.0\n",
    "            mean = float(sum(n_sorted) / len(n_sorted)) if n_sorted else 0.0\n",
    "\n",
    "            print(\n",
    "                f\"[Celda 09][DEBUG] fold_id '{cand}' ‚Üí \"\n",
    "                f\"keys={len(n_list)} | pct_gt1={pct_gt1:.3f} | med={med:.2f} | mean={mean:.2f}\"\n",
    "            )\n",
    "\n",
    "            stats.append((cand, pct_gt1, med, mean))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Celda 09][DEBUG] fold_id '{cand}' ‚Üí error: {e}\")\n",
    "\n",
    "    if not stats:\n",
    "        return None\n",
    "\n",
    "    plausible = [s for s in stats if plausible_min <= s[2] <= plausible_max]\n",
    "\n",
    "    if plausible:\n",
    "        plausible.sort(key=lambda x: (x[1], x[2], x[3]), reverse=True)\n",
    "        best = plausible[0][0]\n",
    "        print(f\"[Celda 09][DEBUG] fold_id ganador (plausible) ‚Üí {best}\")\n",
    "        return best\n",
    "\n",
    "    stats.sort(key=lambda x: (x[1], -x[2]), reverse=True)\n",
    "    best = stats[0][0]\n",
    "    print(f\"[Celda 09][DEBUG] fold_id ganador (fallback sin plausible) ‚Üí {best}\")\n",
    "    return best\n",
    "\n",
    "# ======================== Construcci√≥n de proxy folds ========================\n",
    "def _build_proxy_from_fold_id(\n",
    "    df_events: pl.DataFrame,\n",
    "    fold_id_col: str,\n",
    "    key_cols: List[str],\n",
    ") -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    has_ret  = \"ret\" in df_events.columns\n",
    "    has_succ = \"success\" in df_events.columns\n",
    "\n",
    "    agg_exprs = []\n",
    "    if has_ret:\n",
    "        agg_exprs.append(pl.col(\"ret\").cast(pl.Float64, strict=False).mean().alias(\"ER_fold_proxy\"))\n",
    "    if has_succ:\n",
    "        agg_exprs.append(pl.col(\"success\").cast(pl.Float64, strict=False).mean().alias(\"PD_fold_proxy\"))\n",
    "\n",
    "    folds_proxy = (\n",
    "        df_events\n",
    "        .group_by(key_cols + [fold_id_col])\n",
    "        .agg(agg_exprs)\n",
    "    )\n",
    "\n",
    "    proxy_counts = (\n",
    "        folds_proxy\n",
    "        .group_by(key_cols)\n",
    "        .agg(pl.len().alias(\"n_folds_proxy\"))\n",
    "    )\n",
    "\n",
    "    return folds_proxy, proxy_counts\n",
    "\n",
    "def _median_int_from_counts(proxy_counts: pl.DataFrame) -> Optional[int]:\n",
    "    if proxy_counts.is_empty() or \"n_folds_proxy\" not in proxy_counts.columns:\n",
    "        return None\n",
    "    try:\n",
    "        s = proxy_counts[\"n_folds_proxy\"].to_list()\n",
    "        s = [int(x) for x in s if x is not None]\n",
    "        if not s:\n",
    "            return None\n",
    "        s.sort()\n",
    "        return int(s[len(s)//2])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _proxy_pct_gt1(proxy_counts: pl.DataFrame) -> float:\n",
    "    if proxy_counts.is_empty() or \"n_folds_proxy\" not in proxy_counts.columns:\n",
    "        return 0.0\n",
    "    try:\n",
    "        s = proxy_counts[\"n_folds_proxy\"].to_list()\n",
    "        s = [int(x) for x in s if x is not None]\n",
    "        if not s:\n",
    "            return 0.0\n",
    "        gt1 = sum(1 for x in s if x > 1)\n",
    "        return float(gt1) / float(len(s))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# -------------------- PROXY#0 (Idea B) FIXED: temporal realista por quantiles de tiempo --------------------\n",
    "def _build_time_quantile_proxy(\n",
    "    df_events: pl.DataFrame,\n",
    "    key_cols: List[str],\n",
    "    ts_col: str,\n",
    "    k: int,\n",
    ") -> Tuple[pl.DataFrame, pl.DataFrame, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Construye fold_id temporal realista por quantiles de tiempo POR KEY.\n",
    "    FIX:\n",
    "      - No referencia aliases creados en el mismo with_columns.\n",
    "      - pl.count() -> pl.len()\n",
    "    \"\"\"\n",
    "    if df_events.is_empty() or ts_col not in df_events.columns:\n",
    "        return pl.DataFrame(), pl.DataFrame(), None\n",
    "\n",
    "    has_ret  = \"ret\" in df_events.columns\n",
    "    has_succ = \"success\" in df_events.columns\n",
    "    if not (has_ret or has_succ):\n",
    "        return pl.DataFrame(), pl.DataFrame(), None\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            df_events\n",
    "            .with_columns(\n",
    "                pl.col(ts_col).cast(pl.Datetime, strict=False).alias(\"__ts_dt\")\n",
    "            )\n",
    "            .drop_nulls(subset=[\"__ts_dt\"])\n",
    "        )\n",
    "\n",
    "        if df.is_empty():\n",
    "            return pl.DataFrame(), pl.DataFrame(), None\n",
    "\n",
    "        # orden temporal por KEY\n",
    "        df = df.sort(key_cols + [\"__ts_dt\"])\n",
    "\n",
    "        # 1) rank ordinal por KEY\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"__ts_dt\").rank(\"ordinal\").over(key_cols).alias(\"__rn\")\n",
    "        )\n",
    "\n",
    "        # 2) n_total por KEY\n",
    "        df = df.with_columns(\n",
    "            pl.len().over(key_cols).alias(\"__n_total\")\n",
    "        )\n",
    "\n",
    "        # 3) fold_id cuantiles temporales\n",
    "        df = df.with_columns(\n",
    "            (\n",
    "                ((pl.col(\"__rn\") - 1) * pl.lit(int(k)) / pl.col(\"__n_total\"))\n",
    "                .floor()\n",
    "                .cast(pl.Int32)\n",
    "            ).alias(\"fold_id\")\n",
    "        )\n",
    "\n",
    "        agg_exprs = []\n",
    "        if has_ret:\n",
    "            agg_exprs.append(pl.col(\"ret\").cast(pl.Float64, strict=False).mean().alias(\"ER_fold_proxy\"))\n",
    "        if has_succ:\n",
    "            agg_exprs.append(pl.col(\"success\").cast(pl.Float64, strict=False).mean().alias(\"PD_fold_proxy\"))\n",
    "\n",
    "        agg_exprs += [\n",
    "            pl.min(\"__ts_dt\").alias(\"fold_start\"),\n",
    "            pl.max(\"__ts_dt\").alias(\"fold_end\"),\n",
    "            pl.len().alias(\"n_events_fold\"),\n",
    "        ]\n",
    "\n",
    "        folds_proxy = (\n",
    "            df\n",
    "            .group_by(key_cols + [\"fold_id\"])\n",
    "            .agg(agg_exprs)\n",
    "            .with_columns(\n",
    "                pl.col(\"fold_id\").cast(pl.Int32).alias(\"fold_order\")\n",
    "            )\n",
    "            .sort(key_cols + [\"fold_order\"])\n",
    "        )\n",
    "\n",
    "        counts = (\n",
    "            folds_proxy\n",
    "            .group_by(key_cols)\n",
    "            .agg(pl.len().alias(\"n_folds_proxy\"))\n",
    "        )\n",
    "\n",
    "        return folds_proxy, counts, \"fold_id\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 09][WARN] PROXY#0 TIMEQ fall√≥: {e}\")\n",
    "        return pl.DataFrame(), pl.DataFrame(), None\n",
    "\n",
    "# -------------------- PROXY#2 (fallback): sint√©tico temporal por ts int --------------------\n",
    "def _add_synthetic_time_fold_id(\n",
    "    df_events: pl.DataFrame,\n",
    "    key_cols: List[str],\n",
    "    ts_col: str,\n",
    "    k: int,\n",
    ") -> Tuple[pl.DataFrame, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Crea un fold_id sint√©tico temporal por KEY:\n",
    "      - rank ordinal por ts dentro de key\n",
    "      - bin = floor((rank-1) * k / n_key)\n",
    "    Esto da ~k folds por key sin explosi√≥n.\n",
    "    Mantiene como √∫ltimo fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df_events.with_columns(\n",
    "            pl.col(ts_col).cast(pl.Int64, strict=False).alias(\"__ts_int\")\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"__ts_int\").rank(\"ordinal\").over(key_cols).alias(\"__rn\")\n",
    "        )\n",
    "\n",
    "        counts = df.group_by(key_cols).agg(pl.len().alias(\"__n_total\"))\n",
    "        df = df.join(counts, on=key_cols, how=\"left\")\n",
    "\n",
    "        df = df.with_columns(\n",
    "            (\n",
    "                ((pl.col(\"__rn\") - 1) * pl.lit(int(k)) / pl.col(\"__n_total\"))\n",
    "                .floor()\n",
    "                .cast(pl.Int32)\n",
    "            ).alias(\"__time_bin\")\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.concat_str(\n",
    "                [pl.lit(\"T\"), pl.col(\"__time_bin\").cast(pl.Utf8)],\n",
    "                separator=\"\"\n",
    "            ).alias(\"__synthetic_fold_id\")\n",
    "        )\n",
    "\n",
    "        return df, \"__synthetic_fold_id\"\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 09][WARN] No se pudo construir fold_id sint√©tico temporal: {e}\")\n",
    "        return df_events, None\n",
    "\n",
    "# ======================== PATCH MAYOR: evento-level -> proxy mejorado ========================\n",
    "if (not folds.is_empty()) and (col_ER is None) and (col_PD is None):\n",
    "    has_ret  = \"ret\" in folds.columns\n",
    "    has_succ = \"success\" in folds.columns\n",
    "\n",
    "    if has_ret or has_succ:\n",
    "        folds_events = folds  # conservar original evento-level\n",
    "\n",
    "        # -------- PROXY#0 TIMEQ (Idea B) --------\n",
    "        ts_evt_col = _first_present(folds_events, EVENT_TS_CANDS)\n",
    "\n",
    "        proxy0 = pl.DataFrame()\n",
    "        counts0 = pl.DataFrame()\n",
    "        med0 = None\n",
    "        order0 = None\n",
    "\n",
    "        if ts_evt_col is not None:\n",
    "            print(\n",
    "                \"[Celda 09] INFO: stab_folds evento-level detectado. \"\n",
    "                f\"Intentando PROXY#0 temporal realista por quantiles usando ts='{ts_evt_col}' y K={proxy_target_folds}.\"\n",
    "            )\n",
    "            proxy0, counts0, order0 = _build_time_quantile_proxy(\n",
    "                folds_events, key_cols, ts_evt_col, proxy_target_folds\n",
    "            )\n",
    "            med0 = _median_int_from_counts(counts0) if not counts0.is_empty() else None\n",
    "            pct0 = _proxy_pct_gt1(counts0) if not counts0.is_empty() else 0.0\n",
    "            print(f\"[Celda 09][DEBUG] PROXY#0 TIMEQ ‚Üí rows={proxy0.height}, cols={len(proxy0.columns)} | median_n_folds‚âà{med0} | pct_gt1‚âà{pct0:.3f}\")\n",
    "\n",
    "        # -------- PROXY#1 por fold_id \"sem√°ntico\" --------\n",
    "        fold_id_candidates = [\n",
    "            \"segment\",\n",
    "            \"split_q_ISOOS\",\n",
    "            \"ts_cut_ISOOS\",\n",
    "            \"fold_idx\", \"fold_id\", \"fold\", \"k\", \"idx\", \"index\",\n",
    "            # NOTA: NO ponemos 'ts' aqu√≠ ‚Üí guardrail anti-granularidad absurda\n",
    "        ]\n",
    "\n",
    "        fold_id_col = _choose_best_fold_id_per_key(\n",
    "            folds_events,\n",
    "            fold_id_candidates,\n",
    "            key_cols,\n",
    "            plausible_min=2,\n",
    "            plausible_max=50,\n",
    "        )\n",
    "\n",
    "        proxy1 = pl.DataFrame()\n",
    "        counts1 = pl.DataFrame()\n",
    "        med1 = None\n",
    "\n",
    "        if fold_id_col is not None:\n",
    "            print(\n",
    "                \"[Celda 09] INFO: Construyendo PROXY#1 fold-level usando fold_id sem√°ntico \"\n",
    "                f\"'{fold_id_col}'.\"\n",
    "            )\n",
    "            proxy1, counts1 = _build_proxy_from_fold_id(folds_events, fold_id_col, key_cols)\n",
    "            med1 = _median_int_from_counts(counts1)\n",
    "            pct1 = _proxy_pct_gt1(counts1)\n",
    "            print(f\"[Celda 09][DEBUG] PROXY#1 ‚Üí rows={proxy1.height}, cols={len(proxy1.columns)} | median_n_folds‚âà{med1} | pct_gt1‚âà{pct1:.3f}\")\n",
    "            print(\"[Celda 09][DEBUG] PROXY#1 n_folds_proxy (head 10):\")\n",
    "            print(counts1.sort(\"n_folds_proxy\", descending=True).head(10))\n",
    "\n",
    "        # -------- PROXY#2 sint√©tico temporal (√∫ltimo recurso) --------\n",
    "        proxy2 = pl.DataFrame()\n",
    "        counts2 = pl.DataFrame()\n",
    "        med2 = None\n",
    "        syn_col = None\n",
    "\n",
    "        if ((\"ts\" in folds_events.columns) or (ts_evt_col is not None)):\n",
    "            ts_for_syn = \"ts\" if \"ts\" in folds_events.columns else ts_evt_col\n",
    "\n",
    "            weak0 = (med0 is None) or (med0 < 4)\n",
    "            weak1 = (med1 is None) or (med1 < 4)\n",
    "\n",
    "            if ts_for_syn is not None and (weak0 and weak1):\n",
    "                print(\n",
    "                    f\"[Celda 09][INFO] PROXY#0/1 con soporte bajo. \"\n",
    "                    f\"Intentando PROXY#2 sint√©tico temporal con K={proxy_target_folds} usando '{ts_for_syn}'.\"\n",
    "                )\n",
    "\n",
    "                folds_tmp, syn_col = _add_synthetic_time_fold_id(\n",
    "                    folds_events, key_cols, ts_for_syn, proxy_target_folds\n",
    "                )\n",
    "\n",
    "                if syn_col is not None:\n",
    "                    proxy2, counts2 = _build_proxy_from_fold_id(folds_tmp, syn_col, key_cols)\n",
    "                    med2 = _median_int_from_counts(counts2)\n",
    "                    pct2 = _proxy_pct_gt1(counts2)\n",
    "                    print(f\"[Celda 09][DEBUG] PROXY#2 (synthetic) ‚Üí rows={proxy2.height}, cols={len(proxy2.columns)} | median_n_folds‚âà{med2} | pct_gt1‚âà{pct2:.3f}\")\n",
    "\n",
    "        # -------- Selecci√≥n autom√°tica del mejor proxy --------\n",
    "        candidates = []\n",
    "\n",
    "        if not proxy0.is_empty() and not counts0.is_empty():\n",
    "            candidates.append((\"PROXY0_TIMEQ\", proxy0, counts0, med0, order0))\n",
    "        if not proxy1.is_empty() and not counts1.is_empty():\n",
    "            candidates.append((\"PROXY1_ID\", proxy1, counts1, med1, fold_id_col))\n",
    "        if not proxy2.is_empty() and not counts2.is_empty():\n",
    "            candidates.append((\"PROXY2_SYN\", proxy2, counts2, med2, syn_col))\n",
    "\n",
    "        def _cand_score(name: str, counts: pl.DataFrame, med: Optional[int]) -> Tuple[float, float, int]:\n",
    "            pct = _proxy_pct_gt1(counts)\n",
    "            med_s = float(med) if (med is not None) else 0.0\n",
    "            # preferencia suave: TIMEQ > ID > SYN cuando empatan soporte\n",
    "            pref = 2 if name == \"PROXY0_TIMEQ\" else (1 if name == \"PROXY1_ID\" else 0)\n",
    "            return (pct, med_s, pref)\n",
    "\n",
    "        chosen_proxy = None\n",
    "        chosen_counts = None\n",
    "        chosen_fold_id_name = None\n",
    "        chosen_name = None\n",
    "\n",
    "        if candidates:\n",
    "            candidates.sort(\n",
    "                key=lambda t: _cand_score(t[0], t[2], t[3]),\n",
    "                reverse=True\n",
    "            )\n",
    "            chosen_name, chosen_proxy, chosen_counts, _, chosen_fold_id_name = candidates[0]\n",
    "            print(f\"[Celda 09][INFO] Proxy elegido autom√°ticamente ‚Üí {chosen_name}\")\n",
    "\n",
    "        if chosen_proxy is not None and not chosen_proxy.is_empty():\n",
    "            folds = chosen_proxy\n",
    "\n",
    "            col_ER = \"ER_fold_proxy\" if \"ER_fold_proxy\" in folds.columns else None\n",
    "            col_PD = \"PD_fold_proxy\" if \"PD_fold_proxy\" in folds.columns else None\n",
    "\n",
    "            # orden preferido: fold_order si existe\n",
    "            if \"fold_order\" in folds.columns:\n",
    "                order_col = \"fold_order\"\n",
    "            else:\n",
    "                order_col = chosen_fold_id_name if (chosen_fold_id_name in folds.columns) else (col_TS or chosen_fold_id_name)\n",
    "\n",
    "            print(f\"[Celda 09] cols folds (PROXY FINAL) ‚Üí ER={col_ER}, PD={col_PD}, order_col={order_col}\")\n",
    "\n",
    "            if chosen_counts is not None and not chosen_counts.is_empty():\n",
    "                n_list = chosen_counts[\"n_folds_proxy\"].to_list()\n",
    "                n_list = [int(x) for x in n_list if x is not None]\n",
    "                if n_list:\n",
    "                    nu = len(set(n_list)); mn = min(n_list); mx = max(n_list)\n",
    "                    gt1 = sum(1 for x in n_list if x > 1)\n",
    "                    pct_gt1 = 100.0 * gt1 / max(1, len(n_list))\n",
    "                    print(f\"[Celda 09][DEBUG] n_folds_proxy FINAL ‚Üí n_unique={nu}, min={mn}, max={mx}, keys>1fold={pct_gt1:.2f}%\")\n",
    "\n",
    "        else:\n",
    "            print(\"[Celda 09][WARN] No se pudo construir proxy usable. Se mantiene fallback neutral.\")\n",
    "\n",
    "# ======================== Construcci√≥n de m√©tricas por KEY ========================\n",
    "keys_df = adv.select(key_cols).unique()\n",
    "keys = keys_df.to_dicts()\n",
    "rows_metrics = []\n",
    "\n",
    "def _neutral_rec(key: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    rec = dict(key)\n",
    "    rec.update({\n",
    "        \"n_folds_ER\": None, \"ER_mean_fold\": None, \"ER_std_fold\": None, \"cv_ER_fold\": None, \"min_ER_fold\": None,\n",
    "        \"decay_r_ER\": None, \"decay_penalty_ER\": None,\n",
    "        \"n_folds_PD\": None, \"PD_mean_fold\": None, \"PD_std_fold\": None, \"cv_PD_fold\": None, \"min_PD_fold\": None,\n",
    "        \"decay_r_PD\": None, \"decay_penalty_PD\": None,\n",
    "        \"decay_penalty\": 1.0,\n",
    "        \"EstabScore\": None,\n",
    "        \"flag_stab_data_missing\": True,\n",
    "    })\n",
    "    return rec\n",
    "\n",
    "if folds.is_empty() or (col_ER is None and col_PD is None):\n",
    "    print(\"WARN: stab_folds vac√≠o o sin columnas ER/PD reconocibles.\")\n",
    "    print(\"      Se usar√° EstabScore neutro y decay_penalty=1.0 para todos los keys.\")\n",
    "    for key in keys:\n",
    "        rows_metrics.append(_neutral_rec(key))\n",
    "else:\n",
    "    for key in keys:\n",
    "        rec = dict(key)\n",
    "\n",
    "        # ---- ER ----\n",
    "        if col_ER is not None:\n",
    "            vals_ER = _series_by_key(folds, key, col_ER, order_col)\n",
    "            v_ER = _drop_nan(vals_ER)\n",
    "            n_ER = len(v_ER)\n",
    "            m_ER, sd_ER, cv_ER, min_ER = _mean_std_cv(v_ER)\n",
    "            r_ER = _corr_idx(v_ER)\n",
    "            decay_ER = _decay_penalty_from_corr(r_ER, penalty_lambda)\n",
    "        else:\n",
    "            n_ER = None\n",
    "            m_ER = sd_ER = cv_ER = min_ER = r_ER = decay_ER = None\n",
    "\n",
    "        rec[\"n_folds_ER\"]       = n_ER\n",
    "        rec[\"ER_mean_fold\"]     = m_ER\n",
    "        rec[\"ER_std_fold\"]      = sd_ER\n",
    "        rec[\"cv_ER_fold\"]       = cv_ER\n",
    "        rec[\"min_ER_fold\"]      = min_ER\n",
    "        rec[\"decay_r_ER\"]       = r_ER\n",
    "        rec[\"decay_penalty_ER\"] = decay_ER\n",
    "\n",
    "        # ---- PD ----\n",
    "        if col_PD is not None:\n",
    "            vals_PD = _series_by_key(folds, key, col_PD, order_col)\n",
    "            v_PD = _drop_nan(vals_PD)\n",
    "            n_PD = len(v_PD)\n",
    "            m_PD, sd_PD, cv_PD, min_PD = _mean_std_cv(v_PD)\n",
    "            r_PD = _corr_idx(v_PD)\n",
    "            decay_PD = _decay_penalty_from_corr(r_PD, penalty_lambda)\n",
    "        else:\n",
    "            n_PD = None\n",
    "            m_PD = sd_PD = cv_PD = min_PD = r_PD = decay_PD = None\n",
    "\n",
    "        rec[\"n_folds_PD\"]       = n_PD\n",
    "        rec[\"PD_mean_fold\"]     = m_PD\n",
    "        rec[\"PD_std_fold\"]      = sd_PD\n",
    "        rec[\"cv_PD_fold\"]       = cv_PD\n",
    "        rec[\"min_PD_fold\"]      = min_PD\n",
    "        rec[\"decay_r_PD\"]       = r_PD\n",
    "        rec[\"decay_penalty_PD\"] = decay_PD\n",
    "\n",
    "        # ---- Decay combinado + EstabScore (CV->score * decay) ----\n",
    "        if decay_ER is not None and decay_PD is not None:\n",
    "            decay_comb = min(decay_ER, decay_PD)\n",
    "        elif decay_ER is not None:\n",
    "            decay_comb = decay_ER\n",
    "        elif decay_PD is not None:\n",
    "            decay_comb = decay_PD\n",
    "        else:\n",
    "            decay_comb = None\n",
    "\n",
    "        def _score_from_cv(cv: Optional[float]) -> Optional[float]:\n",
    "            if cv is None or not math.isfinite(cv):\n",
    "                return None\n",
    "            cv_c = max(0.0, min(float(cv), 10.0))\n",
    "            return 1.0 / (1.0 + cv_c)\n",
    "\n",
    "        score_ER = _score_from_cv(cv_ER)\n",
    "        score_PD = _score_from_cv(cv_PD)\n",
    "\n",
    "        if score_ER is None and score_PD is None:\n",
    "            base_score = None\n",
    "        elif score_ER is not None and score_PD is not None:\n",
    "            base_score = 0.5 * (score_ER + score_PD)\n",
    "        else:\n",
    "            base_score = score_ER if score_ER is not None else score_PD\n",
    "\n",
    "        estab_score = (base_score * decay_comb) if (base_score is not None and decay_comb is not None) else base_score\n",
    "\n",
    "        rec[\"decay_penalty\"] = float(decay_comb) if decay_comb is not None else 1.0\n",
    "        rec[\"EstabScore\"]    = estab_score\n",
    "\n",
    "        has_data_ER = (n_ER is not None and n_ER > 0)\n",
    "        has_data_PD = (n_PD is not None and n_PD > 0)\n",
    "        rec[\"flag_stab_data_missing\"] = not (has_data_ER or has_data_PD)\n",
    "\n",
    "        rows_metrics.append(rec)\n",
    "\n",
    "metrics_df = pl.DataFrame(rows_metrics)\n",
    "\n",
    "print(f\"[Celda 09] metrics_df construido (rows={metrics_df.height}, cols={len(metrics_df.columns)})\")\n",
    "print(\"[Celda 09] HEAD(5) de m√©tricas:\")\n",
    "print(metrics_df.head(5))\n",
    "\n",
    "# ======================== Verificaciones fuertes de folds/metrics ========================\n",
    "def _verify_num(df: pl.DataFrame, col: str, label: str):\n",
    "    if df.is_empty() or col not in df.columns:\n",
    "        print(f\"[Celda 09][VERIFY] {label}: columna ausente.\")\n",
    "        return\n",
    "    s = df.get_column(col).cast(pl.Float64, strict=False)\n",
    "    non_null = s.drop_nulls()\n",
    "    nn = non_null.len()\n",
    "    total = df.height\n",
    "    nu = int(non_null.n_unique()) if nn > 0 else 0\n",
    "    mn = float(non_null.min()) if nn > 0 else None\n",
    "    md = float(non_null.median()) if nn > 0 else None\n",
    "    mx = float(non_null.max()) if nn > 0 else None\n",
    "    print(f\"[Celda 09][VERIFY] {label}: non_null={nn}/{total}, n_unique={nu}, min={mn}, med={md}, max={mx}\")\n",
    "\n",
    "_verify_num(metrics_df, \"n_folds_ER\", \"n_folds_ER\")\n",
    "_verify_num(metrics_df, \"n_folds_PD\", \"n_folds_PD\")\n",
    "_verify_num(metrics_df, \"cv_ER_fold\", \"cv_ER_fold\")\n",
    "_verify_num(metrics_df, \"cv_PD_fold\", \"cv_PD_fold\")\n",
    "_verify_num(metrics_df, \"decay_penalty\", \"decay_penalty (metrics_df)\")\n",
    "_verify_num(metrics_df, \"EstabScore\", \"EstabScore (metrics_df)\")\n",
    "\n",
    "if \"flag_stab_data_missing\" in metrics_df.columns and metrics_df.height > 0:\n",
    "    miss_rate = 100.0 * metrics_df.filter(pl.col(\"flag_stab_data_missing\") == True).height / metrics_df.height\n",
    "    print(f\"[Celda 09][VERIFY] flag_stab_data_missing rate = {miss_rate:.2f}%\")\n",
    "\n",
    "# ======================== Join con stability_table ========================\n",
    "adv_join = (\n",
    "    adv\n",
    "    .join(metrics_df, on=key_cols, how=\"left\")\n",
    "    .with_columns([\n",
    "        pl.col(\"pvals_combined_ER\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"pvals_combined_PD\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"pvals_combined_p_event\").cast(pl.Float64, strict=False),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# clamp p-values <= 0\n",
    "adv_join = adv_join.with_columns([\n",
    "    pl.when(pl.col(\"pvals_combined_ER\").is_not_null() & (pl.col(\"pvals_combined_ER\") <= 0.0))\n",
    "      .then(pl.lit(P_MIN)).otherwise(pl.col(\"pvals_combined_ER\")).alias(\"pvals_combined_ER\"),\n",
    "    pl.when(pl.col(\"pvals_combined_PD\").is_not_null() & (pl.col(\"pvals_combined_PD\") <= 0.0))\n",
    "      .then(pl.lit(P_MIN)).otherwise(pl.col(\"pvals_combined_PD\")).alias(\"pvals_combined_PD\"),\n",
    "    pl.when(pl.col(\"pvals_combined_p_event\").is_not_null() & (pl.col(\"pvals_combined_p_event\") <= 0.0))\n",
    "      .then(pl.lit(P_MIN)).otherwise(pl.col(\"pvals_combined_p_event\")).alias(\"pvals_combined_p_event\"),\n",
    "])\n",
    "\n",
    "# ======================== Flags de significancia (DIAGN√ìSTICO) ========================\n",
    "adv_join = adv_join.with_columns([\n",
    "    (pl.col(\"pvals_combined_ER\").is_not_null() & (pl.col(\"pvals_combined_ER\") <= alpha_loose)).alias(\"signif_ER_loose\"),\n",
    "    (pl.col(\"pvals_combined_PD\").is_not_null() & (pl.col(\"pvals_combined_PD\") <= alpha_loose)).alias(\"signif_PD_loose\"),\n",
    "    (pl.col(\"pvals_combined_p_event\").is_not_null() & (pl.col(\"pvals_combined_p_event\") <= alpha_loose)).alias(\"signif_p_event_loose\"),\n",
    "]).with_columns(\n",
    "    (pl.col(\"signif_ER_loose\") | pl.col(\"signif_PD_loose\")).alias(\"signif_any_loose\")\n",
    ")\n",
    "\n",
    "# ======================== EstabScore_base + EstabScore_final ========================\n",
    "has_beta_trend = (\"beta_TR_trend\" in adv_join.columns) and (\"p_beta_TR_trend\" in adv_join.columns)\n",
    "has_beta_npm   = (\"beta_npm_total\" in adv_join.columns) and (\"p_beta_npm_total\" in adv_join.columns)\n",
    "\n",
    "if not (has_beta_trend or has_beta_npm):\n",
    "    print(\"[Celda 09] INFO: No se encontraron betas de decay en stability_table. EstabScore_final = CV+decay.\")\n",
    "\n",
    "cond_decay_claro_expr = pl.lit(False)\n",
    "if has_beta_trend:\n",
    "    cond_decay_claro_expr = cond_decay_claro_expr | ((pl.col(\"beta_TR_trend\") < 0) & (pl.col(\"p_beta_TR_trend\") < alpha_strict))\n",
    "if has_beta_npm:\n",
    "    cond_decay_claro_expr = cond_decay_claro_expr | ((pl.col(\"beta_npm_total\") < 0) & (pl.col(\"p_beta_npm_total\") < alpha_strict))\n",
    "\n",
    "neg_beta_expr = pl.lit(False)\n",
    "if has_beta_trend:\n",
    "    neg_beta_expr = neg_beta_expr | (pl.col(\"beta_TR_trend\") < 0)\n",
    "if has_beta_npm:\n",
    "    neg_beta_expr = neg_beta_expr | (pl.col(\"beta_npm_total\") < 0)\n",
    "\n",
    "adv_join = adv_join.with_columns(cond_decay_claro_expr.alias(\"decay_flag\"))\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.when(pl.col(\"EstabScore\").is_null()).then(pl.lit(0.75)).otherwise(pl.col(\"EstabScore\")).alias(\"EstabScore_base\")\n",
    ")\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.when(pl.col(\"decay_flag\"))\n",
    "      .then(pl.col(\"EstabScore_base\") * 0.5)\n",
    "      .when(neg_beta_expr)\n",
    "      .then(pl.col(\"EstabScore_base\") * 0.75)\n",
    "      .otherwise(pl.col(\"EstabScore_base\"))\n",
    "      .alias(\"EstabScore_final\")\n",
    ")\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    (pl.col(\"EstabScore_final\") >= float(min_estab_loose)).alias(\"keep_stab_base\")\n",
    ")\n",
    "\n",
    "# ======================== Integraci√≥n soft de OOS ========================\n",
    "if (\"flag_oos_ok\" not in adv_join.columns) and (\"oos_fail_reason\" not in adv_join.columns):\n",
    "    adv_join = adv_join.with_columns([\n",
    "        pl.lit(True).alias(\"flag_oos_ok\"),\n",
    "        pl.lit(\"no_oos_stats\", dtype=pl.Utf8).alias(\"oos_fail_reason\"),\n",
    "    ])\n",
    "else:\n",
    "    if \"flag_oos_ok\" not in adv_join.columns:\n",
    "        adv_join = adv_join.with_columns(pl.lit(True).alias(\"flag_oos_ok\"))\n",
    "    if \"oos_fail_reason\" not in adv_join.columns:\n",
    "        adv_join = adv_join.with_columns(pl.lit(None, dtype=pl.Utf8).alias(\"oos_fail_reason\"))\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.col(\"flag_oos_ok\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"flag_oos_ok\"),\n",
    "    pl.col(\"oos_fail_reason\").cast(pl.Utf8, strict=False).alias(\"oos_fail_reason\"),\n",
    ")\n",
    "\n",
    "soft_oos_ok_expr = (\n",
    "    (pl.col(\"flag_oos_ok\") == True)\n",
    "    | (pl.col(\"oos_fail_reason\").is_in([\"no_events\", \"no_oos_events\", \"no_oos_stats\"]))\n",
    "    | (pl.col(\"oos_fail_reason\").is_null())\n",
    ")\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.when(pl.col(\"EstabScore_final\").is_null())\n",
    "      .then(False)\n",
    "      .otherwise(\n",
    "          (pl.col(\"EstabScore_final\") >= float(min_estab_balanced))\n",
    "          & soft_oos_ok_expr\n",
    "      )\n",
    "      .alias(\"passed_stability_gate\")\n",
    ")\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.when(pl.col(\"passed_stability_gate\") == True)\n",
    "      .then(pl.lit(None, dtype=pl.Utf8))\n",
    "      .otherwise(\n",
    "          pl.when(pl.col(\"EstabScore_final\").is_null())\n",
    "            .then(pl.lit(\"no_estab_score\"))\n",
    "          .when(pl.col(\"EstabScore_final\") < float(min_estab_balanced))\n",
    "            .then(pl.lit(\"EstabScore_final_below_min_balanced\"))\n",
    "          .when(\n",
    "              (pl.col(\"flag_oos_ok\") == False)\n",
    "              & (~pl.col(\"oos_fail_reason\").is_in([\"no_events\", \"no_oos_events\", \"no_oos_stats\"]))\n",
    "          )\n",
    "            .then(pl.lit(\"oos_not_ok\"))\n",
    "          .otherwise(pl.lit(\"unknown\"))\n",
    "      )\n",
    "      .alias(\"stability_gate_reason\")\n",
    ")\n",
    "\n",
    "# ======================== PATCH min_TR_required ========================\n",
    "def _detect_min_tr_col(df: pl.DataFrame) -> Optional[str]:\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "    cands = [\n",
    "        \"min_TR_required\", \"min_tr_required\",\n",
    "        \"min_tr_after_cost\", \"min_TR_after_cost\",\n",
    "        \"min_tr_after_cost_trend\", \"min_TR_after_cost_trend\",\n",
    "        \"min_tr_after_cost_range\", \"min_TR_after_cost_range\",\n",
    "        \"min_tr_after_cost_trend_strict\",\n",
    "        \"min_tr_after_cost_trend_balanced\",\n",
    "        \"min_tr_after_cost_trend_loose\",\n",
    "        \"min_tr_after_cost_range_strict\",\n",
    "        \"min_tr_after_cost_range_balanced\",\n",
    "        \"min_tr_after_cost_range_loose\",\n",
    "        \"min_tr_trend\", \"min_tr_range\",\n",
    "    ]\n",
    "    col = _first_present(df, cands)\n",
    "    if col:\n",
    "        return col\n",
    "    for needle in (\"min_tr\", \"min-tr\", \"mintr\"):\n",
    "        col2 = _find_by_substring(df, needle)\n",
    "        if col2:\n",
    "            return col2\n",
    "    return None\n",
    "\n",
    "if ok_thr:\n",
    "    print(f\"[Celda 09][DEBUG] regime_thresholds.columns = {thr.columns}\")\n",
    "\n",
    "min_tr_col = _detect_min_tr_col(thr)\n",
    "\n",
    "if ok_thr and (min_tr_col is not None) and (\"symbol\" in thr.columns):\n",
    "    print(f\"[PATCH 09] regime_thresholds ‚Üí usando columna '{min_tr_col}' como origen de min_TR_required.\")\n",
    "    thr_sel = thr.select([\n",
    "        pl.col(\"symbol\"),\n",
    "        pl.col(min_tr_col).cast(pl.Float64, strict=False).alias(\"min_TR_required\")\n",
    "    ]).unique(subset=[\"symbol\"])\n",
    "    adv_join = adv_join.join(thr_sel, on=\"symbol\", how=\"left\")\n",
    "else:\n",
    "    if ok_thr:\n",
    "        print(\"[PATCH 09] regime_thresholds presente pero no contiene columna min_TR_required reconocible.\")\n",
    "    adv_join = adv_join.with_columns(pl.lit(None).alias(\"min_TR_required\"))\n",
    "\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.col(\"min_TR_required\")\n",
    "      .cast(pl.Float64, strict=False)\n",
    "      .fill_null(DEFAULT_MIN_TR_REQUIRED)\n",
    "      .alias(\"min_TR_required\")\n",
    ")\n",
    "\n",
    "print(f\"[PATCH 09] min_TR_required aplicado con fallback={DEFAULT_MIN_TR_REQUIRED}\")\n",
    "\n",
    "# ======================== Persistir decay_penalty global no-null ========================\n",
    "adv_join = adv_join.with_columns(\n",
    "    pl.col(\"decay_penalty\").cast(pl.Float64, strict=False).fill_null(1.0).clip(0.0, 1.0).alias(\"decay_penalty\")\n",
    ")\n",
    "\n",
    "# ======================== Verificaciones post-join ========================\n",
    "print(\"[Celda 09][VERIFY] Resumen EstabScore (post-join):\")\n",
    "\n",
    "def _verify_num(df: pl.DataFrame, col: str, label: str):\n",
    "    if df.is_empty() or col not in df.columns:\n",
    "        print(f\"[Celda 09][VERIFY] {label}: columna ausente.\")\n",
    "        return\n",
    "    s = df.get_column(col).cast(pl.Float64, strict=False)\n",
    "    non_null = s.drop_nulls()\n",
    "    nn = non_null.len()\n",
    "    total = df.height\n",
    "    nu = int(non_null.n_unique()) if nn > 0 else 0\n",
    "    mn = float(non_null.min()) if nn > 0 else None\n",
    "    md = float(non_null.median()) if nn > 0 else None\n",
    "    mx = float(non_null.max()) if nn > 0 else None\n",
    "    print(f\"[Celda 09][VERIFY] {label}: non_null={nn}/{total}, n_unique={nu}, min={mn}, med={md}, max={mx}\")\n",
    "\n",
    "_verify_num(adv_join, \"EstabScore\", \"EstabScore\")\n",
    "_verify_num(adv_join, \"EstabScore_base\", \"EstabScore_base\")\n",
    "_verify_num(adv_join, \"EstabScore_final\", \"EstabScore_final\")\n",
    "_verify_num(adv_join, \"decay_penalty\", \"decay_penalty (adv_join)\")\n",
    "_verify_num(adv_join, \"min_TR_required\", \"min_TR_required (adv_join)\")\n",
    "\n",
    "if \"EstabScore_final\" in adv_join.columns and adv_join.height > 0:\n",
    "    nu_final = int(adv_join.get_column(\"EstabScore_final\").drop_nulls().n_unique())\n",
    "    if nu_final <= 1:\n",
    "        print(\n",
    "            \"[Celda 09][WARN] EstabScore_final est√° plano. \"\n",
    "            \"Esto suele indicar: (a) pocos folds por key, \"\n",
    "            \"(b) folds evento-level sin fold_id √∫til, \"\n",
    "            \"o (c) EstabScore no pudo calcularse y cay√≥ a fallback 0.75.\"\n",
    "        )\n",
    "\n",
    "if \"passed_stability_gate\" in adv_join.columns and adv_join.height > 0:\n",
    "    n_ok = adv_join.filter(pl.col(\"passed_stability_gate\") == True).height\n",
    "    print(f\"[Celda 09][VERIFY] passed_stability_gate True = {n_ok}/{adv_join.height} ({100.0*n_ok/max(1,adv_join.height):.2f}%)\")\n",
    "\n",
    "# ======================== Persistencia ========================\n",
    "adv_join.write_parquet(str(adv_out_path))\n",
    "\n",
    "rows_adv = adv_join.height\n",
    "cols_adv = len(adv_join.columns)\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(adv_out_path)} (OK, rows={rows_adv}, cols={cols_adv})\")\n",
    "print(f\"[Celda 09] sanity: rows_in={r_tab} -> rows_out={rows_adv} (deber√≠a ser igual)\")\n",
    "\n",
    "# ------------------------ report_stats (diagn√≥stico NO gate) ------------------------\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "\n",
    "if rows_adv > 0 and \"signif_PD_loose\" in adv_join.columns:\n",
    "    n_with_significance = int(adv_join.filter(pl.col(\"signif_PD_loose\") == True).height)\n",
    "    n_after_stability = int(\n",
    "        adv_join.filter(\n",
    "            (pl.col(\"signif_PD_loose\") == True) & (pl.col(\"passed_stability_gate\") == True)\n",
    "        ).height\n",
    "    )\n",
    "else:\n",
    "    n_with_significance = 0\n",
    "    n_after_stability = 0\n",
    "\n",
    "GLOBAL_STATE[\"report_stats\"][\"c09\"] = {\n",
    "    \"universe_size\": int(rows_adv),\n",
    "    \"alpha_loose\": float(alpha_loose),\n",
    "    \"n_with_significance_PD\": int(n_with_significance),\n",
    "    \"n_after_stability\": int(n_after_stability),\n",
    "}\n",
    "\n",
    "print(f\"[Celda 09][Diag] PD significativo (p<=Œ±_loose={alpha_loose}) = {n_with_significance}\")\n",
    "print(f\"[Celda 09][Diag] PD significativo + passed_stability_gate=True = {n_after_stability}\")\n",
    "\n",
    "# ======================== Health check p-values ========================\n",
    "for col_name, label in [\n",
    "    (\"pvals_combined_ER\", \"ER\"),\n",
    "    (\"pvals_combined_PD\", \"PD\"),\n",
    "    (\"pvals_combined_p_event\", \"p_event\"),\n",
    "]:\n",
    "    if col_name not in adv_join.columns:\n",
    "        print(f\"[Celda 09] WARNING: falta {col_name}\")\n",
    "        continue\n",
    "\n",
    "    ser = adv_join.get_column(col_name).drop_nulls()\n",
    "    n_valid = ser.len()\n",
    "    print(f\"[Celda 09] p-values v√°lidos en {col_name} = {n_valid}/{rows_adv}\")\n",
    "\n",
    "    if n_valid > 0:\n",
    "        n_sig = int((ser <= alpha_loose).sum())\n",
    "        rate = 100.0 * n_sig / n_valid\n",
    "        print(f\"[Celda 09] {label}: p<=Œ±_loose ‚Üí {n_sig}/{n_valid} ({rate:.2f}%)\")\n",
    "\n",
    "# ======================== Resumen global de salud ========================\n",
    "print(\"---- [Celda 09][VERIFY] Resumen global de salud ----\")\n",
    "_verify_num(adv_join, \"EstabScore\", \"EstabScore\")\n",
    "_verify_num(adv_join, \"EstabScore_base\", \"EstabScore_base\")\n",
    "_verify_num(adv_join, \"EstabScore_final\", \"EstabScore_final\")\n",
    "_verify_num(adv_join, \"decay_penalty\", \"decay_penalty\")\n",
    "_verify_num(adv_join, \"min_TR_required\", \"min_TR_required\")\n",
    "\n",
    "print(\">>> Celda 09 v3.3.1 PRO :: OK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc38916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 09B v1.1 PRO :: Diagn√≥stico por s√≠mbolo/preset\n",
      "[Celda 09B] RUN_ID              = 20251218_190810\n",
      "[Celda 09B] OUT_STAB_DIR        = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\n",
      "[Celda 09B] stability_table     = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (exists=True)\n",
      "[Celda 09B] stability_table_adv = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet (exists=True)\n",
      "[Celda 09B] stab_folds          = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stab_folds.parquet (exists=True)\n",
      "[Celda 09B] SYMBOL_DEBUG = BTCUSD\n",
      "[Celda 09B] PRESET_DEBUG = ALL\n",
      "[Celda 09B] Filtrado - stability_table     ‚Üí 2 fila(s)\n",
      "[Celda 09B] Filtrado - stability_advanced  ‚Üí 2 fila(s)\n",
      "[Celda 09B] Filtrado - stab_folds          ‚Üí 266406 fila(s)\n",
      "\n",
      "---- RAW (stability_table) ----\n",
      "shape: (2, 11)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ months_IS ‚îÜ short_hist ‚îÜ ‚Ä¶ ‚îÜ pvals_comb ‚îÜ tvals_p_ev ‚îÜ pvals_comb ‚îÜ stat_score ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---       ‚îÜ ory_flag   ‚îÜ   ‚îÜ ined_PD    ‚îÜ ent        ‚îÜ ined_p_eve ‚îÜ _bucket    ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ u32       ‚îÜ ---        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ nt         ‚îÜ ---        ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ           ‚îÜ bool       ‚îÜ   ‚îÜ f64        ‚îÜ f64        ‚îÜ ---        ‚îÜ f64        ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ f64        ‚îÜ            ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ 49        ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.983589   ‚îÜ -108.64002 ‚îÜ 0.0        ‚îÜ 0.001096   ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ 2          ‚îÜ            ‚îÜ            ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ TREND  ‚îÜ 49        ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.98616    ‚îÜ -99.365112 ‚îÜ 0.0        ‚îÜ 0.00092    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "---- ADV (stability_table_advanced) ----\n",
      "shape: (2, 25)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ pvals_comb ‚îÜ pvals_comb ‚îÜ ‚Ä¶ ‚îÜ signif_ER_ ‚îÜ signif_PD_ ‚îÜ signif_p_e ‚îÜ signif_an ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ined_ER    ‚îÜ ined_PD    ‚îÜ   ‚îÜ loose      ‚îÜ loose      ‚îÜ vent_loose ‚îÜ y_loose   ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ ---        ‚îÜ ---        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ ---        ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ f64        ‚îÜ f64        ‚îÜ   ‚îÜ bool       ‚îÜ bool       ‚îÜ bool       ‚îÜ bool      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ 0.983548   ‚îÜ 0.983589   ‚îÜ ‚Ä¶ ‚îÜ false      ‚îÜ false      ‚îÜ true       ‚îÜ false     ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ TREND  ‚îÜ 0.986219   ‚îÜ 0.98616    ‚îÜ ‚Ä¶ ‚îÜ false      ‚îÜ false      ‚îÜ true       ‚îÜ false     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[Celda 09B] Resumen compacto por preset:\n",
      "shape: (2, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ preset ‚îÜ pvals_combined_ ‚îÜ pvals_combined_ ‚îÜ pvals_combined_ ‚îÜ EstabScore_fina ‚îÜ passed_stabilit ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ER              ‚îÜ PD              ‚îÜ p_event         ‚îÜ l               ‚îÜ y_gate          ‚îÇ\n",
      "‚îÇ str    ‚îÜ ---             ‚îÜ ---             ‚îÜ ---             ‚îÜ ---             ‚îÜ ---             ‚îÇ\n",
      "‚îÇ        ‚îÜ f64             ‚îÜ f64             ‚îÜ f64             ‚îÜ f64             ‚îÜ bool            ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ RANGE  ‚îÜ 0.983548        ‚îÜ 0.983589        ‚îÜ 1.0000e-300     ‚îÜ 0.645267        ‚îÜ true            ‚îÇ\n",
      "‚îÇ TREND  ‚îÜ 0.986219        ‚îÜ 0.98616         ‚îÜ 1.0000e-300     ‚îÜ 0.584498        ‚îÜ true            ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "---- FOLDS (stab_folds) ‚Äî evento-level ----\n",
      "[Celda 09B] columnas= ['symbol', 'preset', 'ts', 'ret', 'success', 'segment', 'ts_cut_ISOOS', 'split_q_ISOOS']\n",
      "\n",
      "[Celda 09B] Distribuci√≥n por preset/segment (proxies ret/success):\n",
      "shape: (4, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ preset ‚îÜ segment ‚îÜ n_events ‚îÜ mean_ret  ‚îÜ std_ret  ‚îÜ mean_success ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---     ‚îÜ ---      ‚îÜ ---       ‚îÜ ---      ‚îÜ ---          ‚îÇ\n",
      "‚îÇ str    ‚îÜ str     ‚îÜ u32      ‚îÜ f64       ‚îÜ f64      ‚îÜ f64          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ RANGE  ‚îÜ IS      ‚îÜ 91097    ‚îÜ 0.002916  ‚îÜ 0.999986 ‚îÜ 0.501454     ‚îÇ\n",
      "‚îÇ RANGE  ‚îÜ OOS     ‚îÜ 39041    ‚îÜ 0.000538  ‚îÜ 1.000013 ‚îÜ 0.500269     ‚îÇ\n",
      "‚îÇ TREND  ‚îÜ IS      ‚îÜ 95388    ‚îÜ 0.002443  ‚îÜ 0.999984 ‚îÜ 0.501227     ‚îÇ\n",
      "‚îÇ TREND  ‚îÜ OOS     ‚îÜ 40880    ‚îÜ -0.014677 ‚îÜ 0.999905 ‚îÜ 0.492661     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[Celda 09B] Primeros 20 eventos ordenados por ts:\n",
      "shape: (20, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ segment ‚îÜ ts                  ‚îÜ ret  ‚îÜ success ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---     ‚îÜ ---                 ‚îÜ ---  ‚îÜ ---     ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ str     ‚îÜ datetime[Œºs]        ‚îÜ f64  ‚îÜ i8      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-11-20 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-11-21 00:00:00 ‚îÜ -1.0 ‚îÜ 0       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-11-22 00:00:00 ‚îÜ -1.0 ‚îÜ 0       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-11-23 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-11-24 00:00:00 ‚îÜ -1.0 ‚îÜ 0       ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶                   ‚îÜ ‚Ä¶    ‚îÜ ‚Ä¶       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-12-05 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-12-06 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-12-07 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-12-08 00:00:00 ‚îÜ 1.0  ‚îÜ 1       ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ IS      ‚îÜ 2021-12-09 00:00:00 ‚îÜ -1.0 ‚îÜ 0       ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      ">>> Celda 09B v1.1 PRO :: FIN DIAGN√ìSTICO\n"
     ]
    }
   ],
   "source": [
    "# Celda 09B v1.1 PRO ‚Äî Diagn√≥stico p-values ER/PD/p_event + estabilidad por s√≠mbolo/preset\n",
    "# NOTA: celda QA complementaria. Solo inspecciona outputs de estabilidad.\n",
    "# -------------------------------------------------------------------------\n",
    "# OBJETIVO:\n",
    "#   Inspeccionar un s√≠mbolo (BTCUSD por defecto) en:\n",
    "#     - stability_table.parquet\n",
    "#     - stability_table_advanced.parquet\n",
    "#     - stab_folds.parquet\n",
    "#   para auditar:\n",
    "#     - pvals_combined_ER / PD / p_event\n",
    "#     - tvals_* si existen\n",
    "#     - EstabScore_base / final\n",
    "#     - OOS (flag_oos_ok, oos_fail_reason)\n",
    "#     - distribuci√≥n de eventos en folds (ret/success) por preset/segment\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 09B v1.1 PRO :: Diagn√≥stico por s√≠mbolo/preset\")\n",
    "\n",
    "# ==================== Configuraci√≥n r√°pida ====================\n",
    "SYMBOL_DEBUG = \"BTCUSD\"\n",
    "PRESET_DEBUG = None  # \"RANGE\" / \"TREND\" o None para ver ambos\n",
    "\n",
    "# ==================== Validaciones b√°sicas ====================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas iniciales.\")\n",
    "\n",
    "if \"paths\" not in GLOBAL_STATE or \"stability\" not in GLOBAL_STATE[\"paths\"]:\n",
    "    raise RuntimeError(\"GLOBAL_STATE['paths']['stability'] no est√° definido. Revisa Celda 00/04.\")\n",
    "\n",
    "OUT_STAB_DIR = Path(GLOBAL_STATE[\"paths\"][\"stability\"]).resolve()\n",
    "\n",
    "stab_path   = (OUT_STAB_DIR / \"stability_table.parquet\").resolve()\n",
    "adv_path    = (OUT_STAB_DIR / \"stability_table_advanced.parquet\").resolve()\n",
    "folds_path  = (OUT_STAB_DIR / \"stab_folds.parquet\").resolve()\n",
    "\n",
    "print(f\"[Celda 09B] RUN_ID              = {GLOBAL_STATE.get('run_id')}\")\n",
    "print(f\"[Celda 09B] OUT_STAB_DIR        = {OUT_STAB_DIR}\")\n",
    "print(f\"[Celda 09B] stability_table     = {stab_path} (exists={stab_path.exists()})\")\n",
    "print(f\"[Celda 09B] stability_table_adv = {adv_path} (exists={adv_path.exists()})\")\n",
    "print(f\"[Celda 09B] stab_folds          = {folds_path} (exists={folds_path.exists()})\")\n",
    "\n",
    "if not stab_path.exists():\n",
    "    raise RuntimeError(f\"stability_table.parquet no encontrado en {stab_path}\")\n",
    "if not adv_path.exists():\n",
    "    raise RuntimeError(f\"stability_table_advanced.parquet no encontrado en {adv_path}\")\n",
    "\n",
    "# ==================== Carga de dataframes ====================\n",
    "stab_df  = pl.read_parquet(stab_path)\n",
    "adv_df   = pl.read_parquet(adv_path)\n",
    "folds_df = pl.read_parquet(folds_path) if folds_path.exists() else pl.DataFrame()\n",
    "\n",
    "def _norm(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    out = df\n",
    "    if \"symbol\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    "        )\n",
    "    if \"preset\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"preset\")\n",
    "        )\n",
    "    return out\n",
    "\n",
    "stab_df  = _norm(stab_df)\n",
    "adv_df   = _norm(adv_df)\n",
    "folds_df = _norm(folds_df)\n",
    "\n",
    "SYMBOL_DEBUG = SYMBOL_DEBUG.upper().strip()\n",
    "PRESET_DEBUG = PRESET_DEBUG.upper().strip() if isinstance(PRESET_DEBUG, str) else None\n",
    "\n",
    "print(f\"[Celda 09B] SYMBOL_DEBUG = {SYMBOL_DEBUG}\")\n",
    "print(f\"[Celda 09B] PRESET_DEBUG = {PRESET_DEBUG if PRESET_DEBUG else 'ALL'}\")\n",
    "\n",
    "# ==================== Filtrar por s√≠mbolo/preset ====================\n",
    "def _filter_sym_preset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    q = df.filter(pl.col(\"symbol\") == SYMBOL_DEBUG)\n",
    "    if PRESET_DEBUG and \"preset\" in q.columns:\n",
    "        q = q.filter(pl.col(\"preset\") == PRESET_DEBUG)\n",
    "    return q\n",
    "\n",
    "stab_sym  = _filter_sym_preset(stab_df)\n",
    "adv_sym   = _filter_sym_preset(adv_df)\n",
    "folds_sym = _filter_sym_preset(folds_df)\n",
    "\n",
    "print(f\"[Celda 09B] Filtrado - stability_table     ‚Üí {stab_sym.height} fila(s)\")\n",
    "print(f\"[Celda 09B] Filtrado - stability_advanced  ‚Üí {adv_sym.height} fila(s)\")\n",
    "print(f\"[Celda 09B] Filtrado - stab_folds          ‚Üí {folds_sym.height} fila(s)\")\n",
    "\n",
    "# ==================== RAW ====================\n",
    "if not stab_sym.is_empty():\n",
    "    print(\"\\n---- RAW (stability_table) ----\")\n",
    "    cols_hint = [\n",
    "        \"symbol\", \"preset\",\n",
    "        \"months_IS\", \"short_history_flag\",\n",
    "        \"tvals_ER\", \"pvals_combined_ER\",\n",
    "        \"tvals_PD\", \"pvals_combined_PD\",\n",
    "        \"tvals_p_event\", \"pvals_combined_p_event\",\n",
    "        \"stat_score_bucket\"\n",
    "    ]\n",
    "    cols_hint = [c for c in cols_hint if c in stab_sym.columns]\n",
    "    print(stab_sym.select(cols_hint) if cols_hint else stab_sym)\n",
    "\n",
    "# ==================== ADV ====================\n",
    "if not adv_sym.is_empty():\n",
    "    print(\"\\n---- ADV (stability_table_advanced) ----\")\n",
    "    cols_adv = [\n",
    "        \"symbol\", \"preset\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "        \"tvals_ER\", \"tvals_PD\", \"tvals_p_event\",\n",
    "        \"EstabScore\", \"EstabScore_base\", \"EstabScore_final\",\n",
    "        \"decay_penalty_ER\", \"decay_penalty_PD\", \"decay_penalty\",\n",
    "        \"decay_r_ER\", \"decay_r_PD\",\n",
    "        \"keep_stab_base\", \"passed_stability_gate\", \"stability_gate_reason\",\n",
    "        \"flag_oos_ok\", \"oos_fail_reason\",\n",
    "        \"signif_ER_loose\", \"signif_PD_loose\", \"signif_p_event_loose\", \"signif_any_loose\",\n",
    "    ]\n",
    "    cols_adv = [c for c in cols_adv if c in adv_sym.columns]\n",
    "    print(adv_sym.select(cols_adv))\n",
    "\n",
    "    print(\"\\n[Celda 09B] Resumen compacto por preset:\")\n",
    "    if \"preset\" in adv_sym.columns:\n",
    "        compact = adv_sym.select([\n",
    "            \"preset\",\n",
    "            \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "            \"EstabScore_final\",\n",
    "            \"passed_stability_gate\"\n",
    "        ])\n",
    "        print(compact)\n",
    "    else:\n",
    "        print(adv_sym.select([\n",
    "            \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "            \"EstabScore_final\",\n",
    "            \"passed_stability_gate\"\n",
    "        ]))\n",
    "\n",
    "# ==================== FOLDS (event-level) ====================\n",
    "if not folds_sym.is_empty():\n",
    "    print(\"\\n---- FOLDS (stab_folds) ‚Äî evento-level ----\")\n",
    "    print(f\"[Celda 09B] columnas= {folds_sym.columns}\")\n",
    "\n",
    "    # Vista de distribuci√≥n por preset/segment usando proxies reales del archivo:\n",
    "    #   - ret: retorno del evento\n",
    "    #   - success: indicador binario (si existe) como proxy de PD por evento\n",
    "    agg_exprs = [pl.len().alias(\"n_events\")]\n",
    "    if \"ret\" in folds_sym.columns:\n",
    "        agg_exprs += [\n",
    "            pl.col(\"ret\").mean().alias(\"mean_ret\"),\n",
    "            pl.col(\"ret\").std().alias(\"std_ret\"),\n",
    "        ]\n",
    "    if \"success\" in folds_sym.columns:\n",
    "        agg_exprs += [\n",
    "            pl.col(\"success\").mean().alias(\"mean_success\"),\n",
    "        ]\n",
    "\n",
    "    group_cols = []\n",
    "    if \"preset\" in folds_sym.columns:\n",
    "        group_cols.append(\"preset\")\n",
    "    if \"segment\" in folds_sym.columns:\n",
    "        group_cols.append(\"segment\")\n",
    "\n",
    "    if group_cols:\n",
    "        print(\"\\n[Celda 09B] Distribuci√≥n por preset/segment (proxies ret/success):\")\n",
    "        print(\n",
    "            folds_sym\n",
    "            .group_by(group_cols)\n",
    "            .agg(agg_exprs)\n",
    "            .sort(group_cols)\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[Celda 09B] Resumen global de eventos:\")\n",
    "        print(folds_sym.select([e.meta.output_name() for e in agg_exprs]))\n",
    "\n",
    "    # Muestra cronol√≥gica concisa\n",
    "    show_cols = [\"symbol\"]\n",
    "    for c in [\"preset\", \"segment\", \"ts\", \"ret\", \"success\"]:\n",
    "        if c in folds_sym.columns and c not in show_cols:\n",
    "            show_cols.append(c)\n",
    "\n",
    "    print(\"\\n[Celda 09B] Primeros 20 eventos ordenados por ts:\")\n",
    "    if \"ts\" in folds_sym.columns:\n",
    "        print(folds_sym.sort(\"ts\").select(show_cols).head(20))\n",
    "    else:\n",
    "        print(folds_sym.select(show_cols).head(20))\n",
    "else:\n",
    "    print(\"\\n[Celda 09B] No hay rows en stab_folds para el s√≠mbolo/preset seleccionado.\")\n",
    "\n",
    "print(\"\\n>>> Celda 09B v1.1 PRO :: FIN DIAGN√ìSTICO\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2cd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 09C v1.2 PRO :: QA ret-proxy vs clipping + frecuencia vs edge\n",
      "[09C] RUN_ID = 20251218_190810\n",
      "[09C] stab  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet\n",
      "[09C] adv   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet\n",
      "[09C] folds = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stab_folds.parquet\n",
      "[09C] alpha_loose global = 0.2\n",
      "[09C] alpha_loose QA usado para flags operativos = 0.2\n",
      "\n",
      "[09C] Conteo por modo de ret:\n",
      "shape: (2, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ ret_mode_label ‚îÜ n_pairs ‚îÇ\n",
      "‚îÇ ---            ‚îÜ ---     ‚îÇ\n",
      "‚îÇ str            ‚îÜ u32     ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ OK_CONTINUOUS  ‚îÜ 154     ‚îÇ\n",
      "‚îÇ POSSIBLE_CLIP  ‚îÜ 12      ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[09C] Resumen por (symbol,preset) con flags operativos y score de clipping:\n",
      "shape: (50, 23)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ n_events ‚îÜ mean_ret  ‚îÜ ‚Ä¶ ‚îÜ pvals_combi ‚îÜ edge_suppor ‚îÜ flag_freq_ ‚îÜ stat_score ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ned_p_event ‚îÜ t_loose     ‚îÜ only       ‚îÜ _bucket    ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ u32      ‚îÜ f64       ‚îÜ   ‚îÜ ---         ‚îÜ ---         ‚îÜ ---        ‚îÜ ---        ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ          ‚îÜ           ‚îÜ   ‚îÜ f64         ‚îÜ bool        ‚îÜ bool       ‚îÜ f64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ 130138   ‚îÜ 0.002203  ‚îÜ ‚Ä¶ ‚îÜ 1.0000e-300 ‚îÜ false       ‚îÜ true       ‚îÜ 0.001096   ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ TREND  ‚îÜ 136268   ‚îÜ -0.002693 ‚îÜ ‚Ä¶ ‚îÜ 1.0000e-300 ‚îÜ false       ‚îÜ true       ‚îÜ 0.00092    ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ TREND  ‚îÜ 113697   ‚îÜ 0.01587   ‚îÜ ‚Ä¶ ‚îÜ 4.7652e-81  ‚îÜ false       ‚îÜ true       ‚îÜ 0.088542   ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ RANGE  ‚îÜ 110052   ‚îÜ -0.00011  ‚îÜ ‚Ä¶ ‚îÜ 2.9164e-17  ‚îÜ false       ‚îÜ true       ‚îÜ 0.087075   ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ TREND  ‚îÜ 135763   ‚îÜ 0.003027  ‚îÜ ‚Ä¶ ‚îÜ 1.0000e-300 ‚îÜ false       ‚îÜ true       ‚îÜ 0.001022   ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÇ\n",
      "‚îÇ BCHUSD ‚îÜ RANGE  ‚îÜ 158868   ‚îÜ -0.00002  ‚îÜ ‚Ä¶ ‚îÜ 1.0000e-300 ‚îÜ false       ‚îÜ true       ‚îÜ 0.000698   ‚îÇ\n",
      "‚îÇ CADCHF ‚îÜ TREND  ‚îÜ 116585   ‚îÜ -0.000003 ‚îÜ ‚Ä¶ ‚îÜ 3.7116e-123 ‚îÜ false       ‚îÜ true       ‚îÜ 0.086418   ‚îÇ\n",
      "‚îÇ VECUSD ‚îÜ RANGE  ‚îÜ 102562   ‚îÜ 1.2105e-7 ‚îÜ ‚Ä¶ ‚îÜ 1.0000e-300 ‚îÜ false       ‚îÜ true       ‚îÜ 0.002135   ‚îÇ\n",
      "‚îÇ AUDNZD ‚îÜ RANGE  ‚îÜ 115814   ‚îÜ 0.000001  ‚îÜ ‚Ä¶ ‚îÜ 1.9860e-101 ‚îÜ false       ‚îÜ true       ‚îÜ 0.086663   ‚îÇ\n",
      "‚îÇ EURGBP ‚îÜ TREND  ‚îÜ 115022   ‚îÜ 0.000002  ‚îÜ ‚Ä¶ ‚îÜ 2.4790e-81  ‚îÜ false       ‚îÜ true       ‚îÜ 0.085985   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[09C] Distribuci√≥n por (preset,segment) ‚Äî abs1_rate/zero_rate y proxies evento-level:\n",
      "shape: (4, 8)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ preset ‚îÜ segment ‚îÜ n_events ‚îÜ mean_ret ‚îÜ std_ret  ‚îÜ mean_success ‚îÜ abs1_rate ‚îÜ zero_rate ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---     ‚îÜ ---      ‚îÜ ---      ‚îÜ ---      ‚îÜ ---          ‚îÜ ---       ‚îÜ ---       ‚îÇ\n",
      "‚îÇ str    ‚îÜ str     ‚îÜ u32      ‚îÜ f64      ‚îÜ f64      ‚îÜ f64          ‚îÜ f64       ‚îÜ f64       ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ RANGE  ‚îÜ IS      ‚îÜ 6414553  ‚îÜ 0.000272 ‚îÜ 0.240934 ‚îÜ 0.50347      ‚îÜ 0.041065  ‚îÜ 0.0       ‚îÇ\n",
      "‚îÇ RANGE  ‚îÜ OOS     ‚îÜ 2749049  ‚îÜ 0.000815 ‚îÜ 0.283051 ‚îÜ 0.503841     ‚îÜ 0.065094  ‚îÜ 0.0       ‚îÇ\n",
      "‚îÇ TREND  ‚îÜ IS      ‚îÜ 6383962  ‚îÜ 0.000431 ‚îÜ 0.258221 ‚îÜ 0.504431     ‚îÜ 0.049042  ‚îÜ 0.0       ‚îÇ\n",
      "‚îÇ TREND  ‚îÜ OOS     ‚îÜ 2735943  ‚îÜ 0.001156 ‚îÜ 0.295377 ‚îÜ 0.502334     ‚îÜ 0.072476  ‚îÜ 0.0       ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[09C] Top 'freq-only' (p_event fuerte sin soporte ER/PD) priorizado por clip_risk_score:\n",
      "shape: (50, 14)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ ret_mode_l ‚îÜ ret_sign_p ‚îÜ ‚Ä¶ ‚îÜ pvals_comb ‚îÜ pvals_comb ‚îÜ pvals_comb ‚îÜ stat_scor ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ abel       ‚îÜ roxy_flag  ‚îÜ   ‚îÜ ined_ER    ‚îÜ ined_PD    ‚îÜ ined_p_eve ‚îÜ e_bucket  ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ ---        ‚îÜ ---        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ nt         ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ str        ‚îÜ bool       ‚îÜ   ‚îÜ f64        ‚îÜ f64        ‚îÜ ---        ‚îÜ f64       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ f64        ‚îÜ           ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ POSSIBLE_C ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.983548   ‚îÜ 0.983589   ‚îÜ 1.0000e-30 ‚îÜ 0.001096  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ LIP        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 0          ‚îÜ           ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ TREND  ‚îÜ POSSIBLE_C ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.986219   ‚îÜ 0.98616    ‚îÜ 1.0000e-30 ‚îÜ 0.00092   ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ LIP        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 0          ‚îÜ           ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ TREND  ‚îÜ POSSIBLE_C ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.956725   ‚îÜ 0.959611   ‚îÜ 4.7652e-81 ‚îÜ 0.088542  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ LIP        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ           ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ RANGE  ‚îÜ POSSIBLE_C ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.980466   ‚îÜ 0.978837   ‚îÜ 2.9164e-17 ‚îÜ 0.087075  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ LIP        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ           ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ TREND  ‚îÜ POSSIBLE_C ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.988363   ‚îÜ 0.981004   ‚îÜ 1.0000e-30 ‚îÜ 0.001022  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ LIP        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 0          ‚îÜ           ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶         ‚îÇ\n",
      "‚îÇ BCHUSD ‚îÜ RANGE  ‚îÜ OK_CONTINU ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.998794   ‚îÜ 0.980318   ‚îÜ 1.0000e-30 ‚îÜ 0.000698  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ OUS        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 0          ‚îÜ           ‚îÇ\n",
      "‚îÇ CADCHF ‚îÜ TREND  ‚îÜ OK_CONTINU ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.999984   ‚îÜ 0.978986   ‚îÜ 3.7116e-12 ‚îÜ 0.086418  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ OUS        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 3          ‚îÜ           ‚îÇ\n",
      "‚îÇ VECUSD ‚îÜ RANGE  ‚îÜ OK_CONTINU ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.999999   ‚îÜ 0.937511   ‚îÜ 1.0000e-30 ‚îÜ 0.002135  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ OUS        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 0          ‚îÜ           ‚îÇ\n",
      "‚îÇ AUDNZD ‚îÜ RANGE  ‚îÜ OK_CONTINU ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.999997   ‚îÜ 0.971751   ‚îÜ 1.9860e-10 ‚îÜ 0.086663  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ OUS        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ 1          ‚îÜ           ‚îÇ\n",
      "‚îÇ EURGBP ‚îÜ TREND  ‚îÜ OK_CONTINU ‚îÜ false      ‚îÜ ‚Ä¶ ‚îÜ 0.999991   ‚îÜ 0.991872   ‚îÜ 2.4790e-81 ‚îÜ 0.085985  ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ OUS        ‚îÜ            ‚îÜ   ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ           ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[09C] Top candidatos a revisar upstream por posible clipping real (NO proxy):\n",
      "shape: (12, 14)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ n_events ‚îÜ abs1_rate ‚îÜ ‚Ä¶ ‚îÜ pvals_combi ‚îÜ pvals_combi ‚îÜ flag_freq_ ‚îÜ stat_score ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---      ‚îÜ ---       ‚îÜ   ‚îÜ ned_PD      ‚îÜ ned_p_event ‚îÜ only       ‚îÜ _bucket    ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ u32      ‚îÜ f64       ‚îÜ   ‚îÜ ---         ‚îÜ ---         ‚îÜ ---        ‚îÜ ---        ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ          ‚îÜ           ‚îÜ   ‚îÜ f64         ‚îÜ f64         ‚îÜ bool       ‚îÜ f64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ RANGE  ‚îÜ 130138   ‚îÜ 0.999969  ‚îÜ ‚Ä¶ ‚îÜ 0.983589    ‚îÜ 1.0000e-300 ‚îÜ true       ‚îÜ 0.001096   ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ TREND  ‚îÜ 136268   ‚îÜ 0.999949  ‚îÜ ‚Ä¶ ‚îÜ 0.98616     ‚îÜ 1.0000e-300 ‚îÜ true       ‚îÜ 0.00092    ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ TREND  ‚îÜ 113697   ‚îÜ 0.871237  ‚îÜ ‚Ä¶ ‚îÜ 0.959611    ‚îÜ 4.7652e-81  ‚îÜ true       ‚îÜ 0.088542   ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ RANGE  ‚îÜ 110052   ‚îÜ 0.835414  ‚îÜ ‚Ä¶ ‚îÜ 0.978837    ‚îÜ 2.9164e-17  ‚îÜ true       ‚îÜ 0.087075   ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ TREND  ‚îÜ 135763   ‚îÜ 0.702202  ‚îÜ ‚Ä¶ ‚îÜ 0.981004    ‚îÜ 1.0000e-300 ‚îÜ true       ‚îÜ 0.001022   ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶ ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÇ\n",
      "‚îÇ XAUUSD ‚îÜ RANGE  ‚îÜ 109761   ‚îÜ 0.556937  ‚îÜ ‚Ä¶ ‚îÜ 0.993318    ‚îÜ 2.9101e-14  ‚îÜ true       ‚îÜ 0.086322   ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ TREND  ‚îÜ 162969   ‚îÜ 0.348852  ‚îÜ ‚Ä¶ ‚îÜ 0.923116    ‚îÜ 1.0000e-300 ‚îÜ true       ‚îÜ 0.003205   ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ TREND  ‚îÜ 114094   ‚îÜ 0.331017  ‚îÜ ‚Ä¶ ‚îÜ 0.948044    ‚îÜ 6.1225e-91  ‚îÜ true       ‚îÜ 0.088647   ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ RANGE  ‚îÜ 157771   ‚îÜ 0.264288  ‚îÜ ‚Ä¶ ‚îÜ 0.895473    ‚îÜ 1.0000e-300 ‚îÜ true       ‚îÜ 0.005209   ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ RANGE  ‚îÜ 109026   ‚îÜ 0.263148  ‚îÜ ‚Ä¶ ‚îÜ 0.996082    ‚îÜ 4.7242e-8   ‚îÜ true       ‚îÜ 0.085879   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      ">>> Celda 09C v1.2 PRO :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 09C v1.2 PRO ‚Äî QA ret-proxy vs clipping + frecuencia vs edge =====================\n",
    "# OBJETIVO:\n",
    "#   - Separar dos fen√≥menos:\n",
    "#       (A) ret como PROXY binario/signo (¬±1)  -> abs1_rate alto NO es clipping.\n",
    "#       (B) ret continuo con saturaci√≥n real  -> abs1_rate alto S√ç sugiere clipping upstream.\n",
    "#   - Cruzar contra pvals combinados (ER/PD/p_event) para detectar \"freq-only\":\n",
    "#       p_event significativo pero ER/PD sin soporte.\n",
    "#   - Producir un \"clip_risk_score\" operativo para priorizar auditor√≠a upstream.\n",
    "#   - Output compacto por (symbol,preset) y por (preset,segment).\n",
    "#\n",
    "# NOTA:\n",
    "#   Celda QA. No modifica artefactos del pipeline principal.\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 09C v1.2 PRO :: QA ret-proxy vs clipping + frecuencia vs edge\")\n",
    "\n",
    "# -------------------- Validaciones m√≠nimas --------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta celdas iniciales.\")\n",
    "\n",
    "paths = (GLOBAL_STATE.get(\"paths\", {}) or {})\n",
    "if \"stability\" not in paths:\n",
    "    raise RuntimeError(\"Falta GLOBAL_STATE['paths']['stability'].\")\n",
    "\n",
    "OUT_STAB_DIR = Path(paths[\"stability\"]).resolve()\n",
    "stab_path    = OUT_STAB_DIR / \"stability_table.parquet\"\n",
    "adv_path     = OUT_STAB_DIR / \"stability_table_advanced.parquet\"\n",
    "folds_path   = OUT_STAB_DIR / \"stab_folds.parquet\"\n",
    "\n",
    "if not stab_path.exists():\n",
    "    raise RuntimeError(f\"No existe stability_table.parquet: {stab_path}\")\n",
    "if not adv_path.exists():\n",
    "    raise RuntimeError(f\"No existe stability_table_advanced.parquet: {adv_path}\")\n",
    "if not folds_path.exists():\n",
    "    raise RuntimeError(f\"No existe stab_folds.parquet: {folds_path}\")\n",
    "\n",
    "print(f\"[09C] RUN_ID = {GLOBAL_STATE.get('run_id')}\")\n",
    "print(f\"[09C] stab  = {stab_path}\")\n",
    "print(f\"[09C] adv   = {adv_path}\")\n",
    "print(f\"[09C] folds = {folds_path}\")\n",
    "\n",
    "# -------------------- Cargas --------------------\n",
    "stab  = pl.read_parquet(stab_path)\n",
    "adv   = pl.read_parquet(adv_path)\n",
    "folds = pl.read_parquet(folds_path)\n",
    "\n",
    "# Normalizaci√≥n suave\n",
    "def _norm(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    out = df\n",
    "    if \"symbol\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    "        )\n",
    "    if \"preset\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"preset\")\n",
    "        )\n",
    "    return out\n",
    "\n",
    "stab  = _norm(stab)\n",
    "adv   = _norm(adv)\n",
    "folds = _norm(folds)\n",
    "\n",
    "# -------------------- Checks de columnas --------------------\n",
    "req_folds = [\"symbol\", \"preset\", \"ret\", \"success\"]\n",
    "missing_f = [c for c in req_folds if c not in folds.columns]\n",
    "if missing_f:\n",
    "    raise RuntimeError(f\"[09C] stab_folds no contiene columnas requeridas: {missing_f}\")\n",
    "\n",
    "req_p = [\"symbol\", \"preset\", \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\"]\n",
    "missing_p = [c for c in req_p if c not in adv.columns]\n",
    "if missing_p:\n",
    "    raise RuntimeError(f\"[09C] stability_table_advanced no contiene columnas requeridas: {missing_p}\")\n",
    "\n",
    "# -------------------- Par√°metros --------------------\n",
    "# tolerancia para detectar valores ¬±1 exactos\n",
    "ABS1_THR = 0.999999\n",
    "\n",
    "# umbral de alerta para posible clipping real (solo si NO es proxy)\n",
    "CLIP_RATE_WARN_THR = 0.25\n",
    "\n",
    "# alpha loose desde config con override QA opcional\n",
    "cfg = (GLOBAL_STATE.get(\"config\", {}) or {})\n",
    "stats_sec = (cfg.get(\"stats\", {}) or {})\n",
    "\n",
    "alpha_loose_global = float(stats_sec.get(\"alpha_loose\", 0.2))\n",
    "alpha_loose_QA     = float(stats_sec.get(\"alpha_loose_QA\", alpha_loose_global))\n",
    "\n",
    "alpha_loose = alpha_loose_QA\n",
    "\n",
    "print(f\"[09C] alpha_loose global = {alpha_loose_global}\")\n",
    "print(f\"[09C] alpha_loose QA usado para flags operativos = {alpha_loose}\")\n",
    "\n",
    "# -------------------- M√©tricas evento-level base --------------------\n",
    "folds = folds.with_columns([\n",
    "    pl.col(\"ret\").cast(pl.Float64, strict=False).alias(\"ret\"),\n",
    "    pl.col(\"success\").cast(pl.Float64, strict=False).alias(\"success\"),\n",
    "    (pl.col(\"ret\").abs() >= pl.lit(ABS1_THR)).alias(\"__abs1__\"),\n",
    "    (pl.col(\"ret\") == 0).alias(\"__is_zero__\"),\n",
    "])\n",
    "\n",
    "# -------------------- Resumen por (symbol,preset) --------------------\n",
    "group_cols_sp = [\"symbol\", \"preset\"]\n",
    "\n",
    "# Para evitar errores con std en grupos muy peque√±os, usamos expresiones separadas\n",
    "folds_sp = (\n",
    "    folds\n",
    "    .group_by(group_cols_sp)\n",
    "    .agg([\n",
    "        pl.len().alias(\"n_events\"),\n",
    "        pl.col(\"ret\").mean().alias(\"mean_ret\"),\n",
    "        pl.col(\"ret\").std().alias(\"std_ret\"),\n",
    "        pl.col(\"ret\").min().alias(\"min_ret\"),\n",
    "        pl.col(\"ret\").max().alias(\"max_ret\"),\n",
    "        pl.col(\"ret\").quantile(0.01, interpolation=\"nearest\").alias(\"q01_ret\"),\n",
    "        pl.col(\"ret\").quantile(0.99, interpolation=\"nearest\").alias(\"q99_ret\"),\n",
    "        pl.col(\"ret\").n_unique().alias(\"n_unique_ret\"),\n",
    "        pl.col(\"__abs1__\").cast(pl.Float64).mean().alias(\"abs1_rate\"),\n",
    "        pl.col(\"__is_zero__\").cast(pl.Float64).mean().alias(\"zero_rate\"),\n",
    "        pl.col(\"success\").mean().alias(\"mean_success\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Detector de ret como proxy binario/signo\n",
    "# Heur√≠stica robusta:\n",
    "#   - muy pocos valores √∫nicos\n",
    "#   - extremos cercanos a ¬±1 presentes\n",
    "#   - std alta t√≠pica de mezcla ¬±1\n",
    "folds_sp = folds_sp.with_columns(\n",
    "    (\n",
    "        (pl.col(\"n_unique_ret\") <= 3) &\n",
    "        (pl.col(\"min_ret\") <= -0.999) &\n",
    "        (pl.col(\"max_ret\") >=  0.999) &\n",
    "        (pl.col(\"std_ret\").fill_null(0.0) >= 0.90)\n",
    "    ).alias(\"ret_sign_proxy_flag\")\n",
    ")\n",
    "\n",
    "# Clip efectivo solo si NO es proxy\n",
    "folds_sp = folds_sp.with_columns(\n",
    "    pl.when(pl.col(\"ret_sign_proxy_flag\"))\n",
    "      .then(pl.lit(None))\n",
    "      .otherwise(pl.col(\"abs1_rate\"))\n",
    "      .alias(\"clip_rate_effective\")\n",
    ")\n",
    "\n",
    "# Etiqueta textual para lectura humana\n",
    "folds_sp = folds_sp.with_columns(\n",
    "    pl.when(pl.col(\"ret_sign_proxy_flag\"))\n",
    "      .then(pl.lit(\"RET_SIGN_PROXY\"))\n",
    "      .when(pl.col(\"abs1_rate\") >= pl.lit(CLIP_RATE_WARN_THR))\n",
    "      .then(pl.lit(\"POSSIBLE_CLIP\"))\n",
    "      .otherwise(pl.lit(\"OK_CONTINUOUS\"))\n",
    "      .alias(\"ret_mode_label\")\n",
    ")\n",
    "\n",
    "# -------------------- Score de riesgo de clipping (0..1) --------------------\n",
    "# Dise√±o:\n",
    "#   - penaliza abs1_rate alto cuando NO es proxy\n",
    "#   - penaliza variedad muy baja de ret\n",
    "#   - penaliza compresi√≥n extrema q01/q99 cerca de ¬±1\n",
    "#   - usa zero_rate como se√±al auxiliar de discretizaci√≥n excesiva\n",
    "#\n",
    "# Nota: score es heur√≠stico, solo QA.\n",
    "folds_sp = folds_sp.with_columns([\n",
    "    # variedad inversa normalizada (tope suave)\n",
    "    (1.0 / (1.0 + (pl.col(\"n_unique_ret\").cast(pl.Float64).fill_null(0.0)))).alias(\"__low_variety__\"),\n",
    "\n",
    "    # compresi√≥n en colas cerca de ¬±1\n",
    "    (\n",
    "        (\n",
    "            (pl.col(\"q01_ret\").abs() >= 0.95).cast(pl.Float64) +\n",
    "            (pl.col(\"q99_ret\").abs() >= 0.95).cast(pl.Float64)\n",
    "        ) / 2.0\n",
    "    ).alias(\"__tail_saturation__\"),\n",
    "\n",
    "    # abs1_rate solo relevante si no proxy\n",
    "    pl.when(pl.col(\"ret_sign_proxy_flag\"))\n",
    "      .then(pl.lit(0.0))\n",
    "      .otherwise(pl.col(\"abs1_rate\").fill_null(0.0))\n",
    "      .alias(\"__abs1_effective_for_score__\"),\n",
    "\n",
    "    pl.col(\"zero_rate\").fill_null(0.0).alias(\"__zero_rate__\"),\n",
    "])\n",
    "\n",
    "# combinaci√≥n ponderada\n",
    "folds_sp = folds_sp.with_columns(\n",
    "    (\n",
    "        0.55 * pl.col(\"__abs1_effective_for_score__\") +\n",
    "        0.20 * pl.col(\"__low_variety__\") +\n",
    "        0.15 * pl.col(\"__tail_saturation__\") +\n",
    "        0.10 * pl.col(\"__zero_rate__\")\n",
    "    ).clip(0.0, 1.0).alias(\"clip_risk_score\")\n",
    ")\n",
    "\n",
    "# Limpieza de helpers de score\n",
    "folds_sp = folds_sp.drop([c for c in folds_sp.columns if c.startswith(\"__\")])\n",
    "\n",
    "# -------------------- Vista adicional por (preset,segment) si existe --------------------\n",
    "folds_ps = None\n",
    "if \"segment\" in folds.columns:\n",
    "    folds_ps = (\n",
    "        folds\n",
    "        .group_by([\"preset\", \"segment\"])\n",
    "        .agg([\n",
    "            pl.len().alias(\"n_events\"),\n",
    "            pl.col(\"ret\").mean().alias(\"mean_ret\"),\n",
    "            pl.col(\"ret\").std().alias(\"std_ret\"),\n",
    "            pl.col(\"success\").mean().alias(\"mean_success\"),\n",
    "            pl.col(\"__abs1__\").cast(pl.Float64).mean().alias(\"abs1_rate\"),\n",
    "            pl.col(\"__is_zero__\").cast(pl.Float64).mean().alias(\"zero_rate\"),\n",
    "        ])\n",
    "        .sort([\"preset\", \"segment\"])\n",
    "    )\n",
    "\n",
    "# -------------------- Cruce con p-values para detectar \"freq-only\" --------------------\n",
    "adv_sel = adv.select([\n",
    "    \"symbol\", \"preset\",\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    *([c for c in [\"tvals_ER\", \"tvals_PD\", \"tvals_p_event\", \"stat_score_bucket\"] if c in adv.columns])\n",
    "])\n",
    "\n",
    "qa = (\n",
    "    folds_sp\n",
    "    .join(adv_sel, on=[\"symbol\", \"preset\"], how=\"left\")\n",
    "    .with_columns([\n",
    "        (\n",
    "            (pl.col(\"pvals_combined_ER\") <= pl.lit(alpha_loose)) |\n",
    "            (pl.col(\"pvals_combined_PD\") <= pl.lit(alpha_loose))\n",
    "        ).alias(\"edge_support_loose\"),\n",
    "        (pl.col(\"pvals_combined_p_event\") <= pl.lit(alpha_loose)).alias(\"p_event_loose\"),\n",
    "    ])\n",
    "    .with_columns(\n",
    "        (pl.col(\"p_event_loose\") & (~pl.col(\"edge_support_loose\"))).alias(\"flag_freq_only\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# -------------------- Outputs --------------------\n",
    "print(\"\\n[09C] Conteo por modo de ret:\")\n",
    "print(\n",
    "    qa.group_by(\"ret_mode_label\")\n",
    "      .agg(pl.len().alias(\"n_pairs\"))\n",
    "      .sort(\"n_pairs\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"\\n[09C] Resumen por (symbol,preset) con flags operativos y score de clipping:\")\n",
    "select_cols = [\n",
    "    \"symbol\", \"preset\",\n",
    "    \"n_events\",\n",
    "    \"mean_ret\", \"std_ret\", \"min_ret\", \"max_ret\", \"q01_ret\", \"q99_ret\",\n",
    "    \"mean_success\",\n",
    "    \"n_unique_ret\",\n",
    "    \"abs1_rate\",\n",
    "    \"zero_rate\",\n",
    "    \"clip_rate_effective\",\n",
    "    \"clip_risk_score\",\n",
    "    \"ret_sign_proxy_flag\",\n",
    "    \"ret_mode_label\",\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    \"edge_support_loose\", \"flag_freq_only\",\n",
    "]\n",
    "select_cols += [c for c in [\"stat_score_bucket\"] if c in qa.columns]\n",
    "select_cols = [c for c in select_cols if c in qa.columns]\n",
    "\n",
    "print(\n",
    "    qa.select(select_cols)\n",
    "      .sort(\n",
    "          [\"flag_freq_only\", \"ret_mode_label\", \"clip_risk_score\", \"abs1_rate\", \"n_events\"],\n",
    "          descending=True\n",
    "      )\n",
    "      .head(50)\n",
    ")\n",
    "\n",
    "if folds_ps is not None:\n",
    "    print(\"\\n[09C] Distribuci√≥n por (preset,segment) ‚Äî abs1_rate/zero_rate y proxies evento-level:\")\n",
    "    print(folds_ps)\n",
    "\n",
    "print(\"\\n[09C] Top 'freq-only' (p_event fuerte sin soporte ER/PD) priorizado por clip_risk_score:\")\n",
    "freq_cols = [\n",
    "    \"symbol\", \"preset\",\n",
    "    \"ret_mode_label\", \"ret_sign_proxy_flag\",\n",
    "    \"n_events\", \"mean_ret\", \"mean_success\",\n",
    "    \"abs1_rate\", \"zero_rate\",\n",
    "    \"clip_risk_score\",\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "]\n",
    "freq_cols += [c for c in [\"stat_score_bucket\"] if c in qa.columns]\n",
    "freq_cols = [c for c in freq_cols if c in qa.columns]\n",
    "\n",
    "print(\n",
    "    qa.filter(pl.col(\"flag_freq_only\") == True)\n",
    "      .select(freq_cols)\n",
    "      .sort([\"clip_risk_score\", \"abs1_rate\", \"n_events\"], descending=True)\n",
    "      .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n[09C] Top candidatos a revisar upstream por posible clipping real (NO proxy):\")\n",
    "print(\n",
    "    qa.filter(\n",
    "        (pl.col(\"ret_sign_proxy_flag\") == False) &\n",
    "        (pl.col(\"ret_mode_label\") == \"POSSIBLE_CLIP\")\n",
    "    )\n",
    "    .select([\n",
    "        \"symbol\", \"preset\",\n",
    "        \"n_events\",\n",
    "        \"abs1_rate\", \"zero_rate\",\n",
    "        \"n_unique_ret\",\n",
    "        \"q01_ret\", \"q99_ret\",\n",
    "        \"clip_risk_score\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "        \"flag_freq_only\",\n",
    "        *([c for c in [\"stat_score_bucket\"] if c in qa.columns])\n",
    "    ])\n",
    "    .sort([\"clip_risk_score\", \"abs1_rate\", \"n_events\"], descending=True)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Celda 09C v1.2 PRO :: OK\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9932fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10 v2.2.0 PRO :: SCORE_FINAL v2 + stats suave + estructura + viabilidad + estabilidad\n",
      "[Celda 10] RUN_ID = 20251218_190810\n",
      "[Celda 10] alphas  ‚Üí strict=0.05, balanced=0.1, loose=0.2\n",
      "[Celda 10] thr_trend  ‚Üí {'strict': 0.3, 'balanced': 0.25, 'loose': 0.2}\n",
      "[Celda 10] thr_range  ‚Üí {'strict': 0.3, 'balanced': 0.25, 'loose': 0.2}\n",
      "[Celda 10] min_ndq_trend=0.0, min_ndq_range=0.0\n",
      "[Celda 10] Estab_min=0.6, Estab_min_salvado=0.4, score_super_alto=0.7\n",
      "[Celda 10] min_estab_strict=0.7, min_estab_balanced=0.5, min_estab_loose=0.3\n",
      "[Celda 10] basket_size_max=12, global_min_TR=0.25\n",
      "[Celda 10] default_min_TR_required (fallback) = 0.25\n",
      "[Celda 10] thresholds SCORE_FINAL ‚Üí viable‚âà0.30, interesante‚âà0.40, premium‚âà0.50\n",
      "[Celda 10] weights_main (normalizados) ‚Üí significance=0.250, opportunity=0.200, stability=0.250, viability=0.200, structure=0.100\n",
      "üìÅ INPUT ‚Üí stability_table_advanced = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet (rows=166, cols=77)\n",
      "üìÅ INPUT ‚Üí stability_table          = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet (exists=true)\n",
      "üìÅ INPUT ‚Üí opportunity_table        = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\frequency_opportunity_table.parquet (rows=84, cols=14)\n",
      "üìÅ INPUT ‚Üí economic_viability       = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\economic_viability.parquet (rows=83, cols=13)\n",
      "üìÅ INPUT ‚Üí structure_summary        = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\structure_summary.parquet (exists=true)\n",
      "üìÅ INPUT ‚Üí config.json              = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json\n",
      "[Celda 10] Estructura agregada por s√≠mbolo ‚Üí rows=83, columnas=['symbol', 'structure_score', 'structure_flag_qa']\n",
      "[Celda 10] RUN stats-support @ Œ±_loose=0.2 ‚Üí support_count=1 | ER=0.00% PD=0.00% p_event=98.80%\n",
      "[Celda 10] Componente estad√≠stico ‚Üí weight_stat_component=0.000, run_significance_state=BALANCED\n",
      "[Celda 10] p-values non-null counts (final_df) ‚Üí {'pvals_combined_ER': 166, 'pvals_combined_PD': 166, 'pvals_combined_p_event': 166}\n",
      "---- QA: Distribuciones de sub-scores y SCORE_FINAL ----\n",
      "[Celda 10] stat_score_bucket :: min=0.020, p25=0.601, mediana=0.602, p75=0.604, max=0.609\n",
      "[Celda 10] score_significance :: min=0.000, p25=0.000, mediana=0.000, p75=0.000, max=0.000\n",
      "[Celda 10] score_opportunity :: min=0.000, p25=0.434, mediana=0.468, p75=0.546, max=0.700\n",
      "[Celda 10] score_stability :: min=0.537, p25=0.592, mediana=0.663, p75=0.723, max=0.873\n",
      "[Celda 10] score_viability :: min=0.200, p25=0.273, mediana=0.323, p75=0.387, max=0.708\n",
      "[Celda 10] score_structure :: min=0.000, p25=0.140, mediana=0.250, p75=0.500, max=0.750\n",
      "[Celda 10] SCORE_RAW :: min=0.202, p25=0.327, mediana=0.356, p75=0.387, max=0.488\n",
      "[Celda 10] SCORE_FINAL :: min=0.202, p25=0.327, mediana=0.356, p75=0.387, max=0.488\n",
      "[Celda 10] SCORE_FINAL > 0.30 ‚Üí 141 bucket(s)\n",
      "[Celda 10] SCORE_FINAL > 0.40 ‚Üí 26 bucket(s)\n",
      "[Celda 10] SCORE_FINAL > 0.50 ‚Üí 0 bucket(s)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (OK, rows=166, cols=56)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_meta.json (OK, bytes=1876)\n",
      ">>> Celda 10 v2.2.0 PRO :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 10 v2.2.0 PRO ‚Äî SCORE_FINAL v2 + componente estad√≠stico suave (p-values)\n",
    "#                     + estructura de mercado + flags viabilidad/core/premium\n",
    "#                     + min_TR_required + decay_penalty travel integrados\n",
    "#                     + FIX normalize_by_percentile\n",
    "#                     + Abort p-values alineado al pipeline (solo si faltan o est√°n vac√≠os)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10 v2.2.0 PRO :: SCORE_FINAL v2 + stats suave + estructura + viabilidad + estabilidad\")\n",
    "\n",
    "# ====================== Validaciones de GLOBAL_STATE y rutas base ======================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas iniciales.\")\n",
    "\n",
    "for key in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE incompleto; falta clave '{key}'.\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\")\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "for key in (\"stability\", \"metrics\", \"diagnostics\"):\n",
    "    if key not in paths:\n",
    "        raise RuntimeError(f\"Falta GLOBAL_STATE['paths']['{key}'].\")\n",
    "\n",
    "OUT_STAB_DIR    = Path(paths[\"stability\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_SCORES_DIR  = Path(paths.get(\"scores\", OUT_STAB_DIR.parent / \"scores\")).resolve()\n",
    "\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GLOBAL_STATE[\"paths\"][\"scores\"] = str(OUT_SCORES_DIR)\n",
    "\n",
    "STAB_ADV_PATH   = OUT_STAB_DIR / \"stability_table_advanced.parquet\"\n",
    "STAB_TABLE_PATH = OUT_STAB_DIR / \"stability_table.parquet\"\n",
    "ECO_VIAB_PATH   = OUT_METRICS_DIR / \"economic_viability.parquet\"\n",
    "\n",
    "# ====================== OPORTUNIDAD (contrato real Celda 08 v3) ======================\n",
    "metrics_state = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "opp_path_gs = None\n",
    "if isinstance(metrics_state, dict):\n",
    "    opp_path_gs = metrics_state.get(\"frequency_opportunity_table_path\", None)\n",
    "\n",
    "# Orden de resoluci√≥n:\n",
    "# 1) GLOBAL_STATE si existe\n",
    "# 2) nombre can√≥nico nuevo en disco\n",
    "# 3) fallback legacy si a√∫n vive en algunos runs viejos\n",
    "OPP_TABLE_PATH = Path(opp_path_gs).resolve() if opp_path_gs else (OUT_METRICS_DIR / \"frequency_opportunity_table.parquet\").resolve()\n",
    "OPP_LEGACY_PATH = (OUT_METRICS_DIR / \"opportunity_summary.parquet\").resolve()\n",
    "\n",
    "if not OPP_TABLE_PATH.exists() and OPP_LEGACY_PATH.exists():\n",
    "    OPP_TABLE_PATH = OPP_LEGACY_PATH\n",
    "\n",
    "# ====================== STRUCTURE SUMMARY ======================\n",
    "if isinstance(metrics_state, dict) and \"structure_summary_path\" in metrics_state:\n",
    "    STRUCT_SUMMARY_PATH = Path(metrics_state[\"structure_summary_path\"]).resolve()\n",
    "else:\n",
    "    STRUCT_SUMMARY_PATH = (OUT_METRICS_DIR / \"structure_summary.parquet\").resolve()\n",
    "\n",
    "CONFIG_PATH = Path(GLOBAL_STATE[\"paths\"].get(\"config\", OUT_DIAG_DIR / \"config.json\")).resolve()\n",
    "\n",
    "# ====================== Config & thresholds ======================\n",
    "cfg_state = GLOBAL_STATE.get(\"config\", {})\n",
    "if isinstance(cfg_state, dict) and cfg_state:\n",
    "    cfg = cfg_state\n",
    "else:\n",
    "    if not CONFIG_PATH.exists():\n",
    "        raise RuntimeError(f\"Config no encontrado ni en GLOBAL_STATE ni en disco: {CONFIG_PATH}\")\n",
    "    cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "policy        = cfg.get(\"policy\", {})     or {}\n",
    "stats         = cfg.get(\"stats\", {})      or {}\n",
    "baskets       = cfg.get(\"baskets\", {})    or {}\n",
    "scores_cfg    = cfg.get(\"scores\", {})     or {}\n",
    "stability_cfg = cfg.get(\"stability\", {})  or {}\n",
    "\n",
    "# Alphas (alineados con Celda 09)\n",
    "alpha_strict   = float(stats.get(\"alpha_strict\", 0.02))\n",
    "alpha_balanced = float(stats.get(\"alpha_balanced\", 0.08))\n",
    "alpha_loose    = float(stats.get(\"alpha_loose\", 0.25))\n",
    "\n",
    "# TR thresholds por familia\n",
    "thr_trend = {\n",
    "    \"strict\":   float(policy.get(\"min_tr_after_cost_trend_strict\",   policy.get(\"min_TR_after_cost\", 0.25))),\n",
    "    \"balanced\": float(policy.get(\"min_tr_after_cost_trend_balanced\", policy.get(\"min_TR_after_cost\", 0.18))),\n",
    "    \"loose\":    float(policy.get(\"min_tr_after_cost_trend_loose\",    policy.get(\"min_TR_after_cost\", 0.08))),\n",
    "}\n",
    "thr_range = {\n",
    "    \"strict\":   float(policy.get(\"min_tr_after_cost_range_strict\",   policy.get(\"min_TR_after_cost\", 0.20))),\n",
    "    \"balanced\": float(policy.get(\"min_tr_after_cost_range_balanced\", policy.get(\"min_TR_after_cost\", 0.12))),\n",
    "    \"loose\":    float(policy.get(\"min_tr_after_cost_range_loose\",    policy.get(\"min_TR_after_cost\", 0.05))),\n",
    "}\n",
    "\n",
    "min_ndq_trend = float(policy.get(\"min_NDQ_trend\", 0.00))\n",
    "min_ndq_range = float(policy.get(\"min_NDQ_range\", 0.00))\n",
    "\n",
    "basket_size_max = int(baskets.get(\"size_max\", 5))\n",
    "\n",
    "stab_pol = GLOBAL_STATE.get(\"stability_policy\", {}) or {}\n",
    "Estab_min          = float(stab_pol.get(\"Estab_min\",          policy.get(\"Estab_min\",         0.60)))\n",
    "Estab_min_salvado  = float(stab_pol.get(\"Estab_min_salvado\",  policy.get(\"Estab_min_salvado\", 0.40)))\n",
    "score_super_alto   = float(stab_pol.get(\"score_super_alto\",   policy.get(\"score_super_alto\",  0.70)))\n",
    "min_estab_strict   = float(stab_pol.get(\"min_estab_strict\",   stability_cfg.get(\"min_estab_strict\",   0.70)))\n",
    "min_estab_balanced = float(stab_pol.get(\"min_estab_balanced\", stability_cfg.get(\"min_estab_balanced\", 0.50)))\n",
    "min_estab_loose    = float(stab_pol.get(\"min_estab_loose\",    stability_cfg.get(\"min_estab_loose\",    0.30)))\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"stability_policy\", {})\n",
    "GLOBAL_STATE[\"stability_policy\"].update({\n",
    "    \"Estab_min\": Estab_min,\n",
    "    \"Estab_min_salvado\": Estab_min_salvado,\n",
    "    \"score_super_alto\": score_super_alto,\n",
    "    \"min_estab_strict\": min_estab_strict,\n",
    "    \"min_estab_balanced\": min_estab_balanced,\n",
    "    \"min_estab_loose\": min_estab_loose,\n",
    "})\n",
    "\n",
    "global_min_TR = float(policy.get(\"min_TR_after_cost\", 0.25))\n",
    "default_min_TR_required = float(\n",
    "    stab_pol.get(\"min_TR_required_default\",\n",
    "                 stability_cfg.get(\"min_TR_required\", global_min_TR))\n",
    ")\n",
    "\n",
    "score_thr_cfg         = scores_cfg.get(\"score_thresholds\", {}) or {}\n",
    "score_viable_thr      = float(score_thr_cfg.get(\"viable\",      0.30))\n",
    "score_interesting_thr = float(score_thr_cfg.get(\"interesting\", 0.40))\n",
    "score_premium_thr     = float(score_thr_cfg.get(\"premium\",     0.50))\n",
    "\n",
    "print(f\"[Celda 10] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[Celda 10] alphas  ‚Üí strict={alpha_strict}, balanced={alpha_balanced}, loose={alpha_loose}\")\n",
    "print(f\"[Celda 10] thr_trend  ‚Üí {thr_trend}\")\n",
    "print(f\"[Celda 10] thr_range  ‚Üí {thr_range}\")\n",
    "print(f\"[Celda 10] min_ndq_trend={min_ndq_trend}, min_ndq_range={min_ndq_range}\")\n",
    "print(f\"[Celda 10] Estab_min={Estab_min}, Estab_min_salvado={Estab_min_salvado}, score_super_alto={score_super_alto}\")\n",
    "print(f\"[Celda 10] min_estab_strict={min_estab_strict}, min_estab_balanced={min_estab_balanced}, min_estab_loose={min_estab_loose}\")\n",
    "print(f\"[Celda 10] basket_size_max={basket_size_max}, global_min_TR={global_min_TR}\")\n",
    "print(f\"[Celda 10] default_min_TR_required (fallback) = {default_min_TR_required}\")\n",
    "print(\n",
    "    f\"[Celda 10] thresholds SCORE_FINAL ‚Üí \"\n",
    "    f\"viable‚âà{score_viable_thr:.2f}, interesante‚âà{score_interesting_thr:.2f}, premium‚âà{score_premium_thr:.2f}\"\n",
    ")\n",
    "\n",
    "# ====================== Pesos del Score v2 (incluyendo estructura) ======================\n",
    "W_SIGNIFICANCE = float(scores_cfg.get(\"w_significance\", 0.38))\n",
    "W_OPPORTUNITY  = float(scores_cfg.get(\"w_opportunity\",  0.20))\n",
    "W_STABILITY    = float(scores_cfg.get(\"w_stability\",    0.22))\n",
    "W_VIABILITY    = float(scores_cfg.get(\"w_viability\",    0.12))\n",
    "W_STRUCTURE    = float(scores_cfg.get(\"w_structure\",    0.08))\n",
    "\n",
    "sum_w = W_SIGNIFICANCE + W_OPPORTUNITY + W_STABILITY + W_VIABILITY + W_STRUCTURE\n",
    "if (not math.isfinite(sum_w)) or (sum_w <= 0.0):\n",
    "    W_SIGNIFICANCE, W_OPPORTUNITY, W_STABILITY, W_VIABILITY, W_STRUCTURE = 0.40, 0.20, 0.20, 0.10, 0.10\n",
    "    sum_w = 1.0\n",
    "else:\n",
    "    W_SIGNIFICANCE /= sum_w\n",
    "    W_OPPORTUNITY  /= sum_w\n",
    "    W_STABILITY    /= sum_w\n",
    "    W_VIABILITY    /= sum_w\n",
    "    W_STRUCTURE    /= sum_w\n",
    "\n",
    "print(\n",
    "    \"[Celda 10] weights_main (normalizados) ‚Üí \"\n",
    "    f\"significance={W_SIGNIFICANCE:.3f}, opportunity={W_OPPORTUNITY:.3f}, \"\n",
    "    f\"stability={W_STABILITY:.3f}, viability={W_VIABILITY:.3f}, structure={W_STRUCTURE:.3f}\"\n",
    ")\n",
    "\n",
    "SIG_WEIGHTS = {\"EV\": 0.60, \"PD\": 0.30, \"ER\": 0.10}\n",
    "OPP_WEIGHTS = {\"n\": 0.70, \"c\": 0.30}\n",
    "VIA_WEIGHTS = {\"TR\": 0.60, \"NDQ\": 0.40}\n",
    "\n",
    "# ====================== Utilidades ======================\n",
    "def require_file(p: Path, required: bool = True):\n",
    "    if required and not p.exists():\n",
    "        raise RuntimeError(f\"Missing required input: {p}\")\n",
    "\n",
    "def p_expr_to_unit_score(colname: str) -> pl.Expr:\n",
    "    col = pl.col(colname).cast(pl.Float64, strict=False)\n",
    "    return ((-col.clip(1e-12, 1.0).log10()) / 12.0).clip(0.0, 1.0).fill_null(0.0)\n",
    "\n",
    "def normalize_by_percentile(series: pl.Series, lower: float = 0.05, upper: float = 0.95) -> pl.Series:\n",
    "    if series.len() == 0:\n",
    "        return pl.Series([], dtype=pl.Float64)\n",
    "\n",
    "    s = series.cast(pl.Float64, strict=False)\n",
    "    non_null = s.drop_nulls()\n",
    "\n",
    "    if non_null.len() == 0:\n",
    "        return pl.Series([0.0] * s.len(), dtype=pl.Float64)\n",
    "\n",
    "    lo = float(non_null.quantile(lower))\n",
    "    hi = float(non_null.quantile(upper))\n",
    "\n",
    "    if not (hi > lo):\n",
    "        return pl.Series([0.0] * s.len(), dtype=pl.Float64)\n",
    "\n",
    "    out = ((s - lo) / (hi - lo)).clip(0.0, 1.0)\n",
    "    return out.fill_null(0.0).cast(pl.Float64)\n",
    "\n",
    "def find_first_col(df: pl.DataFrame, candidates: List[str]) -> str:\n",
    "    if df.is_empty():\n",
    "        return \"\"\n",
    "    cols = set(df.columns)\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "        lc = c.lower()\n",
    "        if lc in lower_map:\n",
    "            return lower_map[lc]\n",
    "    return \"\"\n",
    "\n",
    "def safe_select(df: pl.DataFrame, cols: List[str]) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        base = {}\n",
    "        if \"symbol\" in cols:\n",
    "            base[\"symbol\"] = pl.Series([], dtype=pl.Utf8)\n",
    "        if \"preset\" in cols:\n",
    "            base[\"preset\"] = pl.Series([], dtype=pl.Utf8)\n",
    "        out = pl.DataFrame(base) if base else pl.DataFrame()\n",
    "        for c in cols:\n",
    "            if c not in out.columns:\n",
    "                out = out.with_columns(pl.lit(None).alias(c))\n",
    "        return out\n",
    "\n",
    "    present = [c for c in cols if c and c in df.columns]\n",
    "    out = df.select(present) if present else pl.DataFrame()\n",
    "    for m in cols:\n",
    "        if m and m not in out.columns:\n",
    "            out = out.with_columns(pl.lit(None).alias(m))\n",
    "    return out\n",
    "\n",
    "def _sanitize_cols(df: pl.DataFrame, keep_only: List[str] | None = None) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    drop_suffixes = (\"_right\", \"_left\", \"_x\", \"_y\", \"_adv\")\n",
    "    cols = [c for c in df.columns if not c.endswith(drop_suffixes)]\n",
    "    out = df.select(cols)\n",
    "    for bad in (\"symbol_right\", \"symbol_left\"):\n",
    "        if bad in out.columns:\n",
    "            out = out.drop(bad)\n",
    "    if keep_only is not None:\n",
    "        base_keep = [\"symbol\"]\n",
    "        if \"preset\" in out.columns:\n",
    "            base_keep.append(\"preset\")\n",
    "        keep = base_keep + [c for c in keep_only if c in out.columns and c not in base_keep]\n",
    "        out = out.select(keep)\n",
    "    if \"symbol\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    if \"preset\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"preset\").cast(pl.Utf8, strict=False).str.strip_chars()\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def _coerce_preset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty() or \"preset\" in df.columns:\n",
    "        return df\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for cand in (\"preset_name\", \"family\", \"bucket_family\", \"regime_preset\", \"regime_family\"):\n",
    "        if cand in lower_map:\n",
    "            orig = lower_map[cand]\n",
    "            return df.rename({orig: \"preset\"})\n",
    "    return df\n",
    "\n",
    "def _score_tr_soft(tr: float, min_tr: float = 0.30) -> float:\n",
    "    try:\n",
    "        if tr is None or not math.isfinite(float(tr)):\n",
    "            return 0.0\n",
    "        x = float(tr)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    if x <= 0.20:\n",
    "        return 0.0\n",
    "    if x < min_tr:\n",
    "        lo, hi = 0.20, min_tr\n",
    "        if hi <= lo:\n",
    "            return 0.0\n",
    "        frac = (x - lo) / (hi - lo)\n",
    "        frac = max(0.0, min(1.0, frac))\n",
    "        return frac * 0.4\n",
    "\n",
    "    denom_hi = 1.0 - min_tr\n",
    "    if denom_hi <= 0:\n",
    "        return 0.4\n",
    "    frac_hi = (x - min_tr) / denom_hi\n",
    "    frac_hi = max(0.0, min(1.0, frac_hi))\n",
    "    return 0.4 + 0.6 * frac_hi\n",
    "\n",
    "# ====================== Cargar insumos ======================\n",
    "require_file(STAB_ADV_PATH, required=True)\n",
    "require_file(OPP_TABLE_PATH, required=True)\n",
    "require_file(ECO_VIAB_PATH, required=True)\n",
    "require_file(CONFIG_PATH, required=True)\n",
    "require_file(STRUCT_SUMMARY_PATH, required=False)\n",
    "\n",
    "stab_adv_raw = pl.read_parquet(STAB_ADV_PATH)\n",
    "opp_df       = pl.read_parquet(OPP_TABLE_PATH)\n",
    "eco_df       = pl.read_parquet(ECO_VIAB_PATH)\n",
    "stab_raw     = pl.read_parquet(STAB_TABLE_PATH) if STAB_TABLE_PATH.exists() else pl.DataFrame()\n",
    "struct_raw   = pl.read_parquet(STRUCT_SUMMARY_PATH) if STRUCT_SUMMARY_PATH.exists() else pl.DataFrame()\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    if \"preset\" in df.columns:\n",
    "        df = df.with_columns(pl.col(\"preset\").cast(pl.Utf8, strict=False).str.strip_chars())\n",
    "    return df\n",
    "\n",
    "stab_adv_raw = _coerce_preset(_norm_sym(stab_adv_raw))\n",
    "opp_df       = _norm_sym(opp_df)\n",
    "eco_df       = _norm_sym(eco_df)\n",
    "stab_raw     = _coerce_preset(_norm_sym(stab_raw))\n",
    "struct_raw   = _norm_sym(struct_raw)\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí stability_table_advanced = {str(STAB_ADV_PATH)} (rows={stab_adv_raw.height}, cols={len(stab_adv_raw.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí stability_table          = {str(STAB_TABLE_PATH)} (exists={'true' if STAB_TABLE_PATH.exists() else 'false'})\")\n",
    "print(f\"üìÅ INPUT ‚Üí opportunity_table        = {str(OPP_TABLE_PATH)} (rows={opp_df.height}, cols={len(opp_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí economic_viability       = {str(ECO_VIAB_PATH)} (rows={eco_df.height}, cols={len(eco_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí structure_summary        = {str(STRUCT_SUMMARY_PATH)} (exists={'true' if STRUCT_SUMMARY_PATH.exists() else 'false'})\")\n",
    "print(f\"üìÅ INPUT ‚Üí config.json              = {str(CONFIG_PATH)}\")\n",
    "\n",
    "# ====================== Unir stability_table con advanced ======================\n",
    "if not stab_raw.is_empty():\n",
    "    stab_df = _sanitize_cols(stab_raw)\n",
    "\n",
    "    ADV_KEEP = [\n",
    "        \"EstabScore_final\", \"EstabScore_base\", \"decay_flag\",\n",
    "        \"keep_stab_base\", \"passed_stability_gate\", \"flag_oos_ok\",\n",
    "        \"passed_data_gate\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "        \"tvals_ER\", \"tvals_PD\", \"tvals_p_event\",\n",
    "        \"flag_stab_data_missing\", \"oos_fail_reason\",\n",
    "        \"PD_IS\", \"ER_IS\",\n",
    "        \"min_TR_required\", \"decay_penalty\",\n",
    "    ]\n",
    "\n",
    "    stab_adv_df = _sanitize_cols(stab_adv_raw, keep_only=ADV_KEEP)\n",
    "\n",
    "    join_keys = [\"symbol\"]\n",
    "    if (\"preset\" in stab_df.columns) or (\"preset\" in stab_adv_df.columns):\n",
    "        if \"preset\" not in stab_df.columns:\n",
    "            stab_df = stab_df.with_columns(pl.lit(\"DEFAULT\").alias(\"preset\"))\n",
    "        if \"preset\" not in stab_adv_df.columns:\n",
    "            stab_adv_df = stab_adv_df.with_columns(pl.lit(\"DEFAULT\").alias(\"preset\"))\n",
    "        join_keys = [\"symbol\", \"preset\"]\n",
    "\n",
    "    stab_join = stab_df.join(stab_adv_df, on=join_keys, how=\"full\", suffix=\"_adv\")\n",
    "\n",
    "    for col in [c for c in ADV_KEEP if c != \"symbol\"]:\n",
    "        col_adv = f\"{col}_adv\"\n",
    "        if (col in stab_join.columns) and (col_adv in stab_join.columns):\n",
    "            stab_join = (\n",
    "                stab_join\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col(col_adv).is_null())\n",
    "                      .then(pl.col(col))\n",
    "                      .otherwise(pl.col(col_adv))\n",
    "                      .alias(col)\n",
    "                )\n",
    "                .drop(col_adv)\n",
    "            )\n",
    "else:\n",
    "    stab_join = _sanitize_cols(stab_adv_raw)\n",
    "\n",
    "# ====================== Estructura de mercado agregada por s√≠mbolo ======================\n",
    "if struct_raw.is_empty():\n",
    "    print(\"[Celda 10] WARN: structure_summary vac√≠o; score_structure neutro=0.5.\")\n",
    "    struct_sym = pl.DataFrame(\n",
    "        {\n",
    "            \"symbol\": pl.Series([], dtype=pl.Utf8),\n",
    "            \"structure_score\": pl.Series([], dtype=pl.Float64),\n",
    "            \"vol_regime_score\": pl.Series([], dtype=pl.Float64),\n",
    "            \"structure_flag_qa\": pl.Series([], dtype=pl.Utf8),\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    struct_score_src = \"structure_score\" if \"structure_score\" in struct_raw.columns else (\n",
    "        \"structure_score_raw\" if \"structure_score_raw\" in struct_raw.columns else None\n",
    "    )\n",
    "    struct_flag_src  = \"structure_flag_qa\" if \"structure_flag_qa\" in struct_raw.columns else (\n",
    "        \"structure_flag\" if \"structure_flag\" in struct_raw.columns else None\n",
    "    )\n",
    "    vol_regime_src   = \"vol_regime_score\" if \"vol_regime_score\" in struct_raw.columns else (\n",
    "        \"vol_regime_score_raw\" if \"vol_regime_score_raw\" in struct_raw.columns else None\n",
    "    )\n",
    "\n",
    "    if struct_score_src is None:\n",
    "        print(\"[Celda 10] WARN: structure_summary sin score; score_structure=0.5.\")\n",
    "        struct_sym = pl.DataFrame(\n",
    "            {\n",
    "                \"symbol\": pl.Series([], dtype=pl.Utf8),\n",
    "                \"structure_score\": pl.Series([], dtype=pl.Float64),\n",
    "                \"vol_regime_score\": pl.Series([], dtype=pl.Float64),\n",
    "                \"structure_flag_qa\": pl.Series([], dtype=pl.Utf8),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        agg_exprs = [\n",
    "            pl.col(struct_score_src).cast(pl.Float64, strict=False).mean().alias(\"structure_score\"),\n",
    "        ]\n",
    "        if vol_regime_src is not None:\n",
    "            agg_exprs.append(\n",
    "                pl.col(vol_regime_src).cast(pl.Float64, strict=False).mean().alias(\"vol_regime_score\")\n",
    "            )\n",
    "        if struct_flag_src is not None:\n",
    "            agg_exprs += [\n",
    "                (pl.col(struct_flag_src) == \"BAD\").cast(pl.Int64).sum().alias(\"__n_bad__\"),\n",
    "                (pl.col(struct_flag_src) == \"WARN\").cast(pl.Int64).sum().alias(\"__n_warn__\"),\n",
    "                (pl.col(struct_flag_src) == \"OK\").cast(pl.Int64).sum().alias(\"__n_ok__\"),\n",
    "            ]\n",
    "\n",
    "        struct_sym = struct_raw.group_by(\"symbol\").agg(agg_exprs)\n",
    "\n",
    "        if struct_flag_src is not None:\n",
    "            struct_sym = (\n",
    "                struct_sym\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col(\"__n_bad__\") > 0).then(pl.lit(\"BAD\"))\n",
    "                      .when(pl.col(\"__n_warn__\") > 0).then(pl.lit(\"WARN\"))\n",
    "                      .when(pl.col(\"__n_ok__\") > 0).then(pl.lit(\"OK\"))\n",
    "                      .otherwise(pl.lit(None, dtype=pl.Utf8))\n",
    "                      .alias(\"structure_flag_qa\")\n",
    "                )\n",
    "                .drop([c for c in (\"__n_bad__\", \"__n_warn__\", \"__n_ok__\") if c in struct_sym.columns])\n",
    "            )\n",
    "        else:\n",
    "            struct_sym = struct_sym.with_columns(pl.lit(None, dtype=pl.Utf8).alias(\"structure_flag_qa\"))\n",
    "\n",
    "        print(\n",
    "            f\"[Celda 10] Estructura agregada por s√≠mbolo ‚Üí rows={struct_sym.height}, \"\n",
    "            f\"columnas={struct_sym.columns}\"\n",
    "        )\n",
    "\n",
    "# ====================== build_scores_table ======================\n",
    "def build_scores_table(\n",
    "    df_sig: pl.DataFrame,\n",
    "    df_adv: pl.DataFrame,\n",
    "    df_opp: pl.DataFrame,\n",
    "    df_viab: pl.DataFrame,\n",
    ") -> Tuple[pl.DataFrame, Dict]:\n",
    "    meta: Dict = {\"warnings\": [], \"info\": {}}\n",
    "\n",
    "    er_candidates = [\"pvals_combined_ER\", \"p_ER_combined\", \"p_ER\", \"pval_ER\", \"p_value_ER\", \"p_value_er\"]\n",
    "    pd_candidates = [\"pvals_combined_PD\", \"p_PD_combined\", \"p_PD\", \"pval_PD\", \"p_value_PD\", \"p_value_pd\"]\n",
    "    ev_candidates = [\"pvals_combined_p_event\", \"p_event_combined\", \"p_event\", \"pval_event\", \"p_value_event\"]\n",
    "\n",
    "    t_er_candidates = [\"tvals_ER\", \"t_ER\", \"tvalue_ER\"]\n",
    "    t_pd_candidates = [\"tvals_PD\", \"t_PD\", \"tvalue_PD\"]\n",
    "    t_ev_candidates = [\"tvals_p_event\", \"t_event\", \"tvalue_event\"]\n",
    "\n",
    "    col_p_er = find_first_col(df_sig, er_candidates)\n",
    "    col_p_pd = find_first_col(df_sig, pd_candidates)\n",
    "    col_p_ev = find_first_col(df_sig, ev_candidates)\n",
    "\n",
    "    col_t_er = find_first_col(df_sig, t_er_candidates)\n",
    "    col_t_pd = find_first_col(df_sig, t_pd_candidates)\n",
    "    col_t_ev = find_first_col(df_sig, t_ev_candidates)\n",
    "\n",
    "    pd_is_candidates = [\"PD_IS\", \"pwin_IS\", \"p_win_IS\", \"hit_rate_IS\", \"PD_in_sample\", \"PD\"]\n",
    "    er_is_candidates = [\"ER_IS\", \"ER_mean_IS\", \"ER_ISOOS\", \"ER\"]\n",
    "\n",
    "    col_PD_IS = find_first_col(df_sig, pd_is_candidates)\n",
    "    col_ER_IS = find_first_col(df_sig, er_is_candidates)\n",
    "\n",
    "    col_estab_final  = find_first_col(df_adv, [\"EstabScore_final\"])\n",
    "    col_estab_base   = find_first_col(df_adv, [\"EstabScore_base\"])\n",
    "    col_decay_flag   = find_first_col(df_adv, [\"decay_flag\", \"decay\", \"has_decay\"])\n",
    "    col_stab_missing = find_first_col(df_adv, [\"flag_stab_data_missing\", \"stab_data_missing\"])\n",
    "\n",
    "    gate_data_candidates  = [\"passed_data_gate\", \"data_quality_ok\", \"data_gate_ok\"]\n",
    "    gate_oos_candidates   = [\"flag_oos_ok\", \"oos_ok\", \"gate_oos_ok\"]\n",
    "    gate_stab_candidates  = [\"passed_stability_gate\", \"keep_stab_base\"]\n",
    "    oos_reason_candidates = [\"oos_fail_reason\", \"oos_reason\"]\n",
    "\n",
    "    col_gate_data  = find_first_col(df_adv, gate_data_candidates)\n",
    "    col_gate_oos   = find_first_col(df_adv, gate_oos_candidates)\n",
    "    col_gate_stab  = find_first_col(df_adv, gate_stab_candidates)\n",
    "    col_oos_reason = find_first_col(df_adv, oos_reason_candidates)\n",
    "\n",
    "    # ‚úÖ Oportunidad: preferir SAFE si est√° disponible\n",
    "    col_npm = find_first_col(df_opp, [\n",
    "        \"n_per_month_total_safe\",\n",
    "        \"n_per_month_total\",\n",
    "        \"n_per_month\",\n",
    "        \"events_per_month\"\n",
    "    ])\n",
    "    col_cov = find_first_col(df_opp, [\"coverage_p\", \"coverage_ratio\", \"coverage\"])\n",
    "\n",
    "    col_tr_tr  = find_first_col(df_viab, [\"TR_trend_mean\", \"TR_trend\", \"TR_mean_trend\"])\n",
    "    col_tr_rg  = find_first_col(df_viab, [\"TR_range_mean\", \"TR_range\", \"TR_mean_range\"])\n",
    "    col_ndq_tr = find_first_col(df_viab, [\"NDQ_trend_mean\", \"NDQ_trend\", \"NDQ_mean_trend\"])\n",
    "    col_ndq_rg = find_first_col(df_viab, [\"NDQ_range_mean\", \"NDQ_range\", \"NDQ_mean_range\"])\n",
    "\n",
    "    col_spread = find_first_col(df_viab, [\"spread_pct\", \"spread_bps\", \"spread_bp\", \"spread_in_ticks\"])\n",
    "    col_cost   = find_first_col(df_viab, [\"cost_per_trade\", \"commission_pct\", \"commission_bp\", \"total_cost_pct\"])\n",
    "    col_liq    = find_first_col(df_viab, [\"liquidity_score\", \"liq_score\"])\n",
    "\n",
    "    col_min_tr_req = find_first_col(df_adv, [\"min_TR_required\", \"min_tr_required\"])\n",
    "    col_decay_pen  = find_first_col(df_adv, [\"decay_penalty\"])\n",
    "\n",
    "    trend_prop_candidates = [\"prop_trend\", \"trend_prop\", \"regime_trend_prop\", \"prop_regime_trend\"]\n",
    "    range_prop_candidates = [\"prop_range\", \"range_prop\", \"regime_range_prop\", \"prop_regime_range\"]\n",
    "\n",
    "    has_preset = any((\"preset\" in d.columns) for d in (df_sig, df_adv))\n",
    "\n",
    "    if has_preset:\n",
    "        key_cols = [\"symbol\", \"preset\"]\n",
    "\n",
    "        df_sig_sel = safe_select(df_sig, [\"symbol\", \"preset\",\n",
    "                                          col_p_er, col_p_pd, col_p_ev,\n",
    "                                          col_t_er, col_t_pd, col_t_ev,\n",
    "                                          col_PD_IS, col_ER_IS] + trend_prop_candidates + range_prop_candidates)\n",
    "\n",
    "        df_adv_sel = safe_select(df_adv, [\"symbol\", \"preset\",\n",
    "                                          col_estab_final, col_estab_base, col_decay_flag,\n",
    "                                          col_gate_data, col_gate_oos, col_gate_stab,\n",
    "                                          col_stab_missing, col_oos_reason,\n",
    "                                          col_min_tr_req, col_decay_pen])\n",
    "\n",
    "        df_opp_sel = safe_select(df_opp, [\"symbol\", col_npm, col_cov])\n",
    "        df_via_sel = safe_select(df_viab, [\"symbol\",\n",
    "                                           col_tr_tr, col_tr_rg, col_ndq_tr, col_ndq_rg,\n",
    "                                           col_spread, col_cost, col_liq])\n",
    "\n",
    "        keys_list = []\n",
    "        for d in (df_sig_sel, df_adv_sel):\n",
    "            if (not d.is_empty()) and all(k in d.columns for k in key_cols):\n",
    "                keys_list.append(d.select(key_cols).unique())\n",
    "        if keys_list:\n",
    "            df = pl.concat(keys_list, how=\"vertical\").unique()\n",
    "        else:\n",
    "            has_preset = False\n",
    "\n",
    "    if not has_preset:\n",
    "        key_cols = [\"symbol\"]\n",
    "\n",
    "        df_sig_sel = safe_select(df_sig, [\"symbol\",\n",
    "                                          col_p_er, col_p_pd, col_p_ev,\n",
    "                                          col_t_er, col_t_pd, col_t_ev,\n",
    "                                          col_PD_IS, col_ER_IS] + trend_prop_candidates + range_prop_candidates)\n",
    "\n",
    "        df_adv_sel = safe_select(df_adv, [\"symbol\",\n",
    "                                          col_estab_final, col_estab_base, col_decay_flag,\n",
    "                                          col_gate_data, col_gate_oos, col_gate_stab,\n",
    "                                          col_stab_missing, col_oos_reason,\n",
    "                                          col_min_tr_req, col_decay_pen])\n",
    "\n",
    "        df_opp_sel = safe_select(df_opp, [\"symbol\", col_npm, col_cov])\n",
    "        df_via_sel = safe_select(df_viab, [\"symbol\",\n",
    "                                           col_tr_tr, col_tr_rg, col_ndq_tr, col_ndq_rg,\n",
    "                                           col_spread, col_cost, col_liq])\n",
    "\n",
    "        all_symbols = sorted(list(\n",
    "            set(df_sig[\"symbol\"].drop_nulls().to_list() if \"symbol\" in df_sig.columns else []) |\n",
    "            set(df_adv[\"symbol\"].drop_nulls().to_list() if \"symbol\" in df_adv.columns else []) |\n",
    "            set(df_opp[\"symbol\"].drop_nulls().to_list() if \"symbol\" in df_opp.columns else []) |\n",
    "            set(df_viab[\"symbol\"].drop_nulls().to_list() if \"symbol\" in df_viab.columns else [])\n",
    "        ))\n",
    "        df = pl.DataFrame({\"symbol\": all_symbols})\n",
    "\n",
    "    df = df.join(df_sig_sel, on=key_cols, how=\"left\")\n",
    "    df = df.join(df_adv_sel, on=key_cols, how=\"left\")\n",
    "    df = df.join(df_opp_sel, on=\"symbol\", how=\"left\")\n",
    "    df = df.join(df_via_sel, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    trend_prop_src = find_first_col(df, trend_prop_candidates)\n",
    "    range_prop_src = find_first_col(df, range_prop_candidates)\n",
    "\n",
    "    df = df.with_columns([\n",
    "        (pl.col(trend_prop_src).cast(pl.Float64, strict=False) if trend_prop_src else pl.lit(0.5)).alias(\"prop_trend\"),\n",
    "        (pl.col(range_prop_src).cast(pl.Float64, strict=False) if range_prop_src else pl.lit(0.5)).alias(\"prop_range\"),\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        (pl.col(col_p_er) if col_p_er else pl.lit(1.0)).cast(pl.Float64, strict=False).alias(\"pvals_combined_ER\"),\n",
    "        (pl.col(col_p_pd) if col_p_pd else pl.lit(1.0)).cast(pl.Float64, strict=False).alias(\"pvals_combined_PD\"),\n",
    "        (pl.col(col_p_ev) if col_p_ev else pl.lit(1.0)).cast(pl.Float64, strict=False).alias(\"pvals_combined_p_event\"),\n",
    "        (pl.col(col_t_er).cast(pl.Float64, strict=False) if col_t_er else pl.lit(None)).alias(\"tvals_ER\"),\n",
    "        (pl.col(col_t_pd).cast(pl.Float64, strict=False) if col_t_pd else pl.lit(None)).alias(\"tvals_PD\"),\n",
    "        (pl.col(col_t_ev).cast(pl.Float64, strict=False) if col_t_ev else pl.lit(None)).alias(\"tvals_p_event\"),\n",
    "        (pl.col(col_PD_IS).cast(pl.Float64, strict=False) if col_PD_IS else pl.lit(None)).alias(\"PD_IS\"),\n",
    "        (pl.col(col_ER_IS).cast(pl.Float64, strict=False) if col_ER_IS else pl.lit(None)).alias(\"ER_IS\"),\n",
    "        (pl.col(col_estab_final) if col_estab_final\n",
    "         else (pl.col(col_estab_base) if col_estab_base else pl.lit(0.75)))\n",
    "            .cast(pl.Float64, strict=False).alias(\"EstabScore_final\"),\n",
    "        (pl.col(col_decay_flag).cast(pl.Boolean, strict=False) if col_decay_flag else pl.lit(False)).alias(\"decay_flag\"),\n",
    "        (pl.col(col_stab_missing).cast(pl.Boolean, strict=False) if col_stab_missing else pl.lit(False)).alias(\"flag_stab_data_missing\"),\n",
    "        ((pl.col(col_gate_data).cast(pl.Boolean, strict=False) if col_gate_data else pl.lit(True)).fill_null(True)).alias(\"passed_data_gate\"),\n",
    "        ((pl.col(col_gate_oos).cast(pl.Boolean, strict=False) if col_gate_oos else pl.lit(True)).fill_null(True)).alias(\"flag_oos_ok\"),\n",
    "        ((pl.col(col_gate_stab).cast(pl.Boolean, strict=False) if col_gate_stab else pl.lit(True)).fill_null(True)).alias(\"passed_stability_gate\"),\n",
    "        (pl.col(col_oos_reason).cast(pl.Utf8, strict=False) if col_oos_reason else pl.lit(None, dtype=pl.Utf8)).alias(\"oos_fail_reason\"),\n",
    "        (pl.col(col_npm).cast(pl.Float64, strict=False) if col_npm else pl.lit(0.0)).alias(\"n_per_month_total\"),\n",
    "        (pl.col(col_cov).cast(pl.Float64, strict=False) if col_cov else pl.lit(0.0)).alias(\"coverage_p\"),\n",
    "        (pl.col(col_tr_tr).cast(pl.Float64, strict=False) if col_tr_tr else pl.lit(0.0)).alias(\"TR_trend_mean\"),\n",
    "        (pl.col(col_tr_rg).cast(pl.Float64, strict=False) if col_tr_rg else pl.lit(0.0)).alias(\"TR_range_mean\"),\n",
    "        (pl.col(col_ndq_tr).cast(pl.Float64, strict=False) if col_ndq_tr else pl.lit(0.0)).alias(\"NDQ_trend_mean\"),\n",
    "        (pl.col(col_ndq_rg).cast(pl.Float64, strict=False) if col_ndq_rg else pl.lit(0.0)).alias(\"NDQ_range_mean\"),\n",
    "        (pl.col(col_spread).cast(pl.Float64, strict=False) if col_spread else pl.lit(None)).alias(\"spread_cost_raw\"),\n",
    "        (pl.col(col_cost).cast(pl.Float64, strict=False) if col_cost else pl.lit(None)).alias(\"commission_cost_raw\"),\n",
    "        (pl.col(col_liq).cast(pl.Float64, strict=False) if col_liq else pl.lit(None)).alias(\"liquidity_score\"),\n",
    "        (pl.col(col_min_tr_req).cast(pl.Float64, strict=False) if col_min_tr_req else pl.lit(None)).alias(\"min_TR_required\"),\n",
    "        (pl.col(col_decay_pen).cast(pl.Float64, strict=False) if col_decay_pen else pl.lit(None)).alias(\"decay_penalty\"),\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"min_TR_required\").fill_null(default_min_TR_required).alias(\"min_TR_required\"),\n",
    "        pl.col(\"decay_penalty\").fill_null(1.0).alias(\"decay_penalty\"),\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        p_expr_to_unit_score(\"pvals_combined_ER\").alias(\"__sig_ER_p__\"),\n",
    "        p_expr_to_unit_score(\"pvals_combined_PD\").alias(\"__sig_PD_p__\"),\n",
    "        p_expr_to_unit_score(\"pvals_combined_p_event\").alias(\"__sig_EV_p__\"),\n",
    "        ((pl.col(\"PD_IS\") - 0.5).cast(pl.Float64, strict=False).clip(0.0, 0.5) / 0.5).fill_null(0.0).alias(\"__pd_edge__\"),\n",
    "        pl.col(\"ER_IS\").cast(pl.Float64, strict=False).clip(0.0, 1.0).fill_null(0.0).alias(\"__er_edge__\"),\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (SIG_WEIGHTS[\"EV\"] * pl.col(\"__sig_EV_p__\") +\n",
    "         SIG_WEIGHTS[\"PD\"] * pl.col(\"__pd_edge__\") +\n",
    "         SIG_WEIGHTS[\"ER\"] * pl.col(\"__er_edge__\"))\n",
    "        .clip(0.0, 1.0).alias(\"stat_score_bucket\")\n",
    "    )\n",
    "\n",
    "    opp_n = normalize_by_percentile(df[\"n_per_month_total\"])\n",
    "    opp_c = normalize_by_percentile(df[\"coverage_p\"])\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"__opp_n__\", opp_n),\n",
    "        pl.Series(\"__opp_c__\", opp_c),\n",
    "    ]).with_columns(\n",
    "        (OPP_WEIGHTS[\"n\"] * pl.col(\"__opp_n__\") + OPP_WEIGHTS[\"c\"] * pl.col(\"__opp_c__\"))\n",
    "        .clip(0.0, 1.0).alias(\"score_opportunity\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"EstabScore_final\").fill_null(0.75).clip(0.0, 1.0).alias(\"score_stability\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.mean_horizontal([pl.col(\"TR_trend_mean\"), pl.col(\"TR_range_mean\")]).alias(\"__TR_mean__\"),\n",
    "        pl.mean_horizontal([pl.col(\"NDQ_trend_mean\"), pl.col(\"NDQ_range_mean\")]).alias(\"__NDQ_mean__\"),\n",
    "    ])\n",
    "\n",
    "    meta[\"info\"] = {\n",
    "        \"components\": {\n",
    "            \"weights_main\": {\n",
    "                \"significance\": W_SIGNIFICANCE,\n",
    "                \"opportunity\": W_OPPORTUNITY,\n",
    "                \"stability\": W_STABILITY,\n",
    "                \"viability\": W_VIABILITY,\n",
    "                \"structure\": W_STRUCTURE,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return df, meta\n",
    "\n",
    "df_base, meta_info = build_scores_table(\n",
    "    df_sig=stab_join,\n",
    "    df_adv=stab_join,\n",
    "    df_opp=opp_df,\n",
    "    df_viab=eco_df,\n",
    ")\n",
    "\n",
    "# ====================== Componente estad√≠stico RUN-level ======================\n",
    "def _alpha_support(df: pl.DataFrame, col: str, thr: float) -> Tuple[bool, float, int]:\n",
    "    if df.is_empty() or (col not in df.columns):\n",
    "        return False, 0.0, 0\n",
    "    s = df[col].cast(pl.Float64, strict=False).drop_nulls()\n",
    "    n = s.len()\n",
    "    if n == 0:\n",
    "        return False, 0.0, 0\n",
    "    n_lt = int((s < thr).sum())\n",
    "    pct = 100.0 * float(n_lt) / float(n)\n",
    "    return (n_lt > 0), pct, n\n",
    "\n",
    "has_er, pct_er_loose, n_er = _alpha_support(df_base, \"pvals_combined_ER\", alpha_loose)\n",
    "has_pd, pct_pd_loose, n_pd = _alpha_support(df_base, \"pvals_combined_PD\", alpha_loose)\n",
    "has_ev, pct_ev_loose, n_ev = _alpha_support(df_base, \"pvals_combined_p_event\", alpha_loose)\n",
    "\n",
    "support_flags = {\"ER\": bool(has_er), \"PD\": bool(has_pd), \"p_event\": bool(has_ev)}\n",
    "support_pcts  = {\"ER\": float(pct_er_loose), \"PD\": float(pct_pd_loose), \"p_event\": float(pct_ev_loose)}\n",
    "support_count = int(sum(1 for v in support_flags.values() if v))\n",
    "\n",
    "sig_raw_ser = df_base[\"stat_score_bucket\"].cast(pl.Float64, strict=False).drop_nulls()\n",
    "n_sig_raw_valid = sig_raw_ser.len()\n",
    "\n",
    "if n_sig_raw_valid == 0:\n",
    "    weight_stat_component = 0.0\n",
    "else:\n",
    "    p20 = float(sig_raw_ser.quantile(0.20, interpolation=\"nearest\"))\n",
    "    p80 = float(sig_raw_ser.quantile(0.80, interpolation=\"nearest\"))\n",
    "    spread = p80 - p20\n",
    "    if spread <= 0.02:\n",
    "        weight_stat_component = 0.0\n",
    "    else:\n",
    "        lo, hi = 0.02, 0.10\n",
    "        w_disc = 1.0 if spread >= hi else max(0.0, (spread - lo) / (hi - lo))\n",
    "        weight_stat_component = 0.3 + 0.7 * w_disc\n",
    "\n",
    "if support_count >= 2:\n",
    "    run_significance_state = \"STRONG\"\n",
    "elif support_count == 1:\n",
    "    run_significance_state = \"BALANCED\"\n",
    "else:\n",
    "    run_significance_state = \"WEAK\" if weight_stat_component > 0 else \"NO_INFO\"\n",
    "\n",
    "print(\n",
    "    f\"[Celda 10] RUN stats-support @ Œ±_loose={alpha_loose} ‚Üí \"\n",
    "    f\"support_count={support_count} | ER={support_pcts['ER']:.2f}% \"\n",
    "    f\"PD={support_pcts['PD']:.2f}% p_event={support_pcts['p_event']:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"[Celda 10] Componente estad√≠stico ‚Üí \"\n",
    "    f\"weight_stat_component={weight_stat_component:.3f}, run_significance_state={run_significance_state}\"\n",
    ")\n",
    "\n",
    "df_base = df_base.with_columns([\n",
    "    (pl.col(\"stat_score_bucket\") * float(weight_stat_component)).clip(0.0, 1.0).alias(\"score_significance\"),\n",
    "    pl.lit(float(weight_stat_component)).alias(\"weight_stat_component\"),\n",
    "    pl.lit(run_significance_state).cast(pl.Utf8).alias(\"run_significance_state\"),\n",
    "])\n",
    "\n",
    "# ====================== Gates b√°sicos ======================\n",
    "df_base = df_base.with_columns([\n",
    "    pl.col(\"passed_data_gate\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"passed_data_gate\"),\n",
    "    pl.col(\"flag_oos_ok\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"flag_oos_ok\"),\n",
    "    pl.col(\"passed_stability_gate\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"passed_stability_gate\"),\n",
    "])\n",
    "\n",
    "# ====================== Viabilidad econ√≥mica ======================\n",
    "TR_mean  = df_base[\"__TR_mean__\"].cast(pl.Float64, strict=False).fill_null(0.0)\n",
    "NDQ_mean = df_base[\"__NDQ_mean__\"].cast(pl.Float64, strict=False).fill_null(0.0)\n",
    "\n",
    "tr_vals = TR_mean.to_list()\n",
    "score_tr_vals = [_score_tr_soft(v, float(global_min_TR)) for v in tr_vals]\n",
    "score_tr_series = pl.Series(\"__score_TR__\", score_tr_vals)\n",
    "\n",
    "ndq_scaled = normalize_by_percentile(NDQ_mean)\n",
    "score_ndq_series = pl.Series(\"__score_NDQ__\", ndq_scaled.to_list())\n",
    "\n",
    "def _cost_to_score(series: pl.Series) -> pl.Series:\n",
    "    if series.len() == 0:\n",
    "        return pl.Series([1.0] * df_base.height, dtype=pl.Float64)\n",
    "    s = series.cast(pl.Float64, strict=False)\n",
    "    non_null = s.drop_nulls()\n",
    "    if non_null.len() == 0:\n",
    "        return pl.Series([1.0] * s.len(), dtype=pl.Float64)\n",
    "    p75 = float(non_null.quantile(0.75))\n",
    "    p25 = float(non_null.quantile(0.25))\n",
    "    if p75 <= 0:\n",
    "        return pl.Series([1.0] * s.len(), dtype=pl.Float64)\n",
    "    hi = max(p75 * 2.0, p25 + 1e-9)\n",
    "    out = 1.0 - ((s - p25) / (hi - p25))\n",
    "    return out.clip(0.0, 1.0).fill_null(1.0).cast(pl.Float64)\n",
    "\n",
    "spread_ser     = df_base[\"spread_cost_raw\"] if \"spread_cost_raw\" in df_base.columns else pl.Series([], dtype=pl.Float64)\n",
    "commission_ser = df_base[\"commission_cost_raw\"] if \"commission_cost_raw\" in df_base.columns else pl.Series([], dtype=pl.Float64)\n",
    "\n",
    "spread_score = _cost_to_score(spread_ser)\n",
    "comm_score   = _cost_to_score(commission_ser)\n",
    "\n",
    "liq_series = df_base[\"liquidity_score\"] if \"liquidity_score\" in df_base.columns else pl.Series([], dtype=pl.Float64)\n",
    "liq_vals = normalize_by_percentile(liq_series).to_list() if liq_series.len() > 0 else [0.5] * df_base.height\n",
    "\n",
    "df_scores = (\n",
    "    df_base.with_columns([\n",
    "        score_tr_series,\n",
    "        score_ndq_series,\n",
    "        pl.Series(\"__score_spread__\", spread_score),\n",
    "        pl.Series(\"__score_comm__\", comm_score),\n",
    "        pl.Series(\"__score_liq__\", liq_vals),\n",
    "    ])\n",
    "    .with_columns(\n",
    "        (\n",
    "            VIA_WEIGHTS[\"TR\"] * pl.col(\"__score_TR__\") +\n",
    "            VIA_WEIGHTS[\"NDQ\"] * pl.col(\"__score_NDQ__\")\n",
    "        ).clip(0.0, 1.0).alias(\"__score_TR_NDQ__\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            0.6 * pl.col(\"__score_TR_NDQ__\") +\n",
    "            0.2 * pl.col(\"__score_spread__\") +\n",
    "            0.2 * pl.col(\"__score_liq__\")\n",
    "        ).clip(0.0, 1.0).alias(\"score_viability\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ====================== Gate min_TR_required ======================\n",
    "df_scores = df_scores.with_columns(\n",
    "    pl.col(\"min_TR_required\").cast(pl.Float64, strict=False).fill_null(default_min_TR_required).alias(\"min_TR_required\")\n",
    ")\n",
    "\n",
    "df_scores = df_scores.with_columns(\n",
    "    pl.when(pl.col(\"min_TR_required\") <= 0)\n",
    "      .then(pl.lit(True))\n",
    "      .otherwise(pl.col(\"__TR_mean__\") >= pl.col(\"min_TR_required\"))\n",
    "      .alias(\"passed_min_TR_required\")\n",
    ")\n",
    "\n",
    "df_scores = df_scores.with_columns(\n",
    "    (pl.col(\"score_viability\").is_not_null()).alias(\"passed_viability_gate\")\n",
    ").with_columns(\n",
    "    (pl.col(\"passed_viability_gate\") & pl.col(\"passed_min_TR_required\")).alias(\"passed_viability_gate\")\n",
    ")\n",
    "\n",
    "# ====================== Penalizaci√≥n de decay_penalty ======================\n",
    "df_scores = df_scores.with_columns(\n",
    "    pl.col(\"decay_penalty\").cast(pl.Float64, strict=False).fill_null(1.0).clip(0.0, 1.0).alias(\"decay_penalty\")\n",
    ")\n",
    "\n",
    "df_scores = df_scores.with_columns(\n",
    "    (pl.col(\"score_stability\") * pl.col(\"decay_penalty\"))\n",
    "    .clip(0.0, 1.0)\n",
    "    .alias(\"score_stability\")\n",
    ")\n",
    "\n",
    "# ====================== Join de estructura ======================\n",
    "df_scores = df_scores.join(struct_sym, on=\"symbol\", how=\"left\")\n",
    "\n",
    "df_scores = df_scores.with_columns([\n",
    "    pl.col(\"structure_score\").cast(pl.Float64, strict=False).alias(\"__structure_score_raw__\"),\n",
    "    (pl.col(\"vol_regime_score\").cast(pl.Float64, strict=False)\n",
    "     if \"vol_regime_score\" in df_scores.columns else pl.lit(None)).alias(\"vol_regime_score\"),\n",
    "])\n",
    "\n",
    "df_scores = df_scores.with_columns(\n",
    "    pl.when(pl.col(\"__structure_score_raw__\").is_null() & pl.col(\"vol_regime_score\").is_not_null())\n",
    "      .then(pl.col(\"vol_regime_score\"))\n",
    "      .when(pl.col(\"__structure_score_raw__\").is_not_null() & pl.col(\"vol_regime_score\").is_null())\n",
    "      .then(pl.col(\"__structure_score_raw__\"))\n",
    "      .when(pl.col(\"__structure_score_raw__\").is_not_null() & pl.col(\"vol_regime_score\").is_not_null())\n",
    "      .then(0.7 * pl.col(\"__structure_score_raw__\") + 0.3 * pl.col(\"vol_regime_score\"))\n",
    "      .otherwise(0.5)\n",
    "      .cast(pl.Float64, strict=False)\n",
    "      .clip(0.0, 1.0)\n",
    "      .alias(\"score_structure\")\n",
    ")\n",
    "\n",
    "df_scores = df_scores.with_columns([\n",
    "    pl.when(pl.col(\"structure_flag_qa\").is_null())\n",
    "      .then(pl.lit(True))\n",
    "      .otherwise(pl.col(\"structure_flag_qa\") != \"BAD\")\n",
    "      .alias(\"passed_structure_gate_loose\"),\n",
    "    pl.when(pl.col(\"structure_flag_qa\").is_null())\n",
    "      .then(pl.lit(False))\n",
    "      .otherwise(pl.col(\"structure_flag_qa\") == \"OK\")\n",
    "      .alias(\"passed_structure_gate_strict\"),\n",
    "])\n",
    "\n",
    "# ====================== SCORE_RAW + SCORE_FINAL ======================\n",
    "df_scores = df_scores.with_columns(\n",
    "    (\n",
    "        W_SIGNIFICANCE * pl.col(\"score_significance\") +\n",
    "        W_OPPORTUNITY  * pl.col(\"score_opportunity\")  +\n",
    "        W_STABILITY    * pl.col(\"score_stability\")    +\n",
    "        W_VIABILITY    * pl.col(\"score_viability\")    +\n",
    "        W_STRUCTURE    * pl.col(\"score_structure\")\n",
    "    ).clip(0.0, 1.0).alias(\"SCORE_RAW\")\n",
    ")\n",
    "\n",
    "gate_expr = (\n",
    "    pl.col(\"passed_data_gate\") &\n",
    "    pl.col(\"passed_stability_gate\") &\n",
    "    pl.col(\"flag_oos_ok\") &\n",
    "    pl.col(\"passed_viability_gate\")\n",
    ")\n",
    "\n",
    "df_scores = df_scores.with_columns(gate_expr.alias(\"gate_all\"))\n",
    "df_scores = df_scores.with_columns(pl.col(\"SCORE_RAW\").alias(\"SCORE_FINAL\"))\n",
    "\n",
    "# ====================== Reglas t√©cnicas TR/NDQ ======================\n",
    "TR_t  = pl.col(\"TR_trend_mean\")\n",
    "NDQ_t = pl.col(\"NDQ_trend_mean\")\n",
    "TR_r  = pl.col(\"TR_range_mean\")\n",
    "NDQ_r = pl.col(\"NDQ_range_mean\")\n",
    "decay = pl.col(\"decay_flag\")\n",
    "Estab = pl.col(\"EstabScore_final\")\n",
    "g_all = pl.col(\"gate_all\")\n",
    "\n",
    "df_scores = df_scores.with_columns([\n",
    "    ((TR_t >= thr_trend[\"strict\"]) & (~decay) & (Estab >= min_estab_strict) & g_all).alias(\"ok_trend_strict\"),\n",
    "    (((TR_t >= thr_trend[\"balanced\"]) | (NDQ_t >= min_ndq_trend)) & (Estab >= min_estab_balanced) & g_all).alias(\"ok_trend_balanced\"),\n",
    "    (((TR_t >= thr_trend[\"loose\"]) | (NDQ_t >= (min_ndq_trend + 0.10))) & (Estab >= min_estab_loose) & g_all).alias(\"ok_trend_loose\"),\n",
    "\n",
    "    ((TR_r >= thr_range[\"strict\"]) & (~decay) & (Estab >= min_estab_strict) & g_all).alias(\"ok_range_strict\"),\n",
    "    (((TR_r >= thr_range[\"balanced\"]) | (NDQ_r >= min_ndq_range)) & (Estab >= min_estab_balanced) & g_all).alias(\"ok_range_balanced\"),\n",
    "    (((TR_r >= thr_range[\"loose\"]) | (NDQ_r >= (min_ndq_range + 0.10))) & (Estab >= min_estab_loose) & g_all).alias(\"ok_range_loose\"),\n",
    "])\n",
    "\n",
    "# ====================== Flags de viabilidad/core/premium ======================\n",
    "df_scores = df_scores.with_columns([\n",
    "    (pl.col(\"SCORE_FINAL\") >= score_viable_thr).alias(\"flag_viable\"),\n",
    "    (pl.col(\"SCORE_FINAL\") >= score_interesting_thr).alias(\"flag_core\"),\n",
    "    (pl.col(\"SCORE_FINAL\") >= score_premium_thr).alias(\"flag_premium\"),\n",
    "])\n",
    "\n",
    "# ====================== Tabla final base ======================\n",
    "keep_cols = [\n",
    "    \"symbol\", \"preset\",\n",
    "    \"prop_trend\", \"prop_range\",\n",
    "    \"stat_score_bucket\", \"weight_stat_component\", \"run_significance_state\",\n",
    "    \"score_significance\", \"score_opportunity\", \"score_stability\",\n",
    "    \"score_viability\", \"score_structure\",\n",
    "    \"SCORE_RAW\", \"SCORE_FINAL\",\n",
    "    \"PD_IS\", \"ER_IS\",\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    \"tvals_ER\", \"tvals_PD\", \"tvals_p_event\",\n",
    "    \"n_per_month_total\", \"coverage_p\",\n",
    "    \"TR_trend_mean\", \"TR_range_mean\", \"NDQ_trend_mean\", \"NDQ_range_mean\",\n",
    "    \"spread_cost_raw\", \"commission_cost_raw\", \"liquidity_score\",\n",
    "    \"structure_score\", \"vol_regime_score\", \"structure_flag_qa\",\n",
    "    \"passed_structure_gate_loose\", \"passed_structure_gate_strict\",\n",
    "    \"decay_flag\", \"decay_penalty\",\n",
    "    \"flag_stab_data_missing\", \"EstabScore_final\",\n",
    "    \"passed_data_gate\", \"flag_oos_ok\", \"passed_stability_gate\",\n",
    "    \"min_TR_required\", \"passed_min_TR_required\",\n",
    "    \"passed_viability_gate\", \"gate_all\",\n",
    "    \"ok_trend_strict\", \"ok_trend_balanced\", \"ok_trend_loose\",\n",
    "    \"ok_range_strict\", \"ok_range_balanced\", \"ok_range_loose\",\n",
    "    \"flag_viable\", \"flag_core\", \"flag_premium\",\n",
    "]\n",
    "\n",
    "for k in keep_cols:\n",
    "    if k not in df_scores.columns:\n",
    "        df_scores = df_scores.with_columns(pl.lit(None).alias(k))\n",
    "\n",
    "final_df = df_scores.select(keep_cols)\n",
    "\n",
    "# ====================== Salud p-values (solo ausencia real) ======================\n",
    "def _col_exists(df: pl.DataFrame, col: str) -> bool:\n",
    "    return (not df.is_empty()) and (col in df.columns)\n",
    "\n",
    "def _n_nonnull(df: pl.DataFrame, col: str) -> int:\n",
    "    if not _col_exists(df, col):\n",
    "        return 0\n",
    "    return int(df.get_column(col).drop_nulls().len())\n",
    "\n",
    "cols_p = [\"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\"]\n",
    "\n",
    "print(\"[Celda 10] p-values non-null counts (final_df) ‚Üí\",\n",
    "      {c: _n_nonnull(final_df, c) for c in cols_p})\n",
    "\n",
    "missing_or_empty = {c: (not _col_exists(final_df, c)) or (_n_nonnull(final_df, c) == 0) for c in cols_p}\n",
    "\n",
    "if any(missing_or_empty.values()):\n",
    "    raise RuntimeError(\n",
    "        \"p-values combinados faltantes o vac√≠os en: \"\n",
    "        + \", \".join([c for c, bad in missing_or_empty.items() if bad])\n",
    "        + \". Revisa Celda 09 y la persistencia de stability_table_advanced.parquet.\"\n",
    "    )\n",
    "\n",
    "# ====================== QA r√°pido de scores ======================\n",
    "def _score_summary(df: pl.DataFrame, col: str, label: str):\n",
    "    if col not in df.columns:\n",
    "        print(f\"[Celda 10] {label}: columna '{col}' no encontrada.\")\n",
    "        return\n",
    "    s = df[col].drop_nulls()\n",
    "    if s.len() == 0:\n",
    "        print(f\"[Celda 10] {label}: sin valores v√°lidos.\")\n",
    "        return\n",
    "    q25 = float(s.quantile(0.25, interpolation=\"nearest\"))\n",
    "    q50 = float(s.quantile(0.50, interpolation=\"nearest\"))\n",
    "    q75 = float(s.quantile(0.75, interpolation=\"nearest\"))\n",
    "    mi  = float(s.min())\n",
    "    ma  = float(s.max())\n",
    "    print(f\"[Celda 10] {label} :: min={mi:.3f}, p25={q25:.3f}, mediana={q50:.3f}, p75={q75:.3f}, max={ma:.3f}\")\n",
    "\n",
    "print(\"---- QA: Distribuciones de sub-scores y SCORE_FINAL ----\")\n",
    "_score_summary(final_df, \"stat_score_bucket\", \"stat_score_bucket\")\n",
    "_score_summary(final_df, \"score_significance\", \"score_significance\")\n",
    "_score_summary(final_df, \"score_opportunity\",  \"score_opportunity\")\n",
    "_score_summary(final_df, \"score_stability\",    \"score_stability\")\n",
    "_score_summary(final_df, \"score_viability\",    \"score_viability\")\n",
    "_score_summary(final_df, \"score_structure\",    \"score_structure\")\n",
    "_score_summary(final_df, \"SCORE_RAW\",          \"SCORE_RAW\")\n",
    "_score_summary(final_df, \"SCORE_FINAL\",        \"SCORE_FINAL\")\n",
    "\n",
    "print(f\"[Celda 10] SCORE_FINAL > {score_viable_thr:.2f} ‚Üí {final_df.filter(pl.col('SCORE_FINAL') > score_viable_thr).height} bucket(s)\")\n",
    "print(f\"[Celda 10] SCORE_FINAL > {score_interesting_thr:.2f} ‚Üí {final_df.filter(pl.col('SCORE_FINAL') > score_interesting_thr).height} bucket(s)\")\n",
    "print(f\"[Celda 10] SCORE_FINAL > {score_premium_thr:.2f} ‚Üí {final_df.filter(pl.col('SCORE_FINAL') > score_premium_thr).height} bucket(s)\")\n",
    "\n",
    "# ====================== Escritura de artefactos ======================\n",
    "scores_path      = OUT_SCORES_DIR / \"scores_table.parquet\"\n",
    "scores_meta_path = OUT_SCORES_DIR / \"scores_meta.json\"\n",
    "\n",
    "final_df.write_parquet(scores_path)\n",
    "if (not scores_path.exists()) or scores_path.stat().st_size == 0:\n",
    "    raise RuntimeError(\"Failed writing scores_table.parquet\")\n",
    "\n",
    "meta_out = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"weights_main\": {\n",
    "        \"significance\": W_SIGNIFICANCE,\n",
    "        \"opportunity\": W_OPPORTUNITY,\n",
    "        \"stability\": W_STABILITY,\n",
    "        \"viability\": W_VIABILITY,\n",
    "        \"structure\": W_STRUCTURE,\n",
    "    },\n",
    "    \"weights_sub\": {\n",
    "        \"significance\": SIG_WEIGHTS,\n",
    "        \"opportunity\": OPP_WEIGHTS,\n",
    "        \"viability\": VIA_WEIGHTS,\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"stability_table\": str(STAB_TABLE_PATH) if STAB_TABLE_PATH.exists() else None,\n",
    "        \"stability_table_advanced\": str(STAB_ADV_PATH),\n",
    "        \"frequency_opportunity_table\": str(OPP_TABLE_PATH),\n",
    "        \"economic_viability\": str(ECO_VIAB_PATH),\n",
    "        \"structure_summary\": str(STRUCT_SUMMARY_PATH) if STRUCT_SUMMARY_PATH.exists() else None,\n",
    "        \"config\": str(CONFIG_PATH),\n",
    "    },\n",
    "    \"stat_component\": {\n",
    "        \"weight_stat_component\": float(weight_stat_component),\n",
    "        \"run_significance_state\": run_significance_state,\n",
    "        \"alpha_support_flags_loose\": support_flags,\n",
    "        \"alpha_support_pcts_loose\": support_pcts,\n",
    "        \"alpha_support_count_loose\": int(support_count),\n",
    "    },\n",
    "    \"thresholds\": {\n",
    "        \"score_final\": {\n",
    "            \"viable\": score_viable_thr,\n",
    "            \"interesting\": score_interesting_thr,\n",
    "            \"premium\": score_premium_thr,\n",
    "        },\n",
    "        \"min_TR_required_default\": default_min_TR_required,\n",
    "    },\n",
    "}\n",
    "\n",
    "scores_meta_path.write_text(json.dumps(meta_out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "meta_size = scores_meta_path.stat().st_size\n",
    "if meta_size <= 0:\n",
    "    raise RuntimeError(\"scores_meta.json not written\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(scores_path)} (OK, rows={final_df.height}, cols={final_df.width})\")\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(scores_meta_path)} (OK, bytes={meta_size})\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"scores_table_path\"] = str(scores_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"scores_meta_path\"]  = str(scores_meta_path)\n",
    "\n",
    "print(\">>> Celda 10 v2.2.0 PRO :: OK\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e34ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10B v1.0 PRO :: Selecci√≥n operativa post-SCORE + freq-only secondary\n",
      "[10B] OUT_SCORES_DIR = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "[10B] SCORES_PATH   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (exists=True)\n",
      "[10B] META_PATH     = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_meta.json (exists=True)\n",
      "[10B] CONFIG_PATH   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json (exists=True)\n",
      "[10B] alpha_loose = 0.2\n",
      "[10B] SCORE thresholds ‚Üí viable=0.3, core=0.4, premium=0.5\n",
      "[10B] scores_table loaded ‚Üí rows=166, cols=56\n",
      "[10B] columnas clave presentes: symbol=True, preset=True\n",
      "\n",
      "[10B] Conteo por tier_label (operativo):\n",
      "shape: (2, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ tier_label ‚îÜ n_rows ‚îÇ\n",
      "‚îÇ ---        ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str        ‚îÜ u32    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ CORE       ‚îÜ 12     ‚îÇ\n",
      "‚îÇ VIABLE     ‚îÜ 2      ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[10B] FREQ_ONLY_PENALTY aplicado = 0.05\n",
      "[10B] candidates_table.parquet ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\candidates_table.parquet (rows=14)\n",
      "[10B] best_per_symbol.parquet  ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\best_per_symbol.parquet (rows=7)\n",
      "[10B] freq_only_watchlist.parquet ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\freq_only_watchlist.parquet (rows=0)\n",
      "\n",
      "[10B] Top 20 best_per_symbol por SCORE_ADJ:\n",
      "shape: (7, 12)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ best_prese ‚îÜ best_tier ‚îÜ best_score ‚îÜ ‚Ä¶ ‚îÜ best_scor ‚îÜ best_scor ‚îÜ best_scor ‚îÜ best_scor ‚îÇ\n",
      "‚îÇ ---    ‚îÜ t          ‚îÜ ---       ‚îÜ _final     ‚îÜ   ‚îÜ e_opportu ‚îÜ e_stabili ‚îÜ e_viabili ‚îÜ e_structu ‚îÇ\n",
      "‚îÇ str    ‚îÜ ---        ‚îÜ str       ‚îÜ ---        ‚îÜ   ‚îÜ nity      ‚îÜ ty        ‚îÜ ty        ‚îÜ re        ‚îÇ\n",
      "‚îÇ        ‚îÜ str        ‚îÜ           ‚îÜ f64        ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ            ‚îÜ           ‚îÜ            ‚îÜ   ‚îÜ f64       ‚îÜ f64       ‚îÜ f64       ‚îÜ f64       ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BNBUSD ‚îÜ RANGE      ‚îÜ CORE      ‚îÜ 0.488358   ‚îÜ ‚Ä¶ ‚îÜ 0.7       ‚îÜ 0.811142  ‚îÜ 0.477864  ‚îÜ 0.5       ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ TREND      ‚îÜ CORE      ‚îÜ 0.479258   ‚îÜ ‚Ä¶ ‚îÜ 0.433876  ‚îÜ 0.708676  ‚îÜ 0.707754  ‚îÜ 0.737629  ‚îÇ\n",
      "‚îÇ BTCUSD ‚îÜ RANGE      ‚îÜ CORE      ‚îÜ 0.461609   ‚îÜ ‚Ä¶ ‚îÜ 0.547653  ‚îÜ 0.645267  ‚îÜ 0.578808  ‚îÜ 0.75      ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ TREND      ‚îÜ CORE      ‚îÜ 0.461201   ‚îÜ ‚Ä¶ ‚îÜ 0.434113  ‚îÜ 0.722965  ‚îÜ 0.595684  ‚îÜ 0.745004  ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ RANGE      ‚îÜ CORE      ‚îÜ 0.45238    ‚îÜ ‚Ä¶ ‚îÜ 0.546987  ‚îÜ 0.7648    ‚îÜ 0.508911  ‚îÜ 0.5       ‚îÇ\n",
      "‚îÇ XAUUSD ‚îÜ TREND      ‚îÜ CORE      ‚îÜ 0.451056   ‚îÜ ‚Ä¶ ‚îÜ 0.433697  ‚îÜ 0.732288  ‚îÜ 0.540276  ‚îÜ 0.731893  ‚îÇ\n",
      "‚îÇ LVMH   ‚îÜ TREND      ‚îÜ VIABLE    ‚îÜ 0.349748   ‚îÜ ‚Ä¶ ‚îÜ 0.055533  ‚îÜ 0.619977  ‚îÜ 0.554935  ‚îÜ 0.726602  ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[10B] Top 20 WATCH_FREQ por SCORE_ADJ:\n",
      "shape: (0, 12)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ preset ‚îÜ tier_label ‚îÜ SCORE_FINA ‚îÜ ‚Ä¶ ‚îÜ score_oppo ‚îÜ score_stab ‚îÜ score_viab ‚îÜ score_str ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---        ‚îÜ L          ‚îÜ   ‚îÜ rtunity    ‚îÜ ility      ‚îÜ ility      ‚îÜ ucture    ‚îÇ\n",
      "‚îÇ str    ‚îÜ str    ‚îÜ str        ‚îÜ ---        ‚îÜ   ‚îÜ ---        ‚îÜ ---        ‚îÜ ---        ‚îÜ ---       ‚îÇ\n",
      "‚îÇ        ‚îÜ        ‚îÜ            ‚îÜ f64        ‚îÜ   ‚îÜ f64        ‚îÜ f64        ‚îÜ f64        ‚îÜ f64       ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      ">>> Celda 10B v1.0 PRO :: OK\n"
     ]
    }
   ],
   "source": [
    "# ===================== Celda 10B v1.0 PRO ‚Äî Selecci√≥n operativa post-SCORE =====================\n",
    "# Alineada con el comportamiento real del run:\n",
    "#   - Componente de significancia puede quedar en 0.0 (weight_stat_component=0)\n",
    "#   - p_event suele venir fuerte; ER/PD sin soporte en muchos s√≠mbolos\n",
    "#   - \"freq-only\" se trata como se√±al secundaria (no bloqueante, pero penalizada)\n",
    "#\n",
    "# OBJETIVO:\n",
    "#   1) Leer outputs de Celda 10:\n",
    "#        - scores_table.parquet\n",
    "#        - scores_meta.json (si existe)\n",
    "#   2) Reconstruir flags \"freq-only\" desde p-values combinados\n",
    "#   3) Aplicar estrategia expl√≠cita:\n",
    "#        - Si flag_freq_only=True:\n",
    "#            * NO se elimina autom√°ticamente\n",
    "#            * Se clasifica como \"WATCH_FREQ\"\n",
    "#            * Penalizaci√≥n suave sobre SCORE_FINAL para ranking operativo\n",
    "#   4) Producir artefactos listos para construcci√≥n de estrategias por activo:\n",
    "#        - candidates_table.parquet (enriched)\n",
    "#        - best_per_symbol.parquet\n",
    "#        - freq_only_watchlist.parquet\n",
    "#\n",
    "# NOTA:\n",
    "#   Esta celda NO modifica la l√≥gica de scoring; solo ordena/etiqueta/selecciona.\n",
    "#   Mantiene compatibilidad con el pipeline actual.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10B v1.0 PRO :: Selecci√≥n operativa post-SCORE + freq-only secondary\")\n",
    "\n",
    "# ====================== Validaciones de GLOBAL_STATE ======================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta primero las celdas iniciales.\")\n",
    "\n",
    "paths = (GLOBAL_STATE.get(\"paths\", {}) or {})\n",
    "metrics_state = (GLOBAL_STATE.get(\"metrics\", {}) or {})\n",
    "\n",
    "if \"scores\" not in paths:\n",
    "    # fallback razonable: mismo patr√≥n que Celda 10\n",
    "    if \"stability\" in paths:\n",
    "        OUT_SCORES_DIR = Path(paths[\"stability\"]).resolve().parent / \"scores\"\n",
    "        OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        GLOBAL_STATE[\"paths\"][\"scores\"] = str(OUT_SCORES_DIR)\n",
    "    else:\n",
    "        raise RuntimeError(\"No existe GLOBAL_STATE['paths']['scores'] ni 'stability' para fallback.\")\n",
    "\n",
    "OUT_SCORES_DIR  = Path(GLOBAL_STATE[\"paths\"][\"scores\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths.get(\"metrics\", OUT_SCORES_DIR.parent / \"metrics\")).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths.get(\"diagnostics\", OUT_SCORES_DIR.parent / \"diagnostics\")).resolve()\n",
    "\n",
    "SCORES_PATH = Path(metrics_state.get(\"scores_table_path\", OUT_SCORES_DIR / \"scores_table.parquet\")).resolve()\n",
    "META_PATH   = Path(metrics_state.get(\"scores_meta_path\",  OUT_SCORES_DIR / \"scores_meta.json\")).resolve()\n",
    "\n",
    "CONFIG_PATH = Path(paths.get(\"config\", OUT_DIAG_DIR / \"config.json\")).resolve()\n",
    "\n",
    "print(f\"[10B] OUT_SCORES_DIR = {OUT_SCORES_DIR}\")\n",
    "print(f\"[10B] SCORES_PATH   = {SCORES_PATH} (exists={SCORES_PATH.exists()})\")\n",
    "print(f\"[10B] META_PATH     = {META_PATH} (exists={META_PATH.exists()})\")\n",
    "print(f\"[10B] CONFIG_PATH   = {CONFIG_PATH} (exists={CONFIG_PATH.exists()})\")\n",
    "\n",
    "if not SCORES_PATH.exists():\n",
    "    raise RuntimeError(f\"No existe scores_table.parquet. Ejecuta Celda 10 primero: {SCORES_PATH}\")\n",
    "\n",
    "# ====================== Cargar config ======================\n",
    "cfg_state = GLOBAL_STATE.get(\"config\", {})\n",
    "if isinstance(cfg_state, dict) and cfg_state:\n",
    "    cfg = cfg_state\n",
    "else:\n",
    "    if not CONFIG_PATH.exists():\n",
    "        raise RuntimeError(f\"Config no encontrado ni en GLOBAL_STATE ni en disco: {CONFIG_PATH}\")\n",
    "    cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "stats      = cfg.get(\"stats\", {}) or {}\n",
    "scores_cfg = cfg.get(\"scores\", {}) or {}\n",
    "\n",
    "alpha_loose = float(stats.get(\"alpha_loose\", 0.20))\n",
    "\n",
    "score_thr_cfg         = (scores_cfg.get(\"score_thresholds\", {}) or {})\n",
    "score_viable_thr      = float(score_thr_cfg.get(\"viable\",      0.30))\n",
    "score_interesting_thr = float(score_thr_cfg.get(\"interesting\", 0.40))\n",
    "score_premium_thr     = float(score_thr_cfg.get(\"premium\",     0.50))\n",
    "\n",
    "print(f\"[10B] alpha_loose = {alpha_loose}\")\n",
    "print(f\"[10B] SCORE thresholds ‚Üí viable={score_viable_thr}, core={score_interesting_thr}, premium={score_premium_thr}\")\n",
    "\n",
    "# ====================== Cargar scores_table ======================\n",
    "df = pl.read_parquet(SCORES_PATH)\n",
    "\n",
    "def _norm(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    out = df\n",
    "    if \"symbol\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"symbol\")\n",
    "        )\n",
    "    if \"preset\" in out.columns:\n",
    "        out = out.with_columns(\n",
    "            pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars().alias(\"preset\")\n",
    "        )\n",
    "    return out\n",
    "\n",
    "df = _norm(df)\n",
    "\n",
    "print(f\"[10B] scores_table loaded ‚Üí rows={df.height}, cols={df.width}\")\n",
    "print(f\"[10B] columnas clave presentes: symbol={'symbol' in df.columns}, preset={'preset' in df.columns}\")\n",
    "\n",
    "# ====================== Reconstrucci√≥n de flags \"freq-only\" ======================\n",
    "# Definici√≥n operativa:\n",
    "#   - p_event significativo a nivel loose\n",
    "#   - ER y PD NO significativos a nivel loose\n",
    "# Esto replica la l√≥gica QA de 09C sin depender de esa celda.\n",
    "for c in [\"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\"]:\n",
    "    if c not in df.columns:\n",
    "        df = df.with_columns(pl.lit(None).alias(c))\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"pvals_combined_ER\").cast(pl.Float64, strict=False).fill_null(1.0).alias(\"pvals_combined_ER\"),\n",
    "    pl.col(\"pvals_combined_PD\").cast(pl.Float64, strict=False).fill_null(1.0).alias(\"pvals_combined_PD\"),\n",
    "    pl.col(\"pvals_combined_p_event\").cast(pl.Float64, strict=False).fill_null(1.0).alias(\"pvals_combined_p_event\"),\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    (\n",
    "        (pl.col(\"pvals_combined_ER\") <= pl.lit(alpha_loose)) |\n",
    "        (pl.col(\"pvals_combined_PD\") <= pl.lit(alpha_loose))\n",
    "    ).alias(\"edge_support_loose\"),\n",
    "    (pl.col(\"pvals_combined_p_event\") <= pl.lit(alpha_loose)).alias(\"p_event_loose\"),\n",
    "]).with_columns(\n",
    "    (pl.col(\"p_event_loose\") & (~pl.col(\"edge_support_loose\"))).alias(\"flag_freq_only\")\n",
    ")\n",
    "\n",
    "# ====================== Estrategia expl√≠cita de tratamiento \"freq-only\" ======================\n",
    "# Pol√≠tica:\n",
    "#   - No se descarta por defecto: puede ser √∫til como se√±al de liquidez/actividad del r√©gimen.\n",
    "#   - Se rebaja prioridad en ranking:\n",
    "#       SCORE_ADJ = SCORE_FINAL - penalty\n",
    "#   - Se etiqueta como WATCH_FREQ si no alcanza CORE/PREMIUM por otros componentes.\n",
    "#\n",
    "# Par√°metro de penalizaci√≥n:\n",
    "FREQ_ONLY_PENALTY = 0.05  # suave; suficiente para ordenar sin destruir candidatos\n",
    "\n",
    "for c in [\"SCORE_FINAL\", \"SCORE_RAW\", \"gate_all\", \"passed_data_gate\", \"passed_stability_gate\", \"flag_oos_ok\",\n",
    "          \"score_opportunity\", \"score_stability\", \"score_viability\", \"score_structure\"]:\n",
    "    if c not in df.columns:\n",
    "        df = df.with_columns(pl.lit(None).alias(c))\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"SCORE_FINAL\").cast(pl.Float64, strict=False).fill_null(0.0).alias(\"SCORE_FINAL\"),\n",
    "    pl.col(\"gate_all\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"gate_all\"),\n",
    "    pl.col(\"passed_data_gate\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"passed_data_gate\"),\n",
    "    pl.col(\"passed_stability_gate\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"passed_stability_gate\"),\n",
    "    pl.col(\"flag_oos_ok\").cast(pl.Boolean, strict=False).fill_null(True).alias(\"flag_oos_ok\"),\n",
    "])\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"flag_freq_only\"))\n",
    "      .then((pl.col(\"SCORE_FINAL\") - pl.lit(FREQ_ONLY_PENALTY)).clip(0.0, 1.0))\n",
    "      .otherwise(pl.col(\"SCORE_FINAL\"))\n",
    "      .alias(\"SCORE_ADJ\")\n",
    ")\n",
    "\n",
    "# Etiqueta de tier operativa\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"SCORE_FINAL\") >= pl.lit(score_premium_thr)).alias(\"tier_premium\"),\n",
    "    (pl.col(\"SCORE_FINAL\") >= pl.lit(score_interesting_thr)).alias(\"tier_core\"),\n",
    "    (pl.col(\"SCORE_FINAL\") >= pl.lit(score_viable_thr)).alias(\"tier_viable\"),\n",
    "])\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"tier_premium\"))\n",
    "      .then(pl.lit(\"PREMIUM\"))\n",
    "      .when(pl.col(\"tier_core\"))\n",
    "      .then(pl.lit(\"CORE\"))\n",
    "      .when(pl.col(\"tier_viable\"))\n",
    "      .then(pl.lit(\"VIABLE\"))\n",
    "      .when(pl.col(\"flag_freq_only\"))\n",
    "      .then(pl.lit(\"WATCH_FREQ\"))\n",
    "      .otherwise(pl.lit(\"DROP\"))\n",
    "      .alias(\"tier_label\")\n",
    ")\n",
    "\n",
    "# ====================== Reglas de selecci√≥n ======================\n",
    "# 1) Universo operativo base:\n",
    "#    - Gate_all True (si existe l√≥gica de gates dura)\n",
    "#    - y tier_label != DROP\n",
    "#\n",
    "# 2) \"WATCH_FREQ\" se conserva solo como lista secundaria\n",
    "#    - √∫til para exploraci√≥n y para no perder se√±ales de r√©gimen.\n",
    "#\n",
    "df_oper = df.filter(\n",
    "    (pl.col(\"gate_all\") == True) &\n",
    "    (pl.col(\"tier_label\") != \"DROP\")\n",
    ")\n",
    "\n",
    "# ====================== Selecci√≥n \"best per symbol\" ======================\n",
    "has_preset = \"preset\" in df_oper.columns\n",
    "\n",
    "if has_preset:\n",
    "    # Elegimos el mejor preset por s√≠mbolo usando SCORE_ADJ\n",
    "    best_per_symbol = (\n",
    "        df_oper\n",
    "        .sort([\"symbol\", \"SCORE_ADJ\"], descending=[False, True])\n",
    "        .group_by(\"symbol\")\n",
    "        .agg([\n",
    "            pl.first(\"preset\").alias(\"best_preset\"),\n",
    "            pl.first(\"tier_label\").alias(\"best_tier\"),\n",
    "            pl.first(\"SCORE_FINAL\").alias(\"best_score_final\"),\n",
    "            pl.first(\"SCORE_ADJ\").alias(\"best_score_adj\"),\n",
    "            pl.first(\"flag_freq_only\").alias(\"best_flag_freq_only\"),\n",
    "            pl.first(\"edge_support_loose\").alias(\"best_edge_support_loose\"),\n",
    "            pl.first(\"p_event_loose\").alias(\"best_p_event_loose\"),\n",
    "            pl.first(\"score_opportunity\").alias(\"best_score_opportunity\"),\n",
    "            pl.first(\"score_stability\").alias(\"best_score_stability\"),\n",
    "            pl.first(\"score_viability\").alias(\"best_score_viability\"),\n",
    "            pl.first(\"score_structure\").alias(\"best_score_structure\"),\n",
    "        ])\n",
    "        .sort(\"best_score_adj\", descending=True)\n",
    "    )\n",
    "else:\n",
    "    best_per_symbol = (\n",
    "        df_oper\n",
    "        .sort([\"symbol\", \"SCORE_ADJ\"], descending=[False, True])\n",
    "        .group_by(\"symbol\")\n",
    "        .agg([\n",
    "            pl.first(\"tier_label\").alias(\"best_tier\"),\n",
    "            pl.first(\"SCORE_FINAL\").alias(\"best_score_final\"),\n",
    "            pl.first(\"SCORE_ADJ\").alias(\"best_score_adj\"),\n",
    "            pl.first(\"flag_freq_only\").alias(\"best_flag_freq_only\"),\n",
    "            pl.first(\"edge_support_loose\").alias(\"best_edge_support_loose\"),\n",
    "            pl.first(\"p_event_loose\").alias(\"best_p_event_loose\"),\n",
    "            pl.first(\"score_opportunity\").alias(\"best_score_opportunity\"),\n",
    "            pl.first(\"score_stability\").alias(\"best_score_stability\"),\n",
    "            pl.first(\"score_viability\").alias(\"best_score_viability\"),\n",
    "            pl.first(\"score_structure\").alias(\"best_score_structure\"),\n",
    "        ])\n",
    "        .sort(\"best_score_adj\", descending=True)\n",
    "    )\n",
    "\n",
    "# ====================== Watchlist freq-only ======================\n",
    "freq_only_watch = (\n",
    "    df_oper\n",
    "    .filter(pl.col(\"tier_label\") == \"WATCH_FREQ\")\n",
    "    .sort(\"SCORE_ADJ\", descending=True)\n",
    ")\n",
    "\n",
    "# ====================== Outputs ======================\n",
    "candidates_path = OUT_SCORES_DIR / \"candidates_table.parquet\"\n",
    "best_path       = OUT_SCORES_DIR / \"best_per_symbol.parquet\"\n",
    "freq_path       = OUT_SCORES_DIR / \"freq_only_watchlist.parquet\"\n",
    "\n",
    "df_oper.write_parquet(candidates_path)\n",
    "best_per_symbol.write_parquet(best_path)\n",
    "freq_only_watch.write_parquet(freq_path)\n",
    "\n",
    "print(\"\\n[10B] Conteo por tier_label (operativo):\")\n",
    "print(\n",
    "    df_oper.group_by(\"tier_label\")\n",
    "           .agg(pl.len().alias(\"n_rows\"))\n",
    "           .sort(\"n_rows\", descending=True)\n",
    ")\n",
    "\n",
    "print(f\"\\n[10B] FREQ_ONLY_PENALTY aplicado = {FREQ_ONLY_PENALTY}\")\n",
    "print(f\"[10B] candidates_table.parquet ‚Üí {candidates_path} (rows={df_oper.height})\")\n",
    "print(f\"[10B] best_per_symbol.parquet  ‚Üí {best_path} (rows={best_per_symbol.height})\")\n",
    "print(f\"[10B] freq_only_watchlist.parquet ‚Üí {freq_path} (rows={freq_only_watch.height})\")\n",
    "\n",
    "print(\"\\n[10B] Top 20 best_per_symbol por SCORE_ADJ:\")\n",
    "print(best_per_symbol.head(20))\n",
    "\n",
    "print(\"\\n[10B] Top 20 WATCH_FREQ por SCORE_ADJ:\")\n",
    "print(freq_only_watch.select(\n",
    "    [c for c in [\n",
    "        \"symbol\", \"preset\", \"tier_label\",\n",
    "        \"SCORE_FINAL\", \"SCORE_ADJ\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "        \"score_opportunity\", \"score_stability\", \"score_viability\", \"score_structure\"\n",
    "    ] if c in freq_only_watch.columns]\n",
    ").head(20))\n",
    "\n",
    "# ====================== Persistir paths en GLOBAL_STATE ======================\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"candidates_table_path\"]      = str(candidates_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"best_per_symbol_path\"]       = str(best_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"freq_only_watchlist_path\"]   = str(freq_path)\n",
    "\n",
    "print(\"\\n>>> Celda 10B v1.0 PRO :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "118b7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10C :: Modelo dual TREND/RANGE + TYPE unificado + Shortlist por activo\n",
      "[Celda 10C] RUN_ID = 20251218_190810\n",
      "[Celda 10C] Config strategy_profiles ‚Üí min_PROFILE_base=0.35, margin_ambiguous=0.08, type_secondary_margin=0.06, min_SCORE_FINAL_for_shortlist=0.3, min_PROFILE_for_shortlist_trend=0.5, min_PROFILE_for_shortlist_range=0.5, min_PROFILE_for_shortlist_breakout=0.5, max_trend=8, max_mr=8, max_breakout=8, require_structure_loose_gate=True, require_viability_gate=True\n",
      "[Celda 10C] weights_trend_follow = {'regime': 0.35000000000000003, 'ER': 0.35000000000000003, 'TR': 0.20000000000000004, 'stability': 0.10000000000000002}\n",
      "[Celda 10C] weights_mean_reversion = {'regime': 0.35000000000000003, 'PD': 0.35000000000000003, 'NDQ': 0.20000000000000004, 'structure_mid': 0.10000000000000002}\n",
      "[Celda 10C] weights_breakout = {'NDQ_tail': 0.4, 'TR': 0.4, 'viability': 0.2}\n",
      "üìÅ INPUT ‚Üí scores_table = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (rows=166, cols=56)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_profiles.parquet (rows=83, cols=55)\n",
      "[Celda 10C] TREND: sin activos con core_score_trend ‚â• 0.50; fallback best-available.\n",
      "[Celda 10C] RANGE: sin activos con core_score_range ‚â• 0.50; fallback best-available.\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (rows=7, cols=58)\n",
      "---- QA 10C: Diagn√≥stico de selecci√≥n por familia ----\n",
      "[Celda 10C] TREND: candidates_base=7 selected=7 mode=fallback_best_available min_profile_thr=0.5\n",
      "[Celda 10C] RANGE: candidates_base=7 selected=7 mode=fallback_best_available min_profile_thr=0.5\n",
      "[Celda 10C] BREAKOUT: candidates_base=7 selected=4 mode=strict_family_threshold min_profile_thr=0.5\n",
      "---- QA 10C: Distribuci√≥n TYPE/PROFILE ----\n",
      "shape: (2, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ strategy_type_primary ‚îÇ\n",
      "‚îÇ ---                   ‚îÇ\n",
      "‚îÇ struct[2]             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ {\"BREAKOUT\",9}        ‚îÇ\n",
      "‚îÇ {\"MEAN_REVERSION\",74} ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "shape: (3, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ strategy_profile ‚îÇ\n",
      "‚îÇ ---              ‚îÇ\n",
      "‚îÇ struct[2]        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ {\"MR\",18}        ‚îÇ\n",
      "‚îÇ {\"MIXED\",58}     ‚îÇ\n",
      "‚îÇ {\"BREAKOUT\",7}   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- QA 10C: ER/PD missingness ----\n",
      "shape: (1, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ missing_ER ‚îÇ\n",
      "‚îÇ ---        ‚îÇ\n",
      "‚îÇ struct[2]  ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ {false,83} ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "shape: (1, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ missing_PD ‚îÇ\n",
      "‚îÇ ---        ‚îÇ\n",
      "‚îÇ struct[2]  ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ {false,83} ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- QA 10C: Top 10 activos por core_score (cada familia) ----\n",
      "[Celda 10C] Top TREND core_score_trend\n",
      "  rank=1 symbol=BTCUSD core_score_trend=0.2564 SCORE_FINAL=0.4616 TF=0.4464169142371361 MR=0.46160913684711175 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=2 symbol=XAUAUD core_score_trend=0.2228 SCORE_FINAL=0.4793 TF=0.4792580008416095 MR=0.4376551371450595 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=3 symbol=ETHUSD core_score_trend=0.2070 SCORE_FINAL=0.4524 TF=0.42580907877245033 MR=0.4523796737558716 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=4 symbol=XAUUSD core_score_trend=0.2008 SCORE_FINAL=0.4511 TF=0.4510560000612118 MR=0.4491476763765174 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=5 symbol=BNBUSD core_score_trend=0.1676 SCORE_FINAL=0.4884 TF=0.4494545930962193 MR=0.488358254973641 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=6 symbol=XAUEUR core_score_trend=0.1605 SCORE_FINAL=0.4612 TF=0.4612010613585497 MR=0.4483850531541141 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=7 symbol=LVMH core_score_trend=0.1204 SCORE_FINAL=0.3497 TF=0.34974787738309676 MR=0.3458169956478191 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=8 symbol=AAVUSD core_score_trend=0.0985 SCORE_FINAL=0.4118 TF=0.4078365153682241 MR=0.41178330246456324 type=MEAN_REVERSION profile=MIXED\n",
      "  rank=9 symbol=CHFJPY core_score_trend=0.0876 SCORE_FINAL=0.4035 TF=0.36715922009954616 MR=0.403502803536704 type=MEAN_REVERSION profile=MIXED\n",
      "  rank=10 symbol=LTCUSD core_score_trend=0.0866 SCORE_FINAL=0.3729 TF=0.3729135106763135 MR=0.31841375966629815 type=MEAN_REVERSION profile=MIXED\n",
      "[Celda 10C] Top RANGE core_score_range\n",
      "  rank=1 symbol=VECUSD core_score_range=0.3798 SCORE_FINAL=0.3422 TF=0.34223200283193483 MR=0.32527760993967253 type=MEAN_REVERSION profile=MR\n",
      "  rank=2 symbol=CADCHF core_score_range=0.3755 SCORE_FINAL=0.4319 TF=0.43190070369211236 MR=0.40533335427283507 type=MEAN_REVERSION profile=MR\n",
      "  rank=3 symbol=ALGUSD core_score_range=0.3754 SCORE_FINAL=0.3936 TF=0.3935974184910988 MR=0.3840830247013643 type=MEAN_REVERSION profile=MR\n",
      "  rank=4 symbol=BNBUSD core_score_range=0.3753 SCORE_FINAL=0.4884 TF=0.4494545930962193 MR=0.488358254973641 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=5 symbol=NZDCHF core_score_range=0.3746 SCORE_FINAL=0.4128 TF=0.41267695265487 MR=0.4128224562125232 type=MEAN_REVERSION profile=MR\n",
      "  rank=6 symbol=AUDNZD core_score_range=0.3743 SCORE_FINAL=0.3776 TF=0.3775614848124451 MR=0.3727453938145919 type=MEAN_REVERSION profile=MR\n",
      "  rank=7 symbol=AUDCHF core_score_range=0.3740 SCORE_FINAL=0.4283 TF=0.4283048943622819 MR=0.40760288914994225 type=MEAN_REVERSION profile=MR\n",
      "  rank=8 symbol=AUDCAD core_score_range=0.3736 SCORE_FINAL=0.3754 TF=0.3551987930177532 MR=0.3753520742851173 type=MEAN_REVERSION profile=MR\n",
      "  rank=9 symbol=EURJPY core_score_range=0.3733 SCORE_FINAL=0.3967 TF=0.39673007680224626 MR=0.3779236127191127 type=MEAN_REVERSION profile=MR\n",
      "  rank=10 symbol=NZDCAD core_score_range=0.3727 SCORE_FINAL=0.3480 TF=0.3480059418513926 MR=0.3438669876416017 type=MEAN_REVERSION profile=MR\n",
      "[Celda 10C] Top BREAKOUT core_score_breakout\n",
      "  rank=1 symbol=BTCUSD core_score_breakout=0.6908 SCORE_FINAL=0.4616 TF=0.4464169142371361 MR=0.46160913684711175 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=2 symbol=XAUAUD core_score_breakout=0.6344 SCORE_FINAL=0.4793 TF=0.4792580008416095 MR=0.4376551371450595 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=3 symbol=ETHUSD core_score_breakout=0.5517 SCORE_FINAL=0.4524 TF=0.42580907877245033 MR=0.4523796737558716 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=4 symbol=XAUUSD core_score_breakout=0.5480 SCORE_FINAL=0.4511 TF=0.4510560000612118 MR=0.4491476763765174 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=5 symbol=XAUEUR core_score_breakout=0.4855 SCORE_FINAL=0.4612 TF=0.4612010613585497 MR=0.4483850531541141 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=6 symbol=BNBUSD core_score_breakout=0.4570 SCORE_FINAL=0.4884 TF=0.4494545930962193 MR=0.488358254973641 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=7 symbol=LVMH core_score_breakout=0.4189 SCORE_FINAL=0.3497 TF=0.34974787738309676 MR=0.3458169956478191 type=BREAKOUT profile=BREAKOUT\n",
      "  rank=8 symbol=AAVUSD core_score_breakout=0.3035 SCORE_FINAL=0.4118 TF=0.4078365153682241 MR=0.41178330246456324 type=MEAN_REVERSION profile=MIXED\n",
      "  rank=9 symbol=EURNOK core_score_breakout=0.2874 SCORE_FINAL=0.3863 TF=0.38634474352650133 MR=0.37373348528158595 type=BREAKOUT profile=MIXED\n",
      "  rank=10 symbol=EURCZK core_score_breakout=0.2872 SCORE_FINAL=0.3544 TF=0.35438222471720904 MR=0.3402269155017152 type=MEAN_REVERSION profile=MIXED\n",
      ">>> Celda 10C :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 10C ‚Äî Clasificaci√≥n y Shortlist por Activo (modelo dual TREND/RANGE + TYPE unificado)\n",
    "# =============================================================================\n",
    "# Correcciones aplicadas:\n",
    "#   - Usa bucket-level (symbol, preset) sin deduplicar antes de separar familias.\n",
    "#   - Asset-level = 1 fila por symbol.\n",
    "#   - ER_asset/PD_asset se construyen desde ER_IS/PD_IS (contrato Celda 10).\n",
    "#   - Missingness neutral y flags honestos.\n",
    "#   - Gates asset-level expl√≠citos por familia.\n",
    "#   - Shortlist por familia + fallback.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, List, Optional\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10C :: Modelo dual TREND/RANGE + TYPE unificado + Shortlist por activo\")\n",
    "\n",
    "# ============================ Guardas b√°sicas ================================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 10C][ERROR] GLOBAL_STATE no existe. Ejecuta celdas previas.\")\n",
    "\n",
    "for key in (\"paths\", \"run_id\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 10C][ERROR] Falta GLOBAL_STATE['{key}'].\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\")\n",
    "paths: Dict[str, Any] = GLOBAL_STATE[\"paths\"]\n",
    "metrics_state: Dict[str, Any] = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "\n",
    "OUT_SCORES_DIR = Path(paths.get(\"scores\", Path(paths[\"stability\"]).parent / \"scores\")).resolve()\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scores_table_path = Path(\n",
    "    metrics_state.get(\"scores_table_path\", OUT_SCORES_DIR / \"scores_table.parquet\")\n",
    ").resolve()\n",
    "\n",
    "CONFIG_PATH = Path(paths.get(\"config\", Path(paths[\"diagnostics\"]) / \"config.json\")).resolve()\n",
    "\n",
    "def _require_file(p: Path, required: bool = True, label: str = \"\"):\n",
    "    if required and not p.exists():\n",
    "        raise RuntimeError(f\"[Celda 10C][ERROR] Falta input requerido {label}: {p}\")\n",
    "\n",
    "# ============================ Cargar config ==================================\n",
    "if \"config\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"config\"], dict):\n",
    "    cfg = GLOBAL_STATE[\"config\"]\n",
    "else:\n",
    "    _require_file(CONFIG_PATH, required=True, label=\"config\")\n",
    "    cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "policy = cfg.get(\"policy\", {}) or {}\n",
    "spcfg  = cfg.get(\"strategy_profiles\", {}) or {}\n",
    "\n",
    "min_PROFILE_base      = float(spcfg.get(\"min_PROFILE_base\", spcfg.get(\"min_profile_score\", 0.35)))\n",
    "margin_ambiguous      = float(spcfg.get(\"margin_ambiguous\", 0.08))\n",
    "type_secondary_margin = float(spcfg.get(\"type_secondary_margin\", 0.06))\n",
    "\n",
    "min_SCORE_FINAL_for_shortlist = float(spcfg.get(\"min_SCORE_FINAL_for_shortlist\", 0.30))\n",
    "\n",
    "min_PROFILE_for_shortlist_trend = float(spcfg.get(\"min_PROFILE_for_shortlist_trend\",\n",
    "                                                  spcfg.get(\"min_profile_score_for_shortlist\", 0.50)))\n",
    "min_PROFILE_for_shortlist_range = float(spcfg.get(\"min_PROFILE_for_shortlist_range\",\n",
    "                                                  spcfg.get(\"min_profile_score_for_shortlist\", 0.50)))\n",
    "min_PROFILE_for_shortlist_breakout = float(spcfg.get(\"min_PROFILE_for_shortlist_breakout\",\n",
    "                                                     spcfg.get(\"min_profile_score_for_shortlist\", 0.50)))\n",
    "\n",
    "max_trend    = int(spcfg.get(\"max_trend\", 8))\n",
    "max_mr       = int(spcfg.get(\"max_mr\", 8))\n",
    "max_breakout = int(spcfg.get(\"max_breakout\", 8))\n",
    "\n",
    "require_structure_loose_gate = bool(spcfg.get(\"require_structure_loose_gate\", True))\n",
    "require_viability_gate       = bool(spcfg.get(\"require_viability_gate\", True))\n",
    "\n",
    "weights_trend_follow = spcfg.get(\"weights_trend_follow\", None) or {\n",
    "    \"regime\": 0.35, \"ER\": 0.35, \"TR\": 0.20, \"stability\": 0.10\n",
    "}\n",
    "weights_mean_rev = spcfg.get(\"weights_mean_reversion\", None) or {\n",
    "    \"regime\": 0.35, \"PD\": 0.35, \"NDQ\": 0.20, \"structure_mid\": 0.10\n",
    "}\n",
    "weights_breakout = spcfg.get(\"weights_breakout\", None) or {\n",
    "    \"NDQ_tail\": 0.40, \"TR\": 0.40, \"viability\": 0.20\n",
    "}\n",
    "\n",
    "def _normalize_weights(w: Dict[str, float], label: str) -> Dict[str, float]:\n",
    "    ww = {k: float(v) for k, v in (w or {}).items() if v is not None}\n",
    "    s = sum(max(v, 0.0) for v in ww.values())\n",
    "    if not math.isfinite(s) or s <= 0:\n",
    "        print(f\"[Celda 10C][WARN] Pesos inv√°lidos en {label}; usando defaults sin normalizar.\")\n",
    "        return ww\n",
    "    return {k: max(v, 0.0)/s for k, v in ww.items()}\n",
    "\n",
    "weights_trend_follow = _normalize_weights(weights_trend_follow, \"weights_trend_follow\")\n",
    "weights_mean_rev     = _normalize_weights(weights_mean_rev, \"weights_mean_reversion\")\n",
    "weights_breakout     = _normalize_weights(weights_breakout, \"weights_breakout\")\n",
    "\n",
    "global_min_TR = float(policy.get(\"min_TR_after_cost\", 0.25))\n",
    "if not math.isfinite(global_min_TR) or global_min_TR <= 0:\n",
    "    global_min_TR = 0.25\n",
    "\n",
    "print(f\"[Celda 10C] RUN_ID = {RUN_ID}\")\n",
    "print(\n",
    "    \"[Celda 10C] Config strategy_profiles ‚Üí \"\n",
    "    f\"min_PROFILE_base={min_PROFILE_base}, margin_ambiguous={margin_ambiguous}, \"\n",
    "    f\"type_secondary_margin={type_secondary_margin}, \"\n",
    "    f\"min_SCORE_FINAL_for_shortlist={min_SCORE_FINAL_for_shortlist}, \"\n",
    "    f\"min_PROFILE_for_shortlist_trend={min_PROFILE_for_shortlist_trend}, \"\n",
    "    f\"min_PROFILE_for_shortlist_range={min_PROFILE_for_shortlist_range}, \"\n",
    "    f\"min_PROFILE_for_shortlist_breakout={min_PROFILE_for_shortlist_breakout}, \"\n",
    "    f\"max_trend={max_trend}, max_mr={max_mr}, max_breakout={max_breakout}, \"\n",
    "    f\"require_structure_loose_gate={require_structure_loose_gate}, \"\n",
    "    f\"require_viability_gate={require_viability_gate}\"\n",
    ")\n",
    "print(f\"[Celda 10C] weights_trend_follow = {weights_trend_follow}\")\n",
    "print(f\"[Celda 10C] weights_mean_reversion = {weights_mean_rev}\")\n",
    "print(f\"[Celda 10C] weights_breakout = {weights_breakout}\")\n",
    "\n",
    "# ============================ Cargar scores_table ============================\n",
    "_require_file(scores_table_path, required=True, label=\"scores_table\")\n",
    "scores_raw = pl.read_parquet(scores_table_path)\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        df = df.with_columns(pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars())\n",
    "    if \"preset\" in df.columns:\n",
    "        df = df.with_columns(pl.col(\"preset\").cast(pl.Utf8, strict=False).fill_null(\"\").str.to_uppercase().str.strip_chars())\n",
    "    return df\n",
    "\n",
    "scores_raw = _norm_sym(scores_raw)\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí scores_table = {str(scores_table_path)} (rows={scores_raw.height}, cols={len(scores_raw.columns)})\")\n",
    "if scores_raw.is_empty():\n",
    "    raise RuntimeError(\"[Celda 10C][ERROR] scores_table.parquet est√° vac√≠o.\")\n",
    "\n",
    "# ============================ Helpers de tipo ================================\n",
    "def _ensure_float(df: pl.DataFrame, name: str, default: float = 0.0) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Float64, strict=False).fill_null(default).alias(name))\n",
    "    return df.with_columns(pl.lit(default).cast(pl.Float64).alias(name))\n",
    "\n",
    "def _ensure_bool(df: pl.DataFrame, name: str, default: bool = False) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Boolean, strict=False).fill_null(default).alias(name))\n",
    "    return df.with_columns(pl.lit(default).cast(pl.Boolean).alias(name))\n",
    "\n",
    "def _best_per_symbol(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    sort_cols = [c for c in [\n",
    "        \"SCORE_FINAL\",\n",
    "        \"score_stability\",\n",
    "        \"score_viability\",\n",
    "        \"score_structure\",\n",
    "        \"score_significance\",\n",
    "        \"score_opportunity\",\n",
    "    ] if c in df.columns]\n",
    "    if not sort_cols:\n",
    "        sort_cols = [\"symbol\"]\n",
    "    return (\n",
    "        df.sort(by=sort_cols + [\"symbol\"], descending=[True]*len(sort_cols) + [False])\n",
    "          .unique(subset=[\"symbol\"], keep=\"first\")\n",
    "    )\n",
    "\n",
    "def _suffix_cols(df: pl.DataFrame, suf: str) -> pl.DataFrame:\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        if c == \"symbol\":\n",
    "            continue\n",
    "        rename_map[c] = f\"{c}__{suf}\"\n",
    "    return df.rename(rename_map)\n",
    "\n",
    "# ============================ Normalizar columnas base =======================\n",
    "float_candidates = [\n",
    "    \"SCORE_FINAL\",\n",
    "    \"score_significance\",\n",
    "    \"score_opportunity\",\n",
    "    \"score_stability\",\n",
    "    \"score_viability\",\n",
    "    \"score_structure\",\n",
    "    \"TR_trend_mean\", \"TR_range_mean\",\n",
    "    \"NDQ_trend_mean\", \"NDQ_range_mean\",\n",
    "    \"structure_score\",\n",
    "    \"prop_trend\", \"prop_range\",\n",
    "    \"PD_IS\", \"ER_IS\",\n",
    "]\n",
    "for c in float_candidates:\n",
    "    scores_raw = _ensure_float(scores_raw, c, 0.0)\n",
    "\n",
    "bool_candidates = [\n",
    "    \"gate_all\",\n",
    "    \"passed_structure_gate_loose\",\n",
    "    \"passed_structure_gate_strict\",\n",
    "    \"passed_data_gate\",\n",
    "    \"flag_oos_ok\",\n",
    "    \"passed_stability_gate\",\n",
    "    \"passed_viability_gate\",\n",
    "    \"cond_significance\",\n",
    "]\n",
    "default_true = {\n",
    "    \"gate_all\", \"passed_structure_gate_loose\", \"passed_data_gate\",\n",
    "    \"flag_oos_ok\", \"passed_stability_gate\", \"passed_viability_gate\",\n",
    "    \"cond_significance\",\n",
    "}\n",
    "for c in bool_candidates:\n",
    "    scores_raw = _ensure_bool(scores_raw, c, c in default_true)\n",
    "\n",
    "if \"structure_flag_qa\" not in scores_raw.columns:\n",
    "    scores_raw = scores_raw.with_columns(pl.lit(None, dtype=pl.Utf8).alias(\"structure_flag_qa\"))\n",
    "\n",
    "# ============================ Separar por familias TREND/RANGE ============\n",
    "has_preset = \"preset\" in scores_raw.columns\n",
    "preset_values = set(scores_raw.get_column(\"preset\").unique().to_list()) if has_preset else set()\n",
    "use_dual_preset = has_preset and (\"TREND\" in preset_values or \"RANGE\" in preset_values)\n",
    "\n",
    "if use_dual_preset:\n",
    "    df_trend_all = scores_raw.filter(pl.col(\"preset\") == \"TREND\")\n",
    "    df_range_all = scores_raw.filter(pl.col(\"preset\") == \"RANGE\")\n",
    "\n",
    "    df_trend = _best_per_symbol(df_trend_all) if not df_trend_all.is_empty() else pl.DataFrame()\n",
    "    df_range = _best_per_symbol(df_range_all) if not df_range_all.is_empty() else pl.DataFrame()\n",
    "\n",
    "    if df_trend.is_empty() and df_range.is_empty():\n",
    "        print(\"[Celda 10C][WARN] preset existe pero no hay filas TREND/RANGE detectables. Fallback mono.\")\n",
    "        use_dual_preset = False\n",
    "\n",
    "if use_dual_preset:\n",
    "    df_trend_s = _suffix_cols(df_trend, \"trend\")\n",
    "    df_range_s = _suffix_cols(df_range, \"range\")\n",
    "    asset_wide = df_trend_s.join(df_range_s, on=\"symbol\", how=\"full\")\n",
    "else:\n",
    "    dup_ct = scores_raw.height - scores_raw.unique(subset=[\"symbol\"]).height\n",
    "    if dup_ct > 0:\n",
    "        print(f\"[Celda 10C][WARN] {dup_ct} duplicados por symbol sin preset usable; se conserva mejor fila.\")\n",
    "    asset_wide = _best_per_symbol(scores_raw)\n",
    "\n",
    "# ============================ Coalescers asset-level =========================\n",
    "def _coalesce_float(df: pl.DataFrame, cols: List[str], alias: str, default: Optional[float] = 0.0) -> pl.DataFrame:\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    if not existing:\n",
    "        return df.with_columns((pl.lit(None) if default is None else pl.lit(default)).cast(pl.Float64).alias(alias))\n",
    "    expr = None\n",
    "    for c in existing:\n",
    "        e = pl.col(c).cast(pl.Float64, strict=False)\n",
    "        expr = e if expr is None else expr.fill_null(e)\n",
    "    if default is None:\n",
    "        return df.with_columns(expr.alias(alias))\n",
    "    return df.with_columns(expr.fill_null(default).alias(alias))\n",
    "\n",
    "def _max_of(df: pl.DataFrame, cols: List[str], alias: str, default: float = 0.0) -> pl.DataFrame:\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    if not existing:\n",
    "        return df.with_columns(pl.lit(default).cast(pl.Float64).alias(alias))\n",
    "    return df.with_columns(\n",
    "        pl.max_horizontal(*[pl.col(c).cast(pl.Float64, strict=False) for c in existing]).fill_null(default).alias(alias)\n",
    "    )\n",
    "\n",
    "def _or_bool(df: pl.DataFrame, cols: List[str], alias: str, default: bool = False) -> pl.DataFrame:\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    if not existing:\n",
    "        return df.with_columns(pl.lit(default).cast(pl.Boolean).alias(alias))\n",
    "    expr = None\n",
    "    for c in existing:\n",
    "        e = pl.col(c).cast(pl.Boolean, strict=False)\n",
    "        expr = e if expr is None else (expr | e)\n",
    "    return df.with_columns(expr.fill_null(default).alias(alias))\n",
    "\n",
    "def _and_bool(df: pl.DataFrame, cols: List[str], alias: str, default: bool = False) -> pl.DataFrame:\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    if not existing:\n",
    "        return df.with_columns(pl.lit(default).cast(pl.Boolean).alias(alias))\n",
    "    expr = None\n",
    "    for c in existing:\n",
    "        e = pl.col(c).cast(pl.Boolean, strict=False)\n",
    "        expr = e if expr is None else (expr & e)\n",
    "    return df.with_columns(expr.fill_null(default).alias(alias))\n",
    "\n",
    "# ============================ Construir asset_df =============================\n",
    "asset_df = asset_wide\n",
    "\n",
    "if use_dual_preset:\n",
    "    for c in bool_candidates:\n",
    "        asset_df = _ensure_bool(asset_df, f\"{c}__trend\", c in default_true)\n",
    "        asset_df = _ensure_bool(asset_df, f\"{c}__range\", c in default_true)\n",
    "\n",
    "    for c in float_candidates:\n",
    "        asset_df = _ensure_float(asset_df, f\"{c}__trend\", 0.0)\n",
    "        asset_df = _ensure_float(asset_df, f\"{c}__range\", 0.0)\n",
    "\n",
    "    asset_df = asset_df.with_columns([\n",
    "        pl.col(\"SCORE_FINAL__trend\").alias(\"SCORE_FINAL_trend\"),\n",
    "        pl.col(\"SCORE_FINAL__range\").alias(\"SCORE_FINAL_range\"),\n",
    "        pl.col(\"gate_all__trend\").alias(\"gate_all_trend\"),\n",
    "        pl.col(\"gate_all__range\").alias(\"gate_all_range\"),\n",
    "        pl.col(\"passed_structure_gate_loose__trend\").alias(\"passed_structure_gate_loose_trend\"),\n",
    "        pl.col(\"passed_structure_gate_loose__range\").alias(\"passed_structure_gate_loose_range\"),\n",
    "        pl.col(\"passed_viability_gate__trend\").alias(\"passed_viability_gate_trend\"),\n",
    "        pl.col(\"passed_viability_gate__range\").alias(\"passed_viability_gate_range\"),\n",
    "    ])\n",
    "\n",
    "    asset_df = _max_of(asset_df, [\"SCORE_FINAL__trend\", \"SCORE_FINAL__range\"], \"SCORE_FINAL\", 0.0)\n",
    "    for s in [\"score_significance\",\"score_opportunity\",\"score_stability\",\"score_viability\",\"score_structure\"]:\n",
    "        asset_df = _max_of(asset_df, [f\"{s}__trend\", f\"{s}__range\"], s, 0.0)\n",
    "\n",
    "    asset_df = _coalesce_float(asset_df, [\"TR_trend_mean__trend\",\"TR_trend_mean__range\"], \"TR_trend_mean\", 0.0)\n",
    "    asset_df = _coalesce_float(asset_df, [\"TR_range_mean__range\",\"TR_range_mean__trend\"], \"TR_range_mean\", 0.0)\n",
    "    asset_df = _coalesce_float(asset_df, [\"NDQ_trend_mean__trend\",\"NDQ_trend_mean__range\"], \"NDQ_trend_mean\", 0.0)\n",
    "    asset_df = _coalesce_float(asset_df, [\"NDQ_range_mean__range\",\"NDQ_range_mean__trend\"], \"NDQ_range_mean\", 0.0)\n",
    "\n",
    "    asset_df = _coalesce_float(asset_df, [\"structure_score__trend\",\"structure_score__range\"], \"structure_score\", 0.5)\n",
    "\n",
    "    # Preservar nulls reales de r√©gimen\n",
    "    asset_df = _coalesce_float(asset_df, [\"prop_trend__trend\",\"prop_trend__range\"], \"prop_trend\", None)\n",
    "    asset_df = _coalesce_float(asset_df, [\"prop_range__trend\",\"prop_range__range\"], \"prop_range\", None)\n",
    "\n",
    "    # ‚úÖ FIX 2: ER/PD asset-level desde contrato Celda 10\n",
    "    # Missing flags honestos: si ambos presets vienen null ‚Üí missing True\n",
    "    asset_df = asset_df.with_columns([\n",
    "        (pl.col(\"ER_IS__trend\").is_null() & pl.col(\"ER_IS__range\").is_null()).alias(\"missing_ER\"),\n",
    "        (pl.col(\"PD_IS__trend\").is_null() & pl.col(\"PD_IS__range\").is_null()).alias(\"missing_PD\"),\n",
    "    ])\n",
    "\n",
    "    asset_df = _max_of(asset_df, [\"ER_IS__trend\",\"ER_IS__range\"], \"ER_asset\", 0.0)\n",
    "    asset_df = _max_of(asset_df, [\"PD_IS__trend\",\"PD_IS__range\"], \"PD_asset\", 0.0)\n",
    "\n",
    "    asset_df = _or_bool(asset_df, [\"passed_data_gate__trend\",\"passed_data_gate__range\"], \"passed_data_gate\", True)\n",
    "    asset_df = _or_bool(asset_df, [\"flag_oos_ok__trend\",\"flag_oos_ok__range\"], \"flag_oos_ok\", True)\n",
    "    asset_df = _or_bool(asset_df, [\"passed_stability_gate__trend\",\"passed_stability_gate__range\"], \"passed_stability_gate\", True)\n",
    "    asset_df = _or_bool(asset_df, [\"passed_viability_gate__trend\",\"passed_viability_gate__range\"], \"passed_viability_gate\", True)\n",
    "    asset_df = _or_bool(asset_df, [\"passed_structure_gate_loose__trend\",\"passed_structure_gate_loose__range\"], \"passed_structure_gate_loose\", True)\n",
    "    asset_df = _or_bool(asset_df, [\"passed_structure_gate_strict__trend\",\"passed_structure_gate_strict__range\"], \"passed_structure_gate_strict\", False)\n",
    "\n",
    "else:\n",
    "    for c in bool_candidates:\n",
    "        asset_df = _ensure_bool(asset_df, c, c in default_true)\n",
    "    for c in float_candidates:\n",
    "        asset_df = _ensure_float(asset_df, c, 0.0)\n",
    "\n",
    "    asset_df = asset_df.with_columns([\n",
    "        pl.col(\"SCORE_FINAL\").alias(\"SCORE_FINAL_trend\"),\n",
    "        pl.col(\"SCORE_FINAL\").alias(\"SCORE_FINAL_range\"),\n",
    "        pl.col(\"gate_all\").alias(\"gate_all_trend\"),\n",
    "        pl.col(\"gate_all\").alias(\"gate_all_range\"),\n",
    "        pl.col(\"passed_structure_gate_loose\").alias(\"passed_structure_gate_loose_trend\"),\n",
    "        pl.col(\"passed_structure_gate_loose\").alias(\"passed_structure_gate_loose_range\"),\n",
    "        pl.col(\"passed_viability_gate\").alias(\"passed_viability_gate_trend\"),\n",
    "        pl.col(\"passed_viability_gate\").alias(\"passed_viability_gate_range\"),\n",
    "    ])\n",
    "\n",
    "    # ‚úÖ FIX 2 mono: usar ER_IS / PD_IS\n",
    "    asset_df = asset_df.with_columns([\n",
    "        pl.col(\"ER_IS\").alias(\"ER_asset\"),\n",
    "        pl.col(\"PD_IS\").alias(\"PD_asset\"),\n",
    "        pl.col(\"ER_IS\").is_null().alias(\"missing_ER\"),\n",
    "        pl.col(\"PD_IS\").is_null().alias(\"missing_PD\"),\n",
    "    ])\n",
    "\n",
    "# ============================ Missingness neutral (r√©gimen) ==================\n",
    "def _missing_flag(colname: str) -> pl.Expr:\n",
    "    return pl.col(colname).is_null()\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    _missing_flag(\"prop_trend\").alias(\"missing_prop_trend\") if \"prop_trend\" in asset_df.columns else pl.lit(True).alias(\"missing_prop_trend\"),\n",
    "    _missing_flag(\"prop_range\").alias(\"missing_prop_range\") if \"prop_range\" in asset_df.columns else pl.lit(True).alias(\"missing_prop_range\"),\n",
    "])\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.when(pl.col(\"missing_prop_trend\") & pl.col(\"missing_prop_range\"))\n",
    "      .then(pl.lit(0.50))\n",
    "      .when(pl.col(\"missing_prop_trend\") & (~pl.col(\"missing_prop_range\")))\n",
    "      .then((1.0 - pl.col(\"prop_range\")).clip(0.0, 1.0))\n",
    "      .otherwise(pl.col(\"prop_trend\").fill_null(0.50).clip(0.0, 1.0))\n",
    "      .alias(\"prop_trend_eff\"),\n",
    "\n",
    "    pl.when(pl.col(\"missing_prop_trend\") & pl.col(\"missing_prop_range\"))\n",
    "      .then(pl.lit(0.50))\n",
    "      .when(pl.col(\"missing_prop_range\") & (~pl.col(\"missing_prop_trend\")))\n",
    "      .then((1.0 - pl.col(\"prop_trend\")).clip(0.0, 1.0))\n",
    "      .otherwise(pl.col(\"prop_range\").fill_null(0.50).clip(0.0, 1.0))\n",
    "      .alias(\"prop_range_eff\"),\n",
    "])\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.col(\"ER_asset\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"ER_asset\"),\n",
    "    pl.col(\"PD_asset\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"PD_asset\"),\n",
    "])\n",
    "\n",
    "for nm in [\"TR_trend_mean\",\"TR_range_mean\",\"NDQ_trend_mean\",\"NDQ_range_mean\"]:\n",
    "    if nm not in asset_df.columns:\n",
    "        asset_df = asset_df.with_columns(pl.lit(None).cast(pl.Float64).alias(nm))\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    _missing_flag(\"TR_trend_mean\").alias(\"missing_TR_trend\"),\n",
    "    _missing_flag(\"TR_range_mean\").alias(\"missing_TR_range\"),\n",
    "    _missing_flag(\"NDQ_trend_mean\").alias(\"missing_NDQ_trend\"),\n",
    "    _missing_flag(\"NDQ_range_mean\").alias(\"missing_NDQ_range\"),\n",
    "])\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.col(\"TR_trend_mean\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"TR_trend_mean\"),\n",
    "    pl.col(\"TR_range_mean\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"TR_range_mean\"),\n",
    "    pl.col(\"NDQ_trend_mean\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"NDQ_trend_mean\"),\n",
    "    pl.col(\"NDQ_range_mean\").cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0).alias(\"NDQ_range_mean\"),\n",
    "])\n",
    "\n",
    "if \"score_structure\" not in asset_df.columns:\n",
    "    asset_df = asset_df.with_columns(pl.lit(0.50).alias(\"score_structure\"))\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.col(\"score_structure\").cast(pl.Float64, strict=False).fill_null(0.50).clip(0.0, 1.0).alias(\"score_structure\")\n",
    ")\n",
    "\n",
    "# ============================ Componentes auxiliares =========================\n",
    "asset_df = asset_df.with_columns(\n",
    "    (1.0 - (pl.col(\"score_structure\") - 0.5).abs() * 2.0).clip(0.0, 1.0).alias(\"__structure_mid__\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.max_horizontal(\"NDQ_trend_mean\", \"NDQ_range_mean\").clip(0.0, 1.0).alias(\"__ndq_tail__\"),\n",
    "    pl.max_horizontal(\"TR_trend_mean\", \"TR_range_mean\").clip(0.0, 1.0).alias(\"__tr_any__\"),\n",
    "])\n",
    "\n",
    "# ============================ TYPE scores unificados ========================\n",
    "wTF = weights_trend_follow\n",
    "wMR = weights_mean_rev\n",
    "wBO = weights_breakout\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    (\n",
    "        wTF.get(\"regime\",0.0)    * pl.col(\"prop_trend_eff\") +\n",
    "        wTF.get(\"ER\",0.0)        * pl.col(\"ER_asset\") +\n",
    "        wTF.get(\"TR\",0.0)        * pl.col(\"TR_trend_mean\") +\n",
    "        wTF.get(\"stability\",0.0) * pl.col(\"score_stability\")\n",
    "    ).clip(0.0, 1.0).alias(\"score_trend_follow\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    (\n",
    "        wMR.get(\"regime\",0.0)        * pl.col(\"prop_range_eff\") +\n",
    "        wMR.get(\"PD\",0.0)            * pl.col(\"PD_asset\") +\n",
    "        wMR.get(\"NDQ\",0.0)           * pl.col(\"NDQ_range_mean\") +\n",
    "        wMR.get(\"structure_mid\",0.0) * pl.col(\"__structure_mid__\")\n",
    "    ).clip(0.0, 1.0).alias(\"score_mean_reversion\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    (\n",
    "        wBO.get(\"NDQ_tail\",0.0) * pl.col(\"__ndq_tail__\") +\n",
    "        wBO.get(\"TR\",0.0)       * pl.col(\"__tr_any__\") +\n",
    "        wBO.get(\"viability\",0.0)* pl.col(\"score_viability\")\n",
    "    ).clip(0.0, 1.0).alias(\"score_breakout\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.max_horizontal(\"score_trend_follow\",\"score_mean_reversion\",\"score_breakout\").alias(\"__type_max__\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.when(pl.col(\"__type_max__\") == pl.col(\"score_trend_follow\")).then(pl.lit(\"TREND_FOLLOW\"))\n",
    "      .when(pl.col(\"__type_max__\") == pl.col(\"score_mean_reversion\")).then(pl.lit(\"MEAN_REVERSION\"))\n",
    "      .otherwise(pl.lit(\"BREAKOUT\"))\n",
    "      .alias(\"strategy_type_primary\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.when(pl.col(\"__type_max__\") == pl.col(\"score_trend_follow\"))\n",
    "      .then(pl.max_horizontal(\"score_mean_reversion\",\"score_breakout\"))\n",
    "      .when(pl.col(\"__type_max__\") == pl.col(\"score_mean_reversion\"))\n",
    "      .then(pl.max_horizontal(\"score_trend_follow\",\"score_breakout\"))\n",
    "      .otherwise(pl.max_horizontal(\"score_trend_follow\",\"score_mean_reversion\"))\n",
    "      .alias(\"__type_second__\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    (pl.col(\"__type_max__\") - pl.col(\"__type_second__\")).clip(0.0, 1.0).alias(\"__type_margin__\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.when(pl.col(\"__type_margin__\") < type_secondary_margin)\n",
    "      .then(\n",
    "          pl.when(pl.col(\"strategy_type_primary\") == \"TREND_FOLLOW\")\n",
    "            .then(pl.when(pl.col(\"score_mean_reversion\") >= pl.col(\"score_breakout\")).then(pl.lit(\"MEAN_REVERSION\")).otherwise(pl.lit(\"BREAKOUT\")))\n",
    "            .when(pl.col(\"strategy_type_primary\") == \"MEAN_REVERSION\")\n",
    "            .then(pl.when(pl.col(\"score_trend_follow\") >= pl.col(\"score_breakout\")).then(pl.lit(\"TREND_FOLLOW\")).otherwise(pl.lit(\"BREAKOUT\")))\n",
    "            .otherwise(pl.when(pl.col(\"score_trend_follow\") >= pl.col(\"score_mean_reversion\")).then(pl.lit(\"TREND_FOLLOW\")).otherwise(pl.lit(\"MEAN_REVERSION\")))\n",
    "      )\n",
    "      .otherwise(pl.lit(None, dtype=pl.Utf8))\n",
    "      .alias(\"strategy_type_secondary\")\n",
    ")\n",
    "\n",
    "# ============================ PROFILE = traducci√≥n del TYPE ==================\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.col(\"__type_max__\").alias(\"strategy_profile_max\"),\n",
    "    pl.col(\"__type_margin__\").alias(\"strategy_profile_margin\"),\n",
    "])\n",
    "\n",
    "asset_df = asset_df.with_columns(\n",
    "    pl.when(pl.col(\"strategy_profile_max\") < min_PROFILE_base).then(pl.lit(\"MIXED\"))\n",
    "      .when(pl.col(\"strategy_profile_margin\") < margin_ambiguous).then(pl.lit(\"MIXED\"))\n",
    "      .when(pl.col(\"strategy_type_primary\") == \"TREND_FOLLOW\").then(pl.lit(\"TREND\"))\n",
    "      .when(pl.col(\"strategy_type_primary\") == \"MEAN_REVERSION\").then(pl.lit(\"MR\"))\n",
    "      .otherwise(pl.lit(\"BREAKOUT\"))\n",
    "      .alias(\"strategy_profile\")\n",
    ")\n",
    "\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.col(\"score_trend_follow\").alias(\"strategy_profile_score_trend\"),\n",
    "    pl.col(\"score_mean_reversion\").alias(\"strategy_profile_score_mr\"),\n",
    "    pl.col(\"score_breakout\").alias(\"strategy_profile_score_breakout\"),\n",
    "])\n",
    "\n",
    "# ============================ Gates asset-level expl√≠citos ===================\n",
    "asset_df = _ensure_bool(asset_df, \"gate_all_trend\", True)\n",
    "asset_df = _ensure_bool(asset_df, \"gate_all_range\", True)\n",
    "\n",
    "asset_df = _or_bool(asset_df, [\"gate_all_trend\",\"gate_all_range\"], \"gate_asset_any\", True)\n",
    "asset_df = _and_bool(asset_df, [\"gate_all_trend\",\"gate_all_range\"], \"gate_asset_both_strict\", False)\n",
    "\n",
    "# ============================ Core scores por familia/tipo ===================\n",
    "asset_df = asset_df.with_columns([\n",
    "    pl.col(\"score_trend_follow\").alias(\"core_score_trend\"),\n",
    "    pl.col(\"score_mean_reversion\").alias(\"core_score_range\"),\n",
    "    pl.col(\"score_breakout\").alias(\"core_score_breakout\"),\n",
    "])\n",
    "\n",
    "# ============================ OUTPUT perfiles ================================\n",
    "profiles_cols = [\n",
    "    \"symbol\",\n",
    "    \"SCORE_FINAL_trend\",\"SCORE_FINAL_range\",\n",
    "    \"gate_all_trend\",\"gate_all_range\",\n",
    "    \"gate_asset_any\",\"gate_asset_both_strict\",\n",
    "    \"score_trend_follow\",\"score_mean_reversion\",\"score_breakout\",\n",
    "    \"strategy_type_primary\",\"strategy_type_secondary\",\n",
    "    \"__type_max__\",\"__type_second__\",\"__type_margin__\",\n",
    "    \"strategy_profile\",\"strategy_profile_max\",\"strategy_profile_margin\",\n",
    "    \"strategy_profile_score_trend\",\"strategy_profile_score_mr\",\"strategy_profile_score_breakout\",\n",
    "    \"core_score_trend\",\"core_score_range\",\"core_score_breakout\",\n",
    "    \"SCORE_FINAL\",\n",
    "    \"score_significance\",\"score_opportunity\",\"score_stability\",\"score_viability\",\"score_structure\",\n",
    "    \"prop_trend\",\"prop_range\",\"prop_trend_eff\",\"prop_range_eff\",\n",
    "    \"ER_asset\",\"PD_asset\",\n",
    "    \"TR_trend_mean\",\"TR_range_mean\",\"NDQ_trend_mean\",\"NDQ_range_mean\",\n",
    "    \"structure_score\",\"structure_flag_qa\",\n",
    "    \"passed_structure_gate_loose\",\"passed_structure_gate_strict\",\n",
    "    \"passed_data_gate\",\"flag_oos_ok\",\"passed_stability_gate\",\"passed_viability_gate\",\n",
    "    \"missing_prop_trend\",\"missing_prop_range\",\n",
    "    \"missing_ER\",\"missing_PD\",\n",
    "    \"missing_TR_trend\",\"missing_TR_range\",\n",
    "    \"missing_NDQ_trend\",\"missing_NDQ_range\",\n",
    "]\n",
    "\n",
    "profiles_df = asset_df.select([c for c in profiles_cols if c in asset_df.columns])\n",
    "\n",
    "asset_strategy_profiles_path = OUT_SCORES_DIR / \"asset_strategy_profiles.parquet\"\n",
    "profiles_df.write_parquet(asset_strategy_profiles_path)\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(asset_strategy_profiles_path)} (rows={profiles_df.height}, cols={len(profiles_df.columns)})\")\n",
    "\n",
    "# ============================ Shortlist por familia ==========================\n",
    "def _family_base_expr(fam: str) -> pl.Expr:\n",
    "    if fam == \"TREND\":\n",
    "        expr = (pl.col(\"gate_all_trend\") == True) & (pl.col(\"SCORE_FINAL_trend\") >= min_SCORE_FINAL_for_shortlist)\n",
    "        if require_structure_loose_gate and \"passed_structure_gate_loose_trend\" in asset_df.columns:\n",
    "            expr = expr & (pl.col(\"passed_structure_gate_loose_trend\") == True)\n",
    "        if require_viability_gate and \"passed_viability_gate_trend\" in asset_df.columns:\n",
    "            expr = expr & (pl.col(\"passed_viability_gate_trend\") == True)\n",
    "        return expr\n",
    "\n",
    "    if fam == \"RANGE\":\n",
    "        expr = (pl.col(\"gate_all_range\") == True) & (pl.col(\"SCORE_FINAL_range\") >= min_SCORE_FINAL_for_shortlist)\n",
    "        if require_structure_loose_gate and \"passed_structure_gate_loose_range\" in asset_df.columns:\n",
    "            expr = expr & (pl.col(\"passed_structure_gate_loose_range\") == True)\n",
    "        if require_viability_gate and \"passed_viability_gate_range\" in asset_df.columns:\n",
    "            expr = expr & (pl.col(\"passed_viability_gate_range\") == True)\n",
    "        return expr\n",
    "\n",
    "    expr = (pl.col(\"gate_asset_any\") == True) & (pl.col(\"SCORE_FINAL\") >= min_SCORE_FINAL_for_shortlist)\n",
    "    if require_viability_gate:\n",
    "        expr = expr & (pl.col(\"passed_viability_gate\") == True)\n",
    "    return expr\n",
    "\n",
    "def _build_family_shortlist(\n",
    "    df: pl.DataFrame,\n",
    "    family_label: str,\n",
    "    score_col: str,\n",
    "    rank_col: str,\n",
    "    max_n: int,\n",
    "    min_profile_thr: float,\n",
    ") -> Tuple[pl.DataFrame, Dict[str, Any]]:\n",
    "\n",
    "    diag: Dict[str, Any] = {\n",
    "        \"family\": family_label,\n",
    "        \"score_col\": score_col,\n",
    "        \"rank_col\": rank_col,\n",
    "        \"max_n\": int(max_n),\n",
    "        \"min_profile_thr\": float(min_profile_thr),\n",
    "        \"n_candidates_base\": 0,\n",
    "        \"n_selected\": 0,\n",
    "        \"selection_mode\": \"NONE\",\n",
    "    }\n",
    "\n",
    "    if max_n <= 0 or score_col not in df.columns:\n",
    "        return pl.DataFrame({\"symbol\": pl.Series([], dtype=pl.Utf8), rank_col: pl.Series([], dtype=pl.Int64)}), diag\n",
    "\n",
    "    base_expr = _family_base_expr(family_label)\n",
    "    df_candidates = df.filter(base_expr & (pl.col(score_col) > 0.0))\n",
    "    diag[\"n_candidates_base\"] = int(df_candidates.height)\n",
    "\n",
    "    if df_candidates.is_empty():\n",
    "        print(f\"[Celda 10C] {family_label}: sin candidatos que pasen gates base.\")\n",
    "        return pl.DataFrame({\"symbol\": pl.Series([], dtype=pl.Utf8), rank_col: pl.Series([], dtype=pl.Int64)}), diag\n",
    "\n",
    "    df_strict = df_candidates.filter(pl.col(score_col) >= min_profile_thr)\n",
    "\n",
    "    if not df_strict.is_empty():\n",
    "        df_sel = df_strict\n",
    "        diag[\"selection_mode\"] = \"strict_family_threshold\"\n",
    "    else:\n",
    "        print(f\"[Celda 10C] {family_label}: sin activos con {score_col} ‚â• {min_profile_thr:.2f}; fallback best-available.\")\n",
    "        df_sel = df_candidates\n",
    "        diag[\"selection_mode\"] = \"fallback_best_available\"\n",
    "\n",
    "    df_sel = (\n",
    "        df_sel\n",
    "        .sort(by=[score_col, \"SCORE_FINAL\", \"symbol\"], descending=[True, True, False])\n",
    "        .with_row_index(rank_col, offset=1)\n",
    "        .select([\"symbol\", rank_col])\n",
    "        .head(max_n)\n",
    "    )\n",
    "\n",
    "    diag[\"n_selected\"] = int(df_sel.height)\n",
    "    return df_sel, diag\n",
    "\n",
    "selection_diag: Dict[str, Any] = {}\n",
    "\n",
    "asset_df = _ensure_bool(asset_df, \"passed_structure_gate_loose_trend\", True)\n",
    "asset_df = _ensure_bool(asset_df, \"passed_structure_gate_loose_range\", True)\n",
    "asset_df = _ensure_bool(asset_df, \"passed_viability_gate_trend\", True)\n",
    "asset_df = _ensure_bool(asset_df, \"passed_viability_gate_range\", True)\n",
    "\n",
    "trend_rank_df, selection_diag[\"TREND\"] = _build_family_shortlist(\n",
    "    asset_df, \"TREND\", \"core_score_trend\", \"shortlist_rank_trend\",\n",
    "    max_trend, min_PROFILE_for_shortlist_trend\n",
    ")\n",
    "\n",
    "range_rank_df, selection_diag[\"RANGE\"] = _build_family_shortlist(\n",
    "    asset_df, \"RANGE\", \"core_score_range\", \"shortlist_rank_range\",\n",
    "    max_mr, min_PROFILE_for_shortlist_range\n",
    ")\n",
    "\n",
    "break_rank_df, selection_diag[\"BREAKOUT\"] = _build_family_shortlist(\n",
    "    asset_df, \"BREAKOUT\", \"core_score_breakout\", \"shortlist_rank_breakout\",\n",
    "    max_breakout, min_PROFILE_for_shortlist_breakout\n",
    ")\n",
    "\n",
    "# ============================ Integrar ranks en perfiles =====================\n",
    "profiles_df = profiles_df.join(trend_rank_df, on=\"symbol\", how=\"left\")\n",
    "profiles_df = profiles_df.join(range_rank_df, on=\"symbol\", how=\"left\")\n",
    "profiles_df = profiles_df.join(break_rank_df, on=\"symbol\", how=\"left\")\n",
    "\n",
    "# ============================ Construir shortlist final ======================\n",
    "shortlist_expr = (\n",
    "    pl.col(\"shortlist_rank_trend\").is_not_null() |\n",
    "    pl.col(\"shortlist_rank_range\").is_not_null() |\n",
    "    pl.col(\"shortlist_rank_breakout\").is_not_null()\n",
    ")\n",
    "\n",
    "asset_shortlist_df = (\n",
    "    profiles_df\n",
    "    .filter(shortlist_expr)\n",
    "    .with_columns([\n",
    "        pl.min_horizontal([\n",
    "            pl.col(\"shortlist_rank_trend\").fill_null(10_000),\n",
    "            pl.col(\"shortlist_rank_range\").fill_null(10_000),\n",
    "            pl.col(\"shortlist_rank_breakout\").fill_null(10_000),\n",
    "        ]).alias(\"__best_rank__\")\n",
    "    ])\n",
    "    .sort(by=[\"__best_rank__\", \"SCORE_FINAL\", \"symbol\"], descending=[False, True, False])\n",
    "    .drop(\"__best_rank__\")\n",
    ")\n",
    "\n",
    "asset_strategy_shortlist_path = OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\"\n",
    "asset_shortlist_df.write_parquet(asset_strategy_shortlist_path)\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(asset_strategy_shortlist_path)} (rows={asset_shortlist_df.height}, cols={len(asset_shortlist_df.columns)})\")\n",
    "\n",
    "# ============================ Prints QA cr√≠ticos =============================\n",
    "print(\"---- QA 10C: Diagn√≥stico de selecci√≥n por familia ----\")\n",
    "for fam, d in selection_diag.items():\n",
    "    print(\n",
    "        f\"[Celda 10C] {fam}: candidates_base={d['n_candidates_base']} \"\n",
    "        f\"selected={d['n_selected']} mode={d['selection_mode']} \"\n",
    "        f\"min_profile_thr={d['min_profile_thr']}\"\n",
    "    )\n",
    "\n",
    "print(\"---- QA 10C: Distribuci√≥n TYPE/PROFILE ----\")\n",
    "try:\n",
    "    print(asset_df.select(pl.col(\"strategy_type_primary\").value_counts()))\n",
    "    print(asset_df.select(pl.col(\"strategy_profile\").value_counts()))\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 10C] WARN value_counts TYPE/PROFILE: {e}\")\n",
    "\n",
    "print(\"---- QA 10C: ER/PD missingness ----\")\n",
    "try:\n",
    "    if \"missing_ER\" in asset_df.columns:\n",
    "        print(asset_df.select(pl.col(\"missing_ER\").value_counts()))\n",
    "    if \"missing_PD\" in asset_df.columns:\n",
    "        print(asset_df.select(pl.col(\"missing_PD\").value_counts()))\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 10C] WARN value_counts missing ER/PD: {e}\")\n",
    "\n",
    "print(\"---- QA 10C: Top 10 activos por core_score (cada familia) ----\")\n",
    "def _print_top(df: pl.DataFrame, col: str, label: str):\n",
    "    if col not in df.columns:\n",
    "        print(f\"[Celda 10C] {label}: sin columna {col}\")\n",
    "        return\n",
    "    top = df.select([\"symbol\", col, \"SCORE_FINAL\", \"SCORE_FINAL_trend\", \"SCORE_FINAL_range\", \"strategy_type_primary\", \"strategy_profile\"]) \\\n",
    "            .sort(by=[col, \"SCORE_FINAL\"], descending=[True, True], nulls_last=True) \\\n",
    "            .head(10)\n",
    "    print(f\"[Celda 10C] {label}\")\n",
    "    for i, r in enumerate(top.iter_rows(named=True), start=1):\n",
    "        print(f\"  rank={i} symbol={r['symbol']} {col}={float(r[col]):.4f} SCORE_FINAL={float(r['SCORE_FINAL']):.4f} \"\n",
    "              f\"TF={r.get('SCORE_FINAL_trend')} MR={r.get('SCORE_FINAL_range')} \"\n",
    "              f\"type={r.get('strategy_type_primary')} profile={r.get('strategy_profile')}\")\n",
    "\n",
    "_print_top(asset_df, \"core_score_trend\", \"Top TREND core_score_trend\")\n",
    "_print_top(asset_df, \"core_score_range\", \"Top RANGE core_score_range\")\n",
    "_print_top(asset_df, \"core_score_breakout\", \"Top BREAKOUT core_score_breakout\")\n",
    "\n",
    "# ============================ GLOBAL_STATE =====================\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"asset_strategy_profiles_path\"]  = str(asset_strategy_profiles_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"asset_strategy_shortlist_path\"] = str(asset_strategy_shortlist_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c10C\"] = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"use_dual_preset\": bool(use_dual_preset),\n",
    "    \"profiles_rows\": int(profiles_df.height),\n",
    "    \"shortlist_rows\": int(asset_shortlist_df.height),\n",
    "    \"selection_diag\": selection_diag,\n",
    "    \"require_structure_loose_gate\": bool(require_structure_loose_gate),\n",
    "    \"require_viability_gate\": bool(require_viability_gate),\n",
    "}\n",
    "\n",
    "print(\">>> Celda 10C :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5630a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10D :: QA de Celda 10C (Perfiles + Shortlist + Tipo de Estrategia)\n",
      "[Celda 10D] RUN_ID = 20251218_190810\n",
      "üìÅ DIR ‚Üí scores = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "[Celda 10D] thresholds ‚Üí min_SCORE_FINAL_for_shortlist=0.3, min_profile_score_for_shortlist=0.5, require_structure_loose_gate=True, require_viability_gate=True\n",
      "üìÅ INPUT ‚Üí scores_table              = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (rows=166, cols=56)\n",
      "üìÅ INPUT ‚Üí asset_strategy_profiles   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_profiles.parquet (rows=83, cols=55)\n",
      "üìÅ INPUT ‚Üí asset_strategy_shortlist  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (rows=7, cols=58)\n",
      "---- Resumen general ----\n",
      "[Universe] s√≠mbolos perfilados = 83\n",
      "[Shortlist] s√≠mbolos seleccionados = 7\n",
      "---- Distribuciones clave (percentiles) ----\n",
      "[Percentiles] SCORE_FINAL: p10=0.287 | p25=0.343 | p50=0.373 | p75=0.396 | p90=0.428 | p95=0.452\n",
      "[Percentiles] core_score_trend: p10=0.061 | p25=0.067 | p50=0.072 | p75=0.079 | p90=0.088 | p95=0.168\n",
      "[Percentiles] core_score_range: p10=0.295 | p25=0.305 | p50=0.324 | p75=0.354 | p90=0.373 | p95=0.375\n",
      "[Percentiles] score_trend_follow: p10=0.061 | p25=0.067 | p50=0.072 | p75=0.079 | p90=0.088 | p95=0.168\n",
      "[Percentiles] score_mean_reversion: p10=0.295 | p25=0.305 | p50=0.324 | p75=0.354 | p90=0.373 | p95=0.375\n",
      "[Percentiles] score_breakout: p10=0.240 | p25=0.250 | p50=0.261 | p75=0.275 | p90=0.287 | p95=0.486\n",
      "---- Escala de umbrales (diagn√≥stico) ----\n",
      "[Base candidates] pasan gates base = 7 (8.4%)\n",
      "[Strong TREND] core_score_trend ‚â• 0.50 = 0 (0.0% de base)\n",
      "[Strong RANGE] core_score_range ‚â• 0.50 = 0 (0.0% de base)\n",
      "[Celda 10D][WARN] Umbral de TREND fuerte parece agresivo para este universo (solo 0.0% de la base lo supera).\n",
      "[Celda 10D][WARN] Umbral de RANGE/MR fuerte parece agresivo para este universo (solo 0.0% de la base lo supera).\n",
      "---- Coherencia strategy_profile vs strategy_type_primary ----\n",
      "[Alignment] rate aproximado = 100.0% (MIXED cuenta como neutral)\n",
      "[Confusion] profile ‚Üî type (top combinaciones):\n",
      "shape: (4, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ profile  ‚îÜ type           ‚îÜ n   ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---            ‚îÜ --- ‚îÇ\n",
      "‚îÇ str      ‚îÜ str            ‚îÜ u32 ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BREAKOUT ‚îÜ BREAKOUT       ‚îÜ 7   ‚îÇ\n",
      "‚îÇ MIXED    ‚îÜ MEAN_REVERSION ‚îÜ 56  ‚îÇ\n",
      "‚îÇ MIXED    ‚îÜ BREAKOUT       ‚îÜ 2   ‚îÇ\n",
      "‚îÇ MR       ‚îÜ MEAN_REVERSION ‚îÜ 18  ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Top 10 core_score_trend ----\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ core_score_trend ‚îÜ strategy_profile ‚îÜ strategy_type_primar ‚îÜ class_family ‚îÜ SCORE_FINAL ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---              ‚îÜ ---              ‚îÜ y                    ‚îÜ ---          ‚îÜ ---         ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64              ‚îÜ str              ‚îÜ ---                  ‚îÜ str          ‚îÜ f64         ‚îÇ\n",
      "‚îÇ        ‚îÜ                  ‚îÜ                  ‚îÜ str                  ‚îÜ              ‚îÜ             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ 0.256411         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.461609    ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ 0.222835         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.479258    ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ 0.207034         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.45238     ‚îÇ\n",
      "‚îÇ XAUUSD ‚îÜ 0.20084          ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.451056    ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.167599         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.488358    ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ 0.160511         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.461201    ‚îÇ\n",
      "‚îÇ LVMH   ‚îÜ 0.120434         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.349748    ‚îÇ\n",
      "‚îÇ AAVUSD ‚îÜ 0.098545         ‚îÜ MIXED            ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.411783    ‚îÇ\n",
      "‚îÇ CHFJPY ‚îÜ 0.087577         ‚îÜ MIXED            ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.403503    ‚îÇ\n",
      "‚îÇ LTCUSD ‚îÜ 0.086564         ‚îÜ MIXED            ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.372914    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Top 10 core_score_range ----\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ core_score_range ‚îÜ strategy_profile ‚îÜ strategy_type_primar ‚îÜ class_family ‚îÜ SCORE_FINAL ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---              ‚îÜ ---              ‚îÜ y                    ‚îÜ ---          ‚îÜ ---         ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64              ‚îÜ str              ‚îÜ ---                  ‚îÜ str          ‚îÜ f64         ‚îÇ\n",
      "‚îÇ        ‚îÜ                  ‚îÜ                  ‚îÜ str                  ‚îÜ              ‚îÜ             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ VECUSD ‚îÜ 0.379846         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.342232    ‚îÇ\n",
      "‚îÇ CADCHF ‚îÜ 0.375547         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.431901    ‚îÇ\n",
      "‚îÇ ALGUSD ‚îÜ 0.375383         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.393597    ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.375338         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT             ‚îÜ null         ‚îÜ 0.488358    ‚îÇ\n",
      "‚îÇ NZDCHF ‚îÜ 0.374593         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.412822    ‚îÇ\n",
      "‚îÇ AUDNZD ‚îÜ 0.374342         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.377561    ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ 0.374041         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.428305    ‚îÇ\n",
      "‚îÇ AUDCAD ‚îÜ 0.373643         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.375352    ‚îÇ\n",
      "‚îÇ EURJPY ‚îÜ 0.373323         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.39673     ‚îÇ\n",
      "‚îÇ NZDCAD ‚îÜ 0.372715         ‚îÜ MR               ‚îÜ MEAN_REVERSION       ‚îÜ null         ‚îÜ 0.348006    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Top 10 score_trend_follow ----\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ score_trend_follow ‚îÜ strategy_profile ‚îÜ strategy_type_prim ‚îÜ class_family ‚îÜ SCORE_FINAL ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---                ‚îÜ ---              ‚îÜ ary                ‚îÜ ---          ‚îÜ ---         ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64                ‚îÜ str              ‚îÜ ---                ‚îÜ str          ‚îÜ f64         ‚îÇ\n",
      "‚îÇ        ‚îÜ                    ‚îÜ                  ‚îÜ str                ‚îÜ              ‚îÜ             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ 0.256411           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.461609    ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ 0.222835           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.479258    ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ 0.207034           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.45238     ‚îÇ\n",
      "‚îÇ XAUUSD ‚îÜ 0.20084            ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.451056    ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.167599           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.488358    ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ 0.160511           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.461201    ‚îÇ\n",
      "‚îÇ LVMH   ‚îÜ 0.120434           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.349748    ‚îÇ\n",
      "‚îÇ AAVUSD ‚îÜ 0.098545           ‚îÜ MIXED            ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.411783    ‚îÇ\n",
      "‚îÇ CHFJPY ‚îÜ 0.087577           ‚îÜ MIXED            ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.403503    ‚îÇ\n",
      "‚îÇ LTCUSD ‚îÜ 0.086564           ‚îÜ MIXED            ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.372914    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Top 10 score_mean_reversion ----\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ score_mean_reversi ‚îÜ strategy_profile ‚îÜ strategy_type_prim ‚îÜ class_family ‚îÜ SCORE_FINAL ‚îÇ\n",
      "‚îÇ ---    ‚îÜ on                 ‚îÜ ---              ‚îÜ ary                ‚îÜ ---          ‚îÜ ---         ‚îÇ\n",
      "‚îÇ str    ‚îÜ ---                ‚îÜ str              ‚îÜ ---                ‚îÜ str          ‚îÜ f64         ‚îÇ\n",
      "‚îÇ        ‚îÜ f64                ‚îÜ                  ‚îÜ str                ‚îÜ              ‚îÜ             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ VECUSD ‚îÜ 0.379846           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.342232    ‚îÇ\n",
      "‚îÇ CADCHF ‚îÜ 0.375547           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.431901    ‚îÇ\n",
      "‚îÇ ALGUSD ‚îÜ 0.375383           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.393597    ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.375338           ‚îÜ BREAKOUT         ‚îÜ BREAKOUT           ‚îÜ null         ‚îÜ 0.488358    ‚îÇ\n",
      "‚îÇ NZDCHF ‚îÜ 0.374593           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.412822    ‚îÇ\n",
      "‚îÇ AUDNZD ‚îÜ 0.374342           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.377561    ‚îÇ\n",
      "‚îÇ AUDCHF ‚îÜ 0.374041           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.428305    ‚îÇ\n",
      "‚îÇ AUDCAD ‚îÜ 0.373643           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.375352    ‚îÇ\n",
      "‚îÇ EURJPY ‚îÜ 0.373323           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.39673     ‚îÇ\n",
      "‚îÇ NZDCAD ‚îÜ 0.372715           ‚îÜ MR               ‚îÜ MEAN_REVERSION     ‚îÜ null         ‚îÜ 0.348006    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Top 10 score_breakout ----\n",
      "shape: (10, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ symbol ‚îÜ score_breakout ‚îÜ strategy_profile ‚îÜ strategy_type_primary ‚îÜ class_family ‚îÜ SCORE_FINAL ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---            ‚îÜ ---              ‚îÜ ---                   ‚îÜ ---          ‚îÜ ---         ‚îÇ\n",
      "‚îÇ str    ‚îÜ f64            ‚îÜ str              ‚îÜ str                   ‚îÜ str          ‚îÜ f64         ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ BTCUSD ‚îÜ 0.690802       ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.461609    ‚îÇ\n",
      "‚îÇ XAUAUD ‚îÜ 0.634353       ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.479258    ‚îÇ\n",
      "‚îÇ ETHUSD ‚îÜ 0.551731       ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.45238     ‚îÇ\n",
      "‚îÇ XAUUSD ‚îÜ 0.548          ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.451056    ‚îÇ\n",
      "‚îÇ XAUEUR ‚îÜ 0.485518       ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.461201    ‚îÇ\n",
      "‚îÇ BNBUSD ‚îÜ 0.457014       ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.488358    ‚îÇ\n",
      "‚îÇ LVMH   ‚îÜ 0.4189         ‚îÜ BREAKOUT         ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.349748    ‚îÇ\n",
      "‚îÇ AAVUSD ‚îÜ 0.303515       ‚îÜ MIXED            ‚îÜ MEAN_REVERSION        ‚îÜ null         ‚îÜ 0.411783    ‚îÇ\n",
      "‚îÇ EURNOK ‚îÜ 0.287427       ‚îÜ MIXED            ‚îÜ BREAKOUT              ‚îÜ null         ‚îÜ 0.386345    ‚îÇ\n",
      "‚îÇ EURCZK ‚îÜ 0.287205       ‚îÜ MIXED            ‚îÜ MEAN_REVERSION        ‚îÜ null         ‚îÜ 0.354382    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Sanity de shortlist ----\n",
      "[Shortlist] cobertura sobre universo = 8.4%\n",
      "---- Histograms (opcionales) ----\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPNpJREFUeJzt3Qd4FOXa//E7JJAgJXQSOgiCdI2CIAhIE3kVUFE5KKAI5/WADUWIUsQWrFhAEI6ADSmKFPFEEakCIk1EBQGpSkCQJNQEw/yv+3n/u2c3yYYEd8lunu/nugay03bmySTzy1NmwxzHcQQAAMAihfL7AAAAAC42AhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAC/SktLk+eff16++OILShZA0CIAARfoqaeekrCwsItSfm3btjWTy7Jly8x7f/zxx3Kx6fvqufsyZMgQ+fDDD6V58+YX9bgAIC8IQICITJ8+3dzYXVNUVJRUqlRJOnfuLG+88YYcP37cL+X0+++/m/CwefPmAlnus2fPlnnz5sl//vMfKVWqVH4fDgD4FOF7EWCfp59+WmrWrClnz56VpKQkU9Py8MMPy6uvvioLFiyQxo0bu9cdMWKEDB8+PM8BaMyYMVKjRg1p2rRprrf78ssvJVicPn1aIiKy/urQz1U+cOCACT/VqlXLl2MDgNwiAAEeunTpIldddZX7dXx8vHz99dfyP//zP3LzzTfLzz//LEWLFv2/H56IiGyDgD+dOnVKLrnkEilSpEjQfJ+0diw7WnOmzV+h6uTJk1KsWDEpSM6cOWOunUKFAlfZ/9dff8m5c+eC6hoFcoMmMOA8rr/+ehk5cqTs3btXPvjggxz7AC1evFhatWplmn+KFy8udevWlSeeeMIs09qkq6++2nx9zz33uJvbtPlNaR+fhg0byoYNG+S6664zwce1beY+QC4ZGRlmnZiYGHPz1pC2f/9+r3W0tqlfv35Zts1un3rD1PO67LLLTNCJjY2VW265RXbt2pVjH6BNmzaZ8FiyZElz3u3bt5e1a9dm28z4zTffmKBUvnx5c8w9evSQP/74I1fX4bZt2+T2228322oQ1fJ98sknL/hYli9fLv/617+kQoUKUqVKFfdyrcVq3bq1Ob4SJUpI165d5ccff5S8evPNN6VBgwbme1m6dGkTrmfMmOG1zm+//Sb9+/c3Ta6RkZGmBvL++++X9PR09zq//vqr9OzZU8qUKWP2dc0118iiRYu89uPqFzZz5kxTO1m5cmWzbmpqqln+7bffyg033CDR0dFmfps2bcz3Ii/27Nlj3uPll1+W1157TS699FJzzD/99JM53lGjRklcXJx5Dy07LcOlS5f63MfkyZPd+9Cfje+++y7Le86ZM0fq169vrkf9+fj000/N9azXtScNYXpMWt66bsWKFeWf//ynHDt2LE/nCHtQAwTkwt13322ChjZFDRgwINt19AapNUXaTKZNafpLfefOne6bzOWXX27m601i4MCB5uagWrZs6d7H0aNHzc37zjvvlLvuusv8Es/Jc889Z24mw4YNk8OHD5sbQIcOHUwfI1dNVW5pmNLjX7JkiXn/hx56yPR90lC3detWc6Pydd56Lho4Hn/8cSlcuLC8/fbbJlxpwMjcGfqBBx4wYWD06NHmZqjHPHjwYJk1a1aOx7dlyxbzPrp/LT+9AWowW7hwoSmHCzkWDT8apvR7ojVA6v3335e+ffua/l8vvPCCqYWbOHGiCbYarjLfeH2ZMmWKPPjgg3LbbbeZstRwqeegQeQf//iHu0m0WbNmkpycbM6pXr16JhBp53Z9X61VOXTokLlG9LXur2zZsvLuu++asKvraYD09Mwzz5jtHnvsMTMiT7/WWky9rjScaLlrjdC0adNMuF+5cqU5hrzQbfV89Jj1OtdgpkHr3//+t/Tq1cv8jOi1884775hyXLduXZYmXw2Cuo6GFL2GX3zxRRO2Nezp901pyLvjjjukUaNGkpCQYMKMhkUNd5npfjTY6h8XWk67d++W8ePHm++Z/gy69gm4OQCcadOmOfrj8N133/ksjejoaOeKK65wvx49erTZxmXcuHHm9R9//OFzH7p/XUffL7M2bdqYZZMmTcp2mU4uS5cuNetWrlzZSU1Ndc+fPXu2mf/666+751WvXt3p27fvefc5depUs+2rr76aZd1z5865v9Z19Nxdunfv7hQpUsTZtWuXe97vv//ulChRwrnuuuuylHGHDh289vfII4844eHhTnJyspMT3Zfuc+/evT6PLa/H0qpVK+evv/5yzz9+/LhTqlQpZ8CAAV7vkZSUZL7/mefnpFu3bk6DBg1yXKdPnz5OoUKFsr3uXOf18MMPm2NduXKl13HWrFnTqVGjhpORkeF1TdSqVcs5deqU137q1KnjdO7c2ausdB3dR8eOHXN9Trt37zbvUbJkSefw4cNey7Qc09LSvOYdO3bMqVixonPvvfdm2UfZsmWdP//80z1//vz5Zv7ChQvd8xo1auRUqVLFnK/LsmXLzHp6Xbto2ei8Dz/80Ov9ExMTs50PKJrAgFzS5pScRoO5Rj3Nnz/fVMdfCP1rWv+Cza0+ffqYJhoXrW3QZqvPP/88z+/9ySefSLly5UwNTWa+hvtrrZHWinXv3l1q1arlnq/HoLUcq1atcjfBuGitgef+tMZG96NNjL5oE9mKFSvk3nvvzdLB2rWvCzkWrakIDw93v9baLq2N0VqMI0eOuCddR2uPMjfn5ESvB+0Unl2zjtJrREfM3XTTTV79zjKfl34vtYZGa6A8r0UtR61B0+YnT1p75Vn7p7WBO3bsMGWgNYyuc9IaL20e1HLN6/V66623mpozT1pGrn5Aur8///zT9A/Sc9u4cWOWfWjNjtYEurhqRLUGyFU79sMPP5hrXM/XRZvutEYoczOZNrt17NjR6/umNV66bV6+b7AHAQjIpRMnTniFjex+oV977bVy3333maYrbUbSYeF5ublo1X5eOpPWqVMny02zdu3a5saYV9qcpH1q8tKxW4OJNs3odplpk5+ee+Y+SZkDjOsmmFNfDddNUfuA+PNYtL+NJw0KSpuG9AbvOWm40mbG3NJmSb35anjR79OgQYO8+tzo8Wogy+mclAZDX+fkWp6bc9JglPmctMlKm8lSUlJyfV7ZvYeLNs1pE7D2wdGmOn0PbcbKbv/nuw5c56XXc2aZ5+k56ntoX67M56g/t3n5vsEe9AECckH/ktdfsNn9MnbRv7r1r2n9a1N/6ScmJpp+LXoz1ZunZ01DTvvwt5xqb3JzTP7m6z3/r3Xt4spc3q6wqv2AtGN5ZnkJhxpQtm/fLp999pm5FrSG7a233jL9jfRRCBf7nF566SWfj17wrGG5kPdQOkBAOydrDdzQoUNNGNHvtfbd8exEH4jrQM9R308fwJmdzLVVgCIAAbmgN0SlHTpzop1LtVlBJ312kH4khI5S0lCknZP9/eRo11/3njcP7Xjt+bwi/ctam3Uy07+wPZuKtJOzdtDVZyDltsOo3lh0RJHe6LMbsaXlUbVqVfm7XMepnbEDeSyujt56M9Xv19+lI6G0ZlAnHSWlnXy1w7Y+XkGPVztr53ROqnr16j7PybU8N+ek7+WPc/JFO2Tr92nu3Lle17l2ur4QrvPS6zmzzPP0HL/66itTAxuIPyJQMNEEBpyHjqDRkTVa7d+7d2+f62mfh8xcf3FrM4NyPWcmu0ById577z2vfkl6Ezp48KAZ8eN5c9Bh4J7DqrVWInNzkPbr0H4TOnImt3+V61/xnTp1Mv2ePJvddOSSjvLRfit64/27NCzoowGmTp0q+/bty/bY/HEsGnB1HQ2uGgQzy+1wfaX9bTxp06YO59bj1X1rINPaEh3Ftn79+izbu87rxhtvNKOo1qxZ416m/Xd0CLmOSNN95kT7weg1oMPOtTno75xTTlw1Op7XigZqz+POC30sgDYP6jXuedw6mk/7BnnSRyNojab+nGam/ZD89fOGgoUaIMCDPv9F/7LWX5p649Twox1j9a9RfRK0r4cAKh3irk1g+swYXV/7HWiThz5fxtWBVW9E2jl20qRJpj+RBiLtXOurT8X56PBj3bd2nNbj1SHl2kznOVRf+yRpMNJnwOiNQpsjtLki87B27WyqNxt9Ro/ecLVTqt5o9S9rHS7erVu3bI/h2WefdT//SNfTZiIdeq6hT4c2+4t+JIm+x5VXXmk6AGuZadDR5kbXR4v83WPR8KND3vWxB/o+2o9Lw5eGLn0frWHILiBmR8OYNqPpNtonTB+iqdvq9eHqS6ZBS5tHtWOvnpM2m2mA1U692mlbrxV92vhHH31kQq0O79bvufa10WHe2qx2vocc6nLt66Pb6zNy9FrRvmY63F5rJvWcNYT9XfoIBa390WH5eo56fHqda0DLLnjlhpaPXndahnrc2j9Iy1CDkec+tfx0GLw2t+m1oGWvtZhaQ6pl+frrr5sBAoAXBsMB/x0W7Zp0KHVMTIwZIqxDyj2HmvsaBr9kyRIz9LlSpUpme/2/V69ezi+//OK1nQ73rV+/vhMREeE1JF6HpPsaNu1rGPxHH33kxMfHOxUqVHCKFi3qdO3aNcswcfXKK6+YIfORkZHOtdde66xfvz7LPl1Do5988kkzPLpw4cKmDG677TavYeWZh8GrjRs3mmHWxYsXdy655BKnXbt2zurVq3P1qAHXuej/57N161anR48eZqh6VFSUU7duXWfkyJF+OxbPY9J96NB3fZ9LL73U6devnym33Hr77bfN0Hsd7q3lrvsYOnSok5KS4rWefr90OHz58uXNejqMfdCgQV5DyrX89fvgOu9mzZo5n332WZZj1nOaM2dOtsezadMm55ZbbnEfjw4jv/322811m1uuIewvvfRSlmU6xP755583+9X96yMj9Bj1EQyeQ9Zz2kd219bMmTOdevXqmX02bNjQWbBggXPrrbeaeZlNnjzZiYuLMz8L+ugDHUb/+OOPm0chAJmF6T/ekQgAgOClTctaM6e1fcCFog8QACAoaV8pbY7O/JEf33//fbYfDQPkBTVAAJAH2pk8uw7vnvShfKE0Gkk7EJ+vM7QOlc/rcPm/S/t46cg1/VgY7RSt/fO0X5GWr46e02cNAReKTtAAkAerV6+Wdu3anfezsrL7ANpgpSMCz9cRX4ezZ/4Q3EDTRzjoKDbtxK0BTQcNaAfrsWPHEn7wt1EDBAB5oCORNmzYkOM6OtpKP4IjVOgHm+qos5zoM348nxsFhDoCEAAAsA5NYD4eq64fxKfP6vD3k3sBAEBg6MB2fTis9hk73zOyCEDZ0PDjj8f3AwCA/OnXpg+hzQkBKBuup7RqAfrjMf4AACDwUlNTTQWG6z6eEwJQNlzNXhp+CEAAAISW3HRf4UGIAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALBOvgaghIQEufrqq82HllWoUEG6d+8u27dv91rnzJkzMmjQIClbtqwUL15cbr31Vjl06FCO+3UcR0aNGiWxsbFStGhR6dChg+zYsSPAZwMAAEJFvgag5cuXm3Czdu1aWbx4sZw9e1Y6deokJ0+edK/zyCOPyMKFC2XOnDlm/d9//11uueWWHPf74osvyhtvvCGTJk2Sb7/9VooVKyadO3c2YQoAACDM0eqSIPHHH3+YmiANOtddd52kpKRI+fLlZcaMGXLbbbeZdbZt2yaXX365rFmzRq655pos+9DTqVSpkjz66KPy2GOPmXm6n4oVK8r06dPlzjvvPO9xpKamSnR0tNmOT4MHACA05OX+HVR9gPSAVZkyZcz/GzZsMLVC2oTlUq9ePalWrZoJQNnZvXu3JCUleW2jhdG8eXOf26SlpZlC85wAAEDBFSFB4ty5c/Lwww/LtddeKw0bNjTzNMgUKVJESpUq5bWu1ubosuy45us6ud1G+yKNGTPGT2cCwB9qDF8UsILcM7ZrwPYNIDQETQ2Q9gXaunWrzJw586K/d3x8vKl9ck379++/6McAAAAsC0CDBw+Wzz77TJYuXSpVqlRxz4+JiZH09HRJTk72Wl9Hgemy7LjmZx4pltM2kZGRpq3QcwIAAAVXvgYg7bCs4efTTz+Vr7/+WmrWrOm1PC4uTgoXLixLlixxz9Nh8vv27ZMWLVpku0/dhwYdz220T4+OBvO1DQAAsEuh/G72+uCDD8woL30WkPbR0en06dPuzsv9+/eXIUOGmNoh7RR9zz33mCDjOQJMO0ZriFJhYWGmL9Gzzz4rCxYskB9++EH69OljRobpc4YAAADytRP0xIkTzf9t27b1mj9t2jTp16+f+XrcuHFSqFAh8wBEHa2lz/N56623vNbXWiHXCDL1+OOPm2cJDRw40DSftWrVShITEyUqKuqinBcAAAhuQfUcoGDBc4CA/McoMADWPAcIAADgYiAAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWydcAtGLFCrnpppukUqVKEhYWJvPmzfNarvOym1566SWf+3zqqaeyrF+vXr2LcDYAACBU5GsAOnnypDRp0kQmTJiQ7fKDBw96TVOnTjWB5tZbb81xvw0aNPDabtWqVQE6AwAAEIoi8vPNu3TpYiZfYmJivF7Pnz9f2rVrJ7Vq1cpxvxEREVm2BQAACLk+QIcOHZJFixZJ//79z7vujh07TLOaBqXevXvLvn37clw/LS1NUlNTvSYAAFBwhUwAevfdd6VEiRJyyy235Lhe8+bNZfr06ZKYmCgTJ06U3bt3S+vWreX48eM+t0lISJDo6Gj3VLVq1QCcAQAACBYhE4C0/4/W5kRFReW4njap9ezZUxo3biydO3eWzz//XJKTk2X27Nk+t4mPj5eUlBT3tH///gCcAQAACBb52gcot1auXCnbt2+XWbNm5XnbUqVKyWWXXSY7d+70uU5kZKSZAACAHUKiBuidd96RuLg4M2Isr06cOCG7du2S2NjYgBwbAAAIPfkagDScbN682UxK++vo156dlrVD8pw5c+S+++7Ldh/t27eX8ePHu18/9thjsnz5ctmzZ4+sXr1aevToIeHh4dKrV6+LcEYAACAU5GsT2Pr1682wdpchQ4aY//v27Ws6MquZM2eK4zg+A4zW7hw5csT9+sCBA2bdo0ePSvny5aVVq1aydu1a8zUAAIAKczRdwIvWOuloMO0QXbJkSUoHyAc1hi8K2L73jO0asH0DCI37d0j0AQIAAPAnAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYJ18DUArVqyQm266SSpVqiRhYWEyb948r+X9+vUz8z2nG2644bz7nTBhgtSoUUOioqKkefPmsm7dugCeBQAACDX5GoBOnjwpTZo0MYHFFw08Bw8edE8fffRRjvucNWuWDBkyREaPHi0bN240++/cubMcPnw4AGcAAABCUUR+vnmXLl3MlJPIyEiJiYnJ9T5fffVVGTBggNxzzz3m9aRJk2TRokUydepUGT58+N8+ZgAAEPqCvg/QsmXLpEKFClK3bl25//775ejRoz7XTU9Plw0bNkiHDh3c8woVKmRer1mzxud2aWlpkpqa6jUBAICCK6gDkDZ/vffee7JkyRJ54YUXZPny5abGKCMjI9v1jxw5YpZVrFjRa76+TkpK8vk+CQkJEh0d7Z6qVq3q93MBAADBI1+bwM7nzjvvdH/dqFEjady4sVx66aWmVqh9+/Z+e5/4+HjTb8hFa4AIQQAAFFxBXQOUWa1ataRcuXKyc+fObJfrsvDwcDl06JDXfH2dUz8i7WdUsmRJrwkAABRcIRWADhw4YPoAxcbGZru8SJEiEhcXZ5rMXM6dO2det2jR4iIeKQAACGb5GoBOnDghmzdvNpPavXu3+Xrfvn1m2dChQ2Xt2rWyZ88eE2K6desmtWvXNsPaXbQpbPz48e7X2pQ1ZcoUeffdd+Xnn382Had1uL1rVBgAAEC+9gFav369tGvXzv3a1Q+nb9++MnHiRNmyZYsJMsnJyeZhiZ06dZJnnnnGNFm57Nq1y3R+drnjjjvkjz/+kFGjRpmOz02bNpXExMQsHaMBAIC9whzHcfL7IIKNdoLW0WApKSn0BwLySY3hiwK27z1juwZs3wBC4/4dUn2AAAAA/IEABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYJ18D0IoVK+Smm26SSpUqSVhYmMybN8+97OzZszJs2DBp1KiRFCtWzKzTp08f+f3333Pc51NPPWX25TnVq1fvIpwNAAAIFfkagE6ePClNmjSRCRMmZFl26tQp2bhxo4wcOdL8P3fuXNm+fbvcfPPN591vgwYN5ODBg+5p1apVAToDAAAQiiLy8827dOlipuxER0fL4sWLveaNHz9emjVrJvv27ZNq1ar53G9ERITExMT4/XgBAEDBEFJ9gFJSUkyTVqlSpXJcb8eOHabJrFatWtK7d28TmHKSlpYmqampXhMAACi4QiYAnTlzxvQJ6tWrl5QsWdLnes2bN5fp06dLYmKiTJw4UXbv3i2tW7eW48eP+9wmISHB1Di5pqpVqwboLAAAQDAIiQCkHaJvv/12cRzHhJqcaJNaz549pXHjxtK5c2f5/PPPJTk5WWbPnu1zm/j4eFO75Jr2798fgLMAAADBIl/7AOUl/Ozdu1e+/vrrHGt/sqPNZZdddpns3LnT5zqRkZFmAgAAdigUCuFH+/R89dVXUrZs2Tzv48SJE7Jr1y6JjY0NyDECAIDQk68BSMPJ5s2bzaS0v45+rZ2WNfzcdtttsn79evnwww8lIyNDkpKSzJSenu7eR/v27c3oMJfHHntMli9fLnv27JHVq1dLjx49JDw83PQdAgAAyPcmMA037dq1c78eMmSI+b9v377mgYYLFiwwr5s2beq13dKlS6Vt27bma63dOXLkiHvZgQMHTNg5evSolC9fXlq1aiVr1641XwMAAOR7ANIQox2bfclpmYvW9HiaOXOmX44NAAAUXEHdBwgAACAQCEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYJ0LCkDXX3+9JCcnZ5mfmppqlgEAABS4ALRs2TJJT0/PMv/MmTOycuVKfxwXAABAcHwa/JYtW9xf//TTT5KUlOR+nZGRIYmJiVK5cmX/HiEAAEB+BqCmTZtKWFiYmbJr6ipatKi8+eab/jw+AACA/A1Au3fvFsdxpFatWrJu3TopX768e1mRIkWkQoUKEh4e7v+jBAAAyK8AVL16dfP/uXPn/HkMAAAAwRuAPO3YsUOWLl0qhw8fzhKIRo0a5Y9jAwAACJ4ANGXKFLn//vulXLlyEhMTY/oEuejXBCAAAFDgAtCzzz4rzz33nAwbNsz/RwQAAHKtxvBFASutPWO7FtjvxAU9B+jYsWPSs2dP/x8NAABAsAYgDT9ffvml/48GAAAgWJvAateuLSNHjpS1a9dKo0aNpHDhwl7LH3zwQX8dHwAAQHAEoMmTJ0vx4sVl+fLlZvKknaAJQAAAoMAFIH0gIgAAgFV9gAAAAKyrAbr33ntzXD516tQLPR4AAIDgDEA6DN7T2bNnZevWrZKcnJzth6QCAACEfAD69NNPs8zTj8PQp0Nfeuml/jguAACA4O8DVKhQIRkyZIiMGzfOX7sEAAAI/k7Qu3btkr/++sufuwQAAAiOJjCt6fHkOI4cPHhQFi1aJH379vXXsQEAAARPANq0aVOW5q/y5cvLK6+8ct4RYgAAACHZBLZ06VKvacmSJTJz5kwZOHCgRETkPlOtWLFCbrrpJqlUqZJ5gvS8efOy1CyNGjVKYmNjpWjRotKhQwfZsWPHefc7YcIEqVGjhkRFRUnz5s1l3bp1F3KaAACggPpbfYD++OMPWbVqlZn067w6efKkNGnSxASW7Lz44ovyxhtvyKRJk+Tbb7+VYsWKSefOneXMmTM+9zlr1izTRDd69GjZuHGj2b9uc/jw4TwfHwAAKJguKABpcNGmLq2Zue6668yktTj9+/eXU6dO5Xo/Xbp0kWeffVZ69OiRZZnW/rz22msyYsQI6datmzRu3Fjee+89+f3337PUFHl69dVXZcCAAXLPPfdI/fr1TXi65JJLeDgjAAD4ewFIa1j0Q1AXLlxoHn6o0/z58828Rx99VPxBP28sKSnJNHu5REdHmyatNWvWZLtNenq6bNiwwWsb7Z+kr31to9LS0iQ1NdVrAgAABdcFBaBPPvlE3nnnHVODU7JkSTPdeOONMmXKFPn444/9cmAaflTFihW95utr17LMjhw5IhkZGXnaRiUkJJhw5ZqqVq3ql3MAAAAFKABpM1fmkKEqVKiQpyawYBEfHy8pKSnuaf/+/fl9SAAAINgCUIsWLUwnY8/OyKdPn5YxY8aYZf4QExNj/j906JDXfH3tWpZZuXLlJDw8PE/bqMjISHdNlmsCAAAF1wUFIO2c/M0330iVKlWkffv2ZtJmI533+uuv++XAatasaUKLDrF30b45OhrMV8gqUqSIxMXFeW2jn1Gmr/0VzAAAgKUPQmzUqJF5Hs+HH34o27ZtM/N69eolvXv3Ns/rya0TJ07Izp07vTo+b968WcqUKSPVqlWThx9+2IwSq1OnjglEI0eONKPNunfv7t5Gw5eOIhs8eLC7g7Y+jfqqq66SZs2ambCmo9Z0VBgAAMAFByDtNKx9gHS4uaepU6ea5wENGzYsV/tZv369tGvXLstHbGiAmT59ujz++OMmvOgDFnWkWatWrSQxMdE84NDz88e087PLHXfcYY5BH6CoHZ+bNm1qtsmuzxIAALBTmKMP3MkjfcryjBkzpGXLll7ztXnqzjvvNDU5oUyb2nQ0mHaIpj8QkD9qDF8UsH3vGds1YPsGLjZ+Vi7s/n1BfYC0ZkUfgpiZfh6YfigqAABAMLugAOTq8JyZztM+OgAAAAWuD5D2/dEOymfPnpXrr7/ezNORVtpnx19PggYAAAiqADR06FA5evSo/Otf/zIfP6G0Y7J2ftaHCgIAABS4ABQWFiYvvPCCGZb+888/m6HvOlRdHygIAABQIAOQS/HixeXqq6/239EAAAAEaydoAACAUEYAAgAA1iEAAQAA6xCAAACAdf5WJ2jAxsfD8zEKABD6qAECAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwTtAHoBo1akhYWFiWadCgQdmuP3369CzrRkVFXfTjBgAAwStCgtx3330nGRkZ7tdbt26Vjh07Ss+ePX1uU7JkSdm+fbv7tYYgAACAkAlA5cuX93o9duxYufTSS6VNmzY+t9HAExMTcxGODgAAhKKgbwLzlJ6eLh988IHce++9OdbqnDhxQqpXry5Vq1aVbt26yY8//pjjftPS0iQ1NdVrAgAABVdIBaB58+ZJcnKy9OvXz+c6devWlalTp8r8+fNNWDp37py0bNlSDhw44HObhIQEiY6Odk8anAAAQMEVUgHonXfekS5dukilSpV8rtOiRQvp06ePNG3a1DSTzZ071zSjvf322z63iY+Pl5SUFPe0f//+AJ0BAAAIBkHfB8hl79698tVXX5lAkxeFCxeWK664Qnbu3OlzncjISDMBAAA7hEwN0LRp06RChQrStWvXPG2nI8h++OEHiY2NDdixAQCA0BISAUj78WgA6tu3r0REeFdaaXOXNmG5PP300/Lll1/Kr7/+Khs3bpS77rrL1B7dd999+XDkAAAgGIVEE5g2fe3bt8+M/spM5xcq9N8cd+zYMRkwYIAkJSVJ6dKlJS4uTlavXi3169e/yEcNAACCVUgEoE6dOonjONkuW7ZsmdfrcePGmQkAACCkm8AAAAD8iQAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFgnqAPQU089JWFhYV5TvXr1ctxmzpw5Zp2oqChp1KiRfP755xfteAEAQGgI6gCkGjRoIAcPHnRPq1at8rnu6tWrpVevXtK/f3/ZtGmTdO/e3Uxbt269qMcMAACCW9AHoIiICImJiXFP5cqV87nu66+/LjfccIMMHTpULr/8cnnmmWfkyiuvlPHjx1/UYwYAAMEt6APQjh07pFKlSlKrVi3p3bu37Nu3z+e6a9askQ4dOnjN69y5s5mfk7S0NElNTfWaAABAwRUhQax58+Yyffp0qVu3rmn+GjNmjLRu3do0aZUoUSLL+klJSVKxYkWvefpa5+ckISHB7BuAHWoMXxSwfe8Z2zVg+wYKys/KniD4OQnqGqAuXbpIz549pXHjxqYmRzs0Jycny+zZs/36PvHx8ZKSkuKe9u/f79f9AwCA4BLUNUCZlSpVSi677DLZuXNntsu1j9ChQ4e85ulrnZ+TyMhIMwEAADsEdQ1QZidOnJBdu3ZJbGxststbtGghS5Ys8Zq3ePFiMx8AACAkAtBjjz0my5cvlz179pgh7j169JDw8HAz1F316dPHNF+5PPTQQ5KYmCivvPKKbNu2zTxHaP369TJ48OB8PAsAABBsgroJ7MCBAybsHD16VMqXLy+tWrWStWvXmq+VjggrVOi/Ga5ly5YyY8YMGTFihDzxxBNSp04dmTdvnjRs2DAfzwIAAASboA5AM2fOzHH5smXLsszTTtM6AQAAhGQTGAAAQCAQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdYL6ozBQ8NUYvii/D8EKlDMAeKMGCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFgnIr8PAKGhxvBF+X0IVpTFnrFdA7ZvAMB/UQMEAACsQwACAADWIQABAADrEIAAAIB1gjoAJSQkyNVXXy0lSpSQChUqSPfu3WX79u05bjN9+nQJCwvzmqKioi7aMQMAgOAX1AFo+fLlMmjQIFm7dq0sXrxYzp49K506dZKTJ0/muF3JkiXl4MGD7mnv3r0X7ZgBAEDwC+ph8ImJiVlqd7QmaMOGDXLdddf53E5rfWJiYi7CEQIAgFAU1DVAmaWkpJj/y5Qpk+N6J06ckOrVq0vVqlWlW7du8uOPP+a4flpamqSmpnpNAACg4AqZAHTu3Dl5+OGH5dprr5WGDRv6XK9u3boydepUmT9/vnzwwQdmu5YtW8qBAwdy7GsUHR3tnjQ4AQCAgitkApD2Bdq6davMnDkzx/VatGghffr0kaZNm0qbNm1k7ty5Ur58eXn77bd9bhMfH29ql1zT/v37A3AGAAAgWAR1HyCXwYMHy2effSYrVqyQKlWq5GnbwoULyxVXXCE7d+70uU5kZKSZAACAHYK6BshxHBN+Pv30U/n666+lZs2aed5HRkaG/PDDDxIbGxuQYwQAAKEnItibvWbMmGH68+izgJKSksx87adTtGhR87U2d1WuXNn041FPP/20XHPNNVK7dm1JTk6Wl156yQyDv++++/L1XAAAQPAI6gA0ceJE83/btm295k+bNk369etnvt63b58UKvTfiqxjx47JgAEDTFgqXbq0xMXFyerVq6V+/foX+egBAECwigj2JrDzWbZsmdfrcePGmQkAACAk+wABAAAEAgEIAABYJ6ibwAqqGsMXBWS/e8Z2Dch+ARTsn+9QPOZQKwsEH2qAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOuEOY7j5PdBBJvU1FSJjo6WlJQUKVmypN/3X2P4Ir/vEwCAULFnbNd8v39TAwQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAAACAdQhAAADAOiERgCZMmCA1atSQqKgoad68uaxbty7H9efMmSP16tUz6zdq1Eg+//zzi3asAAAg+AV9AJo1a5YMGTJERo8eLRs3bpQmTZpI586d5fDhw9muv3r1aunVq5f0799fNm3aJN27dzfT1q1bL/qxAwCA4BTmOI4jQUxrfK6++moZP368eX3u3DmpWrWqPPDAAzJ8+PAs699xxx1y8uRJ+eyzz9zzrrnmGmnatKlMmjQpV++Zmpoq0dHRkpKSIiVLlhR/qzF8kd/3CQBAqNgztmtA9puX+3eEBLH09HTZsGGDxMfHu+cVKlRIOnToIGvWrMl2G52vNUaetMZo3rx5Pt8nLS3NTC5acK6CDIRzaacCsl8AAEJBaoDur6795qZuJ6gD0JEjRyQjI0MqVqzoNV9fb9u2LdttkpKSsl1f5/uSkJAgY8aMyTJfa5oAAIB/Rb8mAXX8+HFTExSyAehi0Romz1ojbWb7888/pWzZshIWFiahQpOvhrb9+/cHpOnOVpQrZRoquFYpV9uvVcdxTPipVKnSedcN6gBUrlw5CQ8Pl0OHDnnN19cxMTHZbqPz87K+ioyMNJOnUqVKSajSi4kARLmGAq5VyjWUcL2GRpmer+YnJEaBFSlSROLi4mTJkiVetTP6ukWLFtluo/M911eLFy/2uT4AALBPUNcAKW2a6tu3r1x11VXSrFkzee2118wor3vuuccs79Onj1SuXNn041EPPfSQtGnTRl555RXp2rWrzJw5U9avXy+TJ0/O5zMBAADBIugDkA5r/+OPP2TUqFGmI7MOZ09MTHR3dN63b58ZGebSsmVLmTFjhowYMUKeeOIJqVOnjhkB1rBhQynotBlPn5eUuTkPlGuw4VqlXEMJ12vBLNOgfw4QAACAvwV1HyAAAIBAIAABAADrEIAAAIB1CEAAAMA6BKAgN2HCBKlRo4ZERUWZD4Zdt26dz3WnTJkirVu3ltKlS5tJPzMt8/onTpyQwYMHS5UqVaRo0aJSv379XH9IrI1lOnfuXPMIBn0wZrFixcwoxPfff99rHR1HoKMUY2NjTZlque/YsUNs489yPXv2rAwbNkwaNWpklutTXfWRF7///rvYxN/Xqqf//d//NU+610eL2CYQ5frzzz/LzTffbB7Cp+vph3jrKGWbTPBzuQb8fqWjwBCcZs6c6RQpUsSZOnWq8+OPPzoDBgxwSpUq5Rw6dCjb9f/xj384EyZMcDZt2uT8/PPPTr9+/Zzo6GjnwIED7nV0H5deeqmzdOlSZ/fu3c7bb7/thIeHO/Pnz3dskNcy1XKaO3eu89NPPzk7d+50XnvtNVNeiYmJ7nXGjh1rynnevHnO999/79x8881OzZo1ndOnTzu28He5JicnOx06dHBmzZrlbNu2zVmzZo3TrFkzJy4uzrFFIK5VF12vSZMmTqVKlZxx48Y5NglEuer8MmXKOEOHDnU2btxoXuvvVF/7LIhmBqBcA32/IgAFMf2FP2jQIPfrjIwM8wsrISEhV9v/9ddfTokSJZx3333XPa9BgwbO008/7bXelVde6Tz55JOODf5umaorrrjCGTFihPn63LlzTkxMjPPSSy+5l+vNOzIy0vnoo48cW/i7XLOzbt06fWSHs3fvXscGgSpT/YOocuXKztatW53q1atbF4ACUa533HGHc9dddzk2axaAcg30/YomsCCVnp4uGzZsMM0pLvrAR329Zs2aXO3j1KlTpimhTJkyXg+KXLBggfz222+m6Wbp0qXyyy+/SKdOnaSg+7tlquWlH7Oyfft2ue6668y83bt3mwd0eu5Tq8C1+je336dQF4hyzU5KSoppsgnlz+nL7zLVjxK6++67ZejQodKgQQOxTSDKVct00aJFctlll0nnzp2lQoUK5udfH8Bri/QAXa+Bvl8F/ZOgbXXkyBHJyMhwP/HaRV9v27YtV/vQPhTad8LzonzzzTdl4MCBpk01IiLCXKTadyinG4/tZao3Xv24lbS0NPPhvG+99ZZ07NjRLNPw49pH5n26lhV0gSjXzM6cOWOu5169elnxQb+BKtMXXnjB/Nw/+OCDYqNAlOvhw4dNX5WxY8fKs88+a8pYP63glltuMTds/Wimgu5IgK7XQN+vCEAFlP4w6uegLVu2zHRI87yg1q5da1J19erVZcWKFTJo0KAsQQn/VaJECdm8ebP5Jad/pejn09WqVUvatm1LMV2EctVazNtvv938BThx4kTK/ALLVP9Cf/3112Xjxo2mJg3+uVa1Bkh169ZNHnnkEfO1duhdvXq16bBrQwAK1O+AQN+vCEBBqly5ciYRHzp0yGu+vo6Jiclx25dfftkEoK+++koaN27snn/69Gnz+Wiffvqp+aBYpcv1AtRtCnoAutAy1b86ateu7f7FpqM99MN39YfUtZ3uQ0eBee5T17VBIMo1c/jZu3evfP3111bU/gSqTFeuXGlqK6pVq+ZeX/9qf/TRR81IsD179khBF4hy1X1q7YSOUPJ0+eWXy6pVq8QG5QJQrhfjfkUfoCBVpEgRiYuLM6nYRf/S0NctWrTwud2LL74ozzzzjKmC1SGGnvRmopPnh8cqvXBdf8UUZBdappnpNlplq2rWrGl+wD33mZqaKt9++22e9hnKAlGunuFHHymgYb5s2bJii0CUqfb92bJli7mBuCb9S1r7A33xxRdig0CUq+5Th7xr/xVP2ldFay1sUCQA5XpR7ld+6UqNgA0r1NFE06dPN0MFBw4caIYVJiUlmeV33323M3z4cK/h2DoM8eOPP3YOHjzono4fP+5ep02bNqZnvQ4r/PXXX51p06Y5UVFRzltvvWXFdzGvZfr88887X375pbNr1y6z/ssvv+xEREQ4U6ZM8Sp33YcOzdyyZYvTrVs3K4fB+7Nc09PTzeMEqlSp4mzevNnrek5LS3NsEIhrNTMbR4EFolx1OHfhwoWdyZMnOzt27HDefPNNM1x75cqVji1mBqBcA32/IgAFOf1Bqlatmgk2Osxw7dq1XhdH3759vX6ZaabNPI0ePdq9jt5A9PlAOjxRL6S6des6r7zyihnObYu8lKkOt6xdu7Ypq9KlSzstWrQwP+ietOxGjhzpVKxY0fwCaN++vbN9+3bHNv4sV33mR3bXsk76y9AW/r5WM7MxAAWqXN955x33evqMJX0umG3e9HO5Bvp+Fab/+KcuCQAAIDTQBwgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAuB36enpIVWqGRkZ/vuE6QDuE4D/EIAAGHqzfvHFF6V27doSGRkp1apVk+eee84s++GHH+T666+XokWLStmyZWXgwIFy4sQJd8n169dPunfvbtavVKmS1K1b18zfv3+/3H777VKqVCkpU6aMdOvWTfbs2ZOrEl+2bJk0a9ZMihUrZra/9tprZe/eve7lCxculKuvvlqioqKkXLly0qNHD/eyY8eOSZ8+faR06dJyySWXSJcuXWTHjh3u5dOnTzf7XLBggdSvX9+c7759+yQtLU0ee+wxqVy5snnf5s2bm+PIDV/7/O6776Rjx47mGKOjo6VNmzayceNGr23DwsLk3//+tzkHPd46deqY/XjS1zpfz7ddu3by7rvvmu2Sk5Pd66xatUpat25tvk9Vq1aVBx98UE6ePMkVDmSDAATAiI+Pl7Fjx8rIkSPlp59+khkzZkjFihXNDbRz584mTOjNfM6cOfLVV1/J4MGDvUpuyZIlsn37dlm8eLF89tlncvbsWbNdiRIlZOXKlfLNN99I8eLF5YYbbjhvDdFff/1lApWGhS1btsiaNWtM6NIbvlq0aJEJCzfeeKNs2rTJvLeGJc9Atn79ehMadFv9zGddV4/J5dSpU/LCCy+Y4PHjjz9KhQoVzDnp+jNnzjTv27NnT3O8nuEpJ9nt8/jx49K3b18TTtauXWtCjB6Lzvc0ZswYExb1fXV579695c8//zTLdu/eLbfddpspk++//17++c9/ypNPPum1/a5du8yx3nrrrWYfs2bNMu+Z+fsE4P/zy2fKAwhpqampTmRkpDNlypQsyyZPnuyULl3aOXHihHveokWLnEKFCjlJSUnmdd++fZ2KFSs6aWlp7nXef/99p27dus65c+fc83R50aJFnS+++CLH4zl69Kijv56WLVuW7fIWLVo4vXv3znbZL7/8Yrb95ptv3POOHDli3nf27Nnm9bRp08w6mzdvdq+zd+9eJzw83Pntt9+89te+fXsnPj4+x+P1tc/sZGRkOCVKlHAWLlzonqfbjRgxwv1ay1rn/ec//zGvhw0b5jRs2NBrP08++aRZ59ixY+Z1//79nYEDB3qts3LlSvN9On369HmPH7BNhCsIAbDXzz//bJp/2rdvn+2yJk2amCYhF22O0iYzrfHRWiLVqFEjKVKkiHsdranYuXOnqQHydObMGVNbkRNtLtNaHK1B0uajDh06mNqR2NhYs3zz5s0yYMAAn+cSERFhmq9ctNlOm+V0mYsea+PGjd2vtZlP++1cdtllXvvTctHtcyPzPtWhQ4dkxIgRpint8OHD5j20pkibxzx5bqdlXbJkSbO+0nLW5j5PnjVervLWmp8PP/zQPU+zlX6ftAbp8ssvz9U5ALYgAAEwfUb+Ls+ApLSPUFxcnNcN2aV8+fLn3d+0adNMH5bExETTnKMhQpvXrrnmGr8cr+7D1aTmOt7w8HDZsGGD+d+TNt1dyD6VNn8dPXpUXn/9dalevbrpG9SiRYsszYCFCxf2eq37yUsnaj1+bRrTMstM+3MB8EYfIACmX4revLUvTWZac6C1C56dabU/T6FChdydnbNz5ZVXmr4z2g9GO1Z7TtoZODeuuOIK0zdp9erV0rBhQ9MvyVVbkt2xuo5X+xB9++237nkaQLQWRTsn5/ReWjujtS6ZjzcmJuaCrxItKw0l2q+nQYMGJgAdOXIkT/vQctY+TZ60P1bm8ta+W5mPXSfPmjkA/4cABMCMLBo2bJg8/vjj8t5775kmKu2w+84775jOuLpcazK2bt0qS5culQceeEDuvvtud/NXdnQ7HfmkI7+0E7Q2w2gzkIaBAwcO5Fjquq4GH+2QrCO/vvzySxOmXM04o0ePlo8++sj8r81a2nylnY9dYU7fU5vItBOwhre77rrLjOzS+b5o05ces44emzt3rjmGdevWSUJCgul0faH0eN5//31znBrK9D3yWoOlNTvbtm0z36NffvlFZs+ebUadKVeNky7ToKidnrWJUMtr/vz5dIIGfMnvTkgAgoN2zn322Wed6tWrO4ULF3aqVavmPP/882bZli1bnHbt2jlRUVFOmTJlnAEDBjjHjx93b6udoLt165ZlnwcPHnT69OnjlCtXznSyrlWrltk2JSUlx2PRztXdu3d3YmNjnSJFiphjGjVqlDlGl08++cRp2rSpWa77v+WWW9zL/vzzT+fuu+92oqOjTefnzp07m87Rnh2WdVlm6enp5n1q1KhhykDfv0ePHub8z8fXPjdu3OhcddVVpuzq1KnjzJkzx5zPuHHj3Ovor+JPP/3Uazvdl+7TZf78+U7t2rVNObZt29aZOHGi2c6zg/O6deucjh07OsWLF3eKFSvmNG7c2HnuuefOe+yAjcL0H5/pCAAQlPSZS5MmTTLPWgKQd3SCBoAQ8NZbb5mRYDoiTfsVvfTSSzRvAX8DfYAA5AsdWeVr0j5DwUafJu3reJ9//vmAv7/26dE+TNqR+5lnnpFHH31UnnrqqYC/L1BQ0QQGIF/oM4J80Q7L/hjq7k+//fabnD592udzi3QCEDoIQAAAwDo0gQEAAOsQgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArPP/AGYrt6s6IuSLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10D :: OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 10D ‚Äî QA Final de Perfiles, Umbrales y Coherencia (post 10C)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Objetivo:\n",
    "#   Validar que la salida de Celda 10C es:\n",
    "#     1) Estad√≠sticamente razonable (distribuciones/percentiles).\n",
    "#     2) Coherente entre:\n",
    "#          - strategy_profile (macro: TREND/MR/BREAKOUT/MIXED)\n",
    "#          - strategy_type_primary (micro: TREND_FOLLOW/MEAN_REVERSION/BREAKOUT)\n",
    "#     3) Operativamente √∫til:\n",
    "#          - detectar si los umbrales actuales est√°n \"fuera de escala\"\n",
    "#            para este universo (p.ej. min_profile_score_for_shortlist).\n",
    "#\n",
    "# Entradas esperadas:\n",
    "#   - scores_table.parquet (Celda 10)\n",
    "#   - asset_strategy_profiles.parquet (Celda 10C)\n",
    "#   - asset_strategy_shortlist.parquet (Celda 10C)\n",
    "#   - config.json (strategy_profiles)\n",
    "#\n",
    "# Salidas:\n",
    "#   - NO genera nuevos artefactos de negocio.\n",
    "#   - Actualiza:\n",
    "#       GLOBAL_STATE[\"report_stats\"][\"c10D\"]\n",
    "#\n",
    "# Filosof√≠a:\n",
    "#   - Esta celda NO \"arregla\" nada; solo diagnostica con prints claros.\n",
    "#   - Si detecta incoherencias graves, levanta RuntimeError.\n",
    "#   - Si detecta problemas moderados, imprime WARN expl√≠citos.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10D :: QA de Celda 10C (Perfiles + Shortlist + Tipo de Estrategia)\")\n",
    "\n",
    "# ============================ Guardas b√°sicas ================================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 10D][ERROR] GLOBAL_STATE no existe. Ejecuta las celdas previas.\")\n",
    "\n",
    "for key in (\"paths\", \"run_id\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 10D][ERROR] Falta GLOBAL_STATE['{key}'].\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\")\n",
    "paths: Dict[str, Any] = GLOBAL_STATE[\"paths\"]\n",
    "metrics_state: Dict[str, Any] = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "\n",
    "OUT_SCORES_DIR = Path(paths.get(\"scores\", Path(paths[\"stability\"]).parent / \"scores\")).resolve()\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scores_table_path = Path(\n",
    "    metrics_state.get(\"scores_table_path\", OUT_SCORES_DIR / \"scores_table.parquet\")\n",
    ").resolve()\n",
    "\n",
    "asset_profiles_path = Path(\n",
    "    metrics_state.get(\"asset_strategy_profiles_path\", OUT_SCORES_DIR / \"asset_strategy_profiles.parquet\")\n",
    ").resolve()\n",
    "\n",
    "asset_shortlist_path = Path(\n",
    "    metrics_state.get(\"asset_strategy_shortlist_path\", OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\")\n",
    ").resolve()\n",
    "\n",
    "CONFIG_PATH = Path(paths.get(\"config\", Path(paths[\"diagnostics\"]) / \"config.json\")).resolve()\n",
    "\n",
    "def _require_file(p: Path, label: str = \"\"):\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"[Celda 10D][ERROR] Falta input requerido {label}: {p}\")\n",
    "\n",
    "def _safe_read_json(p: Path) -> Dict[str, Any]:\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        return df.with_columns(\n",
    "            pl.col(\"symbol\")\n",
    "              .cast(pl.Utf8, strict=False)\n",
    "              .str.to_uppercase()\n",
    "              .str.strip_chars()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def _ensure_float(df: pl.DataFrame, name: str, default: float = 0.0) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Float64, strict=False).fill_null(default).alias(name))\n",
    "    return df.with_columns(pl.lit(default).cast(pl.Float64).alias(name))\n",
    "\n",
    "def _ensure_str(df: pl.DataFrame, name: str, default: Optional[str] = None) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Utf8, strict=False).alias(name))\n",
    "    return df.with_columns(pl.lit(default, dtype=pl.Utf8).alias(name))\n",
    "\n",
    "def _pct(x: int, n: int) -> float:\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "    return 100.0 * x / n\n",
    "\n",
    "print(f\"[Celda 10D] RUN_ID = {RUN_ID}\")\n",
    "print(f\"üìÅ DIR ‚Üí scores = {str(OUT_SCORES_DIR)}\")\n",
    "\n",
    "# ============================ Cargar config ==================================\n",
    "if \"config\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"config\"], dict):\n",
    "    cfg = GLOBAL_STATE[\"config\"]\n",
    "else:\n",
    "    cfg = _safe_read_json(CONFIG_PATH)\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "policy = cfg.get(\"policy\", {}) or {}\n",
    "strategy_cfg = cfg.get(\"strategy_profiles\", {}) or {}\n",
    "\n",
    "min_profile_score_for_shortlist  = float(strategy_cfg.get(\"min_profile_score_for_shortlist\", 0.50))\n",
    "min_SCORE_FINAL_for_shortlist    = float(strategy_cfg.get(\"min_SCORE_FINAL_for_shortlist\", 0.30))\n",
    "require_structure_loose_gate     = bool(strategy_cfg.get(\"require_structure_loose_gate\", True))\n",
    "require_viability_gate           = bool(strategy_cfg.get(\"require_viability_gate\", True))\n",
    "\n",
    "# Este valor solo se usa para interpretar escalas\n",
    "global_min_TR = float(policy.get(\"min_TR_after_cost\", 0.25))\n",
    "if not math.isfinite(global_min_TR) or global_min_TR <= 0.0:\n",
    "    global_min_TR = 0.25\n",
    "\n",
    "print(\n",
    "    \"[Celda 10D] thresholds ‚Üí \"\n",
    "    f\"min_SCORE_FINAL_for_shortlist={min_SCORE_FINAL_for_shortlist}, \"\n",
    "    f\"min_profile_score_for_shortlist={min_profile_score_for_shortlist}, \"\n",
    "    f\"require_structure_loose_gate={require_structure_loose_gate}, \"\n",
    "    f\"require_viability_gate={require_viability_gate}\"\n",
    ")\n",
    "\n",
    "# ============================ Cargar inputs ==================================\n",
    "_require_file(scores_table_path, \"scores_table\")\n",
    "_require_file(asset_profiles_path, \"asset_strategy_profiles\")\n",
    "_require_file(asset_shortlist_path, \"asset_strategy_shortlist\")\n",
    "\n",
    "scores_df = _norm_sym(pl.read_parquet(scores_table_path))\n",
    "profiles_df = _norm_sym(pl.read_parquet(asset_profiles_path))\n",
    "shortlist_df = _norm_sym(pl.read_parquet(asset_shortlist_path))\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí scores_table              = {str(scores_table_path)} (rows={scores_df.height}, cols={len(scores_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí asset_strategy_profiles   = {str(asset_profiles_path)} (rows={profiles_df.height}, cols={len(profiles_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí asset_strategy_shortlist  = {str(asset_shortlist_path)} (rows={shortlist_df.height}, cols={len(shortlist_df.columns)})\")\n",
    "\n",
    "if profiles_df.is_empty():\n",
    "    raise RuntimeError(\"[Celda 10D][ERROR] asset_strategy_profiles est√° vac√≠o; Celda 10C no gener√≥ perfiles v√°lidos.\")\n",
    "\n",
    "# ============================ Normalizar columnas clave ======================\n",
    "for col in [\"SCORE_FINAL\", \"core_score_trend\", \"core_score_range\",\n",
    "            \"strategy_profile_max\", \"strategy_profile_margin\",\n",
    "            \"strategy_profile_score_trend\", \"strategy_profile_score_mr\",\n",
    "            \"strategy_profile_score_breakout\",\n",
    "            \"score_trend_follow\", \"score_mean_reversion\", \"score_breakout\"]:\n",
    "    profiles_df = _ensure_float(profiles_df, col, 0.0)\n",
    "\n",
    "for col in [\"prop_trend\", \"prop_range\", \"prop_trend_eff\", \"prop_range_eff\",\n",
    "            \"ER_asset\", \"PD_asset\", \"TR_trend_mean\", \"TR_range_mean\",\n",
    "            \"NDQ_trend_mean\", \"NDQ_range_mean\"]:\n",
    "    profiles_df = _ensure_float(profiles_df, col, 0.0)\n",
    "\n",
    "for col in [\"strategy_profile\", \"class_family\", \"strategy_type_primary\", \"strategy_type_secondary\"]:\n",
    "    profiles_df = _ensure_str(profiles_df, col, None)\n",
    "\n",
    "# Gates que podr√≠an existir en perfiles\n",
    "for gcol, default in [\n",
    "    (\"gate_all\", True),\n",
    "    (\"passed_structure_gate_loose\", True),\n",
    "    (\"passed_viability_gate\", True),\n",
    "]:\n",
    "    if gcol in profiles_df.columns:\n",
    "        profiles_df = profiles_df.with_columns(pl.col(gcol).cast(pl.Boolean, strict=False).fill_null(default).alias(gcol))\n",
    "    else:\n",
    "        profiles_df = profiles_df.with_columns(pl.lit(default).cast(pl.Boolean).alias(gcol))\n",
    "\n",
    "universe_size = int(profiles_df.height)\n",
    "shortlist_size = int(shortlist_df.height)\n",
    "\n",
    "# ============================ Resumen r√°pido =================================\n",
    "print(\"---- Resumen general ----\")\n",
    "print(f\"[Universe] s√≠mbolos perfilados = {universe_size}\")\n",
    "print(f\"[Shortlist] s√≠mbolos seleccionados = {shortlist_size}\")\n",
    "\n",
    "# ============================ Percentiles de scores ==========================\n",
    "def _print_percentiles(df: pl.DataFrame, col: str, label: str):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    s = df.select(pl.col(col).cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0)).to_series()\n",
    "    if s.len() == 0:\n",
    "        return\n",
    "    q = df.select([\n",
    "        pl.col(col).quantile(0.10).alias(\"p10\"),\n",
    "        pl.col(col).quantile(0.25).alias(\"p25\"),\n",
    "        pl.col(col).quantile(0.50).alias(\"p50\"),\n",
    "        pl.col(col).quantile(0.75).alias(\"p75\"),\n",
    "        pl.col(col).quantile(0.90).alias(\"p90\"),\n",
    "        pl.col(col).quantile(0.95).alias(\"p95\"),\n",
    "    ]).row(0)\n",
    "    print(\n",
    "        f\"[Percentiles] {label}: \"\n",
    "        f\"p10={q[0]:.3f} | p25={q[1]:.3f} | p50={q[2]:.3f} | \"\n",
    "        f\"p75={q[3]:.3f} | p90={q[4]:.3f} | p95={q[5]:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"---- Distribuciones clave (percentiles) ----\")\n",
    "_print_percentiles(profiles_df, \"SCORE_FINAL\", \"SCORE_FINAL\")\n",
    "_print_percentiles(profiles_df, \"core_score_trend\", \"core_score_trend\")\n",
    "_print_percentiles(profiles_df, \"core_score_range\", \"core_score_range\")\n",
    "if \"score_trend_follow\" in profiles_df.columns:\n",
    "    _print_percentiles(profiles_df, \"score_trend_follow\", \"score_trend_follow\")\n",
    "if \"score_mean_reversion\" in profiles_df.columns:\n",
    "    _print_percentiles(profiles_df, \"score_mean_reversion\", \"score_mean_reversion\")\n",
    "if \"score_breakout\" in profiles_df.columns:\n",
    "    _print_percentiles(profiles_df, \"score_breakout\", \"score_breakout\")\n",
    "\n",
    "# ============================ Diagn√≥stico de umbrales ========================\n",
    "def _count_expr(expr: pl.Expr) -> int:\n",
    "    return int(profiles_df.filter(expr).height)\n",
    "\n",
    "base_expr = (\n",
    "    (pl.col(\"gate_all\") == True) &\n",
    "    (pl.col(\"SCORE_FINAL\") >= min_SCORE_FINAL_for_shortlist)\n",
    ")\n",
    "\n",
    "if require_structure_loose_gate and \"passed_structure_gate_loose\" in profiles_df.columns:\n",
    "    base_expr = base_expr & (pl.col(\"passed_structure_gate_loose\") == True)\n",
    "\n",
    "if require_viability_gate and \"passed_viability_gate\" in profiles_df.columns:\n",
    "    base_expr = base_expr & (pl.col(\"passed_viability_gate\") == True)\n",
    "\n",
    "n_base = _count_expr(base_expr)\n",
    "\n",
    "n_trend_strong = _count_expr(base_expr & (pl.col(\"core_score_trend\") >= min_profile_score_for_shortlist))\n",
    "n_range_strong = _count_expr(base_expr & (pl.col(\"core_score_range\") >= min_profile_score_for_shortlist))\n",
    "\n",
    "print(\"---- Escala de umbrales (diagn√≥stico) ----\")\n",
    "print(f\"[Base candidates] pasan gates base = {n_base} ({_pct(n_base, universe_size):.1f}%)\")\n",
    "print(\n",
    "    f\"[Strong TREND] core_score_trend ‚â• {min_profile_score_for_shortlist:.2f} \"\n",
    "    f\"= {n_trend_strong} ({_pct(n_trend_strong, max(n_base,1)):.1f}% de base)\"\n",
    ")\n",
    "print(\n",
    "    f\"[Strong RANGE] core_score_range ‚â• {min_profile_score_for_shortlist:.2f} \"\n",
    "    f\"= {n_range_strong} ({_pct(n_range_strong, max(n_base,1)):.1f}% de base)\"\n",
    ")\n",
    "\n",
    "# Heur√≠stica de \"umbral fuera de escala\"\n",
    "# Si menos de 5% de la base cumple el umbral, probablemente el umbral es agresivo\n",
    "if n_base > 0:\n",
    "    frac_trend_strong = n_trend_strong / n_base\n",
    "    frac_range_strong = n_range_strong / n_base\n",
    "\n",
    "    if frac_trend_strong < 0.05:\n",
    "        print(\n",
    "            \"[Celda 10D][WARN] Umbral de TREND fuerte parece agresivo para este universo \"\n",
    "            f\"(solo {_pct(n_trend_strong, n_base):.1f}% de la base lo supera).\"\n",
    "        )\n",
    "    if frac_range_strong < 0.05:\n",
    "        print(\n",
    "            \"[Celda 10D][WARN] Umbral de RANGE/MR fuerte parece agresivo para este universo \"\n",
    "            f\"(solo {_pct(n_range_strong, n_base):.1f}% de la base lo supera).\"\n",
    "        )\n",
    "\n",
    "# ============================ Coherencia profile vs type ====================\n",
    "print(\"---- Coherencia strategy_profile vs strategy_type_primary ----\")\n",
    "\n",
    "has_profile = \"strategy_profile\" in profiles_df.columns\n",
    "has_type = \"strategy_type_primary\" in profiles_df.columns\n",
    "\n",
    "alignment_rate = None\n",
    "conf_table = None\n",
    "\n",
    "if has_profile and has_type:\n",
    "    # Mapeo esperado \"razonable\" (no perfecto, pero √∫til)\n",
    "    # TREND  ‚Üî TREND_FOLLOW\n",
    "    # MR     ‚Üî MEAN_REVERSION\n",
    "    # BREAKOUT ‚Üî BREAKOUT\n",
    "    # MIXED  ‚Üî cualquiera (neutral)\n",
    "    df_ct = profiles_df.select([\n",
    "        \"symbol\",\n",
    "        pl.col(\"strategy_profile\").fill_null(\"NA\").alias(\"profile\"),\n",
    "        pl.col(\"strategy_type_primary\").fill_null(\"NA\").alias(\"type\"),\n",
    "    ])\n",
    "\n",
    "    # Tabla de cruces\n",
    "    conf_table = (\n",
    "        df_ct\n",
    "        .group_by([\"profile\", \"type\"])\n",
    "        .agg(pl.len().alias(\"n\"))\n",
    "        .sort([\"profile\", \"n\"], descending=[False, True])\n",
    "    )\n",
    "\n",
    "    # Regla de alineaci√≥n\n",
    "    aligned_expr = (\n",
    "        (pl.col(\"profile\") == \"MIXED\") |\n",
    "        ((pl.col(\"profile\") == \"TREND\") & (pl.col(\"type\") == \"TREND_FOLLOW\")) |\n",
    "        ((pl.col(\"profile\") == \"MR\") & (pl.col(\"type\") == \"MEAN_REVERSION\")) |\n",
    "        ((pl.col(\"profile\") == \"BREAKOUT\") & (pl.col(\"type\") == \"BREAKOUT\"))\n",
    "    )\n",
    "\n",
    "    n_aligned = int(df_ct.filter(aligned_expr).height)\n",
    "    alignment_rate = n_aligned / universe_size if universe_size > 0 else 0.0\n",
    "\n",
    "    print(f\"[Alignment] rate aproximado = {alignment_rate*100:.1f}% (MIXED cuenta como neutral)\")\n",
    "    print(\"[Confusion] profile ‚Üî type (top combinaciones):\")\n",
    "    print(conf_table.head(12))\n",
    "\n",
    "    if alignment_rate < 0.45:\n",
    "        print(\n",
    "            \"[Celda 10D][WARN] La coherencia entre 'strategy_profile' y \"\n",
    "            \"'strategy_type_primary' es baja. Esto no siempre es un bug, \"\n",
    "            \"pero sugiere que tus lentes macro vs micro est√°n usando criterios distintos \"\n",
    "            \"o escalas diferentes.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"[Celda 10D] Aviso: no hay columnas suficientes para validar coherencia profile vs type.\")\n",
    "\n",
    "# ============================ Top rankings operativos ========================\n",
    "def _print_top(df: pl.DataFrame, col: str, k: int = 10, extra: Optional[List[str]] = None, title: str = \"\"):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    cols = [\"symbol\", col]\n",
    "    if extra:\n",
    "        for e in extra:\n",
    "            if e in df.columns and e not in cols:\n",
    "                cols.append(e)\n",
    "    t = (\n",
    "        df\n",
    "        .select(cols)\n",
    "        .sort(by=[col, \"symbol\"], descending=[True, False])\n",
    "        .head(k)\n",
    "    )\n",
    "    hdr = title if title else f\"Top {k} por {col}\"\n",
    "    print(f\"---- {hdr} ----\")\n",
    "    print(t)\n",
    "\n",
    "extra_cols = [\"strategy_profile\", \"strategy_type_primary\", \"class_family\", \"SCORE_FINAL\"]\n",
    "\n",
    "_print_top(profiles_df, \"core_score_trend\", 10, extra_cols, \"Top 10 core_score_trend\")\n",
    "_print_top(profiles_df, \"core_score_range\", 10, extra_cols, \"Top 10 core_score_range\")\n",
    "if \"score_trend_follow\" in profiles_df.columns:\n",
    "    _print_top(profiles_df, \"score_trend_follow\", 10, extra_cols, \"Top 10 score_trend_follow\")\n",
    "if \"score_mean_reversion\" in profiles_df.columns:\n",
    "    _print_top(profiles_df, \"score_mean_reversion\", 10, extra_cols, \"Top 10 score_mean_reversion\")\n",
    "if \"score_breakout\" in profiles_df.columns:\n",
    "    _print_top(profiles_df, \"score_breakout\", 10, extra_cols, \"Top 10 score_breakout\")\n",
    "\n",
    "# ============================ Shortlist sanity ===============================\n",
    "print(\"---- Sanity de shortlist ----\")\n",
    "if shortlist_df.is_empty():\n",
    "    print(\"[Celda 10D][ERROR] Shortlist vac√≠a. Esto debi√≥ haber fallado en 10C.\")\n",
    "    raise RuntimeError(\"[Celda 10D][ERROR] Shortlist vac√≠a detectada post 10C.\")\n",
    "\n",
    "# Comparar cobertura de shortlist vs perfiles\n",
    "short_symbols = set(shortlist_df[\"symbol\"].to_list()) if \"symbol\" in shortlist_df.columns else set()\n",
    "prof_symbols = set(profiles_df[\"symbol\"].to_list()) if \"symbol\" in profiles_df.columns else set()\n",
    "missing_in_profiles = short_symbols - prof_symbols\n",
    "\n",
    "if missing_in_profiles:\n",
    "    print(f\"[Celda 10D][WARN] S√≠mbolos en shortlist que no est√°n en perfiles: {sorted(list(missing_in_profiles))[:20]}\")\n",
    "\n",
    "print(f\"[Shortlist] cobertura sobre universo = {_pct(len(short_symbols), universe_size):.1f}%\")\n",
    "\n",
    "# ============================ Chequeo de duplicados ==========================\n",
    "# Si por alguna raz√≥n volvieron duplicados en perfiles, lo marcamos duro.\n",
    "dup_count = (\n",
    "    profiles_df\n",
    "    .group_by(\"symbol\")\n",
    "    .agg(pl.len().alias(\"n\"))\n",
    "    .filter(pl.col(\"n\") > 1)\n",
    "    .height\n",
    ")\n",
    "if dup_count > 0:\n",
    "    print(f\"[Celda 10D][WARN] Detectados {dup_count} s√≠mbolos duplicados en perfiles. Esto NO deber√≠a ocurrir en final.\")\n",
    "    # No reventamos por defecto, pero lo dejamos registrado.\n",
    "\n",
    "# ============================ (Opcional) Histograms r√°pidos ==================\n",
    "# S√≥lo si est√°s en notebook y tienes matplotlib.\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def _plot_hist(df: pl.DataFrame, col: str, title: str):\n",
    "        if col not in df.columns:\n",
    "            return\n",
    "        data = (\n",
    "            df.select(pl.col(col).cast(pl.Float64, strict=False).fill_null(0.0).clip(0.0, 1.0))\n",
    "              .to_series()\n",
    "              .to_list()\n",
    "        )\n",
    "        if len(data) == 0:\n",
    "            return\n",
    "        plt.figure()\n",
    "        plt.hist(data, bins=20)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "\n",
    "    print(\"---- Histograms (opcionales) ----\")\n",
    "    _plot_hist(profiles_df, \"SCORE_FINAL\", \"Distribuci√≥n SCORE_FINAL\")\n",
    "    _plot_hist(profiles_df, \"core_score_trend\", \"Distribuci√≥n core_score_trend\")\n",
    "    _plot_hist(profiles_df, \"core_score_range\", \"Distribuci√≥n core_score_range\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"[Celda 10D] matplotlib no disponible o entorno no gr√°fico; se omiten histograms.\")\n",
    "\n",
    "# ============================ Report stats ===================================\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "\n",
    "GLOBAL_STATE[\"report_stats\"][\"c10D\"] = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\n",
    "        \"scores_table\": str(scores_table_path),\n",
    "        \"asset_strategy_profiles\": str(asset_profiles_path),\n",
    "        \"asset_strategy_shortlist\": str(asset_shortlist_path),\n",
    "    },\n",
    "    \"universe_size\": universe_size,\n",
    "    \"shortlist_size\": shortlist_size,\n",
    "    \"thresholds\": {\n",
    "        \"min_SCORE_FINAL_for_shortlist\": min_SCORE_FINAL_for_shortlist,\n",
    "        \"min_profile_score_for_shortlist\": min_profile_score_for_shortlist,\n",
    "        \"require_structure_loose_gate\": require_structure_loose_gate,\n",
    "        \"require_viability_gate\": require_viability_gate,\n",
    "    },\n",
    "    \"base_candidates\": {\n",
    "        \"n_base\": n_base,\n",
    "        \"pct_base_of_universe\": _pct(n_base, universe_size),\n",
    "        \"n_trend_strong\": n_trend_strong,\n",
    "        \"pct_trend_strong_of_base\": _pct(n_trend_strong, max(n_base, 1)),\n",
    "        \"n_range_strong\": n_range_strong,\n",
    "        \"pct_range_strong_of_base\": _pct(n_range_strong, max(n_base, 1)),\n",
    "    },\n",
    "    \"alignment_rate_profile_vs_type\": float(alignment_rate) if alignment_rate is not None else None,\n",
    "    \"duplicate_symbol_groups_in_profiles\": int(dup_count),\n",
    "}\n",
    "\n",
    "print(\">>> Celda 10D :: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97581bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10E :: Handoff Operativo + cierre del pipeline\n",
      "[Celda 10E] RUN_ID = 20251218_190810\n",
      "[10E] OUT_SCORES_DIR = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "[10E] SCORES_PATH   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (exists=True)\n",
      "[10E] PROFILES_PATH= C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_profiles.parquet (exists=True)\n",
      "[10E] SHORTLIST_PATH= C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (exists=True)\n",
      "[10E] FREQ_ONLY_PATH= C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\freq_only_watchlist.parquet (exists=True)\n",
      "[10E] CONFIG_PATH  = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json (exists=True)\n",
      "[Celda 10E] thresholds ‚Üí SCORE_FINAL viable=0.3, core=0.4, premium=0.5, min_SCORE_FINAL_for_shortlist=0.3\n",
      "üìÅ INPUT ‚Üí scores_table            = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (rows=166, cols=56)\n",
      "üìÅ INPUT ‚Üí asset_strategy_profiles = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_profiles.parquet (rows=83, cols=55)\n",
      "üìÅ INPUT ‚Üí asset_strategy_shortlist= C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (rows=7, cols=58)\n",
      "[10E] candidates_table loaded ‚Üí rows=14\n",
      "[10E] best_per_symbol loaded  ‚Üí rows=7\n",
      "[10E] freq_only_watchlist loaded ‚Üí rows=0\n",
      "---- Handoff operativo (shortlist) ----\n",
      "[Universe] activos perfilados = 83\n",
      "[Shortlist] activos operativos = 7 (8.4%)\n",
      "---- Tabla final compacta (orden de implementaci√≥n) ----\n",
      "shape: (7, 19)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ priority_r ‚îÜ symbol ‚îÜ tier_label ‚îÜ SCORE_FIN ‚îÜ ‚Ä¶ ‚îÜ score_sta ‚îÜ score_via ‚îÜ score_str ‚îÜ secondary ‚îÇ\n",
      "‚îÇ ank        ‚îÜ ---    ‚îÜ _oper      ‚îÜ AL        ‚îÜ   ‚îÜ bility    ‚îÜ bility    ‚îÜ ucture    ‚îÜ _watch_fr ‚îÇ\n",
      "‚îÇ ---        ‚îÜ str    ‚îÜ ---        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ eq        ‚îÇ\n",
      "‚îÇ u32        ‚îÜ        ‚îÜ str        ‚îÜ f64       ‚îÜ   ‚îÜ f64       ‚îÜ f64       ‚îÜ f64       ‚îÜ ---       ‚îÇ\n",
      "‚îÇ            ‚îÜ        ‚îÜ            ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ bool      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1          ‚îÜ BNBUSD ‚îÜ CORE       ‚îÜ 0.488358  ‚îÜ ‚Ä¶ ‚îÜ 0.811142  ‚îÜ 0.477864  ‚îÜ 0.5       ‚îÜ false     ‚îÇ\n",
      "‚îÇ 2          ‚îÜ BTCUSD ‚îÜ CORE       ‚îÜ 0.461609  ‚îÜ ‚Ä¶ ‚îÜ 0.645267  ‚îÜ 0.578808  ‚îÜ 0.75      ‚îÜ false     ‚îÇ\n",
      "‚îÇ 3          ‚îÜ XAUAUD ‚îÜ CORE       ‚îÜ 0.479258  ‚îÜ ‚Ä¶ ‚îÜ 0.708676  ‚îÜ 0.707754  ‚îÜ 0.737629  ‚îÜ false     ‚îÇ\n",
      "‚îÇ 4          ‚îÜ ETHUSD ‚îÜ CORE       ‚îÜ 0.45238   ‚îÜ ‚Ä¶ ‚îÜ 0.7648    ‚îÜ 0.508911  ‚îÜ 0.5       ‚îÜ false     ‚îÇ\n",
      "‚îÇ 5          ‚îÜ LVMH   ‚îÜ VIABLE     ‚îÜ 0.349748  ‚îÜ ‚Ä¶ ‚îÜ 0.619977  ‚îÜ 0.554935  ‚îÜ 0.726602  ‚îÜ false     ‚îÇ\n",
      "‚îÇ 6          ‚îÜ XAUUSD ‚îÜ CORE       ‚îÜ 0.451056  ‚îÜ ‚Ä¶ ‚îÜ 0.732288  ‚îÜ 0.540276  ‚îÜ 0.731893  ‚îÜ false     ‚îÇ\n",
      "‚îÇ 7          ‚îÜ XAUEUR ‚îÜ CORE       ‚îÜ 0.461201  ‚îÜ ‚Ä¶ ‚îÜ 0.722965  ‚îÜ 0.595684  ‚îÜ 0.745004  ‚îÜ false     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- Resumen por tier operativo ----\n",
      "shape: (2, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ tier_label_oper ‚îÜ n   ‚îÇ\n",
      "‚îÇ ---             ‚îÜ --- ‚îÇ\n",
      "‚îÇ str             ‚îÜ u32 ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ CORE            ‚îÜ 6   ‚îÇ\n",
      "‚îÇ VIABLE          ‚îÜ 1   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "---- WATCH_FREQ (freq-only secondary) ----\n",
      "[10E] No hay freq-only_watchlist para este run o est√° vac√≠a.\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\handoff_operational.parquet (rows=7, cols=19)\n",
      ">>> Celda 10E :: OK ‚Äî Pipeline listo para cerrar y comenzar construcci√≥n de estrategias por activo\n"
     ]
    }
   ],
   "source": [
    "# Celda 10E ‚Äî Handoff Operativo y Cierre del Pipeline (post 10B/10C/10D)\n",
    "# =============================================================================\n",
    "# Objetivo:\n",
    "#   - Consolidar una vista final, operativa y legible por activo.\n",
    "#   - Basarse en:\n",
    "#       * scores_table.parquet (Celda 10)\n",
    "#       * candidates/best/freq_only (Celda 10B) [opcionales]\n",
    "#       * asset_strategy_profiles.parquet (Celda 10C)\n",
    "#       * asset_strategy_shortlist.parquet (Celda 10C)\n",
    "#       * QA ya ejecutado en Celda 10D\n",
    "#   - Generar:\n",
    "#       * Prints finales claros para decidir implementaci√≥n por activo.\n",
    "#       * Un artefacto ligero opcional: handoff_operational.parquet\n",
    "#   - Integrar una estrategia expl√≠cita de \"freq-only\" como se√±al secundaria:\n",
    "#       * Si existe freq_only_watchlist.parquet y tiene filas:\n",
    "#           - Se marca como \"secondary_watch_freq=True\" en el handoff.\n",
    "#           - No entra al shortlist principal por defecto.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "import json\n",
    "import math\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 10E :: Handoff Operativo + cierre del pipeline\")\n",
    "\n",
    "# ============================ Guardas b√°sicas ================================\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 10E][ERROR] GLOBAL_STATE no existe. Ejecuta celdas previas.\")\n",
    "\n",
    "for key in (\"paths\", \"run_id\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 10E][ERROR] Falta GLOBAL_STATE['{key}'].\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE.get(\"run_id\")\n",
    "paths: Dict[str, Any] = GLOBAL_STATE[\"paths\"]\n",
    "metrics_state: Dict[str, Any] = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "\n",
    "OUT_SCORES_DIR = Path(paths.get(\"scores\", Path(paths[\"stability\"]).parent / \"scores\")).resolve()\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scores_table_path = Path(\n",
    "    metrics_state.get(\"scores_table_path\", OUT_SCORES_DIR / \"scores_table.parquet\")\n",
    ").resolve()\n",
    "\n",
    "asset_profiles_path = Path(\n",
    "    metrics_state.get(\"asset_strategy_profiles_path\", OUT_SCORES_DIR / \"asset_strategy_profiles.parquet\")\n",
    ").resolve()\n",
    "\n",
    "asset_shortlist_path = Path(\n",
    "    metrics_state.get(\"asset_strategy_shortlist_path\", OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\")\n",
    ").resolve()\n",
    "\n",
    "# Artefactos 10B (opcionales)\n",
    "candidates_table_path = OUT_SCORES_DIR / \"candidates_table.parquet\"\n",
    "best_per_symbol_path  = OUT_SCORES_DIR / \"best_per_symbol.parquet\"\n",
    "freq_only_path        = OUT_SCORES_DIR / \"freq_only_watchlist.parquet\"\n",
    "\n",
    "CONFIG_PATH = Path(paths.get(\"config\", Path(paths[\"diagnostics\"]) / \"config.json\")).resolve()\n",
    "\n",
    "def _require_file(p: Path, label: str = \"\"):\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"[Celda 10E][ERROR] Falta input requerido {label}: {p}\")\n",
    "\n",
    "def _safe_read_json(p: Path) -> Dict[str, Any]:\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    if \"preset\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"preset\").cast(pl.Utf8, strict=False).fill_null(\"\").str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def _ensure_float(df: pl.DataFrame, name: str, default: float = 0.0) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Float64, strict=False).fill_null(default).alias(name))\n",
    "    return df.with_columns(pl.lit(default).cast(pl.Float64).alias(name))\n",
    "\n",
    "def _ensure_str(df: pl.DataFrame, name: str, default: Optional[str] = None) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df.with_columns(pl.col(name).cast(pl.Utf8, strict=False).alias(name))\n",
    "    return df.with_columns(pl.lit(default, dtype=pl.Utf8).alias(name))\n",
    "\n",
    "def _pct(x: int, n: int) -> float:\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "    return 100.0 * x / n\n",
    "\n",
    "print(f\"[Celda 10E] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[10E] OUT_SCORES_DIR = {str(OUT_SCORES_DIR)}\")\n",
    "print(f\"[10E] SCORES_PATH   = {str(scores_table_path)} (exists={scores_table_path.exists()})\")\n",
    "print(f\"[10E] PROFILES_PATH= {str(asset_profiles_path)} (exists={asset_profiles_path.exists()})\")\n",
    "print(f\"[10E] SHORTLIST_PATH= {str(asset_shortlist_path)} (exists={asset_shortlist_path.exists()})\")\n",
    "print(f\"[10E] FREQ_ONLY_PATH= {str(freq_only_path)} (exists={freq_only_path.exists()})\")\n",
    "print(f\"[10E] CONFIG_PATH  = {str(CONFIG_PATH)} (exists={CONFIG_PATH.exists()})\")\n",
    "\n",
    "# ============================ Config =========================================\n",
    "if \"config\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"config\"], dict):\n",
    "    cfg = GLOBAL_STATE[\"config\"]\n",
    "else:\n",
    "    cfg = _safe_read_json(CONFIG_PATH)\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "scores_cfg   = cfg.get(\"scores\", {}) or {}\n",
    "spcfg        = cfg.get(\"strategy_profiles\", {}) or {}\n",
    "\n",
    "score_thr_cfg = scores_cfg.get(\"score_thresholds\", {}) or {}\n",
    "score_viable_thr      = float(score_thr_cfg.get(\"viable\",      0.30))\n",
    "score_interesting_thr = float(score_thr_cfg.get(\"interesting\", 0.40))\n",
    "score_premium_thr     = float(score_thr_cfg.get(\"premium\",     0.50))\n",
    "\n",
    "min_SCORE_FINAL_for_shortlist = float(spcfg.get(\"min_SCORE_FINAL_for_shortlist\", 0.30))\n",
    "\n",
    "print(\n",
    "    \"[Celda 10E] thresholds ‚Üí \"\n",
    "    f\"SCORE_FINAL viable={score_viable_thr}, core={score_interesting_thr}, premium={score_premium_thr}, \"\n",
    "    f\"min_SCORE_FINAL_for_shortlist={min_SCORE_FINAL_for_shortlist}\"\n",
    ")\n",
    "\n",
    "# ============================ Cargar inputs ==================================\n",
    "_require_file(scores_table_path, \"scores_table\")\n",
    "_require_file(asset_profiles_path, \"asset_strategy_profiles\")\n",
    "_require_file(asset_shortlist_path, \"asset_strategy_shortlist\")\n",
    "\n",
    "scores_df   = _norm_sym(pl.read_parquet(scores_table_path))\n",
    "profiles_df = _norm_sym(pl.read_parquet(asset_profiles_path))\n",
    "shortlist_df = _norm_sym(pl.read_parquet(asset_shortlist_path))\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí scores_table            = {str(scores_table_path)} (rows={scores_df.height}, cols={len(scores_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí asset_strategy_profiles = {str(asset_profiles_path)} (rows={profiles_df.height}, cols={len(profiles_df.columns)})\")\n",
    "print(f\"üìÅ INPUT ‚Üí asset_strategy_shortlist= {str(asset_shortlist_path)} (rows={shortlist_df.height}, cols={len(shortlist_df.columns)})\")\n",
    "\n",
    "if shortlist_df.is_empty():\n",
    "    raise RuntimeError(\"[Celda 10E][ERROR] Shortlist vac√≠a. No hay nada que entregar operativamente.\")\n",
    "\n",
    "# ============================ Cargar 10B opcional =============================\n",
    "candidates_df = pl.read_parquet(candidates_table_path) if candidates_table_path.exists() else pl.DataFrame()\n",
    "best_df       = pl.read_parquet(best_per_symbol_path)  if best_per_symbol_path.exists() else pl.DataFrame()\n",
    "freq_only_df  = pl.read_parquet(freq_only_path)        if freq_only_path.exists() else pl.DataFrame()\n",
    "\n",
    "candidates_df = _norm_sym(candidates_df)\n",
    "best_df       = _norm_sym(best_df)\n",
    "freq_only_df  = _norm_sym(freq_only_df)\n",
    "\n",
    "print(f\"[10E] candidates_table loaded ‚Üí rows={candidates_df.height if not candidates_df.is_empty() else 0}\")\n",
    "print(f\"[10E] best_per_symbol loaded  ‚Üí rows={best_df.height if not best_df.is_empty() else 0}\")\n",
    "print(f\"[10E] freq_only_watchlist loaded ‚Üí rows={freq_only_df.height if not freq_only_df.is_empty() else 0}\")\n",
    "\n",
    "# ============================ Normalizar columnas esperadas ==================\n",
    "for col in [\n",
    "    \"SCORE_FINAL\", \"SCORE_FINAL_trend\", \"SCORE_FINAL_range\",\n",
    "    \"core_score_trend\", \"core_score_range\", \"core_score_breakout\",\n",
    "    \"score_significance\", \"score_opportunity\", \"score_stability\",\n",
    "    \"score_viability\", \"score_structure\",\n",
    "]:\n",
    "    shortlist_df = _ensure_float(shortlist_df, col, 0.0)\n",
    "\n",
    "for col in [\"strategy_type_primary\", \"strategy_type_secondary\", \"strategy_profile\"]:\n",
    "    shortlist_df = _ensure_str(shortlist_df, col, None)\n",
    "\n",
    "# Ranks si existen\n",
    "for col in [\"shortlist_rank_trend\", \"shortlist_rank_range\", \"shortlist_rank_breakout\"]:\n",
    "    if col not in shortlist_df.columns:\n",
    "        shortlist_df = shortlist_df.with_columns(pl.lit(None).cast(pl.Int64).alias(col))\n",
    "\n",
    "# ============================ Estrategia \"freq-only\" secundaria ===============\n",
    "# Filosof√≠a:\n",
    "#   - freq-only NO se mezcla con shortlist principal.\n",
    "#   - Se trata como \"se√±al secundaria\" de monitoreo.\n",
    "#   - Si hay activos en freq_only_watchlist:\n",
    "#       * se crea una marca para visualizaci√≥n en el handoff.\n",
    "#       * se imprimen por separado como WATCH_FREQ.\n",
    "\n",
    "freq_only_symbols = set(freq_only_df[\"symbol\"].to_list()) if (not freq_only_df.is_empty() and \"symbol\" in freq_only_df.columns) else set()\n",
    "\n",
    "shortlist_df = shortlist_df.with_columns(\n",
    "    pl.col(\"symbol\").is_in(list(freq_only_symbols)).alias(\"secondary_watch_freq\")\n",
    ")\n",
    "\n",
    "# ============================ Recomendaciones por activo ======================\n",
    "# recommended_family:\n",
    "#   - Determinada por el mejor rank disponible entre trend/range/breakout.\n",
    "#   - Si no hay ranks, fallback a strategy_profile.\n",
    "\n",
    "shortlist_df = shortlist_df.with_columns([\n",
    "    pl.min_horizontal([\n",
    "        pl.col(\"shortlist_rank_trend\").fill_null(10_000),\n",
    "        pl.col(\"shortlist_rank_range\").fill_null(10_000),\n",
    "        pl.col(\"shortlist_rank_breakout\").fill_null(10_000),\n",
    "    ]).alias(\"__best_rank__\")\n",
    "])\n",
    "\n",
    "shortlist_df = shortlist_df.with_columns(\n",
    "    pl.when(pl.col(\"__best_rank__\") >= 10_000)\n",
    "      .then(\n",
    "          pl.when(pl.col(\"strategy_profile\").is_not_null()).then(pl.col(\"strategy_profile\")).otherwise(pl.lit(\"MIXED\"))\n",
    "      )\n",
    "      .when(pl.col(\"__best_rank__\") == pl.col(\"shortlist_rank_trend\").fill_null(10_000)).then(pl.lit(\"TREND\"))\n",
    "      .when(pl.col(\"__best_rank__\") == pl.col(\"shortlist_rank_range\").fill_null(10_000)).then(pl.lit(\"RANGE\"))\n",
    "      .otherwise(pl.lit(\"BREAKOUT\"))\n",
    "      .alias(\"recommended_family\")\n",
    ")\n",
    "\n",
    "# recommended_type:\n",
    "#   - Por defecto usa strategy_type_primary.\n",
    "#   - Si family sugiere algo distinto y el secondary encaja, lo anotamos.\n",
    "\n",
    "shortlist_df = shortlist_df.with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"recommended_family\") == \"TREND\") &\n",
    "        (pl.col(\"strategy_type_primary\") != \"TREND_FOLLOW\") &\n",
    "        (pl.col(\"strategy_type_secondary\") == \"TREND_FOLLOW\")\n",
    "    ).then(pl.lit(\"TREND_FOLLOW (secondary)\"))\n",
    "     .when(\n",
    "        (pl.col(\"recommended_family\") == \"RANGE\") &\n",
    "        (pl.col(\"strategy_type_primary\") != \"MEAN_REVERSION\") &\n",
    "        (pl.col(\"strategy_type_secondary\") == \"MEAN_REVERSION\")\n",
    "    ).then(pl.lit(\"MEAN_REVERSION (secondary)\"))\n",
    "     .otherwise(pl.col(\"strategy_type_primary\").fill_null(\"NA\"))\n",
    "     .alias(\"recommended_type\")\n",
    ")\n",
    "\n",
    "# Tier operativo simple\n",
    "shortlist_df = shortlist_df.with_columns(\n",
    "    pl.when(pl.col(\"SCORE_FINAL\") >= score_premium_thr).then(pl.lit(\"PREMIUM\"))\n",
    "      .when(pl.col(\"SCORE_FINAL\") >= score_interesting_thr).then(pl.lit(\"CORE\"))\n",
    "      .when(pl.col(\"SCORE_FINAL\") >= score_viable_thr).then(pl.lit(\"VIABLE\"))\n",
    "      .otherwise(pl.lit(\"LOW\"))\n",
    "      .alias(\"tier_label_oper\")\n",
    ")\n",
    "\n",
    "# Orden final operativo\n",
    "handoff_df = (\n",
    "    shortlist_df\n",
    "    .sort(by=[\"__best_rank__\", \"SCORE_FINAL\", \"symbol\"], descending=[False, True, False])\n",
    "    .with_row_index(\"priority_rank\", offset=1)\n",
    ")\n",
    "\n",
    "# ============================ Tabla compacta de handoff =======================\n",
    "handoff_cols = [\n",
    "    \"priority_rank\",\n",
    "    \"symbol\",\n",
    "    \"tier_label_oper\",\n",
    "    \"SCORE_FINAL\",\n",
    "    \"SCORE_FINAL_trend\", \"SCORE_FINAL_range\",\n",
    "    \"recommended_family\",\n",
    "    \"recommended_type\",\n",
    "    \"strategy_type_primary\", \"strategy_type_secondary\",\n",
    "    \"strategy_profile\",\n",
    "    \"core_score_trend\", \"core_score_range\", \"core_score_breakout\",\n",
    "    \"score_opportunity\", \"score_stability\", \"score_viability\", \"score_structure\",\n",
    "    \"secondary_watch_freq\",\n",
    "]\n",
    "\n",
    "for c in handoff_cols:\n",
    "    if c not in handoff_df.columns:\n",
    "        handoff_df = handoff_df.with_columns(pl.lit(None).alias(c))\n",
    "\n",
    "handoff_compact = handoff_df.select(handoff_cols)\n",
    "\n",
    "# ============================ Prints finales =================================\n",
    "universe_size = int(profiles_df.height) if not profiles_df.is_empty() else 0\n",
    "shortlist_size = int(handoff_compact.height)\n",
    "\n",
    "print(\"---- Handoff operativo (shortlist) ----\")\n",
    "print(f\"[Universe] activos perfilados = {universe_size}\")\n",
    "print(f\"[Shortlist] activos operativos = {shortlist_size} ({_pct(shortlist_size, universe_size):.1f}%)\")\n",
    "\n",
    "print(\"---- Tabla final compacta (orden de implementaci√≥n) ----\")\n",
    "print(handoff_compact)\n",
    "\n",
    "print(\"---- Resumen por tier operativo ----\")\n",
    "try:\n",
    "    print(\n",
    "        handoff_compact\n",
    "        .group_by(\"tier_label_oper\")\n",
    "        .agg(pl.len().alias(\"n\"))\n",
    "        .sort(\"n\", descending=True)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[Celda 10E] WARN resumen tier: {e}\")\n",
    "\n",
    "# ============================ FREQ-ONLY como se√±al secundaria =================\n",
    "print(\"---- WATCH_FREQ (freq-only secondary) ----\")\n",
    "if freq_only_df.is_empty():\n",
    "    print(\"[10E] No hay freq-only_watchlist para este run o est√° vac√≠a.\")\n",
    "else:\n",
    "    # Vista ligera: no forzamos columnas internas de 10B\n",
    "    cols_watch = [c for c in [\"symbol\", \"preset\", \"SCORE_FINAL\", \"score_opportunity\", \"n_per_month_total\", \"coverage_p\"] if c in freq_only_df.columns]\n",
    "    if not cols_watch:\n",
    "        cols_watch = [\"symbol\"] if \"symbol\" in freq_only_df.columns else []\n",
    "    watch_view = freq_only_df.select(cols_watch).head(50) if cols_watch else freq_only_df.head(50)\n",
    "    print(watch_view)\n",
    "    print(\n",
    "        \"[10E] Nota operativa: freq-only se mantiene fuera de shortlist principal. \"\n",
    "        \"√ösalo como radar de investigaci√≥n si en pr√≥ximos runs mejora significancia/viabilidad.\"\n",
    "    )\n",
    "\n",
    "# ============================ Artefacto opcional de cierre ===================\n",
    "handoff_path = OUT_SCORES_DIR / \"handoff_operational.parquet\"\n",
    "handoff_compact.write_parquet(handoff_path)\n",
    "\n",
    "if (not handoff_path.exists()) or handoff_path.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 10E][ERROR] No se pudo escribir handoff_operational.parquet\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(handoff_path)} (rows={handoff_compact.height}, cols={handoff_compact.width})\")\n",
    "\n",
    "# ============================ GLOBAL_STATE ===================================\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"handoff_operational_path\"] = str(handoff_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c10E\"] = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"paths\": {\n",
    "        \"scores_table\": str(scores_table_path),\n",
    "        \"asset_strategy_profiles\": str(asset_profiles_path),\n",
    "        \"asset_strategy_shortlist\": str(asset_shortlist_path),\n",
    "        \"handoff_operational\": str(handoff_path),\n",
    "        \"candidates_table\": str(candidates_table_path) if candidates_table_path.exists() else None,\n",
    "        \"best_per_symbol\": str(best_per_symbol_path) if best_per_symbol_path.exists() else None,\n",
    "        \"freq_only_watchlist\": str(freq_only_path) if freq_only_path.exists() else None,\n",
    "    },\n",
    "    \"universe_size\": int(universe_size),\n",
    "    \"shortlist_size\": int(shortlist_size),\n",
    "    \"thresholds\": {\n",
    "        \"score_final\": {\n",
    "            \"viable\": score_viable_thr,\n",
    "            \"core\": score_interesting_thr,\n",
    "            \"premium\": score_premium_thr,\n",
    "        },\n",
    "        \"min_SCORE_FINAL_for_shortlist\": min_SCORE_FINAL_for_shortlist,\n",
    "    },\n",
    "    \"freq_only_secondary\": {\n",
    "        \"enabled\": True,\n",
    "        \"n_watch\": int(freq_only_df.height) if not freq_only_df.is_empty() else 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\">>> Celda 10E :: OK ‚Äî Pipeline listo para cerrar y comenzar construcci√≥n de estrategias por activo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2154620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 10F :: Corr Matrix Builder & QA (M5) [V3.1 TIME FIX + AUTODETECT + POLARS ENGINE FIX]\n",
      "[Celda 10F] RUN_ID = 20251218_190810\n",
      "[Celda 10F] lookback_days = 90\n",
      "[Celda 10F] Bulk candidates scored:\n",
      "   - C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data  score=41\n",
      "   - C:\\Quant\\MT5_Data_Extraction\\bulk_data  score=6\n",
      "   - C:\\Quant\\data\\bulk_data  score=-10\n",
      "[Celda 10F] bulk_data dir     = C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\n",
      "[Celda 10F] processed_data   = C:\\Quant\\MT5_Data_Extraction\\processed_data\n",
      "[Celda 10F] rates/m5 dir     = C:\\Quant\\MT5_Data_Extraction\\data\\bulk_data\\m5_raw\n",
      "üìÅ INPUT ‚Üí asset_strategy_shortlist = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (symbols=7)\n",
      "[Celda 10F] Layout detectado ‚Üí partitioned=True (symbol=* dirs=110)\n",
      "[Celda 10F] Mapping s√≠mbolo ‚Üí archivos: found=7/7, missing=0\n",
      "[Celda 10F] S√≠mbolos con m√∫ltiples parquets (recortados a recientes) = ['XAUEUR', 'ETHUSD', 'XAUUSD', 'BNBUSD', 'BTCUSD', 'XAUAUD', 'LVMH']\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\corr_matrix_builder_mapping.parquet\n",
      "[Celda 10F] Series v√°lidas tras carga = 7\n",
      "üíæ OUTPUT ‚Üí corr_matrix = C:\\Quant\\MT5_Data_Extraction\\processed_data\\corr_matrix_5m.csv (rows=7, cols=7)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\corr_matrix_builder_report.html (OK)\n",
      ">>> Celda 10F :: OK (corr_matrix M5 construida/verificada) [V3.1 TIME FIX + AUTODETECT + POLARS ENGINE FIX]\n"
     ]
    }
   ],
   "source": [
    " # =============================================================================\n",
    "# Celda 10F :: Corr Matrix Builder & QA (M5)  [V3.1 TIME FIX + AUTODETECT]\n",
    "# Objetivo:\n",
    "#   - Asegurar correlaci√≥n real para Celda 11.\n",
    "#   - Detectar layout real de M5:\n",
    "#       (A) Dataset particionado: m5_raw/symbol=XXX/year=YYYY/month=MM/part=*.parquet\n",
    "#       (B) 1 parquet por s√≠mbolo\n",
    "#       (C) carpetas por s√≠mbolo sin \"symbol=\"\n",
    "#   - Construir corr_matrix_5m.csv con lookback configurable.\n",
    "#   - Writes limpios + QA expl√≠cito.\n",
    "#\n",
    "# Fix clave:\n",
    "#   - Tus parquets usan 'timestamp_utc' y 'timestamp_gye'.\n",
    "#   - Se agregan a TIME_CANDS + autodetecci√≥n robusta.\n",
    "#\n",
    "# HOTFIX v3.1.1:\n",
    "#   - Polars DeprecationWarning:\n",
    "#       reemplaza collect(streaming=True) -> collect(engine=\"streaming\")\n",
    "#       con fallback compatible.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import re\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PD_OK = True\n",
    "except Exception:\n",
    "    pd = None\n",
    "    PD_OK = False\n",
    "\n",
    "print(\">>> Celda 10F :: Corr Matrix Builder & QA (M5) [V3.1 TIME FIX + AUTODETECT + POLARS ENGINE FIX]\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Validaciones de estado\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] GLOBAL_STATE no existe. Ejecuta las celdas iniciales.\")\n",
    "\n",
    "for key in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 10F][ERROR] GLOBAL_STATE incompleto; falta clave '{key}'.\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "RUN_ID       = GLOBAL_STATE[\"run_id\"]\n",
    "paths: Dict[str, Any] = GLOBAL_STATE.get(\"paths\", {}) or {}\n",
    "\n",
    "for k in (\"scores\", \"diagnostics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 10F][ERROR] Falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "OUT_SCORES_DIR  = Path(paths[\"scores\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths.get(\"metrics\", OUT_SCORES_DIR.parent / \"metrics\")).resolve()\n",
    "\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Config\n",
    "# -----------------------------------------------------------------------------\n",
    "lookback_days = 90\n",
    "cfg = GLOBAL_STATE.get(\"config\", {}) or {}\n",
    "corr_cfg = (cfg.get(\"corr\", {}) or {}) if isinstance(cfg, dict) else {}\n",
    "try:\n",
    "    lookback_days = int(corr_cfg.get(\"lookback_days\", lookback_days))\n",
    "except Exception:\n",
    "    lookback_days = 90\n",
    "if lookback_days <= 0:\n",
    "    lookback_days = 90\n",
    "\n",
    "print(f\"[Celda 10F] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[Celda 10F] lookback_days = {lookback_days}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Auto-detect de paths\n",
    "# -----------------------------------------------------------------------------\n",
    "def _first_existing(cands: List[Path]) -> Optional[Path]:\n",
    "    for p in cands:\n",
    "        try:\n",
    "            if p is not None and Path(p).exists():\n",
    "                return Path(p).resolve()\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _has_parquet(p: Path) -> bool:\n",
    "    try:\n",
    "        if not p.exists() or not p.is_dir():\n",
    "            return False\n",
    "        if any(p.glob(\"*.parquet\")):\n",
    "            return True\n",
    "        return any(p.rglob(\"*.parquet\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _score_bulk_dir(p: Path) -> int:\n",
    "    try:\n",
    "        if not p.exists() or not p.is_dir():\n",
    "            return -10\n",
    "        score = 0\n",
    "        if (p / \"m5_raw\").exists():\n",
    "            score += 6\n",
    "        if (p / \"rates_5m\").exists():\n",
    "            score += 6\n",
    "        if _has_parquet(p / \"m5_raw\"):\n",
    "            score += 25\n",
    "        if _has_parquet(p / \"rates_5m\"):\n",
    "            score += 25\n",
    "        if _has_parquet(p):\n",
    "            score += 10\n",
    "        return score\n",
    "    except Exception:\n",
    "        return -10\n",
    "\n",
    "bulk_cands = []\n",
    "if paths.get(\"bulk_data\"):\n",
    "    try:\n",
    "        bulk_cands.append(Path(paths[\"bulk_data\"]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "bulk_cands += [\n",
    "    PROJECT_ROOT / \"bulk_data\",\n",
    "    PROJECT_ROOT / \"data\" / \"bulk_data\",\n",
    "    PROJECT_ROOT.parent / \"data\" / \"bulk_data\",\n",
    "]\n",
    "\n",
    "bulk_cands_clean = []\n",
    "seen = set()\n",
    "for c in bulk_cands:\n",
    "    try:\n",
    "        rc = Path(c).resolve()\n",
    "        k = str(rc).lower()\n",
    "        if k not in seen:\n",
    "            bulk_cands_clean.append(rc)\n",
    "            seen.add(k)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "bulk_scores = [(p, _score_bulk_dir(p)) for p in bulk_cands_clean]\n",
    "print(\"[Celda 10F] Bulk candidates scored:\")\n",
    "for p, sc in sorted(bulk_scores, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   - {p}  score={sc}\")\n",
    "\n",
    "bulk_scores_sorted = sorted(bulk_scores, key=lambda x: x[1], reverse=True)\n",
    "if bulk_scores_sorted and bulk_scores_sorted[0][1] > -5:\n",
    "    BULK_DIR = bulk_scores_sorted[0][0]\n",
    "else:\n",
    "    BULK_DIR = _first_existing(bulk_cands_clean) or (PROJECT_ROOT / \"bulk_data\")\n",
    "\n",
    "BULK_DIR = Path(BULK_DIR).resolve()\n",
    "\n",
    "processed_cands = []\n",
    "if paths.get(\"processed_data\"):\n",
    "    try:\n",
    "        processed_cands.append(Path(paths[\"processed_data\"]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "processed_cands += [\n",
    "    PROJECT_ROOT / \"processed_data\",\n",
    "    PROJECT_ROOT / \"data\" / \"processed_data\",\n",
    "    OUT_SCORES_DIR.parent / \"processed_data\",\n",
    "    BULK_DIR.parent / \"processed_data\",\n",
    "]\n",
    "\n",
    "PROCESSED_DIR = _first_existing(processed_cands) or (PROJECT_ROOT / \"processed_data\")\n",
    "PROCESSED_DIR = Path(PROCESSED_DIR).resolve()\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rates_cands = []\n",
    "if paths.get(\"rates_5m\"):\n",
    "    try:\n",
    "        rates_cands.append(Path(paths[\"rates_5m\"]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "rates_cands += [\n",
    "    BULK_DIR / \"rates_5m\",\n",
    "    BULK_DIR / \"m5_raw\",\n",
    "]\n",
    "\n",
    "RATES_5M_DIR = None\n",
    "for cand in rates_cands:\n",
    "    if cand is None:\n",
    "        continue\n",
    "    cand = Path(cand)\n",
    "    if _has_parquet(cand):\n",
    "        RATES_5M_DIR = cand.resolve()\n",
    "        break\n",
    "\n",
    "if RATES_5M_DIR is None:\n",
    "    RATES_5M_DIR = (BULK_DIR / \"m5_raw\").resolve()\n",
    "\n",
    "print(f\"[Celda 10F] bulk_data dir     = {str(BULK_DIR)}\")\n",
    "print(f\"[Celda 10F] processed_data   = {str(PROCESSED_DIR)}\")\n",
    "print(f\"[Celda 10F] rates/m5 dir     = {str(RATES_5M_DIR)}\")\n",
    "\n",
    "paths[\"bulk_data\"]      = str(BULK_DIR)\n",
    "paths[\"processed_data\"] = str(PROCESSED_DIR)\n",
    "paths[\"rates_5m\"]       = str(RATES_5M_DIR)\n",
    "GLOBAL_STATE[\"paths\"]   = paths\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Shortlist\n",
    "# -----------------------------------------------------------------------------\n",
    "metrics_state = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "asset_shortlist_path_str = (\n",
    "    metrics_state.get(\"asset_strategy_shortlist_path\")\n",
    "    or metrics_state.get(\"asset_shortlist_path\")\n",
    ")\n",
    "\n",
    "if asset_shortlist_path_str:\n",
    "    asset_shortlist_path = Path(asset_shortlist_path_str).resolve()\n",
    "else:\n",
    "    cand_strategy = OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\"\n",
    "    cand_legacy   = OUT_SCORES_DIR / \"asset_shortlist.parquet\"\n",
    "    asset_shortlist_path = cand_strategy if cand_strategy.exists() else cand_legacy\n",
    "    asset_shortlist_path = asset_shortlist_path.resolve()\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí asset_strategy_shortlist = {str(asset_shortlist_path)}\", end=\"\")\n",
    "\n",
    "if not asset_shortlist_path.exists() or asset_shortlist_path.stat().st_size == 0:\n",
    "    raise RuntimeError(f\"[Celda 10F][ERROR] Shortlist no encontrada o vac√≠a: {asset_shortlist_path}\")\n",
    "\n",
    "shortlist = pl.read_parquet(str(asset_shortlist_path))\n",
    "if \"symbol\" not in shortlist.columns:\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] Shortlist no tiene columna 'symbol'.\")\n",
    "\n",
    "shortlist = shortlist.with_columns(\n",
    "    pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    ")\n",
    "symbols = shortlist.get_column(\"symbol\").unique().to_list()\n",
    "print(f\" (symbols={len(symbols)})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Detectar layout particionado por \"symbol=XXX\"\n",
    "# -----------------------------------------------------------------------------\n",
    "partition_dirs = []\n",
    "try:\n",
    "    partition_dirs = [p for p in RATES_5M_DIR.glob(\"symbol=*\") if p.is_dir()]\n",
    "except Exception:\n",
    "    partition_dirs = []\n",
    "\n",
    "is_partitioned = len(partition_dirs) > 0\n",
    "print(f\"[Celda 10F] Layout detectado ‚Üí partitioned={is_partitioned} (symbol=* dirs={len(partition_dirs)})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Resolver archivos por s√≠mbolo\n",
    "# -----------------------------------------------------------------------------\n",
    "def _list_parquets_under(p: Path) -> List[Path]:\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    return [f.resolve() for f in p.rglob(\"*.parquet\")]\n",
    "\n",
    "def _recent_files(files: List[Path], max_files: int = 400) -> List[Path]:\n",
    "    try:\n",
    "        files = sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    except Exception:\n",
    "        files = sorted(files)\n",
    "    return files[:max_files]\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    s = (s or \"\").upper().strip()\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", s)\n",
    "\n",
    "def _candidate_name_keys(sym: str) -> List[str]:\n",
    "    s = (sym or \"\").upper().strip()\n",
    "    base = _canon(s)\n",
    "    cands = {\n",
    "        s, base,\n",
    "        f\"{s}_5M\", f\"{base}_5M\",\n",
    "        f\"{s}M5\", f\"{base}M5\",\n",
    "        f\"{s}_M5\", f\"{base}_M5\",\n",
    "    }\n",
    "    return [c for c in cands if c]\n",
    "\n",
    "def _fallback_match_non_partition(sym: str) -> List[Path]:\n",
    "    sym_u = sym.upper().strip()\n",
    "    keys = set(_candidate_name_keys(sym_u))\n",
    "\n",
    "    direct = []\n",
    "    for k in keys:\n",
    "        fp = RATES_5M_DIR / f\"{k}.parquet\"\n",
    "        if fp.exists():\n",
    "            direct.append(fp.resolve())\n",
    "    if direct:\n",
    "        return direct\n",
    "\n",
    "    cand_dirs = [\n",
    "        RATES_5M_DIR / sym_u,\n",
    "        RATES_5M_DIR / _canon(sym_u),\n",
    "    ]\n",
    "    for d in cand_dirs:\n",
    "        if d.exists() and d.is_dir():\n",
    "            files = _list_parquets_under(d)\n",
    "            if files:\n",
    "                return files\n",
    "\n",
    "    files_root = list(RATES_5M_DIR.glob(\"*.parquet\"))\n",
    "    by_stem = [f.resolve() for f in files_root if f.stem.upper().strip() in keys]\n",
    "    return by_stem\n",
    "\n",
    "mapping_rows = []\n",
    "missing_syms = []\n",
    "multi_syms = []\n",
    "sym_to_files: Dict[str, List[Path]] = {}\n",
    "\n",
    "for sym in symbols:\n",
    "    files = []\n",
    "    match_mode = \"NONE\"\n",
    "\n",
    "    if is_partitioned:\n",
    "        part_dir = RATES_5M_DIR / f\"symbol={sym}\"\n",
    "        if part_dir.exists() and part_dir.is_dir():\n",
    "            files = _list_parquets_under(part_dir)\n",
    "            match_mode = \"PARTITION_DIR\"\n",
    "\n",
    "    if not files:\n",
    "        files = _fallback_match_non_partition(sym)\n",
    "        match_mode = \"NON_PARTITION_FALLBACK\" if files else \"NONE\"\n",
    "\n",
    "    files = _recent_files(files, max_files=400)\n",
    "\n",
    "    if not files:\n",
    "        missing_syms.append(sym)\n",
    "        mapping_rows.append({\"symbol\": sym, \"files_found\": 0, \"match_mode\": match_mode, \"example_files\": \"\"})\n",
    "    else:\n",
    "        sym_to_files[sym] = files\n",
    "        if len(files) > 1:\n",
    "            multi_syms.append(sym)\n",
    "        mapping_rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"files_found\": len(files),\n",
    "            \"match_mode\": match_mode,\n",
    "            \"example_files\": \" | \".join(str(p) for p in files[:10]),\n",
    "        })\n",
    "\n",
    "map_df = pl.DataFrame(mapping_rows)\n",
    "\n",
    "found_syms = map_df.filter(pl.col(\"files_found\") > 0)[\"symbol\"].to_list()\n",
    "found_n = len(found_syms)\n",
    "miss_n  = len(missing_syms)\n",
    "\n",
    "print(f\"[Celda 10F] Mapping s√≠mbolo ‚Üí archivos: found={found_n}/{len(symbols)}, missing={miss_n}\")\n",
    "if multi_syms:\n",
    "    print(f\"[Celda 10F] S√≠mbolos con m√∫ltiples parquets (recortados a recientes) = {multi_syms}\")\n",
    "\n",
    "qa_map_path = OUT_DIAG_DIR / \"corr_matrix_builder_mapping.parquet\"\n",
    "map_df.write_parquet(str(qa_map_path))\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(qa_map_path)}\")\n",
    "\n",
    "if found_n < 2:\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 10F][ERROR] Series M5 insuficientes para correlaci√≥n real.\\n\"\n",
    "        f\"Directorio analizado: {RATES_5M_DIR}\\n\"\n",
    "        f\"Missing symbols: {missing_syms}\\n\"\n",
    "        \"Si tu data es particionada, aseg√∫rate de tener carpetas exactas:\\n\"\n",
    "        \"  m5_raw/symbol=TU_SIMBOLO/...\\n\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Cargar series de close (FIX TIME + POLARS ENGINE)\n",
    "# -----------------------------------------------------------------------------\n",
    "if not PD_OK:\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] pandas no disponible; requerido para correlaci√≥n robusta.\")\n",
    "\n",
    "def _find_col(cols: List[str], cands: List[str]) -> Optional[str]:\n",
    "    lower = {c.lower(): c for c in cols}\n",
    "    for cand in cands:\n",
    "        if cand in lower:\n",
    "            return lower[cand]\n",
    "    return None\n",
    "\n",
    "# ‚úÖ FIX: a√±adimos tus columnas reales\n",
    "TIME_CANDS  = [\n",
    "    \"timestamp_utc\", \"timestamp_gye\",   # <- tus nombres reales\n",
    "    \"time_utc\", \"datetime_utc\",\n",
    "    \"timestamp\", \"time\", \"datetime\",\n",
    "    \"dt\", \"ts\", \"time_m5\"\n",
    "]\n",
    "CLOSE_CANDS = [\n",
    "    \"close\", \"last\", \"price_close\", \"close_price\",\n",
    "    \"close_mid\", \"close_bid\", \"close_ask\", \"c\"\n",
    "]\n",
    "\n",
    "def _autodetect_time_col(cols: List[str]) -> Optional[str]:\n",
    "    priority = []\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if \"timestamp\" in cl and \"utc\" in cl:\n",
    "            priority.append(c)\n",
    "    if priority:\n",
    "        return priority[0]\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if \"timestamp\" in cl:\n",
    "            return c\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if cl in (\"time\", \"datetime\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _collect_lazy(lf: pl.LazyFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Evita DeprecationWarning:\n",
    "      - Polars nuevos: collect(engine=\"streaming\")\n",
    "      - Polars antiguos: fallback a collect(streaming=True)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return lf.collect(engine=\"streaming\")\n",
    "    except TypeError:\n",
    "        # compatibilidad con versiones antiguas\n",
    "        return lf.collect(streaming=True)\n",
    "\n",
    "def _load_close_lazy(files: List[Path], lookback_days: int) -> Optional[pd.Series]:\n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df0 = pl.read_parquet(str(files[0]))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    tcol = _find_col(df0.columns, TIME_CANDS)\n",
    "    ccol = _find_col(df0.columns, CLOSE_CANDS)\n",
    "\n",
    "    # ‚úÖ autodetect si no encontr√≥ por lista\n",
    "    if tcol is None:\n",
    "        tcol = _autodetect_time_col(df0.columns)\n",
    "\n",
    "    if tcol is None or ccol is None:\n",
    "        return None\n",
    "\n",
    "    lf = pl.scan_parquet([str(p) for p in files]).select([pl.col(tcol), pl.col(ccol)])\n",
    "    df = _collect_lazy(lf)\n",
    "\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "\n",
    "    # Conversi√≥n robusta de tiempo\n",
    "    try:\n",
    "        dt = df[tcol].dtype\n",
    "    except Exception:\n",
    "        dt = None\n",
    "\n",
    "    if dt in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32):\n",
    "        sample = df[tcol].drop_nulls().head(5).to_list()\n",
    "        unit = \"ms\"\n",
    "        if sample:\n",
    "            mx = max(sample)\n",
    "            unit = \"ms\" if mx > 10_000_000_000 else \"s\"\n",
    "        df = df.with_columns(pl.from_epoch(pl.col(tcol), time_unit=unit).alias(tcol))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col(tcol).cast(pl.Datetime, strict=False).alias(tcol))\n",
    "\n",
    "    df = df.drop_nulls(subset=[tcol, ccol])\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        max_ts = df.select(pl.col(tcol).max()).item()\n",
    "    except Exception:\n",
    "        max_ts = None\n",
    "\n",
    "    if max_ts is not None:\n",
    "        cutoff = max_ts - timedelta(days=lookback_days)\n",
    "        df = df.filter(pl.col(tcol) >= cutoff)\n",
    "\n",
    "    df = df.sort(tcol)\n",
    "\n",
    "    pdf = df.to_pandas()\n",
    "    s = pd.Series(pdf[ccol].values, index=pd.to_datetime(pdf[tcol], utc=True))\n",
    "    s = s[~s.index.duplicated(keep=\"last\")]\n",
    "    return s\n",
    "\n",
    "series_map: Dict[str, pd.Series] = {}\n",
    "load_fail = []\n",
    "\n",
    "for sym, files in sym_to_files.items():\n",
    "    try:\n",
    "        s = _load_close_lazy(files, lookback_days)\n",
    "        if s is not None and len(s) > 80:\n",
    "            series_map[sym] = s\n",
    "        else:\n",
    "            load_fail.append(sym)\n",
    "    except Exception as e:\n",
    "        print(f\"[Celda 10F][WARN] Fallo cargando {sym}: {e}\")\n",
    "        load_fail.append(sym)\n",
    "\n",
    "print(f\"[Celda 10F] Series v√°lidas tras carga = {len(series_map)}\")\n",
    "if load_fail:\n",
    "    print(f\"[Celda 10F] S√≠mbolos fallidos en carga: {sorted(set(load_fail))}\")\n",
    "\n",
    "if len(series_map) < 2:\n",
    "    print(\"---- QA extra: columnas detectadas en 1er archivo de s√≠mbolos fallidos ----\")\n",
    "    for sym in sorted(set(load_fail))[:10]:\n",
    "        fl = sym_to_files.get(sym, [])\n",
    "        if not fl:\n",
    "            continue\n",
    "        try:\n",
    "            dfx = pl.read_parquet(str(fl[0]))\n",
    "            print(f\"[{sym}] file={fl[0].name} cols={dfx.columns[:80]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{sym}] no se pudo leer ejemplo: {e}\")\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"[Celda 10F][ERROR] Series v√°lidas insuficientes tras carga.\\n\"\n",
    "        \"Con tu esquema actual esto NO deber√≠a pasar ya.\\n\"\n",
    "        \"Si pasa, el problema ser√≠a data vac√≠a o timestamps no parseables.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Correlaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "prices_df = pd.DataFrame(series_map).sort_index()\n",
    "prices_df = prices_df.ffill(limit=3)\n",
    "\n",
    "rets_df = np.log(prices_df).diff()\n",
    "rets_df = rets_df.dropna(how=\"all\")\n",
    "\n",
    "if rets_df.shape[0] < 10:\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] Retornos insuficientes tras limpieza/lookback.\")\n",
    "\n",
    "corr = rets_df.corr(method=\"pearson\", min_periods=10)\n",
    "corr = corr.fillna(0.0).clip(-1.0, 1.0)\n",
    "for c in corr.columns:\n",
    "    corr.loc[c, c] = 1.0\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Writes limpios\n",
    "# -----------------------------------------------------------------------------\n",
    "corr_file = PROCESSED_DIR / \"corr_matrix_5m.csv\"\n",
    "tmp_file  = PROCESSED_DIR / f\"corr_matrix_5m__tmp_{RUN_ID}.csv\"\n",
    "\n",
    "corr.to_csv(str(tmp_file), index=True)\n",
    "\n",
    "if (not tmp_file.exists()) or tmp_file.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] Write temporal de corr_matrix fall√≥.\")\n",
    "\n",
    "try:\n",
    "    if corr_file.exists():\n",
    "        corr_file.unlink()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "tmp_file.rename(corr_file)\n",
    "\n",
    "if (not corr_file.exists()) or corr_file.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 10F][ERROR] corr_matrix_5m.csv no qued√≥ escrito correctamente.\")\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"corr_matrix_5m_path\"] = str(corr_file)\n",
    "GLOBAL_STATE[\"metrics\"][\"corr_matrix_5m_source\"] = \"built_10F_v3_1_timefix_engine\"\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí corr_matrix = {str(corr_file)} (rows={corr.shape[0]}, cols={corr.shape[1]})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) Reporte HTML QA\n",
    "# -----------------------------------------------------------------------------\n",
    "def _fmt_num(x):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return \"\"\n",
    "        if isinstance(x, (int, float, np.floating)):\n",
    "            return f\"{float(x):.4f}\"\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def _df_html_pl(df: pl.DataFrame, title: str) -> str:\n",
    "    if df.is_empty():\n",
    "        return f\"<h3>{title}</h3><p>(vac√≠o)</p>\"\n",
    "    thead = \"<tr>\" + \"\".join(f\"<th>{c}</th>\" for c in df.columns) + \"</tr>\"\n",
    "    body = \"\\n\".join(\n",
    "        \"<tr>\" + \"\".join(f\"<td>{_fmt_num(v)}</td>\" for v in r) + \"</tr>\"\n",
    "        for r in df.iter_rows()\n",
    "    )\n",
    "    return (\n",
    "        f\"<h3>{title}</h3>\"\n",
    "        f\"<table border='1' cellspacing='0' cellpadding='4'>\"\n",
    "        f\"<thead>{thead}</thead><tbody>{body}</tbody></table>\"\n",
    "    )\n",
    "\n",
    "now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "summary_df = pl.DataFrame([{\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"lookback_days\": lookback_days,\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"bulk_dir\": str(BULK_DIR),\n",
    "    \"rates_or_m5_dir\": str(RATES_5M_DIR),\n",
    "    \"processed_dir\": str(PROCESSED_DIR),\n",
    "    \"partitioned_layout\": bool(is_partitioned),\n",
    "    \"symbols_shortlist\": len(symbols),\n",
    "    \"symbols_mapped\": found_n,\n",
    "    \"symbols_missing\": miss_n,\n",
    "    \"series_valid\": len(series_map),\n",
    "    \"corr_dim\": int(corr.shape[0]),\n",
    "}])\n",
    "\n",
    "missing_df = pl.DataFrame([{\"symbol\": s} for s in sorted(set(missing_syms))]) if missing_syms else pl.DataFrame(schema={\"symbol\": pl.Utf8})\n",
    "load_fail_df = pl.DataFrame([{\"symbol\": s} for s in sorted(set(load_fail))]) if load_fail else pl.DataFrame(schema={\"symbol\": pl.Utf8})\n",
    "\n",
    "html = []\n",
    "html.append(\"<html><head><meta charset='utf-8'><title>Corr Matrix Builder Report</title></head><body>\")\n",
    "html.append(\"<h2>Corr Matrix Builder & QA ‚Äî M5 (v3.1 timefix + engine fix)</h2>\")\n",
    "html.append(f\"<p><b>Generado:</b> {now_utc}</p>\")\n",
    "html.append(_df_html_pl(summary_df, \"Resumen\"))\n",
    "html.append(_df_html_pl(map_df, \"Mapping s√≠mbolo ‚Üí archivos (recientes, layout-aware)\"))\n",
    "html.append(_df_html_pl(missing_df, \"S√≠mbolos sin archivos detectables\"))\n",
    "html.append(_df_html_pl(load_fail_df, \"S√≠mbolos con archivos pero data no usable\"))\n",
    "html.append(f\"<p><b>corr_matrix_5m.csv:</b> {str(corr_file)}</p>\")\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "report_path = OUT_DIAG_DIR / \"corr_matrix_builder_report.html\"\n",
    "report_path.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(report_path)} (OK)\")\n",
    "print(\">>> Celda 10F :: OK (corr_matrix M5 construida/verificada) [V3.1 TIME FIX + AUTODETECT + POLARS ENGINE FIX]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9727630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 11 :: Baskets decorrelacionadas (TREND/RANGE core) [V2.1 EXCLUSIVITY + BEST LINK 10F]\n",
      "[Celda 11] RUN_ID = 20251218_190810\n",
      "[Celda 11] baskets config ‚Üí target_size_trend=4, target_size_range=4, corr_threshold_start=0.6, corr_threshold_min=0.8, enforce_exclusivity=True\n",
      "[Celda 11] INPUT ‚Üí asset_strategy_shortlist = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (rows=7, cols=58)\n",
      "[Celda 11] Candidatos TREND = 7 s√≠mbolo(s) (source=shortlist_rank)\n",
      "[Celda 11] Candidatos RANGE = 7 s√≠mbolo(s) (source=shortlist_rank)\n",
      "[Celda 11] INPUT ‚Üí corr_matrix = C:\\Quant\\MT5_Data_Extraction\\processed_data\\corr_matrix_5m.csv (source=file, rows=7, cols=8)\n",
      "-------------------------------------------------------------------------------\n",
      "[Celda 11] Cesta TREND core ‚Üí size=4, thr_usado=0.70, basket_avg_abs_corr=0.152\n",
      "[Celda 11] Cesta RANGE core ‚Üí size=2, thr_usado=0.60, basket_avg_abs_corr=0.111\n",
      "[Celda 11] Exclusividad cruzada = True | overlap_n=0\n",
      "[Celda 11] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_trend_core.parquet (rows=4, cols=73)\n",
      "[Celda 11] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_range_core.parquet (rows=2, cols=73)\n",
      "[Celda 11] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\selection_table.parquet (rows=6, cols=2)\n",
      "[Celda 11] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\regimen_selector_report.html (OK)\n",
      ">>> Celda 11 :: OK (baskets decorrelacionadas TREND/RANGE core construidas) [V2.1]\n"
     ]
    }
   ],
   "source": [
    "# Celda 11 ‚Äî Baskets decorrelacionadas (TREND/RANGE core) [BEST LINK 10C]\n",
    "# =============================================================================\n",
    "# V2.1 ‚Äî EXCLUSIVITY + mejor link a 10F + fallback time-fix\n",
    "#\n",
    "# Objetivo:\n",
    "#   Pasar de la shortlist de activos (asset_strategy_shortlist.parquet, Celda 10C)\n",
    "#   a dos cestas \"core\" decorrelacionadas:\n",
    "#\n",
    "#     - basket_trend_core.parquet\n",
    "#     - basket_range_core.parquet\n",
    "#\n",
    "#   Pensadas como candidatos de portafolio de estrategias (TREND / RANGE).\n",
    "#\n",
    "# Ajustes clave V2.1:\n",
    "#   1) Exclusividad cruzada opcional:\n",
    "#        - Evita solapamiento de s√≠mbolos entre TREND y RANGE.\n",
    "#        - Nuevo config:\n",
    "#            \"baskets\": {\n",
    "#               ...\n",
    "#               \"enforce_cross_family_exclusivity\": true\n",
    "#            }\n",
    "#        - Default: True (para alinear con tu hallazgo en Celda 12).\n",
    "#\n",
    "#   2) Mejor resoluci√≥n de corr_matrix:\n",
    "#        - Prioriza GLOBAL_STATE[\"metrics\"][\"corr_matrix_5m_path\"] (Celda 10F),\n",
    "#          luego processed_data/corr_matrix_5m.csv.\n",
    "#\n",
    "#   3) Fallback de correlaci√≥n m√°s robusto:\n",
    "#        - Detecta layout particionado \"symbol=XXX\" y soporta\n",
    "#          timestamps reales: 'timestamp_utc', 'timestamp_gye'.\n",
    "#        - Usa Polars lazy + engine=\"streaming\" con fallback compatible.\n",
    "#\n",
    "# Inputs:\n",
    "#   - asset_strategy_shortlist.parquet  (Celda 10C; fallback: asset_shortlist.parquet)\n",
    "#   - corr_matrix_5m.csv                (preferido; generado por Celda 10F)\n",
    "#   - rates_5m/*.parquet o m5_raw/symbol=XXX/.. (fallback)\n",
    "#   - config.json                       (bloque \"baskets\")\n",
    "#\n",
    "# Config (bloque \"baskets\"):\n",
    "#   \"baskets\": {\n",
    "#     \"target_size_trend\":    4,\n",
    "#     \"target_size_range\":    4,\n",
    "#     \"corr_threshold_start\": 0.6,\n",
    "#     \"corr_threshold_min\":   0.8,\n",
    "#     \"enforce_cross_family_exclusivity\": true\n",
    "#   }\n",
    "#\n",
    "# Sem√°ntica:\n",
    "#   - Intentar construir cestas con |corr| ‚â§ corr_threshold_start.\n",
    "#   - Si no se alcanza el tama√±o objetivo:\n",
    "#       ‚Üí relajar a 0.7, luego 0.8, etc. hasta corr_threshold_min.\n",
    "#   - Greedy por score:\n",
    "#       ‚Üí TREND ordena por core_score_trend.\n",
    "#       ‚Üí RANGE ordena por core_score_range.\n",
    "#\n",
    "# Outputs:\n",
    "#   - baskets/basket_trend_core.parquet\n",
    "#   - baskets/basket_range_core.parquet\n",
    "#       columnas m√≠nimas esperadas downstream:\n",
    "#         symbol, family, preset,\n",
    "#         core_score_trend, core_score_range,\n",
    "#         score_trend_max, score_range_max,\n",
    "#         structure_score_raw, structure_flag,\n",
    "#         rank_in_basket,\n",
    "#         corr_threshold, corr_threshold_used,\n",
    "#         basket_avg_abs_corr,\n",
    "#         avg_abs_corr_to_others,\n",
    "#         source_corr\n",
    "#\n",
    "#   - diagnostics/regimen_selector_report.html\n",
    "#   - scores/selection_table.parquet\n",
    "#\n",
    "# GLOBAL_STATE:\n",
    "#   - Lee:\n",
    "#       GLOBAL_STATE[\"paths\"][\"scores\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"metrics\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"diagnostics\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"processed_data\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"bulk_data\"] / [\"rates_5m\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"asset_strategy_shortlist_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"asset_shortlist_path\"] (legado)\n",
    "#       GLOBAL_STATE[\"metrics\"][\"corr_matrix_5m_path\"] (Celda 10F)\n",
    "#       GLOBAL_STATE[\"config\"] / config.json\n",
    "#   - Escribe:\n",
    "#       GLOBAL_STATE[\"baskets\"][\"paths\"][\"trend_core\"]\n",
    "#       GLOBAL_STATE[\"baskets\"][\"paths\"][\"range_core\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"basket_trend_core_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"basket_range_core_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"selection_table_path\"]\n",
    "#       GLOBAL_STATE[\"report_stats\"][\"c11\"]\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# pandas opcional para correlaci√≥n\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PD_OK = True\n",
    "except Exception:\n",
    "    pd = None\n",
    "    PD_OK = False\n",
    "\n",
    "print(\">>> Celda 11 :: Baskets decorrelacionadas (TREND/RANGE core) [V2.1 EXCLUSIVITY + BEST LINK 10F]\")\n",
    "\n",
    "# ------------------------------- Guardas b√°sicas --------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 11][ERROR] GLOBAL_STATE no existe. Ejecuta las celdas iniciales.\")\n",
    "\n",
    "for key in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 11][ERROR] GLOBAL_STATE incompleto; falta clave '{key}'.\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "RUN_ID       = GLOBAL_STATE[\"run_id\"]\n",
    "paths: Dict[str, Any] = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "for k in (\"scores\", \"metrics\", \"diagnostics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 11][ERROR] Falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "OUT_SCORES_DIR  = Path(paths[\"scores\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_BASKETS_DIR = Path(paths.get(\"baskets\", OUT_SCORES_DIR.parent / \"baskets\")).resolve()\n",
    "\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_BASKETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROCESSED_DIR = Path(paths.get(\"processed_data\", PROJECT_ROOT / \"processed_data\")).resolve()\n",
    "BULK_DIR      = Path(paths.get(\"bulk_data\", PROJECT_ROOT / \"bulk_data\")).resolve()\n",
    "RATES_5M_DIR  = Path(paths.get(\"rates_5m\", BULK_DIR / \"rates_5m\")).resolve()\n",
    "\n",
    "metrics_state = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "\n",
    "# --------- Resoluci√≥n flexible de asset_shortlist / asset_strategy_shortlist ----\n",
    "asset_shortlist_path_str = (\n",
    "    metrics_state.get(\"asset_strategy_shortlist_path\")      # Celda 10C nueva\n",
    "    or metrics_state.get(\"asset_shortlist_path\")            # legado\n",
    ")\n",
    "\n",
    "if asset_shortlist_path_str:\n",
    "    asset_shortlist_path = Path(asset_shortlist_path_str).resolve()\n",
    "else:\n",
    "    cand_strategy = OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\"\n",
    "    cand_legacy   = OUT_SCORES_DIR / \"asset_shortlist.parquet\"\n",
    "    asset_shortlist_path = cand_strategy.resolve() if cand_strategy.exists() else cand_legacy.resolve()\n",
    "\n",
    "# --------- Corr matrix path preferido (10F) ------------------------------------\n",
    "corr_path_str = metrics_state.get(\"corr_matrix_5m_path\")\n",
    "if corr_path_str:\n",
    "    corr_file = Path(corr_path_str).resolve()\n",
    "else:\n",
    "    corr_file = (PROCESSED_DIR / \"corr_matrix_5m.csv\").resolve()\n",
    "\n",
    "config_path = Path(paths.get(\"config\", OUT_DIAG_DIR / \"config.json\")).resolve()\n",
    "report_path = OUT_DIAG_DIR / \"regimen_selector_report.html\"\n",
    "\n",
    "selection_table_path      = OUT_SCORES_DIR / \"selection_table.parquet\"\n",
    "basket_trend_core_path    = OUT_BASKETS_DIR / \"basket_trend_core.parquet\"\n",
    "basket_range_core_path    = OUT_BASKETS_DIR / \"basket_range_core.parquet\"\n",
    "\n",
    "# ------------------------------- Helpers generales -----------------------------\n",
    "def _require_file(p: Path, label: str = \"\", required: bool = True):\n",
    "    if required and (not p.exists() or p.stat().st_size == 0):\n",
    "        raise RuntimeError(f\"[Celda 11][ERROR] Falta input requerido {label}: {p}\")\n",
    "    if (not required) and (not p.exists() or p.stat().st_size == 0):\n",
    "        print(f\"[Celda 11] Aviso: input opcional no encontrado ({label}): {p}\")\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"symbol\")\n",
    "              .cast(pl.Utf8, strict=False)\n",
    "              .str.to_uppercase()\n",
    "              .str.strip_chars()\n",
    "        )\n",
    "    if \"preset\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"preset\")\n",
    "              .cast(pl.Utf8, strict=False)\n",
    "              .fill_null(\"\")\n",
    "              .str.to_uppercase()\n",
    "              .str.strip_chars()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def _fmt_num(x):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return \"\"\n",
    "        if isinstance(x, bool):\n",
    "            return \"OK\" if x else \"‚Äî\"\n",
    "        if isinstance(x, (int, float, np.floating)):\n",
    "            ax = abs(float(x))\n",
    "            if ax != 0.0 and (ax < 1e-4 or ax >= 1e3):\n",
    "                return f\"{float(x):.3e}\"\n",
    "            return f\"{float(x):.4f}\"\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def _df_html(df: pl.DataFrame, title: str) -> str:\n",
    "    if df.is_empty():\n",
    "        return f\"<h3>{title}</h3><p>(vac√≠o)</p>\"\n",
    "    thead = \"<tr>\" + \"\".join(f\"<th>{c}</th>\" for c in df.columns) + \"</tr>\"\n",
    "    body = \"\\n\".join(\n",
    "        \"<tr>\" + \"\".join(f\"<td>{_fmt_num(v)}</td>\" for v in r) + \"</tr>\"\n",
    "        for r in df.iter_rows()\n",
    "    )\n",
    "    return (\n",
    "        f\"<h3>{title}</h3>\"\n",
    "        f\"<table border='1' cellspacing='0' cellpadding='4'>\"\n",
    "        f\"<thead>{thead}</thead><tbody>{body}</tbody></table>\"\n",
    "    )\n",
    "\n",
    "def _sort_by_core(df: pl.DataFrame, core_col: str) -> pl.DataFrame:\n",
    "    if df.is_empty():\n",
    "        return df\n",
    "    try:\n",
    "        return df.sort(by=[core_col, \"symbol\"], descending=[True, False], nulls_last=True)\n",
    "    except TypeError:\n",
    "        return (\n",
    "            df.with_columns(\n",
    "                pl.when(pl.col(core_col).is_null())\n",
    "                  .then(pl.lit(float(\"-inf\")))\n",
    "                  .otherwise(pl.col(core_col))\n",
    "                  .alias(\"__core_tmp__\")\n",
    "            )\n",
    "            .sort(by=[\"__core_tmp__\", \"symbol\"], descending=[True, False])\n",
    "            .drop(\"__core_tmp__\")\n",
    "        )\n",
    "\n",
    "def _ensure_col(df: pl.DataFrame, name: str, dtype: pl.DataType, default=None) -> pl.DataFrame:\n",
    "    if name in df.columns:\n",
    "        return df\n",
    "    return df.with_columns(pl.lit(default).cast(dtype).alias(name))\n",
    "\n",
    "# ------------------------------- Config de baskets -----------------------------\n",
    "_require_file(config_path, label=\"config.json\", required=True)\n",
    "\n",
    "if \"config\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"config\"], dict):\n",
    "    cfg = GLOBAL_STATE[\"config\"]\n",
    "else:\n",
    "    cfg = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "baskets_cfg = cfg.get(\"baskets\", {}) or {}\n",
    "\n",
    "target_size_trend = int(baskets_cfg.get(\"target_size_trend\", 4))\n",
    "target_size_range = int(baskets_cfg.get(\"target_size_range\", 4))\n",
    "corr_thr_start    = float(baskets_cfg.get(\"corr_threshold_start\", 0.60))\n",
    "corr_thr_min      = float(baskets_cfg.get(\"corr_threshold_min\",   0.80))\n",
    "\n",
    "# Nuevo toggle (V2.1)\n",
    "enforce_exclusivity = bool(baskets_cfg.get(\"enforce_cross_family_exclusivity\", True))\n",
    "\n",
    "# Normalizar umbrales (start ‚â§ min, ambos < 1.0)\n",
    "if corr_thr_start <= 0.0 or not np.isfinite(corr_thr_start):\n",
    "    corr_thr_start = 0.60\n",
    "if not np.isfinite(corr_thr_min):\n",
    "    corr_thr_min = corr_thr_start\n",
    "if corr_thr_min < corr_thr_start:\n",
    "    corr_thr_min = corr_thr_start\n",
    "\n",
    "corr_thr_start = float(min(max(corr_thr_start, 0.0), 0.99))\n",
    "corr_thr_min   = float(min(max(corr_thr_min, corr_thr_start), 0.99))\n",
    "\n",
    "print(f\"[Celda 11] RUN_ID = {RUN_ID}\")\n",
    "print(\n",
    "    \"[Celda 11] baskets config ‚Üí \"\n",
    "    f\"target_size_trend={target_size_trend}, \"\n",
    "    f\"target_size_range={target_size_range}, \"\n",
    "    f\"corr_threshold_start={corr_thr_start}, \"\n",
    "    f\"corr_threshold_min={corr_thr_min}, \"\n",
    "    f\"enforce_exclusivity={enforce_exclusivity}\"\n",
    ")\n",
    "\n",
    "# ------------------------------- Cargar asset_shortlist ------------------------\n",
    "_require_file(asset_shortlist_path, label=\"asset_strategy_shortlist/asset_shortlist\", required=True)\n",
    "short_df = _norm_sym(pl.read_parquet(asset_shortlist_path))\n",
    "\n",
    "print(\n",
    "    f\"[Celda 11] INPUT ‚Üí asset_strategy_shortlist = {str(asset_shortlist_path)} \"\n",
    "    f\"(rows={short_df.height}, cols={len(short_df.columns)})\"\n",
    ")\n",
    "\n",
    "if short_df.is_empty():\n",
    "    raise RuntimeError(\"[Celda 11][ERROR] asset_strategy_shortlist est√° vac√≠o; no hay activos para cestas.\")\n",
    "\n",
    "# ------------------------------- Normalizar/compatibilizar columnas -----------\n",
    "short_df = _ensure_col(short_df, \"class_family\", pl.Utf8, None)\n",
    "\n",
    "short_df = _ensure_col(short_df, \"core_score_trend\", pl.Float64, None)\n",
    "short_df = _ensure_col(short_df, \"core_score_range\", pl.Float64, None)\n",
    "\n",
    "if \"score_trend_max\" not in short_df.columns:\n",
    "    if \"SCORE_FINAL_trend\" in short_df.columns:\n",
    "        short_df = short_df.with_columns(\n",
    "            pl.col(\"SCORE_FINAL_trend\").cast(pl.Float64, strict=False).alias(\"score_trend_max\")\n",
    "        )\n",
    "    else:\n",
    "        short_df = short_df.with_columns(pl.lit(None).cast(pl.Float64).alias(\"score_trend_max\"))\n",
    "\n",
    "if \"score_range_max\" not in short_df.columns:\n",
    "    if \"SCORE_FINAL_range\" in short_df.columns:\n",
    "        short_df = short_df.with_columns(\n",
    "            pl.col(\"SCORE_FINAL_range\").cast(pl.Float64, strict=False).alias(\"score_range_max\")\n",
    "        )\n",
    "    else:\n",
    "        short_df = short_df.with_columns(pl.lit(None).cast(pl.Float64).alias(\"score_range_max\"))\n",
    "\n",
    "if \"structure_score_raw\" not in short_df.columns:\n",
    "    if \"structure_score\" in short_df.columns:\n",
    "        short_df = short_df.with_columns(\n",
    "            pl.col(\"structure_score\").cast(pl.Float64, strict=False).alias(\"structure_score_raw\")\n",
    "        )\n",
    "    else:\n",
    "        short_df = short_df.with_columns(pl.lit(None).cast(pl.Float64).alias(\"structure_score_raw\"))\n",
    "\n",
    "if \"structure_flag\" not in short_df.columns:\n",
    "    if \"structure_flag_qa\" in short_df.columns:\n",
    "        short_df = short_df.with_columns(\n",
    "            pl.col(\"structure_flag_qa\").cast(pl.Utf8, strict=False).alias(\"structure_flag\")\n",
    "        )\n",
    "    else:\n",
    "        short_df = short_df.with_columns(pl.lit(None).cast(pl.Utf8).alias(\"structure_flag\"))\n",
    "\n",
    "short_df = _ensure_col(short_df, \"selected_for_trend\", pl.Boolean, False)\n",
    "short_df = _ensure_col(short_df, \"selected_for_range\", pl.Boolean, False)\n",
    "short_df = _ensure_col(short_df, \"shortlist_rank_trend\", pl.Int64, None)\n",
    "short_df = _ensure_col(short_df, \"shortlist_rank_range\", pl.Int64, None)\n",
    "\n",
    "# ------------------------------- Pools de candidatos (BEST LINK 10C) ----------\n",
    "def _build_pool(df: pl.DataFrame, family: str) -> Tuple[pl.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Prioridad:\n",
    "      1) selected_for_{fam} si hay al menos un True (legado).\n",
    "      2) shortlist_rank_{fam} is not null (10C).\n",
    "      3) fallback: core_score_{fam} > 0 ordenado por score.\n",
    "    \"\"\"\n",
    "    if family == \"TREND\":\n",
    "        flag_col = \"selected_for_trend\"\n",
    "        rank_col = \"shortlist_rank_trend\"\n",
    "        core_col = \"core_score_trend\"\n",
    "    else:\n",
    "        flag_col = \"selected_for_range\"\n",
    "        rank_col = \"shortlist_rank_range\"\n",
    "        core_col = \"core_score_range\"\n",
    "\n",
    "    df = df.with_columns(pl.col(core_col).cast(pl.Float64, strict=False))\n",
    "\n",
    "    try:\n",
    "        has_any_flag = df.filter(pl.col(flag_col) == True).height > 0\n",
    "    except Exception:\n",
    "        has_any_flag = False\n",
    "\n",
    "    if has_any_flag:\n",
    "        pool = df.filter(pl.col(flag_col) == True)\n",
    "        return pool, \"selected_flag\"\n",
    "\n",
    "    if rank_col in df.columns:\n",
    "        pool = df.filter(pl.col(rank_col).is_not_null())\n",
    "        if not pool.is_empty():\n",
    "            return pool, \"shortlist_rank\"\n",
    "\n",
    "    pool = df.filter(pl.col(core_col).is_not_null() & (pl.col(core_col) > 0.0))\n",
    "    return pool, \"core_score_fallback\"\n",
    "\n",
    "trend_pool, trend_source = _build_pool(short_df, \"TREND\")\n",
    "range_pool, range_source = _build_pool(short_df, \"RANGE\")\n",
    "\n",
    "print(f\"[Celda 11] Candidatos TREND = {trend_pool.height} s√≠mbolo(s) (source={trend_source})\")\n",
    "print(f\"[Celda 11] Candidatos RANGE = {range_pool.height} s√≠mbolo(s) (source={range_source})\")\n",
    "\n",
    "if trend_pool.is_empty() and range_pool.is_empty():\n",
    "    print(\"[Celda 11][WARN] Shortlist sin candidatos TREND/RANGE tras heur√≠sticas. \"\n",
    "          \"Se generar√°n cestas core vac√≠as (revisa gates/umbrales en 10C).\")\n",
    "\n",
    "# Universo de s√≠mbolos para correlaciones (solo candidatos relevantes)\n",
    "universe_syms = sorted(set(\n",
    "    (trend_pool[\"symbol\"].to_list() if \"symbol\" in trend_pool.columns else []) +\n",
    "    (range_pool[\"symbol\"].to_list() if \"symbol\" in range_pool.columns else [])\n",
    "))\n",
    "\n",
    "# ------------------------------- Correlaciones ---------------------------------\n",
    "FALLBACK_SYMBOLS: set = set()\n",
    "source_corr_label: str = \"none\"\n",
    "\n",
    "def _normalize_stem(s: str) -> str:\n",
    "    return \"\".join(ch for ch in (s or \"\").lower() if ch.isalnum())\n",
    "\n",
    "def _has_parquet(p: Path) -> bool:\n",
    "    try:\n",
    "        if not p.exists() or not p.is_dir():\n",
    "            return False\n",
    "        if any(p.glob(\"*.parquet\")):\n",
    "            return True\n",
    "        return any(p.rglob(\"*.parquet\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _list_parquets_under(p: Path) -> List[Path]:\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    return [f.resolve() for f in p.rglob(\"*.parquet\")]\n",
    "\n",
    "def _recent_files(files: List[Path], max_files: int = 300) -> List[Path]:\n",
    "    try:\n",
    "        files = sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    except Exception:\n",
    "        files = sorted(files)\n",
    "    return files[:max_files]\n",
    "\n",
    "def _index_rates_flat(rdir: Path) -> Dict[str, Path]:\n",
    "    if not rdir.exists():\n",
    "        return {}\n",
    "    return {_normalize_stem(p.stem): p for p in rdir.glob(\"*.parquet\")}\n",
    "\n",
    "def _find_rates_file_flat(sym: str, idx: Dict[str, Path]) -> Optional[Path]:\n",
    "    cand = [\n",
    "        sym,\n",
    "        sym.replace(\".\", \"_\"), sym.replace(\".\", \"\"),\n",
    "        sym.replace(\"/\", \"_\"), sym.replace(\"/\", \"\"),\n",
    "        sym.replace(\"-\", \"_\"), sym.replace(\"-\", \"\"),\n",
    "        sym.replace(\" \", \"_\"), sym.replace(\" \", \"\"),\n",
    "    ]\n",
    "    for c in cand:\n",
    "        k = _normalize_stem(c)\n",
    "        if k in idx:\n",
    "            return idx[k]\n",
    "    return None\n",
    "\n",
    "# Time/cierre candidates robustos (alineado con tu data real)\n",
    "TIME_CANDS  = [\n",
    "    \"timestamp_utc\", \"timestamp_gye\",\n",
    "    \"time_utc\", \"datetime_utc\",\n",
    "    \"timestamp\", \"time\", \"datetime\",\n",
    "    \"dt\", \"ts\", \"time_m5\"\n",
    "]\n",
    "CLOSE_CANDS = [\n",
    "    \"close\", \"last\", \"price_close\", \"close_price\",\n",
    "    \"close_mid\", \"close_bid\", \"close_ask\", \"c\"\n",
    "]\n",
    "\n",
    "def _find_col(cols: List[str], cands: List[str]) -> Optional[str]:\n",
    "    lower = {c.lower(): c for c in cols}\n",
    "    for cand in cands:\n",
    "        if cand in lower:\n",
    "            return lower[cand]\n",
    "    return None\n",
    "\n",
    "def _autodetect_time_col(cols: List[str]) -> Optional[str]:\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if \"timestamp\" in cl and \"utc\" in cl:\n",
    "            return c\n",
    "    for c in cols:\n",
    "        if \"timestamp\" in c.lower():\n",
    "            return c\n",
    "    for c in cols:\n",
    "        if c.lower() in (\"time\", \"datetime\", \"dt\", \"ts\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _collect_lazy(lf: pl.LazyFrame) -> pl.DataFrame:\n",
    "    try:\n",
    "        return lf.collect(engine=\"streaming\")\n",
    "    except TypeError:\n",
    "        return lf.collect(streaming=True)\n",
    "\n",
    "def _load_close_series_from_files(files: List[Path], lookback_days: int = 90) -> Optional[pd.Series]:\n",
    "    if not PD_OK or not files:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df0 = pl.read_parquet(str(files[0]))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    tcol = _find_col(df0.columns, TIME_CANDS) or _autodetect_time_col(df0.columns)\n",
    "    ccol = _find_col(df0.columns, CLOSE_CANDS)\n",
    "\n",
    "    if tcol is None or ccol is None:\n",
    "        return None\n",
    "\n",
    "    lf = pl.scan_parquet([str(p) for p in files]).select([pl.col(tcol), pl.col(ccol)])\n",
    "    df = _collect_lazy(lf)\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "\n",
    "    # Parse timestamp robusto\n",
    "    try:\n",
    "        dt = df[tcol].dtype\n",
    "    except Exception:\n",
    "        dt = None\n",
    "\n",
    "    if dt in (pl.Int64, pl.Int32, pl.UInt64, pl.UInt32):\n",
    "        sample = df[tcol].drop_nulls().head(5).to_list()\n",
    "        unit = \"ms\"\n",
    "        if sample:\n",
    "            mx = max(sample)\n",
    "            unit = \"ms\" if mx > 10_000_000_000 else \"s\"\n",
    "        df = df.with_columns(pl.from_epoch(pl.col(tcol), time_unit=unit).alias(tcol))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col(tcol).cast(pl.Datetime, strict=False).alias(tcol))\n",
    "\n",
    "    df = df.drop_nulls(subset=[tcol, ccol])\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        max_ts = df.select(pl.col(tcol).max()).item()\n",
    "    except Exception:\n",
    "        max_ts = None\n",
    "\n",
    "    if max_ts is not None:\n",
    "        cutoff = max_ts - timedelta(days=lookback_days)\n",
    "        df = df.filter(pl.col(tcol) >= cutoff)\n",
    "\n",
    "    df = df.sort(tcol)\n",
    "    pdf = df.to_pandas()\n",
    "    s = pd.Series(pdf[ccol].values, index=pd.to_datetime(pdf[tcol], utc=True))\n",
    "    s = s[~s.index.duplicated(keep=\"last\")]\n",
    "    return s\n",
    "\n",
    "def build_corr_from_returns(symbols: List[str], rates_dir: Path, lookback_days: int = 90) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Fallback robusto:\n",
    "      - Soporta:\n",
    "          (A) layout particionado: rates_dir/symbol=XXX/**/*.parquet\n",
    "          (B) layout plano: rates_dir/*.parquet (1 por s√≠mbolo)\n",
    "          (C) carpetas por s√≠mbolo sin \"symbol=\"\n",
    "      - Usa columnas de tiempo reales detectadas.\n",
    "    \"\"\"\n",
    "    FALLBACK_SYMBOLS.clear()\n",
    "\n",
    "    if not PD_OK or not symbols:\n",
    "        return pl.DataFrame({\"symbol\": []})\n",
    "\n",
    "    # Detectar particionado symbol=*\n",
    "    part_dirs = []\n",
    "    try:\n",
    "        part_dirs = [p for p in rates_dir.glob(\"symbol=*\") if p.is_dir()]\n",
    "    except Exception:\n",
    "        part_dirs = []\n",
    "\n",
    "    is_partitioned = len(part_dirs) > 0\n",
    "\n",
    "    idx_flat = _index_rates_flat(rates_dir) if (rates_dir.exists() and rates_dir.is_dir()) else {}\n",
    "    series = {}\n",
    "\n",
    "    for sym in symbols:\n",
    "        files: List[Path] = []\n",
    "\n",
    "        if is_partitioned:\n",
    "            pdir = rates_dir / f\"symbol={sym}\"\n",
    "            if pdir.exists() and pdir.is_dir():\n",
    "                files = _list_parquets_under(pdir)\n",
    "\n",
    "        if not files:\n",
    "            # fallback a carpeta por s√≠mbolo sin prefix\n",
    "            d1 = rates_dir / sym\n",
    "            d2 = rates_dir / _normalize_stem(sym).upper()\n",
    "            for d in (d1, d2):\n",
    "                if d.exists() and d.is_dir():\n",
    "                    files = _list_parquets_under(d)\n",
    "                    if files:\n",
    "                        break\n",
    "\n",
    "        if not files:\n",
    "            # fallback plano *.parquet\n",
    "            p = _find_rates_file_flat(sym, idx_flat)\n",
    "            if p is not None:\n",
    "                files = [p]\n",
    "\n",
    "        files = _recent_files(files, max_files=300)\n",
    "\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        s_close = _load_close_series_from_files(files, lookback_days=lookback_days)\n",
    "        if s_close is None or len(s_close) < 80:\n",
    "            continue\n",
    "\n",
    "        series[sym] = s_close\n",
    "        FALLBACK_SYMBOLS.add(sym)\n",
    "\n",
    "    if not series:\n",
    "        return pl.DataFrame({\"symbol\": []})\n",
    "\n",
    "    prices_df = pd.DataFrame(series).sort_index()\n",
    "    prices_df = prices_df.ffill(limit=3)\n",
    "    rets_df = np.log(prices_df).diff().dropna(how=\"all\")\n",
    "\n",
    "    if rets_df.shape[0] < 10:\n",
    "        return pl.DataFrame({\"symbol\": []})\n",
    "\n",
    "    corr = rets_df.corr(method=\"pearson\", min_periods=10)\n",
    "    corr = corr.fillna(0.0).clip(-1.0, 1.0)\n",
    "    for c in corr.columns:\n",
    "        corr.loc[c, c] = 1.0\n",
    "\n",
    "    order = symbols\n",
    "    corr = corr.reindex(index=order, columns=order).fillna(0.0)\n",
    "\n",
    "    cpd2 = corr.copy()\n",
    "    cpd2.insert(0, \"symbol\", cpd2.index)\n",
    "    return pl.from_pandas(cpd2.reset_index(drop=True))\n",
    "\n",
    "def load_corr_df() -> Tuple[pl.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Intenta leer corr_matrix_5m.csv (preferido).\n",
    "    Si no existe o falla, reconstruye correlaci√≥n desde rates_5m (fallback).\n",
    "    \"\"\"\n",
    "    if corr_file.exists() and corr_file.stat().st_size > 0:\n",
    "        try:\n",
    "            if PD_OK:\n",
    "                cpd = pd.read_csv(corr_file)\n",
    "                if cpd.shape[1] < 2:\n",
    "                    raise RuntimeError(\"corr_matrix debe tener al menos 2 columnas.\")\n",
    "                cpd.rename(columns={cpd.columns[0]: \"symbol\"}, inplace=True)\n",
    "                cpd[\"symbol\"] = cpd[\"symbol\"].astype(str).str.upper()\n",
    "                cpd = cpd.set_index(\"symbol\")\n",
    "\n",
    "                if universe_syms:\n",
    "                    cpd = cpd.reindex(index=universe_syms, columns=universe_syms)\n",
    "\n",
    "                cpd = cpd.clip(-1.0, 1.0).fillna(0.0)\n",
    "                cpd2 = cpd.copy()\n",
    "                cpd2.insert(0, \"symbol\", cpd2.index)\n",
    "                return pl.from_pandas(cpd2.reset_index(drop=True)), \"file\"\n",
    "            else:\n",
    "                tmp = pl.read_csv(str(corr_file))\n",
    "                if tmp.width < 2:\n",
    "                    raise RuntimeError(\"corr_matrix debe tener al menos 2 columnas.\")\n",
    "                if tmp.columns[0].lower() != \"symbol\":\n",
    "                    tmp = tmp.rename({tmp.columns[0]: \"symbol\"})\n",
    "                tmp = tmp.with_columns(pl.col(\"symbol\").cast(pl.Utf8).str.to_uppercase())\n",
    "\n",
    "                if universe_syms:\n",
    "                    base = pl.DataFrame({\"symbol\": universe_syms})\n",
    "                    tmp = base.join(tmp, on=\"symbol\", how=\"left\")\n",
    "\n",
    "                num_cols = [c for c in tmp.columns if c != \"symbol\"]\n",
    "                tmp = tmp.with_columns(\n",
    "                    [pl.col(c).cast(pl.Float64).clip(-1.0, 1.0).fill_null(0.0).alias(c) for c in num_cols]\n",
    "                )\n",
    "                return tmp, \"file\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Celda 11] WARN: fallo leyendo corr_matrix_5m.csv ({e}); se usar√° fallback.\")\n",
    "\n",
    "    print(\"[Celda 11] FALLBACK ‚Üí reconstruyendo correlaci√≥n M5 (√∫ltimos 90 d√≠as) desde rates/m5\")\n",
    "    return build_corr_from_returns(universe_syms, RATES_5M_DIR, 90), \"fallback\"\n",
    "\n",
    "corr_df, source_corr_label = load_corr_df()\n",
    "print(\n",
    "    f\"[Celda 11] INPUT ‚Üí corr_matrix = {str(corr_file)} \"\n",
    "    f\"(source={source_corr_label}, rows={corr_df.height}, cols={len(corr_df.columns)})\"\n",
    ")\n",
    "\n",
    "# Preparar lookup de correlaciones\n",
    "if corr_df.is_empty() or \"symbol\" not in corr_df.columns or len(corr_df.columns) < 2:\n",
    "    corr_cols: List[str] = []\n",
    "    corr_rows: List[str] = []\n",
    "    corr_np = None\n",
    "else:\n",
    "    corr_cols = [c for c in corr_df.columns if c != \"symbol\"]\n",
    "    corr_rows = corr_df.get_column(\"symbol\").to_list()\n",
    "    try:\n",
    "        corr_np   = corr_df.select(corr_cols).to_numpy()\n",
    "    except Exception:\n",
    "        corr_np = None\n",
    "\n",
    "row_index = {s: i for i, s in enumerate(corr_rows)}\n",
    "col_index = {s: j for j, s in enumerate(corr_cols)}\n",
    "\n",
    "def _get_corr_val(sym_a: str, sym_b: str) -> Optional[float]:\n",
    "    if corr_np is None:\n",
    "        return None\n",
    "    if sym_a == sym_b:\n",
    "        return 1.0\n",
    "    ia = row_index.get(sym_a)\n",
    "    jb = col_index.get(sym_b)\n",
    "    if ia is None or jb is None:\n",
    "        return None\n",
    "    try:\n",
    "        v = float(corr_np[ia, jb])\n",
    "    except Exception:\n",
    "        return None\n",
    "    if not np.isfinite(v):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "def _avg_abs_corr_in_basket(symbols: List[str]) -> float:\n",
    "    n = len(symbols)\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "    vals: List[float] = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            c = _get_corr_val(symbols[i], symbols[j])\n",
    "            if c is None:\n",
    "                continue\n",
    "            vals.append(abs(c))\n",
    "    return float(sum(vals) / len(vals)) if vals else 0.0\n",
    "\n",
    "def _avg_abs_corr_per_symbol(symbols: List[str]) -> Dict[str, float]:\n",
    "    n = len(symbols)\n",
    "    out: Dict[str, float] = {}\n",
    "    if n <= 1:\n",
    "        for s in symbols:\n",
    "            out[s] = 0.0\n",
    "        return out\n",
    "    for i, s in enumerate(symbols):\n",
    "        vals: List[float] = []\n",
    "        for j, t in enumerate(symbols):\n",
    "            if i == j:\n",
    "                continue\n",
    "            c = _get_corr_val(s, t)\n",
    "            if c is None:\n",
    "                continue\n",
    "            vals.append(abs(c))\n",
    "        out[s] = float(sum(vals) / len(vals)) if vals else 0.0\n",
    "    return out\n",
    "\n",
    "if corr_np is None and universe_syms:\n",
    "    print(\"[Celda 11][WARN] Matriz de correlaci√≥n vac√≠a/no usable. \"\n",
    "          \"La selecci√≥n decorrelacionada ignorar√° pares sin dato. \"\n",
    "          \"Recomendaci√≥n: ejecutar Celda 10F para generar corr_matrix_5m.csv.\")\n",
    "\n",
    "# ------------------------------- Selecci√≥n greedy de cestas --------------------\n",
    "def build_core_basket(\n",
    "    candidates: pl.DataFrame,\n",
    "    core_col: str,\n",
    "    family_label: str,\n",
    "    target_size: int,\n",
    "    thr_start: float,\n",
    "    thr_min: float,\n",
    ") -> Tuple[pl.DataFrame, float, float]:\n",
    "    \"\"\"\n",
    "    Construye una cesta decorrelacionada greedy:\n",
    "      - Ordena candidatos por core_col DESC.\n",
    "      - Prueba umbrales: thr_start, thr_start+0.1, ... thr_min.\n",
    "      - A√±ade s√≠mbolo si |corr| con todos los ya dentro ‚â§ thr\n",
    "        (si no hay dato de corr ‚Üí se ignora ese par).\n",
    "      - Si no logra seleccionar nada:\n",
    "          fallback: Top-N por core_col ignorando correlaci√≥n.\n",
    "      - Si logra s√≥lo cestas parciales en todos los umbrales:\n",
    "          conserva la mejor (mayor tama√±o) encontrada.\n",
    "    \"\"\"\n",
    "\n",
    "    base_empty = (\n",
    "        short_df\n",
    "        .select(short_df.columns)\n",
    "        .head(0)\n",
    "        .with_columns(\n",
    "            pl.lit(family_label).alias(\"family\"),\n",
    "            pl.lit(\"BALANCED\").alias(\"preset\"),\n",
    "            pl.lit(None).cast(pl.Int64).alias(\"rank_in_basket\"),\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"corr_threshold\"),\n",
    "            pl.lit(None).cast(pl.Float64).alias(\"corr_threshold_used\"),\n",
    "            pl.lit(0.0).cast(pl.Float64).alias(\"basket_avg_abs_corr\"),\n",
    "            pl.lit(0.0).cast(pl.Float64).alias(\"avg_abs_corr_to_others\"),\n",
    "            pl.lit(source_corr_label).alias(\"source_corr\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if candidates.is_empty():\n",
    "        return base_empty, float(\"nan\"), 0.0\n",
    "\n",
    "    cand = candidates\n",
    "    cand = _ensure_col(cand, \"score_trend_max\", pl.Float64, None)\n",
    "    cand = _ensure_col(cand, \"score_range_max\", pl.Float64, None)\n",
    "    cand = _ensure_col(cand, \"structure_score_raw\", pl.Float64, None)\n",
    "    cand = _ensure_col(cand, \"structure_flag\", pl.Utf8, None)\n",
    "    cand = _ensure_col(cand, \"class_family\", pl.Utf8, None)\n",
    "\n",
    "    cand = cand.with_columns(pl.col(core_col).cast(pl.Float64, strict=False))\n",
    "    cand = _sort_by_core(cand, core_col)\n",
    "\n",
    "    syms_order = cand[\"symbol\"].to_list()\n",
    "    core_vals  = cand[core_col].to_list()\n",
    "\n",
    "    if not syms_order:\n",
    "        return base_empty, float(\"nan\"), 0.0\n",
    "\n",
    "    thresholds: List[float] = []\n",
    "    t = thr_start\n",
    "    while t <= thr_min + 1e-9:\n",
    "        thresholds.append(float(round(t, 4)))\n",
    "        t += 0.10\n",
    "    thresholds = sorted(set(thresholds))\n",
    "\n",
    "    selected_syms: List[str] = []\n",
    "    thr_used: float = float(\"nan\")\n",
    "\n",
    "    for thr in thresholds:\n",
    "        current: List[str] = []\n",
    "        for sym, core in zip(syms_order, core_vals):\n",
    "            if core is None:\n",
    "                continue\n",
    "            ok = True\n",
    "            for s_sel in current:\n",
    "                c = _get_corr_val(sym, s_sel)\n",
    "                if c is not None and abs(c) > thr + 1e-12:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                current.append(sym)\n",
    "                if len(current) >= target_size:\n",
    "                    break\n",
    "\n",
    "        if current:\n",
    "            if len(current) >= target_size:\n",
    "                selected_syms = current\n",
    "                thr_used = thr\n",
    "                break\n",
    "            if len(current) > len(selected_syms):\n",
    "                selected_syms = current\n",
    "                thr_used = thr\n",
    "\n",
    "    if not selected_syms:\n",
    "        top_n = min(max(1, target_size), cand.height)\n",
    "        selected_syms = syms_order[:top_n]\n",
    "        thr_used = 1.0  # marca fallback sin restricci√≥n efectiva\n",
    "\n",
    "    basket_df = (\n",
    "        cand\n",
    "        .filter(pl.col(\"symbol\").is_in(selected_syms))\n",
    "        .with_columns(pl.lit(family_label).alias(\"family\"))\n",
    "        .with_columns(pl.lit(\"BALANCED\").alias(\"preset\"))\n",
    "        .sort(by=[core_col, \"symbol\"], descending=[True, False], nulls_last=True)\n",
    "        .with_columns(pl.arange(1, pl.len() + 1).alias(\"rank_in_basket\"))\n",
    "    )\n",
    "\n",
    "    symbols_final = basket_df[\"symbol\"].to_list()\n",
    "    basket_avg = _avg_abs_corr_in_basket(symbols_final)\n",
    "    per_sym_avg = _avg_abs_corr_per_symbol(symbols_final)\n",
    "\n",
    "    basket_df = basket_df.with_columns([\n",
    "        pl.lit(float(thr_used)).alias(\"corr_threshold\"),\n",
    "        pl.lit(float(thr_used)).alias(\"corr_threshold_used\"),\n",
    "        pl.lit(float(basket_avg)).alias(\"basket_avg_abs_corr\"),\n",
    "        pl.col(\"symbol\").map_elements(lambda s: per_sym_avg.get(s, 0.0), return_dtype=pl.Float64)\n",
    "                        .alias(\"avg_abs_corr_to_others\"),\n",
    "        pl.lit(source_corr_label).alias(\"source_corr\"),\n",
    "    ])\n",
    "\n",
    "    basket_df = _ensure_col(basket_df, \"core_score_trend\", pl.Float64, None)\n",
    "    basket_df = _ensure_col(basket_df, \"core_score_range\", pl.Float64, None)\n",
    "\n",
    "    return basket_df, float(thr_used), float(basket_avg)\n",
    "\n",
    "# ------------------------------- Construir TREND primero -----------------------\n",
    "trend_basket_df, thr_trend_used, avg_corr_trend = build_core_basket(\n",
    "    candidates=trend_pool,\n",
    "    core_col=\"core_score_trend\",\n",
    "    family_label=\"TREND\",\n",
    "    target_size=target_size_trend,\n",
    "    thr_start=corr_thr_start,\n",
    "    thr_min=corr_thr_min,\n",
    ")\n",
    "\n",
    "trend_syms_selected = trend_basket_df[\"symbol\"].to_list() if (not trend_basket_df.is_empty() and \"symbol\" in trend_basket_df.columns) else []\n",
    "\n",
    "# ------------------------------- Exclusividad cruzada (V2.1) -------------------\n",
    "range_pool_eff = range_pool\n",
    "range_source_eff = range_source\n",
    "\n",
    "if enforce_exclusivity and trend_syms_selected:\n",
    "    range_pool_excl = range_pool.filter(~pl.col(\"symbol\").is_in(trend_syms_selected))\n",
    "    if range_pool_excl.is_empty():\n",
    "        print(\n",
    "            \"[Celda 11][WARN] Exclusividad activada pero el pool RANGE quedar√≠a vac√≠o tras excluir TREND. \"\n",
    "            \"Se mantiene pool RANGE original; el basket RANGE podr√≠a solaparse.\"\n",
    "        )\n",
    "    else:\n",
    "        range_pool_eff = range_pool_excl\n",
    "        range_source_eff = f\"{range_source} + excl_trend\"\n",
    "\n",
    "# ------------------------------- Construir RANGE con pool efectivo -------------\n",
    "range_basket_df, thr_range_used, avg_corr_range = build_core_basket(\n",
    "    candidates=range_pool_eff,\n",
    "    core_col=\"core_score_range\",\n",
    "    family_label=\"RANGE\",\n",
    "    target_size=target_size_range,\n",
    "    thr_start=corr_thr_start,\n",
    "    thr_min=corr_thr_min,\n",
    ")\n",
    "\n",
    "range_syms_selected = range_basket_df[\"symbol\"].to_list() if (not range_basket_df.is_empty() and \"symbol\" in range_basket_df.columns) else []\n",
    "\n",
    "# ------------------------------- QA de solapamiento ----------------------------\n",
    "overlap_syms = sorted(set(trend_syms_selected).intersection(set(range_syms_selected)))\n",
    "overlap_n = len(overlap_syms)\n",
    "\n",
    "if enforce_exclusivity and overlap_n > 0:\n",
    "    print(\n",
    "        \"[Celda 11][WARN] Se detect√≥ solapamiento pese a exclusividad. \"\n",
    "        \"Esto puede ocurrir si el pool RANGE no pudo excluir TREND por falta de candidatos.\"\n",
    "    )\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(f\"[Celda 11] Cesta TREND core ‚Üí size={trend_basket_df.height}, thr_usado={thr_trend_used:.2f}, basket_avg_abs_corr={avg_corr_trend:.3f}\")\n",
    "print(f\"[Celda 11] Cesta RANGE core ‚Üí size={range_basket_df.height}, thr_usado={thr_range_used:.2f}, basket_avg_abs_corr={avg_corr_range:.3f}\")\n",
    "print(f\"[Celda 11] Exclusividad cruzada = {enforce_exclusivity} | overlap_n={overlap_n}\")\n",
    "if overlap_syms:\n",
    "    print(f\"[Celda 11] Overlap symbols: {overlap_syms}\")\n",
    "\n",
    "# ------------------------------- Escritura de parquet --------------------------\n",
    "trend_basket_df.write_parquet(basket_trend_core_path)\n",
    "range_basket_df.write_parquet(basket_range_core_path)\n",
    "\n",
    "print(f\"[Celda 11] OUTPUT ‚Üí {str(basket_trend_core_path)} (rows={trend_basket_df.height}, cols={trend_basket_df.width})\")\n",
    "print(f\"[Celda 11] OUTPUT ‚Üí {str(basket_range_core_path)} (rows={range_basket_df.height}, cols={range_basket_df.width})\")\n",
    "\n",
    "# ------------------------------- selection_table (union de cestas) -------------\n",
    "selection_df = pl.concat(\n",
    "    [\n",
    "        trend_basket_df.select([\"symbol\", \"family\"]) if not trend_basket_df.is_empty() else pl.DataFrame({\"symbol\": [], \"family\": []}),\n",
    "        range_basket_df.select([\"symbol\", \"family\"]) if not range_basket_df.is_empty() else pl.DataFrame({\"symbol\": [], \"family\": []}),\n",
    "    ],\n",
    "    how=\"vertical_relaxed\",\n",
    ").unique(subset=[\"symbol\", \"family\"])\n",
    "\n",
    "selection_df.write_parquet(selection_table_path)\n",
    "print(f\"[Celda 11] OUTPUT ‚Üí {str(selection_table_path)} (rows={selection_df.height}, cols={selection_df.width})\")\n",
    "\n",
    "# ------------------------------- Reporte HTML ----------------------------------\n",
    "ts_now = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "summary_df = pl.DataFrame(\n",
    "    {\n",
    "        \"basket\": [\"TREND_core\", \"RANGE_core\"],\n",
    "        \"size\": [trend_basket_df.height, range_basket_df.height],\n",
    "        \"corr_threshold_used\": [thr_trend_used, thr_range_used],\n",
    "        \"basket_avg_abs_corr\": [avg_corr_trend, avg_corr_range],\n",
    "        \"candidate_source\": [trend_source, range_source_eff],\n",
    "        \"corr_source\": [source_corr_label, source_corr_label],\n",
    "    }\n",
    ")\n",
    "\n",
    "overlap_df = pl.DataFrame({\"symbol\": overlap_syms}) if overlap_syms else pl.DataFrame(schema={\"symbol\": pl.Utf8})\n",
    "\n",
    "report_sections = [\n",
    "    \"<html><head><meta charset='utf-8'><title>Regimen Selector Report</title></head><body>\",\n",
    "    \"<h2>Regimen Selector Report ‚Äî Baskets decorrelacionadas (TREND/RANGE core)</h2>\",\n",
    "    f\"<p><b>Generado:</b> {ts_now}</p>\",\n",
    "    f\"<p><b>RUN_ID:</b> {RUN_ID}</p>\",\n",
    "    f\"<p><b>Fuente de correlaci√≥n:</b> {source_corr_label} (archivo={str(corr_file)})</p>\",\n",
    "    \"<p><b>Par√°metros de cestas:</b> \"\n",
    "    f\"target_size_trend={target_size_trend}, \"\n",
    "    f\"target_size_range={target_size_range}, \"\n",
    "    f\"corr_threshold_start={corr_thr_start}, \"\n",
    "    f\"corr_threshold_min={corr_thr_min}, \"\n",
    "    f\"enforce_exclusivity={enforce_exclusivity}</p>\",\n",
    "    _df_html(summary_df, \"Resumen de cestas core (TREND/RANGE)\"),\n",
    "    _df_html(overlap_df, \"Solapamiento TREND vs RANGE (s√≠mbolos)\"),\n",
    "]\n",
    "\n",
    "trend_view = (\n",
    "    trend_basket_df.select(\n",
    "        [c for c in [\n",
    "            \"rank_in_basket\",\n",
    "            \"symbol\",\n",
    "            \"class_family\",\n",
    "            \"core_score_trend\",\n",
    "            \"score_trend_max\",\n",
    "            \"structure_score_raw\",\n",
    "            \"structure_flag\",\n",
    "            \"avg_abs_corr_to_others\",\n",
    "        ] if c in trend_basket_df.columns]\n",
    "    )\n",
    ") if not trend_basket_df.is_empty() else trend_basket_df\n",
    "\n",
    "range_view = (\n",
    "    range_basket_df.select(\n",
    "        [c for c in [\n",
    "            \"rank_in_basket\",\n",
    "            \"symbol\",\n",
    "            \"class_family\",\n",
    "            \"core_score_range\",\n",
    "            \"score_range_max\",\n",
    "            \"structure_score_raw\",\n",
    "            \"structure_flag\",\n",
    "            \"avg_abs_corr_to_others\",\n",
    "        ] if c in range_basket_df.columns]\n",
    "    )\n",
    ") if not range_basket_df.is_empty() else range_basket_df\n",
    "\n",
    "report_sections.append(_df_html(trend_view, \"Cesta TREND core (ordenada por core_score_trend)\"))\n",
    "report_sections.append(_df_html(range_view, \"Cesta RANGE core (ordenada por core_score_range)\"))\n",
    "report_sections.append(\"<p>Correlaciones reportadas como medias absolutas |œÅ|.</p>\")\n",
    "report_sections.append(\"</body></html>\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(report_sections), encoding=\"utf-8\")\n",
    "print(f\"[Celda 11] OUTPUT ‚Üí {str(report_path)} (OK)\")\n",
    "\n",
    "# ------------------------------- Actualizar GLOBAL_STATE -----------------------\n",
    "GLOBAL_STATE.setdefault(\"baskets\", {})\n",
    "GLOBAL_STATE[\"baskets\"].setdefault(\"paths\", {})\n",
    "GLOBAL_STATE[\"baskets\"][\"paths\"][\"trend_core\"] = str(basket_trend_core_path)\n",
    "GLOBAL_STATE[\"baskets\"][\"paths\"][\"range_core\"] = str(basket_range_core_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"basket_trend_core_path\"] = str(basket_trend_core_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"basket_range_core_path\"] = str(basket_range_core_path)\n",
    "GLOBAL_STATE[\"metrics\"][\"selection_table_path\"]   = str(selection_table_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c11\"] = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"trend_candidate_source\": trend_source,\n",
    "    \"range_candidate_source\": range_source_eff,\n",
    "    \"n_shortlist_trend\": int(trend_pool.height),\n",
    "    \"n_shortlist_range\": int(range_pool.height),\n",
    "    \"n_basket_trend\": int(trend_basket_df.height),\n",
    "    \"n_basket_range\": int(range_basket_df.height),\n",
    "    \"target_size_trend\": int(target_size_trend),\n",
    "    \"target_size_range\": int(target_size_range),\n",
    "    \"corr_threshold_start\": float(corr_thr_start),\n",
    "    \"corr_threshold_min\": float(corr_thr_min),\n",
    "    \"corr_threshold_trend_used\": float(thr_trend_used) if np.isfinite(thr_trend_used) else None,\n",
    "    \"corr_threshold_range_used\": float(thr_range_used) if np.isfinite(thr_range_used) else None,\n",
    "    \"basket_avg_abs_corr_trend\": float(avg_corr_trend),\n",
    "    \"basket_avg_abs_corr_range\": float(avg_corr_range),\n",
    "    \"corr_source\": source_corr_label,\n",
    "    \"universe_syms_n\": int(len(universe_syms)),\n",
    "    \"fallback_symbols_n\": int(len(FALLBACK_SYMBOLS)),\n",
    "    \"enforce_exclusivity\": bool(enforce_exclusivity),\n",
    "    \"overlap_n\": int(overlap_n),\n",
    "    \"overlap_symbols\": overlap_syms,\n",
    "    \"corr_file_used\": str(corr_file),\n",
    "}\n",
    "\n",
    "print(\">>> Celda 11 :: OK (baskets decorrelacionadas TREND/RANGE core construidas) [V2.1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b15c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 12 :: Export & QA de Baskets core (TREND/RANGE) [V2 SANITY + ATOMIC WRITES]\n",
      "[Celda 12] RUN_ID = 20251218_190810\n",
      "[Celda 12] baskets config ‚Üí target_size_trend=4, target_size_range=4\n",
      "[Celda 12] Sanity pre-lectura...\n",
      "TREND rows (disk): 4\n",
      "RANGE rows (disk): 2\n",
      "SELECTION rows (disk): 6\n",
      "‚ö†Ô∏è [Celda 12][SANITY] Tama√±os distintos al objetivo del config:\n",
      "  TREND disk = 4 vs target = 4\n",
      "  RANGE disk = 2 vs target = 4\n",
      "Esto puede ser NORMAL si la decorrelaci√≥n impide completar tama√±o.\n",
      "[Celda 12] Sanity OK.\n",
      "üìÅ INPUT ‚Üí basket_trend_core   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_trend_core.parquet (rows=4, cols=73)\n",
      "üìÅ INPUT ‚Üí basket_range_core   = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_range_core.parquet (rows=2, cols=73)\n",
      "üìÅ INPUT ‚Üí selection_table     = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\selection_table.parquet (rows=6, cols=2)\n",
      "üìÅ INPUT ‚Üí asset_shortlist     = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\asset_strategy_shortlist.parquet (rows=7, cols=58)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\selection_table_enriched.parquet (rows=6, cols=6)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_trend_core_symbols.txt (n=4)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_range_core_symbols.txt (n=2)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\selection_symbols.txt (n=6)\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\baskets_core_export_report.html (OK)\n",
      ">>> Celda 12 :: OK (export & QA de baskets core) [V2 SANITY + ATOMIC WRITES]\n"
     ]
    }
   ],
   "source": [
    "# Celda 12 ‚Äî Export & QA de Baskets core (TREND/RANGE)  [V2 SANITY + ATOMIC WRITES]\n",
    "# =============================================================================\n",
    "# Objetivo:\n",
    "#   Tomar outputs de Celda 11 y:\n",
    "#     - validar tama√±os vs objetivos\n",
    "#     - exportar listas de s√≠mbolos\n",
    "#     - enriquecer selection_table con metadatos de shortlist\n",
    "#     - generar reporte HTML de QA\n",
    "#\n",
    "# Inputs:\n",
    "#   - baskets/basket_trend_core.parquet\n",
    "#   - baskets/basket_range_core.parquet\n",
    "#   - scores/selection_table.parquet\n",
    "#   - scores/asset_strategy_shortlist.parquet (Celda 10C)\n",
    "#   - config.json  (bloque \"baskets\")\n",
    "#\n",
    "# Outputs:\n",
    "#   - scores/selection_table_enriched.parquet\n",
    "#   - baskets/basket_trend_core_symbols.txt\n",
    "#   - baskets/basket_range_core_symbols.txt\n",
    "#   - baskets/selection_symbols.txt\n",
    "#   - diagnostics/baskets_core_export_report.html\n",
    "#\n",
    "# GLOBAL_STATE:\n",
    "#   - Lee:\n",
    "#       GLOBAL_STATE[\"paths\"][\"scores\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"diagnostics\"]\n",
    "#       GLOBAL_STATE[\"paths\"][\"metrics\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"basket_trend_core_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"basket_range_core_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"selection_table_path\"]\n",
    "#       GLOBAL_STATE[\"metrics\"][\"asset_strategy_shortlist_path\"]\n",
    "#       GLOBAL_STATE[\"config\"]\n",
    "#   - Escribe:\n",
    "#       GLOBAL_STATE[\"metrics\"][\"selection_table_enriched_path\"]\n",
    "#       GLOBAL_STATE[\"baskets\"][\"paths\"][\"trend_core_symbols_txt\"]\n",
    "#       GLOBAL_STATE[\"baskets\"][\"paths\"][\"range_core_symbols_txt\"]\n",
    "#       GLOBAL_STATE[\"baskets\"][\"paths\"][\"selection_symbols_txt\"]\n",
    "#       GLOBAL_STATE[\"report_stats\"][\"c12\"]\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 12 :: Export & QA de Baskets core (TREND/RANGE) [V2 SANITY + ATOMIC WRITES]\")\n",
    "\n",
    "# ------------------------------- Guardas b√°sicas --------------------------------\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"[Celda 12][ERROR] GLOBAL_STATE no existe. Ejecuta celdas previas.\")\n",
    "\n",
    "for key in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if key not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"[Celda 12][ERROR] GLOBAL_STATE incompleto; falta clave '{key}'.\")\n",
    "\n",
    "PROJECT_ROOT = Path(GLOBAL_STATE[\"project_root\"]).resolve()\n",
    "RUN_ID       = str(GLOBAL_STATE[\"run_id\"])\n",
    "paths: Dict[str, Any] = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "for k in (\"scores\", \"metrics\", \"diagnostics\"):\n",
    "    if k not in paths:\n",
    "        raise RuntimeError(f\"[Celda 12][ERROR] Falta GLOBAL_STATE['paths']['{k}'].\")\n",
    "\n",
    "OUT_SCORES_DIR  = Path(paths[\"scores\"]).resolve()\n",
    "OUT_METRICS_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR    = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_BASKETS_DIR = Path(paths.get(\"baskets\", OUT_SCORES_DIR.parent / \"baskets\")).resolve()\n",
    "\n",
    "OUT_SCORES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_BASKETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_state = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "config_path = Path(paths.get(\"config\", OUT_DIAG_DIR / \"config.json\")).resolve()\n",
    "\n",
    "# ------------------------------- Toggles de SANITY -----------------------------\n",
    "# Esto ataca tu alerta real: que Celda 12 lea conteos distintos a los que Celda 11 dijo escribir.\n",
    "SANITY_HARD_FAIL_IO        = True   # mismatch entre TREND/RANGE vs SELECTION en disk = ERROR\n",
    "SANITY_HARD_FAIL_SIZES     = False  # tama√±os != target del config (puede ser normal por correlaci√≥n)\n",
    "SANITY_WARN_RUN_MIX        = True   # alerta si los paths no parecen del mismo RUN\n",
    "SANITY_CHECK_MTIME_ORDER   = True   # alerta si selection_table parece m√°s viejo que baskets\n",
    "\n",
    "# ------------------------------- Helpers generales -----------------------------\n",
    "def _require_file(p: Path, label: str = \"\", required: bool = True):\n",
    "    if required and (not p.exists() or p.stat().st_size == 0):\n",
    "        raise RuntimeError(f\"[Celda 12][ERROR] Falta input requerido {label}: {p}\")\n",
    "    if (not required) and (not p.exists() or p.stat().st_size == 0):\n",
    "        print(f\"[Celda 12] Aviso: input opcional no encontrado ({label}): {p}\")\n",
    "\n",
    "def _norm_sym(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    if \"family\" in df.columns:\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"family\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def _fmt_num(x):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return \"\"\n",
    "        if isinstance(x, bool):\n",
    "            return \"‚úÖ\" if x else \"‚Äî\"\n",
    "        if isinstance(x, (int, float)):\n",
    "            ax = abs(float(x))\n",
    "            if ax != 0.0 and (ax < 1e-4 or ax >= 1e3):\n",
    "                return f\"{float(x):.3e}\"\n",
    "            return f\"{float(x):.4f}\"\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def _df_html(df: pl.DataFrame, title: str) -> str:\n",
    "    if df.is_empty():\n",
    "        return f\"<h3>{title}</h3><p>(vac√≠o)</p>\"\n",
    "    thead = \"<tr>\" + \"\".join(f\"<th>{c}</th>\" for c in df.columns) + \"</tr>\"\n",
    "    body = \"\\n\".join(\n",
    "        \"<tr>\" + \"\".join(f\"<td>{_fmt_num(v)}</td>\" for v in r) + \"</tr>\"\n",
    "        for r in df.iter_rows()\n",
    "    )\n",
    "    return (\n",
    "        f\"<h3>{title}</h3>\"\n",
    "        f\"<table border='1' cellspacing='0' cellpadding='4'>\"\n",
    "        f\"<thead>{thead}</thead><tbody>{body}</tbody></table>\"\n",
    "    )\n",
    "\n",
    "def _atomic_write_text(path: Path, text: str):\n",
    "    tmp = path.with_name(path.name + f\".__tmp__{RUN_ID}\")\n",
    "    tmp.write_text(text, encoding=\"utf-8\")\n",
    "    if not tmp.exists() or tmp.stat().st_size == 0:\n",
    "        raise RuntimeError(f\"[Celda 12][ERROR] Fallo escritura temporal: {tmp}\")\n",
    "    # reemplazo limpio\n",
    "    try:\n",
    "        if path.exists():\n",
    "            path.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "    tmp.rename(path)\n",
    "\n",
    "def _write_symbols_txt_atomic(symbols: List[str], path: Path):\n",
    "    symbols = [s for s in symbols if isinstance(s, str) and s.strip() != \"\"]\n",
    "    symbols = sorted(dict.fromkeys([s.upper().strip() for s in symbols]))\n",
    "    _atomic_write_text(path, \"\\n\".join(symbols))\n",
    "    print(f\"üíæ OUTPUT ‚Üí {str(path)} (n={len(symbols)})\")\n",
    "    return symbols\n",
    "\n",
    "def _ensure_col(df: pl.DataFrame, name: str, dtype: pl.DataType):\n",
    "    if name not in df.columns:\n",
    "        return df.with_columns(pl.lit(None, dtype=dtype).alias(name))\n",
    "    return df\n",
    "\n",
    "def _path_has_run_id(p: Path, run_id: str) -> bool:\n",
    "    try:\n",
    "        return run_id in str(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _mtime(p: Path) -> float:\n",
    "    try:\n",
    "        return p.stat().st_mtime\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ------------------------------- Cargar config ---------------------------------\n",
    "_require_file(config_path, label=\"config.json\", required=True)\n",
    "\n",
    "if \"config\" in GLOBAL_STATE and isinstance(GLOBAL_STATE[\"config\"], dict):\n",
    "    cfg = GLOBAL_STATE[\"config\"]\n",
    "else:\n",
    "    cfg = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "baskets_cfg = cfg.get(\"baskets\", {}) or {}\n",
    "target_size_trend = int(baskets_cfg.get(\"target_size_trend\", 4))\n",
    "target_size_range = int(baskets_cfg.get(\"target_size_range\", 4))\n",
    "\n",
    "print(f\"[Celda 12] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[Celda 12] baskets config ‚Üí target_size_trend={target_size_trend}, target_size_range={target_size_range}\")\n",
    "\n",
    "# ------------------------------- Resolver paths inputs -------------------------\n",
    "basket_trend_core_path = Path(\n",
    "    metrics_state.get(\"basket_trend_core_path\", OUT_BASKETS_DIR / \"basket_trend_core.parquet\")\n",
    ").resolve()\n",
    "basket_range_core_path = Path(\n",
    "    metrics_state.get(\"basket_range_core_path\", OUT_BASKETS_DIR / \"basket_range_core.parquet\")\n",
    ").resolve()\n",
    "selection_table_path = Path(\n",
    "    metrics_state.get(\"selection_table_path\", OUT_SCORES_DIR / \"selection_table.parquet\")\n",
    ").resolve()\n",
    "\n",
    "asset_shortlist_path = Path(\n",
    "    metrics_state.get(\"asset_strategy_shortlist_path\", OUT_SCORES_DIR / \"asset_strategy_shortlist.parquet\")\n",
    ").resolve()\n",
    "\n",
    "selection_table_enriched_path = OUT_SCORES_DIR / \"selection_table_enriched.parquet\"\n",
    "report_path = OUT_DIAG_DIR / \"baskets_core_export_report.html\"\n",
    "\n",
    "trend_symbols_txt = OUT_BASKETS_DIR / \"basket_trend_core_symbols.txt\"\n",
    "range_symbols_txt = OUT_BASKETS_DIR / \"basket_range_core_symbols.txt\"\n",
    "selection_symbols_txt = OUT_BASKETS_DIR / \"selection_symbols.txt\"\n",
    "\n",
    "# ------------------------------- Requeridos ------------------------------------\n",
    "_require_file(basket_trend_core_path, label=\"basket_trend_core\", required=True)\n",
    "_require_file(basket_range_core_path, label=\"basket_range_core\", required=True)\n",
    "_require_file(selection_table_path, label=\"selection_table\", required=True)\n",
    "_require_file(asset_shortlist_path, label=\"asset_shortlist (10C)\", required=True)\n",
    "\n",
    "# ------------------------------- SANITY pre-lectura ----------------------------\n",
    "print(\"[Celda 12] Sanity pre-lectura...\")\n",
    "\n",
    "_trend_disk_h = pl.read_parquet(basket_trend_core_path).height\n",
    "_range_disk_h = pl.read_parquet(basket_range_core_path).height\n",
    "_sel_disk_h   = pl.read_parquet(selection_table_path).height\n",
    "\n",
    "print(\"TREND rows (disk):\", _trend_disk_h)\n",
    "print(\"RANGE rows (disk):\", _range_disk_h)\n",
    "print(\"SELECTION rows (disk):\", _sel_disk_h)\n",
    "\n",
    "_expected_sel = _trend_disk_h + _range_disk_h\n",
    "\n",
    "if _sel_disk_h != _expected_sel:\n",
    "    msg = (\n",
    "        \"[Celda 12][SANITY][ALERTA REAL] Mismatch entre outputs en DISCO:\\n\"\n",
    "        f\"  TREND disk = {_trend_disk_h}\\n\"\n",
    "        f\"  RANGE disk = {_range_disk_h}\\n\"\n",
    "        f\"  SELECTION disk = {_sel_disk_h}\\n\"\n",
    "        f\"  Esperado SELECTION = TREND + RANGE = {_expected_sel}\\n\"\n",
    "        \"Causas t√≠picas:\\n\"\n",
    "        \"  - Ejecutaste Celda 12 sin re-ejecutar Celda 11 tras cambios.\\n\"\n",
    "        \"  - Est√°s leyendo archivos de un RUN anterior mezclados.\\n\"\n",
    "        \"  - Se sobreescribi√≥ solo una parte de los outputs.\\n\"\n",
    "        \"Acci√≥n recomendada:\\n\"\n",
    "        \"  1) Re-ejecuta Celda 11.\\n\"\n",
    "        \"  2) Re-ejecuta inmediatamente Celda 12.\\n\"\n",
    "    )\n",
    "    if SANITY_HARD_FAIL_IO:\n",
    "        raise RuntimeError(msg)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è \" + msg)\n",
    "\n",
    "# Warning si los paths no parecen del mismo RUN\n",
    "if SANITY_WARN_RUN_MIX:\n",
    "    run_hits = sum([\n",
    "        _path_has_run_id(basket_trend_core_path, RUN_ID),\n",
    "        _path_has_run_id(basket_range_core_path, RUN_ID),\n",
    "        _path_has_run_id(selection_table_path, RUN_ID),\n",
    "    ])\n",
    "    if run_hits < 2:\n",
    "        print(\n",
    "            \"‚ö†Ô∏è [Celda 12][SANITY] Los paths no parecen contener tu RUN_ID actual.\\n\"\n",
    "            f\"  RUN_ID={RUN_ID}\\n\"\n",
    "            f\"  trend_path={basket_trend_core_path}\\n\"\n",
    "            f\"  range_path={basket_range_core_path}\\n\"\n",
    "            f\"  selection_path={selection_table_path}\\n\"\n",
    "            \"Esto puede indicar mezcla de runs.\"\n",
    "        )\n",
    "\n",
    "# Chequeo de orden por mtime (selection deber√≠a ser >= baskets si fue generado por Celda 11)\n",
    "if SANITY_CHECK_MTIME_ORDER:\n",
    "    mt_tr = _mtime(basket_trend_core_path)\n",
    "    mt_rg = _mtime(basket_range_core_path)\n",
    "    mt_sl = _mtime(selection_table_path)\n",
    "    if mt_sl + 1e-6 < max(mt_tr, mt_rg):\n",
    "        print(\n",
    "            \"‚ö†Ô∏è [Celda 12][SANITY] selection_table parece M√ÅS VIEJO que alg√∫n basket.\\n\"\n",
    "            \"Posible ejecuci√≥n fuera de orden. Considera re-ejecutar Celda 11 y luego Celda 12.\"\n",
    "        )\n",
    "\n",
    "# Chequeo de tama√±os vs target (puede ser normal por correlaci√≥n)\n",
    "if (_trend_disk_h != target_size_trend) or (_range_disk_h != target_size_range):\n",
    "    msg = (\n",
    "        \"[Celda 12][SANITY] Tama√±os distintos al objetivo del config:\\n\"\n",
    "        f\"  TREND disk = {_trend_disk_h} vs target = {target_size_trend}\\n\"\n",
    "        f\"  RANGE disk = {_range_disk_h} vs target = {target_size_range}\\n\"\n",
    "        \"Esto puede ser NORMAL si la decorrelaci√≥n impide completar tama√±o.\"\n",
    "    )\n",
    "    if SANITY_HARD_FAIL_SIZES:\n",
    "        raise RuntimeError(msg)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è \" + msg)\n",
    "\n",
    "print(\"[Celda 12] Sanity OK.\")\n",
    "\n",
    "# ------------------------------- Cargar inputs --------------------------------\n",
    "trend_df     = _norm_sym(pl.read_parquet(basket_trend_core_path))\n",
    "range_df     = _norm_sym(pl.read_parquet(basket_range_core_path))\n",
    "selection_df = _norm_sym(pl.read_parquet(selection_table_path))\n",
    "short_df     = _norm_sym(pl.read_parquet(asset_shortlist_path))\n",
    "\n",
    "print(f\"üìÅ INPUT ‚Üí basket_trend_core   = {str(basket_trend_core_path)} (rows={trend_df.height}, cols={trend_df.width})\")\n",
    "print(f\"üìÅ INPUT ‚Üí basket_range_core   = {str(basket_range_core_path)} (rows={range_df.height}, cols={range_df.width})\")\n",
    "print(f\"üìÅ INPUT ‚Üí selection_table     = {str(selection_table_path)} (rows={selection_df.height}, cols={selection_df.width})\")\n",
    "print(f\"üìÅ INPUT ‚Üí asset_shortlist     = {str(asset_shortlist_path)} (rows={short_df.height}, cols={short_df.width})\")\n",
    "\n",
    "# ------------------------------- Enriquecer selection_table -------------------\n",
    "# Alias de compatibilidad por si 10C cambi√≥ nombres\n",
    "if \"class_family\" not in short_df.columns and \"strategy_profile\" in short_df.columns:\n",
    "    short_df = short_df.with_columns(pl.col(\"strategy_profile\").cast(pl.Utf8).alias(\"class_family\"))\n",
    "\n",
    "for col, dt in [\n",
    "    (\"class_family\", pl.Utf8),\n",
    "    (\"core_score_trend\", pl.Float64),\n",
    "    (\"core_score_range\", pl.Float64),\n",
    "    (\"structure_flag\", pl.Utf8),\n",
    "]:\n",
    "    short_df = _ensure_col(short_df, col, dt)\n",
    "\n",
    "short_meta = short_df.select([\n",
    "    \"symbol\",\n",
    "    \"class_family\",\n",
    "    \"core_score_trend\",\n",
    "    \"core_score_range\",\n",
    "    \"structure_flag\",\n",
    "])\n",
    "\n",
    "selection_enriched = (\n",
    "    selection_df\n",
    "    .join(short_meta, on=\"symbol\", how=\"left\")\n",
    "    .select([\n",
    "        \"symbol\", \"family\",\n",
    "        \"class_family\",\n",
    "        \"core_score_trend\",\n",
    "        \"core_score_range\",\n",
    "        \"structure_flag\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Write at√≥mico parquet (v√≠a tmp + rename)\n",
    "tmp_enriched = selection_table_enriched_path.with_name(\n",
    "    selection_table_enriched_path.name + f\".__tmp__{RUN_ID}\"\n",
    ")\n",
    "selection_enriched.write_parquet(tmp_enriched)\n",
    "if not tmp_enriched.exists() or tmp_enriched.stat().st_size == 0:\n",
    "    raise RuntimeError(\"[Celda 12][ERROR] Write temporal selection_table_enriched fall√≥.\")\n",
    "try:\n",
    "    if selection_table_enriched_path.exists():\n",
    "        selection_table_enriched_path.unlink()\n",
    "except Exception:\n",
    "    pass\n",
    "tmp_enriched.rename(selection_table_enriched_path)\n",
    "\n",
    "print(\n",
    "    f\"üíæ OUTPUT ‚Üí {str(selection_table_enriched_path)} \"\n",
    "    f\"(rows={selection_enriched.height}, cols={selection_enriched.width})\"\n",
    ")\n",
    "\n",
    "# ------------------------------- Exportar s√≠mbolos ----------------------------\n",
    "trend_syms = trend_df[\"symbol\"].to_list() if \"symbol\" in trend_df.columns else []\n",
    "range_syms = range_df[\"symbol\"].to_list() if \"symbol\" in range_df.columns else []\n",
    "\n",
    "trend_syms = _write_symbols_txt_atomic(trend_syms, trend_symbols_txt)\n",
    "range_syms = _write_symbols_txt_atomic(range_syms, range_symbols_txt)\n",
    "\n",
    "union_syms = sorted(dict.fromkeys(trend_syms + range_syms))\n",
    "selection_syms = _write_symbols_txt_atomic(union_syms, selection_symbols_txt)\n",
    "\n",
    "# ------------------------------- QA tama√±os -----------------------------------\n",
    "n_trend = len(trend_syms)\n",
    "n_range = len(range_syms)\n",
    "n_union = len(selection_syms)\n",
    "\n",
    "summary_df = pl.DataFrame({\n",
    "    \"basket\": [\"TREND_core\", \"RANGE_core\", \"UNION\"],\n",
    "    \"size\": [n_trend, n_range, n_union],\n",
    "    \"target_size\": [target_size_trend, target_size_range, target_size_trend + target_size_range],\n",
    "})\n",
    "\n",
    "# ------------------------------- Reporte HTML ---------------------------------\n",
    "ts_now = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "trend_preview = (\n",
    "    trend_df.select([c for c in [\n",
    "        \"rank_in_basket\", \"symbol\", \"class_family\", \"core_score_trend\", \"avg_abs_corr_to_others\"\n",
    "    ] if c in trend_df.columns]).head(12)\n",
    ") if not trend_df.is_empty() else trend_df\n",
    "\n",
    "range_preview = (\n",
    "    range_df.select([c for c in [\n",
    "        \"rank_in_basket\", \"symbol\", \"class_family\", \"core_score_range\", \"avg_abs_corr_to_others\"\n",
    "    ] if c in range_df.columns]).head(12)\n",
    ") if not range_df.is_empty() else range_df\n",
    "\n",
    "report_sections = [\n",
    "    \"<html><head><meta charset='utf-8'><title>Baskets Core Export Report</title></head><body>\",\n",
    "    \"<h2>Baskets Core Export Report ‚Äî Celda 12</h2>\",\n",
    "    f\"<p><b>Generado:</b> {ts_now}</p>\",\n",
    "    f\"<p><b>RUN_ID:</b> {RUN_ID}</p>\",\n",
    "    \"<p><b>Inputs:</b></p>\",\n",
    "    f\"<ul>\"\n",
    "    f\"<li>trend_core: {str(basket_trend_core_path)}</li>\"\n",
    "    f\"<li>range_core: {str(basket_range_core_path)}</li>\"\n",
    "    f\"<li>selection_table: {str(selection_table_path)}</li>\"\n",
    "    f\"<li>asset_shortlist: {str(asset_shortlist_path)}</li>\"\n",
    "    f\"</ul>\",\n",
    "    _df_html(summary_df, \"Resumen tama√±os vs objetivos\"),\n",
    "    _df_html(trend_preview, \"Preview TREND core\"),\n",
    "    _df_html(range_preview, \"Preview RANGE core\"),\n",
    "    \"<p>Nota: Si alguna cesta sale menor al objetivo, puede ser normal si \"\n",
    "    \"la decorrelaci√≥n impide completar tama√±o. Revisa los umbrales en config.json.</p>\",\n",
    "    \"</body></html>\",\n",
    "]\n",
    "\n",
    "_atomic_write_text(report_path, \"\\n\".join(report_sections))\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(report_path)} (OK)\")\n",
    "\n",
    "# ------------------------------- GLOBAL_STATE ---------------------------------\n",
    "GLOBAL_STATE.setdefault(\"baskets\", {})\n",
    "GLOBAL_STATE[\"baskets\"].setdefault(\"paths\", {})\n",
    "GLOBAL_STATE[\"baskets\"][\"paths\"][\"trend_core_symbols_txt\"] = str(trend_symbols_txt)\n",
    "GLOBAL_STATE[\"baskets\"][\"paths\"][\"range_core_symbols_txt\"] = str(range_symbols_txt)\n",
    "GLOBAL_STATE[\"baskets\"][\"paths\"][\"selection_symbols_txt\"]  = str(selection_symbols_txt)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"metrics\", {})\n",
    "GLOBAL_STATE[\"metrics\"][\"selection_table_enriched_path\"] = str(selection_table_enriched_path)\n",
    "\n",
    "GLOBAL_STATE.setdefault(\"report_stats\", {})\n",
    "GLOBAL_STATE[\"report_stats\"][\"c12\"] = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"n_trend\": int(n_trend),\n",
    "    \"n_range\": int(n_range),\n",
    "    \"n_union\": int(n_union),\n",
    "    \"target_size_trend\": int(target_size_trend),\n",
    "    \"target_size_range\": int(target_size_range),\n",
    "    \"trend_symbols\": trend_syms,\n",
    "    \"range_symbols\": range_syms,\n",
    "    \"trend_path\": str(basket_trend_core_path),\n",
    "    \"range_path\": str(basket_range_core_path),\n",
    "    \"selection_path\": str(selection_table_path),\n",
    "    \"selection_enriched_path\": str(selection_table_enriched_path),\n",
    "}\n",
    "\n",
    "print(\">>> Celda 12 :: OK (export & QA de baskets core) [V2 SANITY + ATOMIC WRITES]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bac976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 13 :: Exports EA (familia+preset) :: v1.0.0 (SANITY + EXCLUSIVITY AWARE)\n",
      "[13] OUT_BASKETS_DIR = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\n",
      "[13] Baskets detectadas:\n",
      " - basket_range_core.parquet | rows=2\n",
      " - basket_trend_core.parquet | rows=4\n",
      "[13] INPUT scores_table = C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet (rows=166)\n",
      "[13] enforce_exclusivity (config) = True\n",
      "[13][SANITY] trend_core rows=4 | range_core rows=2 | overlap_n=0\n",
      "[13] OUTPUT ‚Üí ea_universe_range_core.csv | rows=2 | md5=9ea9979c70466e7352ec13060cd80bb2\n",
      "[13] OUTPUT ‚Üí ea_universe_range_core.parquet | rows=2 | md5=cab4cf3410b6fb5fbca6d6dad90e98e3\n",
      "[13] OUTPUT ‚Üí ea_universe_trend_core.csv | rows=4 | md5=e927650cbfabb5f8db096f7c9369a505\n",
      "[13] OUTPUT ‚Üí ea_universe_trend_core.parquet | rows=4 | md5=a5b3a66afb03655369e86bc7d831b6bf\n",
      "[13] Resumen export EA: rows_total=6 | families=2 | presets=1\n",
      "[13] Muestra export EA (hasta 10 filas)\n",
      "  symbol=BTCUSD | family=TREND | preset=CORE | SCORE_FINAL=0.46160913684711175 | score_significance=0.0 | score_viability=0.578808280454672 | decay_flag=False | n_per_month_total=5345.94 | coverage_p=1.0\n",
      "  symbol=XAUAUD | family=TREND | preset=CORE | SCORE_FINAL=0.4792580008416095 | score_significance=0.0 | score_viability=0.7077541356617449 | decay_flag=False | n_per_month_total=4491.98 | coverage_p=1.0\n",
      "  symbol=LVMH | family=TREND | preset=CORE | SCORE_FINAL=0.34974787738309676 | score_significance=0.0 | score_viability=0.5549349433618809 | decay_flag=False | n_per_month_total=1652.32 | coverage_p=1.0\n",
      "  symbol=ETHUSD | family=RANGE | preset=CORE | SCORE_FINAL=0.4523796737558716 | score_significance=0.0 | score_viability=0.5089111631293538 | decay_flag=False | n_per_month_total=5340.94 | coverage_p=1.0\n",
      "  symbol=XAUUSD | family=RANGE | preset=CORE | SCORE_FINAL=0.4510560000612118 | score_significance=0.0 | score_viability=0.5402764788589638 | decay_flag=False | n_per_month_total=4490.64 | coverage_p=1.0\n",
      "  symbol=BNBUSD | family=TREND | preset=CORE | SCORE_FINAL=0.488358254973641 | score_significance=0.0 | score_viability=0.4778642396563776 | decay_flag=False | n_per_month_total=6489.38 | coverage_p=1.0\n",
      "[13] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\exports\\ea_universe_all.parquet | rows=6 | md5=ae9a5006be565921df86955c1ce1af7c\n",
      "[13] OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\exports_hashes.json (OK)\n",
      ">>> Celda 13 :: OK\n"
     ]
    }
   ],
   "source": [
    "# >>> Celda 13 :: Exports EA (familia+preset) :: v1.0.0 (SANITY + EXCLUSIVITY AWARE)\n",
    "# Interfaz oficial con los EAs en MT5.\n",
    "# Detecta cestas reales en OUT_BASKETS_DIR (basket_*.parquet) y genera:\n",
    "#   1) Exports por cesta: 1 CSV + 1 Parquet por archivo detectado.\n",
    "#   2) Export unificado estable: ea_universe_all.{csv,parquet}\n",
    "#\n",
    "# Robustez:\n",
    "#   - Auto-discovery: no asume cestas fijas.\n",
    "#   - Fail-fast SOLO si no existe ninguna cesta basket_*.parquet.\n",
    "#   - family/preset se fuerzan desde el nombre del archivo.\n",
    "#   - No toma SCORE_FINAL desde basket.\n",
    "#   - Unificado hace unique por (symbol,family,preset).\n",
    "#   - Hashes MD5 de todos los exports.\n",
    "#\n",
    "# SANITY a√±adido:\n",
    "#   - Si existen basket_trend_core y basket_range_core:\n",
    "#       * puede chequear coherencia con selection_table.parquet\n",
    "#       * puede chequear exclusividad cruzada seg√∫n config\n",
    "#\n",
    "# GLOBAL_STATE:\n",
    "#   - GLOBAL_STATE[\"exports\"][\"parquet_path\"]\n",
    "#   - GLOBAL_STATE[\"exports\"][\"hashes_path\"]\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "import polars as pl\n",
    "\n",
    "print(\">>> Celda 13 :: Exports EA (familia+preset) :: v1.0.0 (SANITY + EXCLUSIVITY AWARE)\")\n",
    "\n",
    "# ======================= Descubrimiento de rutas (GLOBAL_STATE o fallback) =======================\n",
    "\n",
    "def _discover_paths() -> Tuple[Path, Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Retorna (OUT_SCORES_DIR, OUT_BASKETS_DIR, OUT_EXPORTS_DIR, OUT_DIAG_DIR)\n",
    "    Priorizando GLOBAL_STATE['paths']; si no existe, intenta reconstruir por convenci√≥n:\n",
    "      outputs/er_filter_5m/<RUN_ID m√°s reciente>/...\n",
    "    \"\"\"\n",
    "    if \"GLOBAL_STATE\" in globals() and isinstance(GLOBAL_STATE, dict) and \"paths\" in GLOBAL_STATE:\n",
    "        paths = GLOBAL_STATE[\"paths\"]\n",
    "        try:\n",
    "            scores_dir = Path(paths[\"scores\"]).resolve()\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"No se pudo resolver rutas: falta 'scores' en GLOBAL_STATE['paths'].\")\n",
    "        baskets_dir = Path(paths.get(\"baskets\", scores_dir.parent / \"baskets\")).resolve()\n",
    "        diagnostics_dir = Path(paths.get(\"diagnostics\", scores_dir.parent / \"diagnostics\")).resolve()\n",
    "        exports_dir = Path(paths.get(\"exports\", scores_dir.parent / \"exports\")).resolve()\n",
    "        return scores_dir, baskets_dir, exports_dir, diagnostics_dir\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    candidate = None\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        probe = p / \"outputs\" / \"er_filter_5m\"\n",
    "        if probe.exists() and probe.is_dir():\n",
    "            subdirs = [d for d in probe.iterdir() if d.is_dir()]\n",
    "            if subdirs:\n",
    "                candidate = sorted(subdirs)[-1]\n",
    "                break\n",
    "    if candidate is None:\n",
    "        raise RuntimeError(\n",
    "            \"No se pudieron resolver rutas: GLOBAL_STATE no disponible \"\n",
    "            \"y outputs/er_filter_5m no encontrado.\"\n",
    "        )\n",
    "\n",
    "    run_root = candidate.resolve()\n",
    "    scores_dir = (run_root / \"scores\").resolve()\n",
    "    baskets_dir = (run_root / \"baskets\").resolve()\n",
    "    diagnostics_dir = (run_root / \"diagnostics\").resolve()\n",
    "    exports_dir = (run_root / \"exports\").resolve()\n",
    "    return scores_dir, baskets_dir, exports_dir, diagnostics_dir\n",
    "\n",
    "\n",
    "OUT_SCORES_DIR, OUT_BASKETS_DIR, OUT_EXPORTS_DIR, OUT_DIAG_DIR = _discover_paths()\n",
    "OUT_BASKETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = (globals().get(\"GLOBAL_STATE\", {}) or {}).get(\"run_id\", None)\n",
    "PROJECT_ROOT = (globals().get(\"GLOBAL_STATE\", {}) or {}).get(\"project_root\", None)\n",
    "\n",
    "# ======================= Inputs base =======================\n",
    "\n",
    "scores_path = OUT_SCORES_DIR / \"scores_table.parquet\"\n",
    "selection_table_path = OUT_SCORES_DIR / \"selection_table.parquet\"\n",
    "config_path = OUT_DIAG_DIR / \"config.json\"\n",
    "\n",
    "summary_path = OUT_BASKETS_DIR / \"basket_summary.json\"  # opcional (solo meta)\n",
    "\n",
    "# ======================= Descubrimiento de cestas reales =======================\n",
    "\n",
    "def _discover_baskets(baskets_dir: Path) -> List[Path]:\n",
    "    files = sorted([p for p in baskets_dir.glob(\"basket_*.parquet\") if p.is_file()])\n",
    "    return files\n",
    "\n",
    "basket_files = _discover_baskets(OUT_BASKETS_DIR)\n",
    "\n",
    "print(f\"[13] OUT_BASKETS_DIR = {str(OUT_BASKETS_DIR)}\")\n",
    "print(\"[13] Baskets detectadas:\")\n",
    "\n",
    "_inputs_listing_cache: Dict[str, Tuple[bool, int]] = {}\n",
    "\n",
    "for p in basket_files:\n",
    "    try:\n",
    "        r = pl.read_parquet(str(p)).height\n",
    "    except Exception:\n",
    "        r = 0\n",
    "        print(f\"[13][WARN] No se pudo leer {p.name}; se exportar√° vac√≠o.\")\n",
    "    print(f\" - {p.name} | rows={r}\")\n",
    "    _inputs_listing_cache[str(p)] = (True, r)\n",
    "\n",
    "if len(basket_files) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No se detect√≥ ninguna cesta basket_*.parquet en OUT_BASKETS_DIR.\\n\"\n",
    "        f\"OUT_BASKETS_DIR={str(OUT_BASKETS_DIR)}\\n\"\n",
    "        \"Acci√≥n: ejecuta/valida Celda 11 y revisa GLOBAL_STATE['paths'].\"\n",
    "    )\n",
    "\n",
    "if not scores_path.exists():\n",
    "    raise RuntimeError(\"Missing required input: scores_table.parquet\")\n",
    "\n",
    "try:\n",
    "    scores_df = pl.read_parquet(str(scores_path))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo leer scores_table.parquet | reason={e}\")\n",
    "\n",
    "print(f\"[13] INPUT scores_table = {str(scores_path)} (rows={scores_df.height})\")\n",
    "\n",
    "# ======================= Config (para exclusividad opcional) =======================\n",
    "\n",
    "enforce_exclusivity = False\n",
    "if config_path.exists():\n",
    "    try:\n",
    "        cfg = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "        baskets_cfg = cfg.get(\"baskets\", {}) or {}\n",
    "        # soporta ambos nombres posibles\n",
    "        enforce_exclusivity = bool(\n",
    "            baskets_cfg.get(\"enforce_exclusivity\", False)\n",
    "            or baskets_cfg.get(\"enforce_cross_family_exclusivity\", False)\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"[13] enforce_exclusivity (config) = {enforce_exclusivity}\")\n",
    "\n",
    "# ======================= Columnas y utilidades =======================\n",
    "\n",
    "EXPORT_SCHEMA_COLS: List[str] = [\n",
    "    \"symbol\",\n",
    "    \"family\",\n",
    "    \"preset\",\n",
    "    \"SCORE_FINAL\",\n",
    "    \"score_significance\",\n",
    "    \"score_opportunity\",\n",
    "    \"score_stability\",\n",
    "    \"score_viability\",\n",
    "    \"pvals_combined_ER\",\n",
    "    \"pvals_combined_PD\",\n",
    "    \"pvals_combined_p_event\",\n",
    "    \"TR_trend_mean\",\n",
    "    \"TR_range_mean\",\n",
    "    \"NDQ_trend_mean\",\n",
    "    \"NDQ_range_mean\",\n",
    "    \"n_per_month_total\",\n",
    "    \"coverage_p\",\n",
    "    \"decay_flag\",\n",
    "    \"EstabScore_final\",\n",
    "    \"min_TR_required\",\n",
    "]\n",
    "\n",
    "SCORES_EXPECTED_COLS: List[str] = [c for c in EXPORT_SCHEMA_COLS if c not in (\"family\", \"preset\")]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"SCORE_FINAL\", \"score_significance\", \"score_opportunity\", \"score_stability\", \"score_viability\",\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    \"TR_trend_mean\", \"TR_range_mean\", \"NDQ_trend_mean\", \"NDQ_range_mean\",\n",
    "    \"n_per_month_total\", \"coverage_p\", \"EstabScore_final\", \"min_TR_required\",\n",
    "]\n",
    "BOOL_COLS = [\"decay_flag\"]\n",
    "STR_COLS = [\"symbol\", \"family\", \"preset\"]\n",
    "\n",
    "NOTES: List[str] = []\n",
    "\n",
    "def _md5sum(path: Path, chunk_size: int = 1 << 20) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _parse_family_preset_from_name(p: Path) -> Tuple[str, str]:\n",
    "    # Espera formato: basket_{family}_{preset}.parquet\n",
    "    stem = p.stem\n",
    "    parts = stem.split(\"_\", 2)\n",
    "    if len(parts) >= 3:\n",
    "        family = parts[1]\n",
    "        preset = parts[2]\n",
    "        return family.upper(), preset.upper()\n",
    "    return \"UNKNOWN\", \"UNKNOWN\"\n",
    "\n",
    "def _read_basket_or_empty(p: Path) -> pl.DataFrame:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df = pl.read_parquet(str(p))\n",
    "        except Exception:\n",
    "            print(f\"[13][WARN] No se pudo leer {p.name}; se exportar√° vac√≠o.\")\n",
    "            NOTES.append(f\"basket unreadable: {p.name}; exporting empty\")\n",
    "            df = pl.DataFrame(schema={\"symbol\": pl.Utf8, \"family\": pl.Utf8, \"preset\": pl.Utf8})\n",
    "    else:\n",
    "        df = pl.DataFrame(schema={\"symbol\": pl.Utf8, \"family\": pl.Utf8, \"preset\": pl.Utf8})\n",
    "\n",
    "    for c in (\"symbol\", \"family\", \"preset\"):\n",
    "        if c not in df.columns:\n",
    "            df = df.with_columns(pl.lit(None).alias(c))\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"family\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"family\"),\n",
    "        pl.col(\"preset\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"preset\"),\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def _prepare_scores(scores_df_in: pl.DataFrame) -> pl.DataFrame:\n",
    "    out = scores_df_in.clone()\n",
    "\n",
    "    missing = [c for c in SCORES_EXPECTED_COLS if c not in out.columns]\n",
    "    if missing:\n",
    "        print(f\"[13][WARN] scores_table missing columns: {missing}; imputed.\")\n",
    "        NOTES.append(f\"scores_table missing columns: {missing}; imputed\")\n",
    "        for c in missing:\n",
    "            if c in BOOL_COLS:\n",
    "                out = out.with_columns(pl.lit(False).alias(c))\n",
    "            elif c in STR_COLS:\n",
    "                out = out.with_columns(pl.lit(None).alias(c))\n",
    "            else:\n",
    "                out = out.with_columns(pl.lit(None).alias(c))\n",
    "\n",
    "    out = out.with_columns([\n",
    "        pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().alias(\"symbol\"),\n",
    "        pl.col(\"SCORE_FINAL\").cast(pl.Float64, strict=False).alias(\"SCORE_FINAL\"),\n",
    "    ])\n",
    "\n",
    "    out = (\n",
    "        out\n",
    "        .sort(by=[\"symbol\", \"SCORE_FINAL\"], descending=[False, True])\n",
    "        .unique(subset=[\"symbol\"], keep=\"first\")\n",
    "    )\n",
    "\n",
    "    cols_cast = []\n",
    "    for c in NUMERIC_COLS:\n",
    "        if c in out.columns:\n",
    "            cols_cast.append(pl.col(c).cast(pl.Float64, strict=False).alias(c))\n",
    "    for c in BOOL_COLS:\n",
    "        if c in out.columns:\n",
    "            cols_cast.append(pl.col(c).cast(pl.Boolean, strict=False).fill_null(False).alias(c))\n",
    "    if cols_cast:\n",
    "        out = out.with_columns(cols_cast)\n",
    "\n",
    "    keep = [\"symbol\"] + [c for c in SCORES_EXPECTED_COLS if c in out.columns and c != \"symbol\"]\n",
    "    keep = list(dict.fromkeys(keep))\n",
    "    return out.select(keep)\n",
    "\n",
    "# ======================= SANITY espec√≠fico para core TREND/RANGE =======================\n",
    "\n",
    "def _find_by_name(name: str) -> Optional[Path]:\n",
    "    for p in basket_files:\n",
    "        if p.name.lower() == name.lower():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "trend_core_file = _find_by_name(\"basket_trend_core.parquet\")\n",
    "range_core_file = _find_by_name(\"basket_range_core.parquet\")\n",
    "\n",
    "if trend_core_file and range_core_file:\n",
    "    tr_df = _read_basket_or_empty(trend_core_file)\n",
    "    rg_df = _read_basket_or_empty(range_core_file)\n",
    "\n",
    "    tr_syms = set(tr_df[\"symbol\"].drop_nulls().to_list()) if \"symbol\" in tr_df.columns else set()\n",
    "    rg_syms = set(rg_df[\"symbol\"].drop_nulls().to_list()) if \"symbol\" in rg_df.columns else set()\n",
    "    overlap = sorted(tr_syms.intersection(rg_syms))\n",
    "\n",
    "    print(f\"[13][SANITY] trend_core rows={tr_df.height} | range_core rows={rg_df.height} | overlap_n={len(overlap)}\")\n",
    "\n",
    "    if enforce_exclusivity and len(overlap) > 0:\n",
    "        raise RuntimeError(\n",
    "            \"[Celda 13][ERROR] Exclusividad cruzada activada en config, pero hay overlap entre TREND y RANGE.\\n\"\n",
    "            f\"overlap_syms={overlap}\\n\"\n",
    "            \"Acci√≥n: Re-ejecuta Celda 11 V2.1 y luego Celda 12/13.\"\n",
    "        )\n",
    "\n",
    "    if selection_table_path.exists():\n",
    "        try:\n",
    "            sel_h = pl.read_parquet(selection_table_path).height\n",
    "            expected = tr_df.height + rg_df.height\n",
    "            if sel_h != expected:\n",
    "                print(\n",
    "                    \"[13][SANITY][WARN] selection_table no coincide con suma de TREND+RANGE core.\\n\"\n",
    "                    f\"selection_rows={sel_h} | expected={expected}\\n\"\n",
    "                    \"Acci√≥n recomendada: re-ejecutar Celda 11 y luego Celda 12.\"\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ======================= Exportaci√≥n por cesta =======================\n",
    "\n",
    "def build_export_for_basket(\n",
    "    basket_path: Path,\n",
    "    scores_prepared: pl.DataFrame,\n",
    "    out_csv: Path,\n",
    "    out_parquet: Path,\n",
    ") -> Tuple[dict, pl.DataFrame]:\n",
    "\n",
    "    family_hint, preset_hint = _parse_family_preset_from_name(basket_path)\n",
    "\n",
    "    basket_df = _read_basket_or_empty(basket_path)\n",
    "    has_rank = \"rank_in_basket\" in basket_df.columns\n",
    "\n",
    "    # forzar family/preset desde el archivo\n",
    "    if basket_df.height > 0:\n",
    "        basket_df = basket_df.with_columns([\n",
    "            pl.lit(family_hint).alias(\"family\"),\n",
    "            pl.lit(preset_hint).alias(\"preset\"),\n",
    "        ])\n",
    "\n",
    "    select_cols = [\"symbol\", \"family\", \"preset\"]\n",
    "    if has_rank:\n",
    "        select_cols.append(\"rank_in_basket\")\n",
    "\n",
    "    export_df = basket_df.select(select_cols)\n",
    "    export_df = export_df.join(scores_prepared, on=\"symbol\", how=\"left\")\n",
    "\n",
    "    for col in EXPORT_SCHEMA_COLS:\n",
    "        if col not in export_df.columns:\n",
    "            if col in BOOL_COLS:\n",
    "                export_df = export_df.with_columns(pl.lit(False).alias(col))\n",
    "            elif col in STR_COLS:\n",
    "                export_df = export_df.with_columns(pl.lit(None).alias(col))\n",
    "            else:\n",
    "                export_df = export_df.with_columns(pl.lit(None).alias(col))\n",
    "\n",
    "    # ordenar estable\n",
    "    try:\n",
    "        if has_rank:\n",
    "            export_df = export_df.with_columns(pl.col(\"rank_in_basket\").cast(pl.Int64, strict=False))\n",
    "            export_df = export_df.sort(\n",
    "                by=[\"rank_in_basket\", \"symbol\"],\n",
    "                descending=[False, False],\n",
    "                nulls_last=True,\n",
    "            )\n",
    "        else:\n",
    "            export_df = export_df.sort(\n",
    "                by=[\"SCORE_FINAL\", \"symbol\"],\n",
    "                descending=[True, False],\n",
    "                nulls_last=True,\n",
    "            )\n",
    "    except TypeError:\n",
    "        if has_rank:\n",
    "            export_df = export_df.sort(by=[\"rank_in_basket\", \"symbol\"], descending=[False, False])\n",
    "        else:\n",
    "            export_df = export_df.sort(by=[\"SCORE_FINAL\", \"symbol\"], descending=[True, False])\n",
    "\n",
    "    export_df = export_df.select(EXPORT_SCHEMA_COLS)\n",
    "\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    export_df.write_csv(str(out_csv))\n",
    "    if (not out_csv.exists()) or out_csv.stat().st_size == 0:\n",
    "        raise RuntimeError(f\"Failed writing export: {str(out_csv)}\")\n",
    "\n",
    "    export_df.write_parquet(str(out_parquet))\n",
    "    if (not out_parquet.exists()) or out_parquet.stat().st_size == 0:\n",
    "        raise RuntimeError(f\"Failed writing export: {str(out_parquet)}\")\n",
    "\n",
    "    info = {\n",
    "        \"rows_csv\": export_df.height,\n",
    "        \"bytes_csv\": out_csv.stat().st_size,\n",
    "        \"md5_csv\": _md5sum(out_csv),\n",
    "        \"rows_parquet\": export_df.height,\n",
    "        \"bytes_parquet\": out_parquet.stat().st_size,\n",
    "        \"md5_parquet\": _md5sum(out_parquet),\n",
    "    }\n",
    "    return info, export_df\n",
    "\n",
    "files_metrics: Dict[str, Dict] = {}\n",
    "export_dfs: List[pl.DataFrame] = []\n",
    "\n",
    "scores_prepared = _prepare_scores(scores_df)\n",
    "\n",
    "for bpath in basket_files:\n",
    "    family_hint, preset_hint = _parse_family_preset_from_name(bpath)\n",
    "\n",
    "    out_csv = OUT_EXPORTS_DIR / f\"ea_universe_{family_hint.lower()}_{preset_hint.lower()}.csv\"\n",
    "    out_parquet = OUT_EXPORTS_DIR / f\"ea_universe_{family_hint.lower()}_{preset_hint.lower()}.parquet\"\n",
    "\n",
    "    info, exp_df = build_export_for_basket(bpath, scores_prepared, out_csv, out_parquet)\n",
    "    export_dfs.append(exp_df)\n",
    "\n",
    "    print(f\"[13] OUTPUT ‚Üí {out_csv.name} | rows={info['rows_csv']} | md5={info['md5_csv']}\")\n",
    "    print(f\"[13] OUTPUT ‚Üí {out_parquet.name} | rows={info['rows_parquet']} | md5={info['md5_parquet']}\")\n",
    "\n",
    "    files_metrics[out_csv.name] = {\"rows\": info[\"rows_csv\"], \"bytes\": info[\"bytes_csv\"], \"md5\": info[\"md5_csv\"]}\n",
    "    files_metrics[out_parquet.name] = {\"rows\": info[\"rows_parquet\"], \"bytes\": info[\"bytes_parquet\"], \"md5\": info[\"md5_parquet\"]}\n",
    "\n",
    "# ======================= Export unificado =======================\n",
    "\n",
    "export_all = pl.concat(export_dfs, how=\"vertical\") if export_dfs else pl.DataFrame()\n",
    "\n",
    "if not export_all.is_empty():\n",
    "    export_all = export_all.unique(subset=[\"symbol\", \"family\", \"preset\"], keep=\"first\")\n",
    "\n",
    "rows_total = export_all.height if not export_all.is_empty() else 0\n",
    "n_families = export_all[\"family\"].n_unique() if (not export_all.is_empty() and \"family\" in export_all.columns) else 0\n",
    "n_presets  = export_all[\"preset\"].n_unique() if (not export_all.is_empty() and \"preset\" in export_all.columns) else 0\n",
    "\n",
    "print(f\"[13] Resumen export EA: rows_total={rows_total} | families={n_families} | presets={n_presets}\")\n",
    "\n",
    "# muestra\n",
    "print(\"[13] Muestra export EA (hasta 10 filas)\")\n",
    "if export_all.is_empty():\n",
    "    print(\"  (export vac√≠o)\")\n",
    "else:\n",
    "    sample_cols = [\n",
    "        \"symbol\", \"family\", \"preset\",\n",
    "        \"SCORE_FINAL\", \"score_significance\",\n",
    "        \"score_viability\", \"decay_flag\",\n",
    "        \"n_per_month_total\", \"coverage_p\",\n",
    "    ]\n",
    "    sample_cols = [c for c in sample_cols if c in export_all.columns]\n",
    "    for row in export_all.select(sample_cols).head(10).iter_rows(named=True):\n",
    "        row_str = \" | \".join(f\"{k}={row.get(k)}\" for k in sample_cols)\n",
    "        print(\"  \" + row_str)\n",
    "\n",
    "# escritura unificado\n",
    "export_csv_final = OUT_EXPORTS_DIR / \"ea_universe_all.csv\"\n",
    "export_parquet_final = OUT_EXPORTS_DIR / \"ea_universe_all.parquet\"\n",
    "\n",
    "export_all.write_csv(str(export_csv_final))\n",
    "export_all.write_parquet(str(export_parquet_final))\n",
    "\n",
    "if (not export_parquet_final.exists()) or export_parquet_final.stat().st_size == 0:\n",
    "    raise RuntimeError(\"Failed writing unified EA parquet export\")\n",
    "\n",
    "files_metrics[export_csv_final.name] = {\n",
    "    \"rows\": export_all.height,\n",
    "    \"bytes\": export_csv_final.stat().st_size,\n",
    "    \"md5\": _md5sum(export_csv_final),\n",
    "}\n",
    "files_metrics[export_parquet_final.name] = {\n",
    "    \"rows\": export_all.height,\n",
    "    \"bytes\": export_parquet_final.stat().st_size,\n",
    "    \"md5\": _md5sum(export_parquet_final),\n",
    "}\n",
    "\n",
    "print(f\"[13] OUTPUT ‚Üí {str(export_parquet_final)} | rows={export_all.height} | md5={files_metrics[export_parquet_final.name]['md5']}\")\n",
    "\n",
    "# ======================= Hashes/meta consolidado =======================\n",
    "\n",
    "basket_size_max = None\n",
    "corr_threshold_cfg = None\n",
    "corr_thresholds_preset = None\n",
    "summary_found = False\n",
    "\n",
    "if summary_path.exists():\n",
    "    summary_found = True\n",
    "    try:\n",
    "        with summary_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            js = json.load(fh)\n",
    "        basket_size_max = js.get(\"basket_size_max\", None)\n",
    "        corr_threshold_cfg = js.get(\"corr_threshold_cfg\", None)\n",
    "        corr_thresholds_preset = js.get(\"corr_thresholds_preset\", None)\n",
    "        if isinstance(basket_size_max, dict):\n",
    "            basket_size_max = None\n",
    "    except Exception:\n",
    "        NOTES.append(\"basket_summary.json malformed; meta omitted\")\n",
    "\n",
    "unique_symbols_all: set = set()\n",
    "if (not export_all.is_empty()) and (\"symbol\" in export_all.columns):\n",
    "    unique_symbols_all = set(export_all[\"symbol\"].drop_nulls().cast(pl.Utf8).to_list())\n",
    "\n",
    "exports_hashes = {\n",
    "    \"run_meta\": {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"project_root\": PROJECT_ROOT,\n",
    "        \"generated_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    },\n",
    "    \"baskets_meta\": {\n",
    "        \"basket_files_detected\": [p.name for p in basket_files],\n",
    "        \"basket_size_max\": basket_size_max,\n",
    "        \"corr_threshold_cfg\": corr_threshold_cfg,\n",
    "        \"corr_thresholds_preset\": corr_thresholds_preset,\n",
    "        \"summary_file_found\": bool(summary_found),\n",
    "        \"enforce_exclusivity_config\": bool(enforce_exclusivity),\n",
    "    },\n",
    "    \"files\": files_metrics,\n",
    "    \"summary\": {\n",
    "        \"total_files\": int(len(files_metrics)),\n",
    "        \"unique_symbols_exported\": int(len(unique_symbols_all)),\n",
    "        \"notes\": NOTES,\n",
    "    },\n",
    "}\n",
    "\n",
    "hashes_path = OUT_DIAG_DIR / \"exports_hashes.json\"\n",
    "hashes_path.write_text(json.dumps(exports_hashes, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "if hashes_path.stat().st_size <= 0:\n",
    "    raise RuntimeError(\"Failed writing exports_hashes.json\")\n",
    "\n",
    "print(f\"[13] OUTPUT ‚Üí {str(hashes_path)} (OK)\")\n",
    "\n",
    "# ======================= GLOBAL_STATE y cierre =======================\n",
    "\n",
    "if \"GLOBAL_STATE\" in globals() and isinstance(GLOBAL_STATE, dict):\n",
    "    GLOBAL_STATE.setdefault(\"exports\", {})\n",
    "    GLOBAL_STATE[\"exports\"][\"parquet_path\"] = str(export_parquet_final)\n",
    "    GLOBAL_STATE[\"exports\"][\"hashes_path\"] = str(hashes_path)\n",
    "\n",
    "print(\">>> Celda 13 :: OK\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37461387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 14 :: Run Summary + Reporte HTML (sin backtesting) :: v0.9.9d (FINAL)\n",
      "üìÅ Fuentes clave para el reporte:\n",
      "  - coverage_table_5m: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\coverage_table_5m.parquet\n",
      "  - regime_thresholds: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_thresholds.parquet\n",
      "  - regime_labels: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "  - economic_viability: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\economic_viability.parquet\n",
      "  - opportunity_summary: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\opportunity_summary.parquet\n",
      "  - multiple_testing_summary: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\multiple_testing_summary.parquet\n",
      "  - multiple_testing_bh: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\multiple_testing_bh.parquet\n",
      "  - drift_log: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\drift_log.parquet\n",
      "  - stability_table: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet\n",
      "  - stability_table_advanced: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet\n",
      "  - scores_table: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet\n",
      "  - config_json: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\config.json\n",
      "  - run_metadata: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\logs\\run_metadata.json\n",
      "  - ea_universe_all: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\exports\\ea_universe_all.parquet\n",
      "  - exports_hashes: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\exports_hashes.json\n",
      "  - basket_detected: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_range_core.parquet\n",
      "  - basket_detected: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\\basket_trend_core.parquet\n",
      "üíæ OUTPUT ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\reports\\regimen_selector_report.html (bytes=92024)\n",
      ">>> Celda 14 :: OK (run summary + reporte HTML sin backtesting)\n"
     ]
    }
   ],
   "source": [
    "# >>> Celda 14 :: Run Summary + Reporte HTML (sin backtesting) :: v0.9.9d (FINAL)\n",
    "# Genera un resumen PRODUCTIVO del RUN + reporte HTML ligero.\n",
    "# No ejecuta backtesting.\n",
    "#\n",
    "# Robustez:\n",
    "#   - Descubre rutas por GLOBAL_STATE o convenci√≥n.\n",
    "#   - Lee fuentes de forma segura: si faltan, se reporta \"missing\" sin crash.\n",
    "#   - Detecta cestas reales basket_*.parquet.\n",
    "#   - Usa export EA unificado si existe.\n",
    "#\n",
    "# Output:\n",
    "#   - reports/regimen_selector_report.html\n",
    "# (Nombre mantenido por compatibilidad con tu pipeline actual)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "import html\n",
    "import polars as pl\n",
    "\n",
    "# ======================= Descubrimiento de rutas (GLOBAL_STATE o fallback) =======================\n",
    "\n",
    "def _discover_paths_runroot() -> Tuple[Path, Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Retorna (RUN_ROOT, dirs_dict)\n",
    "    dirs_dict contiene rutas est√°ndar esperadas si existen:\n",
    "      scores, baskets, exports, diagnostics, metrics, stability, logs, reports\n",
    "    \"\"\"\n",
    "    if \"GLOBAL_STATE\" in globals() and isinstance(GLOBAL_STATE, dict) and \"paths\" in GLOBAL_STATE:\n",
    "        paths = GLOBAL_STATE[\"paths\"]\n",
    "        scores_dir = Path(paths[\"scores\"]).resolve()\n",
    "        run_root = scores_dir.parent.resolve()\n",
    "    else:\n",
    "        cwd = Path.cwd()\n",
    "        candidate = None\n",
    "        for p in [cwd] + list(cwd.parents):\n",
    "            probe = p / \"outputs\" / \"er_filter_5m\"\n",
    "            if probe.exists() and probe.is_dir():\n",
    "                subdirs = [d for d in probe.iterdir() if d.is_dir()]\n",
    "                if subdirs:\n",
    "                    candidate = sorted(subdirs)[-1]\n",
    "                    break\n",
    "        if candidate is None:\n",
    "            raise RuntimeError(\n",
    "                \"No se pudieron resolver rutas del RUN: \"\n",
    "                \"GLOBAL_STATE no disponible y outputs/er_filter_5m no encontrado.\"\n",
    "            )\n",
    "        run_root = candidate.resolve()\n",
    "\n",
    "    dirs = {\n",
    "        \"scores\": run_root / \"scores\",\n",
    "        \"baskets\": run_root / \"baskets\",\n",
    "        \"exports\": run_root / \"exports\",\n",
    "        \"diagnostics\": run_root / \"diagnostics\",\n",
    "        \"metrics\": run_root / \"metrics\",\n",
    "        \"stability\": run_root / \"stability\",\n",
    "        \"logs\": run_root / \"logs\",\n",
    "        \"reports\": run_root / \"reports\",\n",
    "    }\n",
    "    for d in dirs.values():\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return run_root, dirs\n",
    "\n",
    "\n",
    "RUN_ROOT, DIRS = _discover_paths_runroot()\n",
    "\n",
    "RUN_ID = (globals().get(\"GLOBAL_STATE\", {}) or {}).get(\"run_id\", RUN_ROOT.name)\n",
    "PROJECT_ROOT = (globals().get(\"GLOBAL_STATE\", {}) or {}).get(\"project_root\", None)\n",
    "\n",
    "# ======================= Fuentes esperadas =======================\n",
    "\n",
    "def _safe_read_parquet(p: Path) -> Optional[pl.DataFrame]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return pl.read_parquet(str(p))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_read_json(p: Path) -> Optional[dict]:\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Detecta cestas reales\n",
    "basket_files = sorted([p for p in (DIRS[\"baskets\"]).glob(\"basket_*.parquet\") if p.is_file()])\n",
    "\n",
    "# Archivos clave (compatibles con tu log anterior)\n",
    "sources: Dict[str, Path] = {\n",
    "    \"coverage_table_5m\": DIRS[\"diagnostics\"] / \"coverage_table_5m.parquet\",\n",
    "    \"regime_thresholds\": DIRS[\"metrics\"] / \"regime_thresholds.parquet\",\n",
    "    \"regime_labels\": DIRS[\"metrics\"] / \"regime_labels.parquet\",\n",
    "    \"economic_viability\": DIRS[\"metrics\"] / \"economic_viability.parquet\",\n",
    "    \"opportunity_summary\": DIRS[\"metrics\"] / \"opportunity_summary.parquet\",\n",
    "    \"multiple_testing_summary\": DIRS[\"metrics\"] / \"multiple_testing_summary.parquet\",\n",
    "    \"multiple_testing_bh\": DIRS[\"metrics\"] / \"multiple_testing_bh.parquet\",\n",
    "    \"drift_log\": DIRS[\"metrics\"] / \"drift_log.parquet\",\n",
    "    \"stability_table\": DIRS[\"stability\"] / \"stability_table.parquet\",\n",
    "    \"stability_table_advanced\": DIRS[\"stability\"] / \"stability_table_advanced.parquet\",\n",
    "    \"scores_table\": DIRS[\"scores\"] / \"scores_table.parquet\",\n",
    "    \"config_json\": DIRS[\"diagnostics\"] / \"config.json\",\n",
    "    \"run_metadata\": DIRS[\"logs\"] / \"run_metadata.json\",\n",
    "    \"ea_universe_all\": DIRS[\"exports\"] / \"ea_universe_all.parquet\",\n",
    "    \"exports_hashes\": DIRS[\"diagnostics\"] / \"exports_hashes.json\",\n",
    "}\n",
    "\n",
    "# ======================= Prints de fuentes =======================\n",
    "\n",
    "print(\">>> Celda 14 :: Run Summary + Reporte HTML (sin backtesting) :: v0.9.9d (FINAL)\")\n",
    "print(\"üìÅ Fuentes clave para el reporte:\")\n",
    "\n",
    "for k, p in sources.items():\n",
    "    print(f\"  - {k}: {str(p)}\")\n",
    "\n",
    "if basket_files:\n",
    "    for bf in basket_files:\n",
    "        print(f\"  - basket_detected: {str(bf)}\")\n",
    "else:\n",
    "    print(\"  - basket_detected: (none)\")\n",
    "\n",
    "# ======================= Carga segura de data =======================\n",
    "\n",
    "df_scores = _safe_read_parquet(sources[\"scores_table\"])\n",
    "df_export_all = _safe_read_parquet(sources[\"ea_universe_all\"])\n",
    "df_cov = _safe_read_parquet(sources[\"coverage_table_5m\"])\n",
    "df_reg_thr = _safe_read_parquet(sources[\"regime_thresholds\"])\n",
    "df_reg_lbl = _safe_read_parquet(sources[\"regime_labels\"])\n",
    "df_econ = _safe_read_parquet(sources[\"economic_viability\"])\n",
    "df_opp = _safe_read_parquet(sources[\"opportunity_summary\"])\n",
    "df_mts = _safe_read_parquet(sources[\"multiple_testing_summary\"])\n",
    "df_mtb = _safe_read_parquet(sources[\"multiple_testing_bh\"])\n",
    "df_drift = _safe_read_parquet(sources[\"drift_log\"])\n",
    "df_stab = _safe_read_parquet(sources[\"stability_table\"])\n",
    "df_stab_adv = _safe_read_parquet(sources[\"stability_table_advanced\"])\n",
    "\n",
    "js_config = _safe_read_json(sources[\"config_json\"]) or {}\n",
    "js_meta = _safe_read_json(sources[\"run_metadata\"]) or {}\n",
    "js_hashes = _safe_read_json(sources[\"exports_hashes\"]) or {}\n",
    "\n",
    "# ======================= Helpers HTML =======================\n",
    "\n",
    "def _df_to_html_table(df: Optional[pl.DataFrame], title: str, max_rows: int = 20) -> str:\n",
    "    if df is None or df.is_empty():\n",
    "        return f\"<h3>{html.escape(title)}</h3><p><em>missing/empty</em></p>\"\n",
    "\n",
    "    show = df.head(max_rows)\n",
    "    cols = show.columns\n",
    "    rows = show.to_dicts()\n",
    "\n",
    "    out = [f\"<h3>{html.escape(title)}</h3>\"]\n",
    "    out.append(\"<div class='table-wrap'><table>\")\n",
    "    out.append(\"<thead><tr>\" + \"\".join(f\"<th>{html.escape(str(c))}</th>\" for c in cols) + \"</tr></thead>\")\n",
    "    out.append(\"<tbody>\")\n",
    "    for r in rows:\n",
    "        out.append(\"<tr>\" + \"\".join(f\"<td>{html.escape(str(r.get(c)))}</td>\" for c in cols) + \"</tr>\")\n",
    "    out.append(\"</tbody></table></div>\")\n",
    "    out.append(f\"<p class='note'>rows_shown={min(max_rows, show.height)} | rows_total={df.height}</p>\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _kv_block(d: dict, title: str) -> str:\n",
    "    if not d:\n",
    "        return f\"<h3>{html.escape(title)}</h3><p><em>missing/empty</em></p>\"\n",
    "    items = sorted(d.items(), key=lambda x: str(x[0]))\n",
    "    out = [f\"<h3>{html.escape(title)}</h3>\", \"<ul>\"]\n",
    "    for k, v in items:\n",
    "        out.append(f\"<li><b>{html.escape(str(k))}</b>: {html.escape(str(v))}</li>\")\n",
    "    out.append(\"</ul>\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ======================= Res√∫menes del export EA =======================\n",
    "\n",
    "export_summary_html = \"<h3>Export EA unificado</h3><p><em>missing/empty</em></p>\"\n",
    "dist_html = \"\"\n",
    "top_html = \"\"\n",
    "\n",
    "if df_export_all is not None and not df_export_all.is_empty():\n",
    "    rows_total = df_export_all.height\n",
    "    families = df_export_all[\"family\"].n_unique() if \"family\" in df_export_all.columns else 0\n",
    "    presets = df_export_all[\"preset\"].n_unique() if \"preset\" in df_export_all.columns else 0\n",
    "\n",
    "    export_summary_html = (\n",
    "        \"<h3>Export EA unificado</h3>\"\n",
    "        f\"<ul>\"\n",
    "        f\"<li><b>rows_total</b>: {rows_total}</li>\"\n",
    "        f\"<li><b>families</b>: {families}</li>\"\n",
    "        f\"<li><b>presets</b>: {presets}</li>\"\n",
    "        f\"</ul>\"\n",
    "    )\n",
    "\n",
    "    # distribuci√≥n family/preset\n",
    "    try:\n",
    "        dist = (\n",
    "            df_export_all\n",
    "            .group_by([\"family\", \"preset\"])\n",
    "            .agg(pl.len().alias(\"n\"))\n",
    "            .sort(by=[\"family\", \"preset\"])\n",
    "        )\n",
    "        dist_html = _df_to_html_table(dist, \"Distribuci√≥n por family/preset\", max_rows=50)\n",
    "    except Exception:\n",
    "        dist_html = \"<h3>Distribuci√≥n por family/preset</h3><p><em>error building table</em></p>\"\n",
    "\n",
    "    # top por SCORE_FINAL\n",
    "    if \"SCORE_FINAL\" in df_export_all.columns:\n",
    "        try:\n",
    "            top = (\n",
    "                df_export_all\n",
    "                .sort(by=[\"SCORE_FINAL\"], descending=[True])\n",
    "                .select([c for c in [\"symbol\", \"family\", \"preset\", \"SCORE_FINAL\", \"score_significance\",\n",
    "                                    \"score_opportunity\", \"score_stability\", \"score_viability\",\n",
    "                                    \"n_per_month_total\", \"coverage_p\", \"decay_flag\"]\n",
    "                         if c in df_export_all.columns])\n",
    "                .head(20)\n",
    "            )\n",
    "            top_html = _df_to_html_table(top, \"Top s√≠mbolos por SCORE_FINAL (export)\", max_rows=20)\n",
    "        except Exception:\n",
    "            top_html = \"<h3>Top s√≠mbolos por SCORE_FINAL (export)</h3><p><em>error building table</em></p>\"\n",
    "\n",
    "# ======================= Secci√≥n cestas detectadas =======================\n",
    "\n",
    "baskets_html_parts = [\"<h3>Cestas detectadas</h3>\"]\n",
    "if not basket_files:\n",
    "    baskets_html_parts.append(\"<p><em>none</em></p>\")\n",
    "else:\n",
    "    baskets_html_parts.append(\"<ul>\")\n",
    "    for bf in basket_files:\n",
    "        try:\n",
    "            r = pl.read_parquet(str(bf)).height\n",
    "        except Exception:\n",
    "            r = 0\n",
    "        fam, pre = (\"UNKNOWN\", \"UNKNOWN\")\n",
    "        stem = bf.stem.split(\"_\", 2)\n",
    "        if len(stem) >= 3:\n",
    "            fam, pre = stem[1].upper(), stem[2].upper()\n",
    "        baskets_html_parts.append(\n",
    "            f\"<li><b>{html.escape(bf.name)}</b> ‚Üí family={fam} | preset={pre} | rows={r}</li>\"\n",
    "        )\n",
    "    baskets_html_parts.append(\"</ul>\")\n",
    "\n",
    "baskets_html = \"\\n\".join(baskets_html_parts)\n",
    "\n",
    "# ======================= Construcci√≥n HTML =======================\n",
    "\n",
    "report_path = DIRS[\"reports\"] / \"regimen_selector_report.html\"\n",
    "\n",
    "generated_utc = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "css = \"\"\"\n",
    "<style>\n",
    "body{font-family:system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin:24px; color:#111;}\n",
    "h1{font-size:22px; margin-bottom:6px;}\n",
    "h2{font-size:18px; margin-top:22px; border-bottom:1px solid #ddd; padding-bottom:6px;}\n",
    "h3{font-size:15px; margin-top:16px;}\n",
    ".note{color:#666; font-size:12px;}\n",
    ".table-wrap{overflow-x:auto; border:1px solid #eee; border-radius:8px; padding:8px;}\n",
    "table{border-collapse:collapse; width:100%;}\n",
    "th, td{border:1px solid #eee; padding:6px 8px; font-size:12px; text-align:left;}\n",
    "th{background:#fafafa;}\n",
    "code{background:#f6f6f6; padding:2px 4px; border-radius:4px;}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html_parts: List[str] = []\n",
    "html_parts.append(\"<html><head><meta charset='utf-8'/>\")\n",
    "html_parts.append(\"<title>ER Filter 5M - Run Report</title>\")\n",
    "html_parts.append(css)\n",
    "html_parts.append(\"</head><body>\")\n",
    "\n",
    "html_parts.append(\"<h1>Run Summary - ER_FILTER_5M (sin backtesting)</h1>\")\n",
    "html_parts.append(f\"<div class='note'>RUN_ID={html.escape(str(RUN_ID))} | RUN_ROOT={html.escape(str(RUN_ROOT.name))} | generated_utc={generated_utc}</div>\")\n",
    "\n",
    "if PROJECT_ROOT:\n",
    "    html_parts.append(f\"<div class='note'>project_root={html.escape(str(PROJECT_ROOT))}</div>\")\n",
    "\n",
    "html_parts.append(\"<h2>Meta</h2>\")\n",
    "html_parts.append(_kv_block(js_meta, \"run_metadata.json (resumen)\"))\n",
    "html_parts.append(_kv_block(js_config, \"config.json (resumen)\"))\n",
    "html_parts.append(_kv_block(js_hashes.get(\"summary\", {}), \"exports_hashes.json (summary)\"))\n",
    "\n",
    "html_parts.append(\"<h2>Cestas & Exports EA</h2>\")\n",
    "html_parts.append(baskets_html)\n",
    "html_parts.append(export_summary_html)\n",
    "if dist_html:\n",
    "    html_parts.append(dist_html)\n",
    "if top_html:\n",
    "    html_parts.append(top_html)\n",
    "\n",
    "html_parts.append(\"<h2>Tablas estad√≠sticas (si existen)</h2>\")\n",
    "html_parts.append(_df_to_html_table(df_cov, \"coverage_table_5m\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_reg_thr, \"regime_thresholds\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_reg_lbl, \"regime_labels\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_econ, \"economic_viability\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_opp, \"opportunity_summary\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_mts, \"multiple_testing_summary\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_mtb, \"multiple_testing_bh\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_drift, \"drift_log\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_stab, \"stability_table\", max_rows=20))\n",
    "html_parts.append(_df_to_html_table(df_stab_adv, \"stability_table_advanced\", max_rows=20))\n",
    "\n",
    "html_parts.append(\"</body></html>\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(html_parts), encoding=\"utf-8\")\n",
    "\n",
    "bytes_html = report_path.stat().st_size if report_path.exists() else 0\n",
    "print(f\"üíæ OUTPUT ‚Üí {str(report_path)} (bytes={bytes_html})\")\n",
    "\n",
    "# Opcional: guardar ruta en GLOBAL_STATE\n",
    "if \"GLOBAL_STATE\" in globals() and isinstance(GLOBAL_STATE, dict):\n",
    "    GLOBAL_STATE.setdefault(\"reports\", {})\n",
    "    GLOBAL_STATE[\"reports\"][\"html_path\"] = str(report_path)\n",
    "\n",
    "print(\">>> Celda 14 :: OK (run summary + reporte HTML sin backtesting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91cef102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Celda 15 v2.0 ‚Äî DIAGN√ìSTICO INTEGRAL + CIERRE TECH + VALIDACI√ìN FINAL\n",
      "[Celda 15] RUN_ID = 20251218_190810\n",
      "[Celda 15] paths ‚Üí stability=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\n",
      "[Celda 15] paths ‚Üí metrics=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\n",
      "[Celda 15] paths ‚Üí diagnostics=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\n",
      "[Celda 15] paths ‚Üí scores=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "[Celda 15] paths ‚Üí events=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\n",
      "[Celda 15] paths ‚Üí baskets=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\n",
      "[Celda 15] paths ‚Üí exports=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\exports\n",
      "[Celda 15] paths ‚Üí logs=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\logs\n",
      "[Celda 15] paths ‚Üí reports=C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\reports\n",
      "[Celda 15] thresholds ‚Üí alpha_loose(stats)=0.2, global_min_TR=0.25\n",
      "[Celda 15] SCORE_FINAL thresholds ‚Üí viable=0.3, core=0.4, premium=0.5\n",
      "\n",
      "=== PRUEBA 1/3 :: Integridad de artefactos ===\n",
      "[Celda 15] structure_summary encontrado ‚Üí C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\structure_summary.parquet\n",
      "[OK] stability_table rows=166, cols=49\n",
      "[OK] stability_table_advanced rows=166, cols=77\n",
      "[OK] opportunity rows=84, cols=14  ‚Üí frequency_opportunity_table.parquet\n",
      "[OK] economic_viability rows=83, cols=13\n",
      "[OK] structure_summary rows=166\n",
      "‚úÖ PRUEBA 1 PASADA\n",
      "\n",
      "=== PRUEBA 2/3 :: Consistencia inter-celdas ===\n",
      "[Celda 15] non-null pvals_combined_ER = 100.00%\n",
      "[Celda 15] non-null pvals_combined_PD = 100.00%\n",
      "[Celda 15] non-null pvals_combined_p_event = 100.00%\n",
      "[Celda 15] non-null EstabScore_final = 100.00%\n",
      "[Celda 15] non-null decay_penalty = 100.00%\n",
      "[Celda 15] non-null min_TR_required = 100.00%\n",
      "‚úÖ PRUEBA 2 PASADA\n",
      "\n",
      "=== PRUEBA 3/3 :: Diagn√≥stico real de Trading ===\n",
      "[Celda 15] scores_table rows=166, cols=56\n",
      "[Celda 15] economic_viability contrato ‚Üí canonical=False, legacy=True\n",
      "[Celda 15] economic_viability columns ‚Üí ['NDQ_range', 'NDQ_trend', 'TR_after_cost_min', 'TR_ci_low_range', 'TR_ci_low_trend', 'TR_range', 'TR_trend', 'range_candidates', 'range_success', 'symbol', 'trend_candidates', 'trend_success', 'viab_flag_tr']\n",
      "[Celda 15] Universe total=166, core(>= 0.40)=26 (15.66%)\n",
      "\n",
      "[Celda 15] Gate health en CORE (% True):\n",
      "  passed_data_gate ‚Üí 100.00\n",
      "  passed_stability_gate ‚Üí 100.00\n",
      "  flag_oos_ok ‚Üí 100.00\n",
      "  passed_min_TR_required ‚Üí 46.15\n",
      "  passed_viability_gate ‚Üí 46.15\n",
      "  gate_all ‚Üí 46.15\n",
      "\n",
      "[Celda 15] TR_mean(core) real stats ‚Üí min=0.0, p25=2.4772250125409516e-05, mediana=0.12535585408272626, max=0.9542410014561505\n",
      "\n",
      "[Celda 15] Close policy ‚Üí WARN gate_all<15.00%, WARN minTR<10.00%\n",
      "\n",
      "‚úÖ PRUEBA 3 COMPLETADA (diagn√≥stico econ√≥mico)\n",
      ">>> CIERRE TECH :: GO ‚úÖ (con advertencias econ√≥micas)\n",
      "  [WARN] economic_viability en contrato LEGACY (no bloquea cierre t√©cnico).\n",
      "\n",
      "Siguiente paso recomendado:\n",
      "  - NO tocar el notebook base.\n",
      "  - Pasar a construcci√≥n de estrategias por activo usando selecci√≥n estricta por s√≠mbolo/preset.\n",
      "\n",
      "====================================================================================================\n",
      "BLOQUE 0 ‚Äî Listado r√°pido de artefactos .parquet por carpeta\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Baskets disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\baskets\n",
      "  - basket_range_core.parquet\n",
      "  - basket_trend_core.parquet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Exports disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\exports\n",
      "  - ea_universe_all.parquet\n",
      "  - ea_universe_range_core.parquet\n",
      "  - ea_universe_trend_core.parquet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Stability artefacts disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\n",
      "  - stab_folds.parquet\n",
      "  - stability_table.parquet\n",
      "  - stability_table_advanced.parquet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Scores artefacts disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\n",
      "  - asset_strategy_profiles.parquet\n",
      "  - asset_strategy_shortlist.parquet\n",
      "  - best_per_symbol.parquet\n",
      "  - candidates_table.parquet\n",
      "  - freq_only_watchlist.parquet\n",
      "  - handoff_operational.parquet\n",
      "  - scores_table.parquet\n",
      "  - selection_table.parquet\n",
      "  - selection_table_enriched.parquet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Metrics artefacts disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\n",
      "  - economic_viability.parquet\n",
      "  - er_series.parquet\n",
      "  - frequency_opportunity_table.parquet\n",
      "  - microstructure_summary.parquet\n",
      "  - pd_series.parquet\n",
      "  - regime_labels.parquet\n",
      "  - regime_thresholds.parquet\n",
      "  - regime_volatility_summary.parquet\n",
      "  - structure_summary.parquet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[DIR] Events artefacts disponibles\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\n",
      "  - range_events.parquet\n",
      "  - trend_events.parquet\n",
      "\n",
      "====================================================================================================\n",
      "BLOQUE 1 ‚Äî Checks de artefactos clave\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] regime_labels.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\regime_labels.parquet\n",
      "  ‚Üí OK: rows=24522466, cols=10\n",
      "  Columnas clave: symbol: nulls=0 | time_utc: nulls=0 | regime: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] trend_events.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\trend_events.parquet\n",
      "  ‚Üí OK: rows=9119905, cols=9\n",
      "  Columnas clave: symbol: nulls=0 | success: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] range_events.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\events\\range_events.parquet\n",
      "  ‚Üí OK: rows=9163602, cols=9\n",
      "  Columnas clave: symbol: nulls=0 | success: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] coverage_table_5m.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\diagnostics\\coverage_table_5m.parquet\n",
      "  ‚Üí OK: rows=84, cols=22\n",
      "  Columnas clave: symbol: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] opportunity_summary.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\metrics\\opportunity_summary.parquet\n",
      "  ‚Üí NO EXISTE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] stability_table.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table.parquet\n",
      "  ‚Üí OK: rows=166, cols=49\n",
      "  Columnas clave: symbol: nulls=0 | pvals_combined_ER: nulls=0 | pvals_combined_PD: nulls=0 | pvals_combined_p_event: nulls=0 | tvals_ER: nulls=0 | tvals_PD: nulls=0 | tvals_p_event: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] stability_table_advanced.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\stability\\stability_table_advanced.parquet\n",
      "  ‚Üí OK: rows=166, cols=77\n",
      "  Columnas clave: symbol: nulls=0 | EstabScore_final: nulls=0 | keep_stab_base: nulls=0 | decay_penalty: nulls=0 | decay_flag: nulls=0 | pvals_combined_ER: nulls=0 | pvals_combined_PD: nulls=0 | pvals_combined_p_event: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] scores_table.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_table.parquet\n",
      "  ‚Üí OK: rows=166, cols=56\n",
      "  Columnas clave: symbol: nulls=0 | preset: nulls=0 | SCORE_FINAL: nulls=0 | passed_data_gate: nulls=0 | flag_oos_ok: nulls=0 | passed_stability_gate: nulls=0 | gate_all: nulls=0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] scores_eligible.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\scores_eligible.parquet\n",
      "  ‚Üí NO EXISTE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[CHECK] selection_audit.parquet\n",
      "Ruta: C:\\Quant\\MT5_Data_Extraction\\outputs\\er_filter_5m\\20251218_190810\\scores\\selection_audit.parquet\n",
      "  ‚Üí NO EXISTE\n",
      "====================================================================================================\n",
      "RESUMEN R√ÅPIDO (rows por artefacto):\n",
      "  regime_labels.rows             = 24522466\n",
      "  trend_events.rows              = 9119905\n",
      "  range_events.rows              = 9163602\n",
      "  coverage_table_5m.rows         = 84\n",
      "  opportunity_summary.rows       = None\n",
      "  stability_table.rows           = 166\n",
      "  stability_table_advanced.rows  = 166\n",
      "  scores_table.rows              = 166\n",
      "  scores_eligible.rows           = None\n",
      "  selection_audit.rows           = None\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "BLOQUE A ‚Äî Resumen de gates en scores_table\n",
      "\n",
      "[gate_all] Conteo True/False:\n",
      "shape: (2, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ gate_all ‚îÜ count ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---   ‚îÇ\n",
      "‚îÇ bool     ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ false    ‚îÜ 152   ‚îÇ\n",
      "‚îÇ true     ‚îÜ 14    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[passed_data_gate] Conteo True/False:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ passed_data_gate ‚îÜ count ‚îÇ\n",
      "‚îÇ ---              ‚îÜ ---   ‚îÇ\n",
      "‚îÇ bool             ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ true             ‚îÜ 166   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[flag_oos_ok] Conteo True/False:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ flag_oos_ok ‚îÜ count ‚îÇ\n",
      "‚îÇ ---         ‚îÜ ---   ‚îÇ\n",
      "‚îÇ bool        ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ true        ‚îÜ 166   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "[passed_stability_gate] Conteo True/False:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ passed_stability_gate ‚îÜ count ‚îÇ\n",
      "‚îÇ ---                   ‚îÜ ---   ‚îÇ\n",
      "‚îÇ bool                  ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ true                  ‚îÜ 166   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "====================================================================================================\n",
      "BLOQUE B ‚Äî Resumen de p-values combinados\n",
      "[INFO] Usando alpha_loose (diag) = 0.2000\n",
      "  - pvals_combined_ER: p < alpha_loose en   0.00% de s√≠mbolos\n",
      "  - pvals_combined_PD: p < alpha_loose en   0.00% de s√≠mbolos\n",
      "  - pvals_combined_p_event: p < alpha_loose en  98.80% de s√≠mbolos\n",
      "\n",
      "[INFO] BLOQUE C ‚Äî scores_eligible no existe o est√° vac√≠o; se omite resumen de buckets.\n",
      "\n",
      "[INFO] BLOQUE D ‚Äî selection_audit no existe o est√° vac√≠o; se omite resumen de razones.\n",
      "\n",
      "====================================================================================================\n",
      "BLOQUE E ‚Äî Resumen final del RUN\n",
      "  universe_size (scores_table.rows) = 166\n",
      "  gate_all=True  ‚âà 14\n",
      "  gate_all=False ‚âà 152\n",
      "  eligibles_total (keep_final=True) = None\n",
      "  ‚ö†Ô∏è No se pudo construir breakdown por bucket.\n",
      "\n",
      "  No se puede resumir reason_code (selection_audit vac√≠o o sin columna).\n",
      "====================================================================================================\n",
      "BLOQUE E finalizado. Resumen global del RUN listo.\n",
      "====================================================================================================\n",
      "\n",
      "==============================================================================================================\n",
      "BLOQUE F ‚Äî Validaci√≥n final 100% certeza (min_TR, decay, alpha_loose, baskets vs exports)\n",
      "==============================================================================================================\n",
      "  ‚úÖ OK ‚Üí scores_table | rows=166, cols=56\n",
      "  ‚ùå NO EXISTE ‚Üí scores_eligible | scores_eligible.parquet\n",
      "  ‚ùå NO EXISTE ‚Üí selection_audit | selection_audit.parquet\n",
      "  ‚úÖ OK ‚Üí stability_table | rows=166, cols=49\n",
      "  ‚úÖ OK ‚Üí stability_table_advanced | rows=166, cols=77\n",
      "  ‚úÖ OK ‚Üí ea_universe_all | rows=6, cols=20\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1) ‚úÖ Verificar origen/presencia de min_TR_required\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  ‚úÖ scores_table: PRESENT\n",
      "  ‚ùå scores_eligible: MISSING_DF\n",
      "  ‚ùå selection_audit: MISSING_DF\n",
      "  ‚ö†Ô∏è stability_table: ABSENT\n",
      "  ‚úÖ stability_table_advanced: PRESENT\n",
      "  ‚úÖ ea_universe_all: PRESENT\n",
      "\n",
      "  ‚úÖ OK: min_TR_required est√° disponible en scores_table o no es requerido por tu versi√≥n actual.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "2) ‚úÖ Confirmar estado de decay_penalty / decay_flag\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  ‚úÖ decay_penalty existe | nulls=0 / rows=166\n",
      "  ‚úÖ OK: decay_penalty tiene valores no nulos en parte del universo.\n",
      "\n",
      "  [decay_flag] value_counts:\n",
      "shape: (1, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ decay_flag ‚îÜ count ‚îÇ\n",
      "‚îÇ ---        ‚îÜ ---   ‚îÇ\n",
      "‚îÇ bool       ‚îÜ u32   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ false      ‚îÜ 166   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "3) ‚úÖ Alinear alpha_loose (config.json vs GLOBAL_STATE)\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  alpha_loose.stats (config.json)      = 0.2\n",
      "  alpha_loose.selection (config.json)  = None\n",
      "  alpha_loose (GLOBAL_STATE.selection) = None\n",
      "  ‚ö†Ô∏è AVISO: alpha_loose solo fue encontrado en una de las fuentes.\n",
      "     Si tu notebook usa ambas rutas, esto puede causar discrepancias de diagn√≥stico.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "4) ‚úÖ Revisi√≥n de 'unique_symbols_exported' vs uni√≥n de baskets CORE\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  Baskets CORE detectadas:\n",
      "   - basket_range_core.parquet\n",
      "   - basket_trend_core.parquet\n",
      "  ‚úÖ OK ‚Üí basket_core::basket_range_core | rows=2, cols=73\n",
      "  ‚úÖ OK ‚Üí basket_core::basket_trend_core | rows=4, cols=73\n",
      "\n",
      "  Resumen CORE por archivo:\n",
      "   ‚Ä¢ basket_range_core.parquet: rows=2, unique_symbols=2\n",
      "   ‚Ä¢ basket_trend_core.parquet: rows=4, unique_symbols=4\n",
      "\n",
      "  ‚úÖ Uni√≥n global CORE unique_symbols = 6\n",
      "  ‚úÖ ea_universe_all unique_symbols(global) = 6\n",
      "\n",
      "  Comparativa CORE vs EXPORT(all):\n",
      "   ‚Ä¢ intersecci√≥n (CORE ‚à© EXPORT) = 6\n",
      "   ‚Ä¢ solo CORE                    = 0\n",
      "   ‚Ä¢ solo EXPORT                  = 0\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "5) ‚úÖ Resumen final de coherencia (flags tipo Celda X)\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  ‚úÖ TODO OK: no se detectaron inconsistencias relevantes para cierre del RUN.\n",
      "\n",
      "==============================================================================================================\n",
      "‚úÖ BLOQUE F completado.\n",
      "üìå Variable disponible en memoria: baskets_core_export_report\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Celda 15 v2.0 ‚Äî DIAGN√ìSTICO INTEGRAL + CIERRE TECH + VALIDACI√ìN FINAL\n",
    "# Notebook: ER_FILTER_5M_V1\n",
    "#\n",
    "# Integra:\n",
    "#   - Antigua Celda 15 (cierre t√©cnico + diagn√≥stico trading).\n",
    "#   - Celda 99 (diagn√≥stico r√°pido del pipeline).\n",
    "#   - Celda X (validaci√≥n fuerte de min_TR_required, decay_penalty, alpha_loose, baskets/export).\n",
    "#\n",
    "# Objetivos:\n",
    "#   1) Bloquear SOLO por fallos t√©cnicos / contrato inter-celdas (NO por econom√≠a, salvo modo STRICT).\n",
    "#   2) Dejar en consola un diagn√≥stico rico del RUN (artefactos, gates, p-values, elegibles, reasons).\n",
    "#   3) Validar coherencia entre baskets CORE y export EA unificado.\n",
    "#\n",
    "# Modo:\n",
    "#   STRICT_TRADING_CLOSE = False  -> cierre t√©cnico con advertencias econ√≥micas.\n",
    "#   STRICT_TRADING_CLOSE = True   -> cierre t√©cnico + bloqueo econ√≥mico.\n",
    "#\n",
    "# No escribe archivos ni modifica artefactos del RUN.\n",
    "# Deja en memoria:\n",
    "#   - baskets_core_export_report  (dict con coherencia baskets/export/alpha/min_TR/decay).\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import json\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Celda 15 v2.0 requiere 'polars'. Inst√°lalo con pip antes de seguir.\") from e\n",
    "\n",
    "STRICT_TRADING_CLOSE = False  # <-- deja False para tu objetivo actual\n",
    "\n",
    "print(\">>> Celda 15 v2.0 ‚Äî DIAGN√ìSTICO INTEGRAL + CIERRE TECH + VALIDACI√ìN FINAL\")\n",
    "\n",
    "# =============================================================================\n",
    "# 0) GLOBAL_STATE & PATHS\n",
    "# =============================================================================\n",
    "\n",
    "if \"GLOBAL_STATE\" not in globals() or not isinstance(GLOBAL_STATE, dict):\n",
    "    raise RuntimeError(\"GLOBAL_STATE no existe. Ejecuta las celdas iniciales.\")\n",
    "\n",
    "for k in (\"project_root\", \"run_id\", \"paths\"):\n",
    "    if k not in GLOBAL_STATE:\n",
    "        raise RuntimeError(f\"GLOBAL_STATE incompleto; falta '{k}'.\")\n",
    "\n",
    "RUN_ID = GLOBAL_STATE[\"run_id\"]\n",
    "paths = GLOBAL_STATE[\"paths\"]\n",
    "\n",
    "required_path_keys = (\"metrics\", \"events\", \"diagnostics\", \"stability\", \"scores\")\n",
    "missing_keys = [k for k in required_path_keys if k not in paths]\n",
    "if missing_keys:\n",
    "    raise RuntimeError(\n",
    "        f\"Faltan rutas m√≠nimas en GLOBAL_STATE['paths']: {missing_keys}. \"\n",
    "        \"Revisa celdas iniciales de configuraci√≥n de paths.\"\n",
    "    )\n",
    "\n",
    "# Directorios base\n",
    "OUT_STAB_DIR = Path(paths[\"stability\"]).resolve()\n",
    "OUT_METR_DIR = Path(paths[\"metrics\"]).resolve()\n",
    "OUT_DIAG_DIR = Path(paths[\"diagnostics\"]).resolve()\n",
    "OUT_SCO_DIR  = Path(paths.get(\"scores\", OUT_STAB_DIR.parent / \"scores\")).resolve()\n",
    "\n",
    "METRICS_DIR = OUT_METR_DIR\n",
    "STAB_DIR    = OUT_STAB_DIR\n",
    "DIAG_DIR    = OUT_DIAG_DIR\n",
    "SCORES_DIR  = OUT_SCO_DIR\n",
    "EVENTS_DIR  = Path(paths[\"events\"]).resolve()\n",
    "\n",
    "# Opcionales / recomendados\n",
    "BASKETS_DIR = Path(paths[\"baskets\"]).resolve() if \"baskets\" in paths else None\n",
    "EXPORTS_DIR = Path(paths[\"exports\"]).resolve() if \"exports\" in paths else None\n",
    "LOGS_DIR    = Path(paths[\"logs\"]).resolve()    if \"logs\" in paths else None\n",
    "REPORTS_DIR = Path(paths[\"reports\"]).resolve() if \"reports\" in paths else None\n",
    "\n",
    "print(f\"[Celda 15] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[Celda 15] paths ‚Üí stability={OUT_STAB_DIR}\")\n",
    "print(f\"[Celda 15] paths ‚Üí metrics={OUT_METR_DIR}\")\n",
    "print(f\"[Celda 15] paths ‚Üí diagnostics={OUT_DIAG_DIR}\")\n",
    "print(f\"[Celda 15] paths ‚Üí scores={OUT_SCO_DIR}\")\n",
    "print(f\"[Celda 15] paths ‚Üí events={EVENTS_DIR}\")\n",
    "if BASKETS_DIR: print(f\"[Celda 15] paths ‚Üí baskets={BASKETS_DIR}\")\n",
    "if EXPORTS_DIR: print(f\"[Celda 15] paths ‚Üí exports={EXPORTS_DIR}\")\n",
    "if LOGS_DIR:    print(f\"[Celda 15] paths ‚Üí logs={LOGS_DIR}\")\n",
    "if REPORTS_DIR: print(f\"[Celda 15] paths ‚Üí reports={REPORTS_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Resolver Config + thresholds\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG_PATH = Path(paths.get(\"config\", OUT_DIAG_DIR / \"config.json\")).resolve()\n",
    "cfg_state = GLOBAL_STATE.get(\"config\", {})\n",
    "if isinstance(cfg_state, dict) and cfg_state:\n",
    "    cfg = cfg_state\n",
    "else:\n",
    "    if not CONFIG_PATH.exists():\n",
    "        raise RuntimeError(f\"Config no encontrado: {CONFIG_PATH}\")\n",
    "    cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "    GLOBAL_STATE[\"config\"] = cfg\n",
    "\n",
    "policy        = cfg.get(\"policy\", {}) or {}\n",
    "stats_cfg     = cfg.get(\"stats\", {})  or {}\n",
    "scores_cfg    = cfg.get(\"scores\", {}) or {}\n",
    "stability_cfg = cfg.get(\"stability\", {}) or {}\n",
    "\n",
    "# alpha_loose \"oficial\" de stats (se usa como referencia para p-values)\n",
    "alpha_loose_stats = float(stats_cfg.get(\"alpha_loose\", 0.20))\n",
    "global_min_TR     = float(policy.get(\"min_TR_after_cost\", 0.25))\n",
    "\n",
    "# thresholds de SCORE (si no existen en config, defaults razonables)\n",
    "score_thr_cfg     = scores_cfg.get(\"score_thresholds\", {}) or {}\n",
    "score_viable_thr  = float(score_thr_cfg.get(\"viable\",     0.30))\n",
    "score_core_thr    = float(score_thr_cfg.get(\"interesting\", 0.40))\n",
    "score_premium_thr = float(score_thr_cfg.get(\"premium\",    0.50))\n",
    "\n",
    "print(f\"[Celda 15] thresholds ‚Üí alpha_loose(stats)={alpha_loose_stats}, global_min_TR={global_min_TR}\")\n",
    "print(\n",
    "    f\"[Celda 15] SCORE_FINAL thresholds ‚Üí \"\n",
    "    f\"viable={score_viable_thr}, core={score_core_thr}, premium={score_premium_thr}\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Helpers\n",
    "# =============================================================================\n",
    "\n",
    "def _exists(p: Path) -> bool:\n",
    "    return p.exists() and p.is_file()\n",
    "\n",
    "def _require(p: Path, label: str):\n",
    "    if not _exists(p):\n",
    "        raise RuntimeError(f\"[NO-GO TECH] Falta artefacto requerido: {label} ‚Üí {p}\")\n",
    "\n",
    "def _pick_first_existing(paths_list: List[Path]) -> Optional[Path]:\n",
    "    for p in paths_list:\n",
    "        if _exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _pct_nonnull(df: pl.DataFrame, col: str) -> float:\n",
    "    if df.is_empty() or col not in df.columns:\n",
    "        return 0.0\n",
    "    nn = df.get_column(col).drop_nulls().len()\n",
    "    return 100.0 * nn / max(1, df.height)\n",
    "\n",
    "def _ensure_upper_symbol(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"symbol\" in df.columns:\n",
    "        return df.with_columns(\n",
    "            pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase().str.strip_chars()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def _median_safe(s: pl.Series):\n",
    "    s2 = s.drop_nulls()\n",
    "    if s2.len() == 0:\n",
    "        return None\n",
    "    return float(s2.median())\n",
    "\n",
    "def _quantile_safe(s: pl.Series, q: float):\n",
    "    s2 = s.drop_nulls()\n",
    "    if s2.len() == 0:\n",
    "        return None\n",
    "    return float(s2.quantile(q, interpolation=\"nearest\"))\n",
    "\n",
    "def _pct_true(df: pl.DataFrame, col: str) -> float:\n",
    "    if col not in df.columns or df.is_empty():\n",
    "        return 0.0\n",
    "    s = df.get_column(col).cast(pl.Boolean, strict=False).fill_null(False)\n",
    "    return 100.0 * float(s.sum()) / max(1, df.height)\n",
    "\n",
    "def _list_parquets_in_dir(dir_path: Optional[Path], label: str) -> None:\n",
    "    if dir_path is None:\n",
    "        return\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"[DIR] {label}\")\n",
    "    print(f\"Ruta: {dir_path}\")\n",
    "    if not dir_path.exists():\n",
    "        print(\"  ‚Üí NO EXISTE\")\n",
    "        return\n",
    "    files = sorted([p.name for p in dir_path.glob(\"*.parquet\")])\n",
    "    if not files:\n",
    "        print(\"  ‚Üí (sin .parquet)\")\n",
    "        return\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "def _check_parquet(path: Path, label: str, cols_clave: Optional[List[str]] = None) -> Optional[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Lee un parquet (si existe) y muestra:\n",
    "      - filas, columnas\n",
    "      - para algunas columnas clave, n¬∫ de nulos\n",
    "    No lanza excepci√≥n si no existe: solo informa.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"[CHECK] {label}\")\n",
    "    print(f\"Ruta: {path}\")\n",
    "\n",
    "    if not path.exists():\n",
    "        print(\"  ‚Üí NO EXISTE\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pl.read_parquet(str(path))\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚Üí ERROR al leer: {e.__class__.__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "    rows, cols = df.height, len(df.columns)\n",
    "    print(f\"  ‚Üí OK: rows={rows}, cols={cols}\")\n",
    "    if rows == 0:\n",
    "        print(\"  ‚ö†Ô∏è AVISO: DataFrame VAC√çO\")\n",
    "\n",
    "    if cols_clave:\n",
    "        presentes = [c for c in cols_clave if c in df.columns]\n",
    "        if presentes:\n",
    "            info = []\n",
    "            for c in presentes:\n",
    "                try:\n",
    "                    n_null = int(df.get_column(c).null_count())\n",
    "                except Exception:\n",
    "                    n_null = \"?\"\n",
    "                info.append(f\"{c}: nulls={n_null}\")\n",
    "            print(\"  Columnas clave:\", \" | \".join(info))\n",
    "        else:\n",
    "            print(\"  Columnas clave no encontradas en este DF.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def _safe_read_parquet(p: Path, label: str) -> Optional[pl.DataFrame]:\n",
    "    if not p.exists():\n",
    "        print(f\"  ‚ùå NO EXISTE ‚Üí {label} | {p.name}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pl.read_parquet(str(p))\n",
    "        print(f\"  ‚úÖ OK ‚Üí {label} | rows={df.height}, cols={len(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR leyendo ‚Üí {label} | {p.name} | {e.__class__.__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _nulls_report(df: pl.DataFrame, col: str) -> Optional[int]:\n",
    "    if df is None or col not in df.columns:\n",
    "        return None\n",
    "    try:\n",
    "        return int(df.get_column(col).null_count())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# 3) PRUEBA 1/3 :: Integridad de artefactos (n√∫cleo t√©cnico)\n",
    "# =============================================================================\n",
    "\n",
    "STAB_TABLE_PATH = STAB_DIR / \"stability_table.parquet\"\n",
    "STAB_ADV_PATH   = STAB_DIR / \"stability_table_advanced.parquet\"\n",
    "\n",
    "OPP_A_PATH = METRICS_DIR / \"opportunity_summary.parquet\"\n",
    "OPP_B_PATH = METRICS_DIR / \"frequency_opportunity_table.parquet\"\n",
    "\n",
    "ECO_VIAB_PATH = METRICS_DIR / \"economic_viability.parquet\"\n",
    "STRUCT_PATH   = METRICS_DIR / \"structure_summary.parquet\"\n",
    "REG_THR_PATH  = METRICS_DIR / \"regime_thresholds.parquet\"\n",
    "\n",
    "SCORES_TABLE_PATH = SCORES_DIR / \"scores_table.parquet\"\n",
    "\n",
    "print(\"\\n=== PRUEBA 1/3 :: Integridad de artefactos ===\")\n",
    "_require(STAB_TABLE_PATH, \"stability_table\")\n",
    "_require(STAB_ADV_PATH, \"stability_table_advanced\")\n",
    "_require(ECO_VIAB_PATH, \"economic_viability\")\n",
    "_require(CONFIG_PATH, \"config.json\")\n",
    "_require(SCORES_TABLE_PATH, \"scores_table (Celda 10)\")\n",
    "\n",
    "opp_path = _pick_first_existing([OPP_A_PATH, OPP_B_PATH])\n",
    "if opp_path is None:\n",
    "    raise RuntimeError(\n",
    "        \"[NO-GO TECH] Falta opportunity_summary/frequency_opportunity_table. \"\n",
    "        f\"Buscado en: {OPP_A_PATH} | {OPP_B_PATH}\"\n",
    "    )\n",
    "\n",
    "if _exists(STRUCT_PATH):\n",
    "    print(f\"[Celda 15] structure_summary encontrado ‚Üí {STRUCT_PATH}\")\n",
    "else:\n",
    "    print(f\"[Celda 15][WARN] structure_summary no encontrado ‚Üí {STRUCT_PATH} (no bloquea cierre)\")\n",
    "\n",
    "stab_df   = pl.read_parquet(STAB_TABLE_PATH)\n",
    "adv_df    = pl.read_parquet(STAB_ADV_PATH)\n",
    "opp_df    = pl.read_parquet(opp_path)\n",
    "eco_df    = pl.read_parquet(ECO_VIAB_PATH)\n",
    "struct_df = pl.read_parquet(STRUCT_PATH) if _exists(STRUCT_PATH) else pl.DataFrame()\n",
    "reg_thr_df = pl.read_parquet(REG_THR_PATH) if _exists(REG_THR_PATH) else pl.DataFrame()\n",
    "\n",
    "print(f\"[OK] stability_table rows={stab_df.height}, cols={len(stab_df.columns)}\")\n",
    "print(f\"[OK] stability_table_advanced rows={adv_df.height}, cols={len(adv_df.columns)}\")\n",
    "print(f\"[OK] opportunity rows={opp_df.height}, cols={len(opp_df.columns)}  ‚Üí {opp_path.name}\")\n",
    "print(f\"[OK] economic_viability rows={eco_df.height}, cols={len(eco_df.columns)}\")\n",
    "print(f\"[OK] structure_summary rows={struct_df.height if not struct_df.is_empty() else 0}\")\n",
    "\n",
    "if stab_df.height != adv_df.height:\n",
    "    raise RuntimeError(\n",
    "        f\"[NO-GO TECH] mismatch rows stability_table ({stab_df.height}) vs advanced ({adv_df.height}).\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ PRUEBA 1 PASADA\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4) PRUEBA 2/3 :: Consistencia inter-celdas (stability_table_advanced)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== PRUEBA 2/3 :: Consistencia inter-celdas ===\")\n",
    "\n",
    "critical_adv_cols = [\n",
    "    \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    \"EstabScore_final\", \"decay_penalty\", \"min_TR_required\",\n",
    "]\n",
    "\n",
    "for c in critical_adv_cols:\n",
    "    pct = _pct_nonnull(adv_df, c)\n",
    "    print(f\"[Celda 15] non-null {c} = {pct:.2f}%\")\n",
    "    if pct < 95.0:\n",
    "        raise RuntimeError(\n",
    "            f\"[NO-GO TECH] Columna cr√≠tica '{c}' con cobertura insuficiente ({pct:.2f}%). \"\n",
    "            \"Revisa Celda 09.\"\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ PRUEBA 2 PASADA\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5) PRUEBA 3/3 :: Diagn√≥stico real de Trading (n√∫cleo econ√≥mico)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== PRUEBA 3/3 :: Diagn√≥stico real de Trading ===\")\n",
    "\n",
    "scores_df = pl.read_parquet(SCORES_TABLE_PATH)\n",
    "if scores_df.is_empty():\n",
    "    raise RuntimeError(\"[NO-GO TECH] scores_table.parquet est√° vac√≠o.\")\n",
    "\n",
    "scores_df = _ensure_upper_symbol(scores_df)\n",
    "eco_df    = _ensure_upper_symbol(eco_df)\n",
    "\n",
    "print(f\"[Celda 15] scores_table rows={scores_df.height}, cols={len(scores_df.columns)}\")\n",
    "\n",
    "eco_cols = set(eco_df.columns)\n",
    "canonical = {\"TR_trend_mean\", \"TR_range_mean\", \"NDQ_trend_mean\", \"NDQ_range_mean\"}\n",
    "legacy    = {\"TR_trend\", \"TR_range\", \"NDQ_trend\", \"NDQ_range\"}\n",
    "\n",
    "has_canonical = len(canonical.intersection(eco_cols)) >= 2\n",
    "has_legacy    = len(legacy.intersection(eco_cols)) >= 2\n",
    "\n",
    "print(f\"[Celda 15] economic_viability contrato ‚Üí canonical={has_canonical}, legacy={has_legacy}\")\n",
    "print(f\"[Celda 15] economic_viability columns ‚Üí {sorted(list(eco_cols))}\")\n",
    "\n",
    "if (not has_canonical) and (not has_legacy):\n",
    "    raise RuntimeError(\n",
    "        \"[NO-GO TECH] economic_viability no contiene columnas TR/NDQ reconocibles.\"\n",
    "    )\n",
    "\n",
    "# Normalizar contrato legacy en memoria\n",
    "eco_norm = eco_df\n",
    "if \"TR_trend_mean\" not in eco_norm.columns and \"TR_trend\" in eco_norm.columns:\n",
    "    eco_norm = eco_norm.with_columns(\n",
    "        pl.col(\"TR_trend\").cast(pl.Float64, strict=False).alias(\"TR_trend_mean\")\n",
    "    )\n",
    "if \"TR_range_mean\" not in eco_norm.columns and \"TR_range\" in eco_norm.columns:\n",
    "    eco_norm = eco_norm.with_columns(\n",
    "        pl.col(\"TR_range\").cast(pl.Float64, strict=False).alias(\"TR_range_mean\")\n",
    "    )\n",
    "if \"NDQ_trend_mean\" not in eco_norm.columns and \"NDQ_trend\" in eco_norm.columns:\n",
    "    eco_norm = eco_norm.with_columns(\n",
    "        pl.col(\"NDQ_trend\").cast(pl.Float64, strict=False).alias(\"NDQ_trend_mean\")\n",
    "    )\n",
    "if \"NDQ_range_mean\" not in eco_norm.columns and \"NDQ_range\" in eco_norm.columns:\n",
    "    eco_norm = eco_norm.with_columns(\n",
    "        pl.col(\"NDQ_range\").cast(pl.Float64, strict=False).alias(\"NDQ_range_mean\")\n",
    "    )\n",
    "\n",
    "eco_keep = [\n",
    "    c for c in [\n",
    "        \"symbol\", \"TR_trend_mean\", \"TR_range_mean\", \"NDQ_trend_mean\", \"NDQ_range_mean\",\n",
    "        \"TR_ci_low_trend\", \"TR_ci_low_range\", \"TR_after_cost_min\", \"viab_flag_tr\",\n",
    "    ] if c in eco_norm.columns\n",
    "]\n",
    "eco_norm = eco_norm.select(eco_keep).unique(subset=[\"symbol\"])\n",
    "\n",
    "# Join suave por si scores no trae esos campos\n",
    "scores_df = scores_df.join(eco_norm, on=\"symbol\", how=\"left\")\n",
    "\n",
    "# TR_mean real (robusto)\n",
    "scores_df = scores_df.with_columns([\n",
    "    pl.mean_horizontal([\n",
    "        pl.col(\"TR_trend_mean\").cast(pl.Float64, strict=False).fill_null(0.0),\n",
    "        pl.col(\"TR_range_mean\").cast(pl.Float64, strict=False).fill_null(0.0),\n",
    "    ]).alias(\"__TR_mean_real__\"),\n",
    "    pl.mean_horizontal([\n",
    "        pl.col(\"NDQ_trend_mean\").cast(pl.Float64, strict=False).fill_null(0.0),\n",
    "        pl.col(\"NDQ_range_mean\").cast(pl.Float64, strict=False).fill_null(0.0),\n",
    "    ]).alias(\"__NDQ_mean_real__\"),\n",
    "])\n",
    "\n",
    "core_df = scores_df.filter(pl.col(\"SCORE_FINAL\") >= score_core_thr)\n",
    "\n",
    "n_total = scores_df.height\n",
    "n_core  = core_df.height\n",
    "print(\n",
    "    f\"[Celda 15] Universe total={n_total}, \"\n",
    "    f\"core(>= {score_core_thr:.2f})={n_core} ({100.0 * n_core / max(1, n_total):.2f}%)\"\n",
    ")\n",
    "\n",
    "if n_core == 0:\n",
    "    raise RuntimeError(\n",
    "        \"[NO-GO TECH] No hay universo core: SCORE_FINAL no est√° produciendo candidatos.\"\n",
    "    )\n",
    "\n",
    "gate_cols = [\n",
    "    \"passed_data_gate\",\n",
    "    \"passed_stability_gate\",\n",
    "    \"flag_oos_ok\",\n",
    "    \"passed_min_TR_required\",\n",
    "    \"passed_viability_gate\",\n",
    "    \"gate_all\",\n",
    "]\n",
    "\n",
    "print(\"\\n[Celda 15] Gate health en CORE (% True):\")\n",
    "gate_health: Dict[str, float] = {}\n",
    "for c in gate_cols:\n",
    "    gate_health[c] = _pct_true(core_df, c)\n",
    "    print(f\"  {c} ‚Üí {gate_health[c]:.2f}\")\n",
    "\n",
    "tr_core = core_df.get_column(\"__TR_mean_real__\").cast(pl.Float64, strict=False)\n",
    "p25_tr  = _quantile_safe(tr_core, 0.25)\n",
    "med_tr  = _median_safe(tr_core)\n",
    "mx_tr   = float(tr_core.drop_nulls().max()) if tr_core.drop_nulls().len() > 0 else None\n",
    "mn_tr   = float(tr_core.drop_nulls().min()) if tr_core.drop_nulls().len() > 0 else None\n",
    "\n",
    "print(\n",
    "    f\"\\n[Celda 15] TR_mean(core) real stats ‚Üí \"\n",
    "    f\"min={mn_tr}, p25={p25_tr}, mediana={med_tr}, max={mx_tr}\"\n",
    ")\n",
    "\n",
    "# Pol√≠tica de cierre (solo warnings econ√≥micos)\n",
    "WARNINGS: List[str] = []\n",
    "\n",
    "if has_legacy and not has_canonical:\n",
    "    WARNINGS.append(\"economic_viability en contrato LEGACY (no bloquea cierre t√©cnico).\")\n",
    "\n",
    "pct_gate_all = gate_health.get(\"gate_all\", 0.0)\n",
    "pct_minTR    = gate_health.get(\"passed_min_TR_required\", 0.0)\n",
    "\n",
    "MIN_GATE_ALL_WARN = float(stability_cfg.get(\"min_gate_all_core_warn\", 15.0))\n",
    "MIN_MINTR_WARN    = float(stability_cfg.get(\"min_passed_minTR_core_warn\", 10.0))\n",
    "\n",
    "if pct_gate_all < MIN_GATE_ALL_WARN:\n",
    "    WARNINGS.append(\n",
    "        f\"gate_all en core bajo ({pct_gate_all:.2f}%). Universo econ√≥micamente filtrable es peque√±o.\"\n",
    "    )\n",
    "\n",
    "if pct_minTR < MIN_MINTR_WARN:\n",
    "    WARNINGS.append(\n",
    "        f\"passed_min_TR_required en core bajo ({pct_minTR:.2f}%). \"\n",
    "        \"min_TR_required global es exigente vs universo actual.\"\n",
    "    )\n",
    "\n",
    "if med_tr is None:\n",
    "    WARNINGS.append(\"No se pudo calcular mediana TR_mean real en core.\")\n",
    "elif med_tr <= 1e-9:\n",
    "    WARNINGS.append(\n",
    "        \"TR_mean real mediana del core ~0.0 ‚Üí econom√≠a del universo core d√©bil con umbrales actuales.\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"\\n[Celda 15] Close policy ‚Üí WARN gate_all<{MIN_GATE_ALL_WARN:.2f}%, \"\n",
    "    f\"WARN minTR<{MIN_MINTR_WARN:.2f}%\"\n",
    ")\n",
    "\n",
    "# Decisi√≥n de cierre (solo t√©cnico)\n",
    "if WARNINGS and STRICT_TRADING_CLOSE:\n",
    "    raise RuntimeError(\"[NO-GO TRADING] \" + \" | \".join(WARNINGS))\n",
    "\n",
    "print(\"\\n‚úÖ PRUEBA 3 COMPLETADA (diagn√≥stico econ√≥mico)\")\n",
    "if WARNINGS:\n",
    "    print(\">>> CIERRE TECH :: GO ‚úÖ (con advertencias econ√≥micas)\")\n",
    "    for w in WARNINGS:\n",
    "        print(f\"  [WARN] {w}\")\n",
    "    print(\"\\nSiguiente paso recomendado:\")\n",
    "    print(\"  - NO tocar el notebook base.\")\n",
    "    print(\"  - Pasar a construcci√≥n de estrategias por activo usando selecci√≥n estricta por s√≠mbolo/preset.\")\n",
    "else:\n",
    "    print(\">>> CIERRE TECH :: GO ‚úÖ (sin advertencias relevantes)\")\n",
    "    print(\"Puedes empezar construcci√≥n de estrategias por activo.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6) BLOQUE 0 ‚Äî Listados r√°pidos de carpetas (tipo Celda 99)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BLOQUE 0 ‚Äî Listado r√°pido de artefactos .parquet por carpeta\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "_list_parquets_in_dir(BASKETS_DIR, \"Baskets disponibles\")\n",
    "_list_parquets_in_dir(EXPORTS_DIR, \"Exports disponibles\")\n",
    "_list_parquets_in_dir(STAB_DIR,    \"Stability artefacts disponibles\")\n",
    "_list_parquets_in_dir(SCORES_DIR,  \"Scores artefacts disponibles\")\n",
    "_list_parquets_in_dir(METRICS_DIR, \"Metrics artefacts disponibles\")\n",
    "_list_parquets_in_dir(EVENTS_DIR,  \"Events artefacts disponibles\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7) BLOQUE 1 ‚Äî Checks de artefactos clave (tipo Celda 99)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BLOQUE 1 ‚Äî Checks de artefactos clave\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "regime = _check_parquet(\n",
    "    METRICS_DIR / \"regime_labels.parquet\",\n",
    "    \"regime_labels.parquet\",\n",
    "    cols_clave=[\"symbol\", \"time_utc\", \"regime\"],\n",
    ")\n",
    "\n",
    "trend_ev = _check_parquet(\n",
    "    EVENTS_DIR / \"trend_events.parquet\",\n",
    "    \"trend_events.parquet\",\n",
    "    cols_clave=[\"symbol\", \"success\"],\n",
    ")\n",
    "\n",
    "range_ev = _check_parquet(\n",
    "    EVENTS_DIR / \"range_events.parquet\",\n",
    "    \"range_events.parquet\",\n",
    "    cols_clave=[\"symbol\", \"success\"],\n",
    ")\n",
    "\n",
    "coverage_tbl = _check_parquet(\n",
    "    DIAG_DIR / \"coverage_table_5m.parquet\",\n",
    "    \"coverage_table_5m.parquet\",\n",
    "    cols_clave=[\"symbol\"],\n",
    ")\n",
    "\n",
    "opp_df_99 = _check_parquet(\n",
    "    METRICS_DIR / \"opportunity_summary.parquet\",\n",
    "    \"opportunity_summary.parquet\",\n",
    "    cols_clave=[\n",
    "        \"symbol\", \"n_per_month_total\", \"coverage_p\",\n",
    "        \"opp_flag_strict\", \"opp_flag_balanced\", \"opp_flag_loose\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "stab_base_99 = _check_parquet(\n",
    "    STAB_DIR / \"stability_table.parquet\",\n",
    "    \"stability_table.parquet\",\n",
    "    cols_clave=[\n",
    "        \"symbol\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "        \"tvals_ER\", \"tvals_PD\", \"tvals_p_event\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "stab_adv_99 = _check_parquet(\n",
    "    STAB_DIR / \"stability_table_advanced.parquet\",\n",
    "    \"stability_table_advanced.parquet\",\n",
    "    cols_clave=[\n",
    "        \"symbol\",\n",
    "        \"EstabScore_final\", \"keep_stab_base\",\n",
    "        \"decay_penalty\", \"decay_flag\",\n",
    "        \"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "scores_table_99 = _check_parquet(\n",
    "    SCORES_DIR / \"scores_table.parquet\",\n",
    "    \"scores_table.parquet\",\n",
    "    cols_clave=[\n",
    "        \"symbol\", \"family\", \"preset\",\n",
    "        \"SCORE_FINAL\",\n",
    "        \"passed_data_gate\", \"flag_oos_ok\", \"passed_stability_gate\", \"gate_all\",\n",
    "        \"ok_sig_strict\", \"ok_sig_balanced\", \"ok_sig_loose\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "scores_eligible_99 = _check_parquet(\n",
    "    SCORES_DIR / \"scores_eligible.parquet\",\n",
    "    \"scores_eligible.parquet\",\n",
    "    cols_clave=[\n",
    "        \"symbol\", \"family\", \"preset\",\n",
    "        \"keep_final\",\n",
    "        \"eligible_strict\", \"eligible_balanced\", \"eligible_loose\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "selection_audit_path = SCORES_DIR / \"selection_audit.parquet\"\n",
    "metrics_state = GLOBAL_STATE.get(\"metrics\", {}) or {}\n",
    "if metrics_state.get(\"selection_audit_path\"):\n",
    "    try:\n",
    "        selection_audit_path = Path(metrics_state[\"selection_audit_path\"]).resolve()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "selection_audit_99 = _check_parquet(\n",
    "    selection_audit_path,\n",
    "    \"selection_audit.parquet\",\n",
    "    cols_clave=[\"symbol\", \"family\", \"preset\", \"reason_code\"],\n",
    ")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RESUMEN R√ÅPIDO (rows por artefacto):\")\n",
    "\n",
    "def _rows(df: Optional[pl.DataFrame]) -> Optional[int]:\n",
    "    try:\n",
    "        return int(df.height) if df is not None else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(f\"  regime_labels.rows             = {_rows(regime)}\")\n",
    "print(f\"  trend_events.rows              = {_rows(trend_ev)}\")\n",
    "print(f\"  range_events.rows              = {_rows(range_ev)}\")\n",
    "print(f\"  coverage_table_5m.rows         = {_rows(coverage_tbl)}\")\n",
    "print(f\"  opportunity_summary.rows       = {_rows(opp_df_99)}\")\n",
    "print(f\"  stability_table.rows           = {_rows(stab_base_99)}\")\n",
    "print(f\"  stability_table_advanced.rows  = {_rows(stab_adv_99)}\")\n",
    "print(f\"  scores_table.rows              = {_rows(scores_table_99)}\")\n",
    "print(f\"  scores_eligible.rows           = {_rows(scores_eligible_99)}\")\n",
    "print(f\"  selection_audit.rows           = {_rows(selection_audit_99)}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# =============================================================================\n",
    "# 8) BLOQUE A ‚Äî Resumen de gates en scores_table\n",
    "# =============================================================================\n",
    "\n",
    "if scores_table_99 is not None and scores_table_99.height > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BLOQUE A ‚Äî Resumen de gates en scores_table\")\n",
    "\n",
    "    df_scores = scores_table_99\n",
    "\n",
    "    gate_cols_base = [\"passed_data_gate\", \"flag_oos_ok\", \"passed_stability_gate\"]\n",
    "    for col in gate_cols_base:\n",
    "        if col in df_scores.columns:\n",
    "            df_scores = df_scores.with_columns(pl.col(col).cast(pl.Boolean).alias(col))\n",
    "\n",
    "    if (\"gate_all\" not in df_scores.columns) and all(c in df_scores.columns for c in gate_cols_base):\n",
    "        df_scores = df_scores.with_columns(\n",
    "            (pl.col(\"passed_data_gate\") & pl.col(\"flag_oos_ok\") & pl.col(\"passed_stability_gate\"))\n",
    "            .alias(\"gate_all\")\n",
    "        )\n",
    "        print(\"  [INFO] gate_all reconstruido desde gates base.\")\n",
    "\n",
    "    if \"gate_all\" in df_scores.columns:\n",
    "        print(\"\\n[gate_all] Conteo True/False:\")\n",
    "        print(df_scores[\"gate_all\"].value_counts())\n",
    "    else:\n",
    "        print(\"\\n[gate_all] No existe la columna 'gate_all' en scores_table.\")\n",
    "\n",
    "    for col in gate_cols_base:\n",
    "        if col in df_scores.columns:\n",
    "            print(f\"\\n[{col}] Conteo True/False:\")\n",
    "            print(df_scores[col].value_counts())\n",
    "        else:\n",
    "            print(f\"\\n[{col}] No existe en scores_table.\")\n",
    "\n",
    "    scores_table_99 = df_scores\n",
    "else:\n",
    "    print(\"\\n[INFO] BLOQUE A ‚Äî scores_table no existe o est√° vac√≠o; se omite resumen de gates.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9) BLOQUE B ‚Äî Resumen de p-values combinados\n",
    "# =============================================================================\n",
    "\n",
    "if scores_table_99 is not None and scores_table_99.height > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BLOQUE B ‚Äî Resumen de p-values combinados\")\n",
    "\n",
    "    alpha_loose_diag = alpha_loose_stats  # usamos alpha_loose de stats como referencia\n",
    "    try:\n",
    "        cfg_sections = GLOBAL_STATE.get(\"config_sections\", {}) or {}\n",
    "        sel_cfg = cfg_sections.get(\"selection\") or {}\n",
    "        alpha_loose_diag = float(sel_cfg.get(\"alpha_loose\", alpha_loose_diag))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[INFO] Usando alpha_loose (diag) = {alpha_loose_diag:.4f}\")\n",
    "\n",
    "    pval_cols = [\"pvals_combined_ER\", \"pvals_combined_PD\", \"pvals_combined_p_event\"]\n",
    "\n",
    "    def _p_share(df: pl.DataFrame, col: str, alpha: float) -> Optional[float]:\n",
    "        if col not in df.columns:\n",
    "            return None\n",
    "        s = df.select(pl.col(col).cast(pl.Float64).alias(col)).drop_nulls()\n",
    "        if s.height == 0:\n",
    "            return 0.0\n",
    "        below = s.filter(pl.col(col) < alpha).height\n",
    "        total = s.height\n",
    "        return (below / total) if total > 0 else 0.0\n",
    "\n",
    "    for col in pval_cols:\n",
    "        share = _p_share(scores_table_99, col, alpha_loose_diag)\n",
    "        if share is None:\n",
    "            print(f\"  - {col}: columna no existe en scores_table.\")\n",
    "        else:\n",
    "            print(f\"  - {col}: p < alpha_loose en {share * 100:6.2f}% de s√≠mbolos\")\n",
    "else:\n",
    "    print(\"\\n[INFO] BLOQUE B ‚Äî scores_table no existe o est√° vac√≠o; se omite resumen de p-values.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 10) BLOQUE C ‚Äî Resumen de elegibles por bucket en scores_eligible\n",
    "# =============================================================================\n",
    "\n",
    "if scores_eligible_99 is not None and scores_eligible_99.height > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BLOQUE C ‚Äî Resumen de elegibles por (family, preset)\")\n",
    "\n",
    "    df_elig = scores_eligible_99\n",
    "\n",
    "    if \"keep_final\" in df_elig.columns:\n",
    "        df_elig = df_elig.with_columns(pl.col(\"keep_final\").cast(pl.Boolean).alias(\"keep_final\"))\n",
    "        df_keep = df_elig.filter(pl.col(\"keep_final\") == True)\n",
    "    else:\n",
    "        df_elig = df_elig.with_columns(pl.lit(True).alias(\"keep_final\"))\n",
    "        df_keep = df_elig\n",
    "\n",
    "    group_cols = [c for c in (\"family\", \"preset\") if c in df_elig.columns]\n",
    "\n",
    "    if group_cols:\n",
    "        df_group = (\n",
    "            df_keep\n",
    "            .group_by(group_cols)\n",
    "            .agg(pl.len().alias(\"n_eligibles\"))\n",
    "            .sort(group_cols)\n",
    "        )\n",
    "        print(\"\\n[Conteo de elegibles (keep_final=True) por (family, preset)]:\")\n",
    "        print(df_group)\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No hay columnas 'family'/'preset' en scores_eligible.\")\n",
    "\n",
    "    elig_flag_cols = [\n",
    "        c for c in (\"eligible_strict\", \"eligible_balanced\", \"eligible_loose\")\n",
    "        if c in df_elig.columns\n",
    "    ]\n",
    "    if elig_flag_cols:\n",
    "        print(\"\\n[Conteo de flags eligible_* True]:\")\n",
    "        for c in elig_flag_cols:\n",
    "            try:\n",
    "                n_true = int(df_elig.filter(pl.col(c) == True).height)\n",
    "                print(f\"  - {c}: {n_true}\")\n",
    "            except Exception:\n",
    "                print(f\"  - {c}: (no se pudo calcular)\")\n",
    "\n",
    "    scores_eligible_99 = df_elig\n",
    "else:\n",
    "    print(\"\\n[INFO] BLOQUE C ‚Äî scores_eligible no existe o est√° vac√≠o; se omite resumen de buckets.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11) BLOQUE D ‚Äî Resumen de reason_code y passed_* en selection_audit\n",
    "# =============================================================================\n",
    "\n",
    "if selection_audit_99 is not None and selection_audit_99.height > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BLOQUE D ‚Äî Resumen de reason_code y passed_* en selection_audit\")\n",
    "\n",
    "    df_audit = selection_audit_99\n",
    "\n",
    "    if \"reason_code\" in df_audit.columns:\n",
    "        print(\"\\n[reason_code] Conteo por motivo:\")\n",
    "        print(\n",
    "            df_audit\n",
    "            .group_by(\"reason_code\")\n",
    "            .agg(pl.len().alias(\"n_symbols\"))\n",
    "            .sort(\"n_symbols\", descending=True)\n",
    "        )\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è La columna 'reason_code' no existe en selection_audit.\")\n",
    "\n",
    "    gate_cols_audit = [c for c in df_audit.columns if c.startswith(\"passed_\")]\n",
    "    if gate_cols_audit:\n",
    "        print(\"\\n[passed_*] Conteo True/False por gate:\")\n",
    "        for col in gate_cols_audit:\n",
    "            print(f\"\\nGate: {col}\")\n",
    "            try:\n",
    "                print(df_audit[col].cast(pl.Boolean).value_counts())\n",
    "            except Exception:\n",
    "                print(df_audit[col].value_counts())\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No hay columnas 'passed_*' en selection_audit.\")\n",
    "\n",
    "    selection_audit_99 = df_audit\n",
    "else:\n",
    "    print(\"\\n[INFO] BLOQUE D ‚Äî selection_audit no existe o est√° vac√≠o; se omite resumen de razones.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 12) BLOQUE E ‚Äî Mini resumen final del RUN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BLOQUE E ‚Äî Resumen final del RUN\")\n",
    "\n",
    "universe_size = scores_table_99.height if scores_table_99 is not None else 0\n",
    "gate_all_true = None\n",
    "gate_all_false = None\n",
    "\n",
    "if scores_table_99 is not None and scores_table_99.height > 0 and \"gate_all\" in scores_table_99.columns:\n",
    "    try:\n",
    "        vc_gate = scores_table_99[\"gate_all\"].cast(pl.Boolean).value_counts()\n",
    "        try:\n",
    "            gate_all_true = int(vc_gate.filter(pl.col(\"gate_all\") == True)[\"count\"][0])\n",
    "        except Exception:\n",
    "            gate_all_true = 0\n",
    "        try:\n",
    "            gate_all_false = int(vc_gate.filter(pl.col(\"gate_all\") == False)[\"count\"][0])\n",
    "        except Exception:\n",
    "            gate_all_false = 0\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    if scores_table_99 is not None and scores_table_99.height > 0:\n",
    "        print(\"  ‚ö†Ô∏è gate_all no existe en scores_table (o no se pudo calcular).\")\n",
    "\n",
    "print(f\"  universe_size (scores_table.rows) = {universe_size}\")\n",
    "print(f\"  gate_all=True  ‚âà {gate_all_true}\")\n",
    "print(f\"  gate_all=False ‚âà {gate_all_false}\")\n",
    "\n",
    "eligibles_total = None\n",
    "bucket_breakdown = None\n",
    "\n",
    "if scores_eligible_99 is not None and scores_eligible_99.height > 0:\n",
    "    df_elig = scores_eligible_99\n",
    "    if \"keep_final\" in df_elig.columns:\n",
    "        df_keep = df_elig.filter(pl.col(\"keep_final\") == True)\n",
    "        eligibles_total = df_keep.height\n",
    "        group_cols = [c for c in (\"family\", \"preset\") if c in df_elig.columns]\n",
    "        if group_cols:\n",
    "            bucket_breakdown = (\n",
    "                df_keep\n",
    "                .group_by(group_cols)\n",
    "                .agg(pl.len().alias(\"n_eligibles\"))\n",
    "                .sort(group_cols)\n",
    "            )\n",
    "    else:\n",
    "        eligibles_total = df_elig.height\n",
    "\n",
    "print(f\"  eligibles_total (keep_final=True) = {eligibles_total}\")\n",
    "\n",
    "if bucket_breakdown is not None:\n",
    "    print(\"\\n  Breakdown por (family, preset) ‚Äî keep_final=True:\")\n",
    "    print(bucket_breakdown)\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No se pudo construir breakdown por bucket.\")\n",
    "\n",
    "if selection_audit_99 is not None and selection_audit_99.height > 0 and \"reason_code\" in selection_audit_99.columns:\n",
    "    print(\"\\n  Top 3 reason_code por n¬∫ de s√≠mbolos:\")\n",
    "    top_reason = (\n",
    "        selection_audit_99\n",
    "        .group_by(\"reason_code\")\n",
    "        .agg(pl.len().alias(\"n_symbols\"))\n",
    "        .sort(\"n_symbols\", descending=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    print(top_reason)\n",
    "else:\n",
    "    print(\"\\n  No se puede resumir reason_code (selection_audit vac√≠o o sin columna).\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"BLOQUE E finalizado. Resumen global del RUN listo.\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# =============================================================================\n",
    "# 13) BLOQUE F ‚Äî Validaci√≥n final tipo Celda X\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"BLOQUE F ‚Äî Validaci√≥n final 100% certeza (min_TR, decay, alpha_loose, baskets vs exports)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Artefactos clave para este bloque\n",
    "scores_eligible_path  = SCORES_DIR / \"scores_eligible.parquet\"\n",
    "selection_audit_path_X = SCORES_DIR / \"selection_audit.parquet\"\n",
    "stab_base_path_X      = STAB_DIR / \"stability_table.parquet\"\n",
    "stab_adv_path_X       = STAB_DIR / \"stability_table_advanced.parquet\"\n",
    "config_json_path      = CONFIG_PATH\n",
    "export_all_path       = (EXPORTS_DIR / \"ea_universe_all.parquet\") if EXPORTS_DIR else Path(\"__missing__\")\n",
    "\n",
    "scores_table_X     = _safe_read_parquet(SCORES_TABLE_PATH, \"scores_table\")\n",
    "scores_eligible_X  = _safe_read_parquet(scores_eligible_path, \"scores_eligible\")\n",
    "selection_audit_X  = _safe_read_parquet(selection_audit_path_X, \"selection_audit\")\n",
    "stab_base_X        = _safe_read_parquet(stab_base_path_X, \"stability_table\")\n",
    "stab_adv_X         = _safe_read_parquet(stab_adv_path_X,  \"stability_table_advanced\")\n",
    "export_all_X       = _safe_read_parquet(export_all_path,  \"ea_universe_all\") if EXPORTS_DIR else None\n",
    "\n",
    "print(\"\\n\" + \"-\" * 110)\n",
    "print(\"1) ‚úÖ Verificar origen/presencia de min_TR_required\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "min_tr_presence: Dict[str, str] = {}\n",
    "for name, df in [\n",
    "    (\"scores_table\",            scores_table_X),\n",
    "    (\"scores_eligible\",         scores_eligible_X),\n",
    "    (\"selection_audit\",         selection_audit_X),\n",
    "    (\"stability_table\",         stab_base_X),\n",
    "    (\"stability_table_advanced\",stab_adv_X),\n",
    "    (\"ea_universe_all\",         export_all_X),\n",
    "]:\n",
    "    if df is None:\n",
    "        min_tr_presence[name] = \"MISSING_DF\"\n",
    "        continue\n",
    "    min_tr_presence[name] = \"PRESENT\" if \"min_TR_required\" in df.columns else \"ABSENT\"\n",
    "\n",
    "for k, v in min_tr_presence.items():\n",
    "    icon = \"‚úÖ\" if v == \"PRESENT\" else (\"‚ö†Ô∏è\" if v == \"ABSENT\" else \"‚ùå\")\n",
    "    print(f\"  {icon} {k}: {v}\")\n",
    "\n",
    "if scores_table_X is not None and \"min_TR_required\" not in scores_table_X.columns:\n",
    "    print(\"\\n  ‚ö†Ô∏è ALERTA:\")\n",
    "    print(\"     'min_TR_required' NO est√° en scores_table.\")\n",
    "    print(\"     Si tu export lo necesita, el origen recomendado suele ser estabilidad/thresholds.\")\n",
    "    print(\"     Revisa Celda 09 (o el bloque donde se define min_TR_required) y su merge a scores.\")\n",
    "else:\n",
    "    print(\"\\n  ‚úÖ OK: min_TR_required est√° disponible en scores_table o no es requerido por tu versi√≥n actual.\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 110)\n",
    "print(\"2) ‚úÖ Confirmar estado de decay_penalty / decay_flag\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "decay_report: Dict[str, Optional[object]] = {\n",
    "    \"exists\": False,\n",
    "    \"all_null\": None,\n",
    "    \"nulls\": None,\n",
    "    \"rows\": None,\n",
    "}\n",
    "\n",
    "if stab_adv_X is None:\n",
    "    print(\"  ‚ùå No se pudo evaluar decay_penalty: stability_table_advanced no disponible.\")\n",
    "else:\n",
    "    decay_report[\"rows\"] = stab_adv_X.height\n",
    "    if \"decay_penalty\" in stab_adv_X.columns:\n",
    "        decay_report[\"exists\"] = True\n",
    "        n_null = _nulls_report(stab_adv_X, \"decay_penalty\")\n",
    "        decay_report[\"nulls\"] = n_null\n",
    "        decay_report[\"all_null\"] = (n_null == stab_adv_X.height)\n",
    "        print(f\"  ‚úÖ decay_penalty existe | nulls={n_null} / rows={stab_adv_X.height}\")\n",
    "        if decay_report[\"all_null\"]:\n",
    "            print(\"  ‚ö†Ô∏è ALERTA: decay_penalty est√° 100% NULL.\")\n",
    "            print(\"     Si esperas penalizaci√≥n de decay activa, revisa condici√≥n de c√°lculo y asignaci√≥n.\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ OK: decay_penalty tiene valores no nulos en parte del universo.\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è decay_penalty NO existe en stability_table_advanced.\")\n",
    "        print(\"     Si tu dise√±o actual no lo usa, esto puede ser OK.\")\n",
    "        print(\"     Si s√≠ lo usa, revisa Celda 09 (Alpha Decay).\")\n",
    "\n",
    "if stab_adv_X is not None and \"decay_flag\" in stab_adv_X.columns:\n",
    "    try:\n",
    "        vc = stab_adv_X[\"decay_flag\"].value_counts()\n",
    "        print(\"\\n  [decay_flag] value_counts:\")\n",
    "        print(vc)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"-\" * 110)\n",
    "print(\"3) ‚úÖ Alinear alpha_loose (config.json vs GLOBAL_STATE)\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "alpha_from_config: Optional[float] = None\n",
    "alpha_from_state:  Optional[float] = None\n",
    "alpha_stats_cfg:   Optional[float] = None\n",
    "alpha_sel_cfg_cfg: Optional[float] = None\n",
    "\n",
    "# Desde config.json\n",
    "if config_json_path.exists():\n",
    "    try:\n",
    "        cfg_raw = json.loads(config_json_path.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(cfg_raw, dict):\n",
    "            # alpha_loose en secci√≥n stats (esquema actual)\n",
    "            stats_block = cfg_raw.get(\"stats\") or {}\n",
    "            if \"alpha_loose\" in stats_block:\n",
    "                alpha_stats_cfg = float(stats_block[\"alpha_loose\"])\n",
    "            sel_block = cfg_raw.get(\"selection\") or cfg_raw.get(\"config\", {}).get(\"selection\")\n",
    "            if isinstance(sel_block, dict) and \"alpha_loose\" in sel_block:\n",
    "                alpha_sel_cfg_cfg = float(sel_block[\"alpha_loose\"])\n",
    "\n",
    "            # Tomamos alpha_from_config como preferencia: selection > stats\n",
    "            if alpha_sel_cfg_cfg is not None:\n",
    "                alpha_from_config = alpha_sel_cfg_cfg\n",
    "            elif alpha_stats_cfg is not None:\n",
    "                alpha_from_config = alpha_stats_cfg\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è No se pudo leer alpha_loose de config.json | {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# Desde GLOBAL_STATE.config_sections\n",
    "try:\n",
    "    cfg_sections = GLOBAL_STATE.get(\"config_sections\", {}) or {}\n",
    "    sel_cfg_state = cfg_sections.get(\"selection\") or {}\n",
    "    if \"alpha_loose\" in sel_cfg_state:\n",
    "        alpha_from_state = float(sel_cfg_state[\"alpha_loose\"])\n",
    "except Exception:\n",
    "    alpha_from_state = None\n",
    "\n",
    "print(f\"  alpha_loose.stats (config.json)      = {alpha_stats_cfg}\")\n",
    "print(f\"  alpha_loose.selection (config.json)  = {alpha_sel_cfg_cfg}\")\n",
    "print(f\"  alpha_loose (GLOBAL_STATE.selection) = {alpha_from_state}\")\n",
    "\n",
    "if alpha_from_config is None and alpha_from_state is None:\n",
    "    print(\"  ‚ö†Ô∏è ALERTA: no se pudo recuperar alpha_loose de ninguna fuente 'selection'.\")\n",
    "    print(\"     Para diagn√≥stico, se usa stats.alpha_loose en PRUEBA 3 y BLOQUE B.\")\n",
    "elif alpha_from_config is not None and alpha_from_state is not None:\n",
    "    if abs(alpha_from_config - alpha_from_state) < 1e-9:\n",
    "        print(\"  ‚úÖ OK: alpha_loose est√° alineado entre config.json y GLOBAL_STATE.\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è ALERTA: alpha_loose DIFERENTE entre config.json y GLOBAL_STATE.\")\n",
    "        print(\"     Acci√≥n: estandariza lectura en gates/diagn√≥sticos usando un √∫nico origen.\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è AVISO: alpha_loose solo fue encontrado en una de las fuentes.\")\n",
    "    print(\"     Si tu notebook usa ambas rutas, esto puede causar discrepancias de diagn√≥stico.\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 110)\n",
    "print(\"4) ‚úÖ Revisi√≥n de 'unique_symbols_exported' vs uni√≥n de baskets CORE\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "core_baskets: List[Path] = []\n",
    "if BASKETS_DIR is not None and BASKETS_DIR.exists():\n",
    "    core_baskets = sorted(BASKETS_DIR.glob(\"basket_*_core.parquet\"))\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è BASKETS_DIR no definido o no existe; se omite chequeo de baskets CORE.\")\n",
    "\n",
    "core_symbols = set()\n",
    "core_by_file: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "if not core_baskets:\n",
    "    print(\"  ‚ö†Ô∏è No se encontraron baskets *_core.parquet.\")\n",
    "else:\n",
    "    print(\"  Baskets CORE detectadas:\")\n",
    "    for p in core_baskets:\n",
    "        print(f\"   - {p.name}\")\n",
    "\n",
    "    for p in core_baskets:\n",
    "        df_b = _safe_read_parquet(p, f\"basket_core::{p.stem}\")\n",
    "        if df_b is None or df_b.height == 0 or \"symbol\" not in df_b.columns:\n",
    "            core_by_file[p.name] = {\"rows\": 0, \"unique_symbols\": 0}\n",
    "            continue\n",
    "\n",
    "        syms = (\n",
    "            df_b\n",
    "            .select(pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase())\n",
    "            .to_series()\n",
    "            .drop_nulls()\n",
    "            .to_list()\n",
    "        )\n",
    "        sset = set(syms)\n",
    "        core_by_file[p.name] = {\"rows\": df_b.height, \"unique_symbols\": len(sset)}\n",
    "        core_symbols |= sset\n",
    "\n",
    "    print(\"\\n  Resumen CORE por archivo:\")\n",
    "    for k, v in core_by_file.items():\n",
    "        print(f\"   ‚Ä¢ {k}: rows={v['rows']}, unique_symbols={v['unique_symbols']}\")\n",
    "\n",
    "print(f\"\\n  ‚úÖ Uni√≥n global CORE unique_symbols = {len(core_symbols)}\")\n",
    "\n",
    "export_symbols = set()\n",
    "if export_all_X is None or export_all_X.is_empty():\n",
    "    print(\"  ‚ö†Ô∏è ea_universe_all est√° ausente o vac√≠o (o EXPORTS_DIR no definido).\")\n",
    "else:\n",
    "    if \"symbol\" in export_all_X.columns:\n",
    "        export_symbols = set(\n",
    "            export_all_X\n",
    "            .select(pl.col(\"symbol\").cast(pl.Utf8, strict=False).str.to_uppercase())\n",
    "            .to_series()\n",
    "            .drop_nulls()\n",
    "            .to_list()\n",
    "        )\n",
    "        print(f\"  ‚úÖ ea_universe_all unique_symbols(global) = {len(export_symbols)}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è ea_universe_all no tiene columna 'symbol'.\")\n",
    "\n",
    "if core_symbols and export_symbols:\n",
    "    inter = core_symbols & export_symbols\n",
    "    only_core = core_symbols - export_symbols\n",
    "    only_export = export_symbols - core_symbols\n",
    "\n",
    "    print(\"\\n  Comparativa CORE vs EXPORT(all):\")\n",
    "    print(f\"   ‚Ä¢ intersecci√≥n (CORE ‚à© EXPORT) = {len(inter)}\")\n",
    "    print(f\"   ‚Ä¢ solo CORE                    = {len(only_core)}\")\n",
    "    print(f\"   ‚Ä¢ solo EXPORT                  = {len(only_export)}\")\n",
    "\n",
    "    if len(only_core) > 0:\n",
    "        print(\"   ‚ö†Ô∏è S√≠mbolos en CORE pero NO en export_all:\")\n",
    "        print(\"     \", sorted(list(only_core))[:50])\n",
    "\n",
    "    if len(only_export) > 0:\n",
    "        print(\"   ‚ÑπÔ∏è S√≠mbolos en export_all pero NO en CORE:\")\n",
    "        print(\"     \", sorted(list(only_export))[:50])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 110)\n",
    "print(\"5) ‚úÖ Resumen final de coherencia (flags tipo Celda X)\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "summary_flags: List[str] = []\n",
    "\n",
    "if scores_table_X is not None and \"min_TR_required\" not in scores_table_X.columns:\n",
    "    summary_flags.append(\"min_TR_required_missing_in_scores_table\")\n",
    "\n",
    "if decay_report.get(\"exists\") and decay_report.get(\"all_null\") is True:\n",
    "    summary_flags.append(\"decay_penalty_all_null\")\n",
    "\n",
    "if alpha_from_config is not None and alpha_from_state is not None:\n",
    "    if abs(alpha_from_config - alpha_from_state) >= 1e-9:\n",
    "        summary_flags.append(\"alpha_loose_mismatch\")\n",
    "\n",
    "if core_symbols and export_symbols:\n",
    "    if len(core_symbols - export_symbols) > 0:\n",
    "        summary_flags.append(\"core_symbols_not_in_export_all\")\n",
    "\n",
    "if not summary_flags:\n",
    "    print(\"  ‚úÖ TODO OK: no se detectaron inconsistencias relevantes para cierre del RUN.\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è ALERTAS detectadas:\")\n",
    "    for f in summary_flags:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "# Dict final en memoria (equivalente a baskets_core_export_report de Celda X)\n",
    "baskets_core_export_report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\n",
    "        \"baskets_dir\": str(BASKETS_DIR) if BASKETS_DIR else None,\n",
    "        \"exports_dir\": str(EXPORTS_DIR) if EXPORTS_DIR else None,\n",
    "        \"scores_dir\": str(SCORES_DIR),\n",
    "        \"stability_dir\": str(STAB_DIR),\n",
    "        \"config_json\": str(config_json_path),\n",
    "    },\n",
    "    \"min_TR_required_presence\": min_tr_presence,\n",
    "    \"decay_penalty\": decay_report,\n",
    "    \"alpha_loose\": {\n",
    "        \"stats_in_config\": alpha_stats_cfg,\n",
    "        \"selection_in_config\": alpha_sel_cfg_cfg,\n",
    "        \"selection_in_state\": alpha_from_state,\n",
    "    },\n",
    "    \"core_baskets_detected\": [p.name for p in core_baskets],\n",
    "    \"core_unique_symbols\": len(core_symbols),\n",
    "    \"export_all_unique_symbols\": len(export_symbols) if export_symbols else 0,\n",
    "    \"core_minus_export\": sorted(list(core_symbols - export_symbols))[:200],\n",
    "    \"export_minus_core\": sorted(list(export_symbols - core_symbols))[:200],\n",
    "    \"alerts\": summary_flags,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"‚úÖ BLOQUE F completado.\")\n",
    "print(\"üìå Variable disponible en memoria: baskets_core_export_report\")\n",
    "print(\"=\" * 110)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
